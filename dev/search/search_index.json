{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"api/overview/","text":"Overview \u00b6 anomaly \u00b6 Anomaly detection. The estimators in the anomaly module have a slightly different API. Instead of a predict_one method, each anomaly detector has a score_one . The latter returns an anomaly score for a given set of features. High scores indicate anomalies, whereas low scores indicate normal observations. Note that the range of the scores is relative to each estimator. HalfSpaceTrees base \u00b6 Base interfaces. Every estimator in river is a class, and as such inherits from at least one base interface. These are used to categorize, organize, and standardize the many estimators that river contains. This module contains mixin classes, which are all suffixed by Mixin . Their purpose is to provide additional functionality to an estimator, and thus need to be used in conjunction with a non-mixin base class. This module also contains utilities for type hinting and tagging estimators. AnomalyDetector Base Classifier Clusterer DriftDetector EnsembleMixin Estimator MiniBatchClassifier MiniBatchRegressor MultiOutputMixin Regressor SupervisedTransformer Transformer WrapperMixin cluster \u00b6 Unsupervised clustering. CluStream DBSTREAM DenStream KMeans STREAMKMeans compat \u00b6 Compatibility tools. This module contains adapters for making river estimators compatible with other libraries, and vice-versa whenever possible. The relevant adapters will only be usable if you have installed the necessary library. For instance, you have to install scikit-learn in order to use the compat.convert_sklearn_to_river function. Classes River2SKLClassifier River2SKLClusterer River2SKLRegressor River2SKLTransformer SKL2RiverClassifier SKL2RiverRegressor Functions convert_river_to_sklearn convert_sklearn_to_river compose \u00b6 Model composition. This module contains utilities for merging multiple modeling steps into a single pipeline. Although pipelines are not the only way to process a stream of data, we highly encourage you to use them. Discard FuncTransformer Grouper Pipeline Renamer Select SelectType TransformerUnion datasets \u00b6 Datasets. This module contains a collection of datasets for multiple tasks: classification, regression, etc. The data corresponds to popular datasets and are conveniently wrapped to easily iterate over the data in a stream fashion. All datasets have fixed size. Please refer to river.synth if you are interested in infinite synthetic data generators. AirlinePassengers Bananas Bikes ChickWeights CreditCard Elec2 HTTP Higgs ImageSegments Insects MaliciousURL MovieLens100K Music Phishing Restaurants SMSSpam SMTP SolarFlare TREC07 Taxis TrumpApproval drift \u00b6 Concept Drift Detection. This module contains concept drift detection methods. The purpose of a drift detector is to raise an alarm if the data distribution changes. A good drift detector method is the one that maximizes the true positives while keeping the number of false positives to a minimum. ADWIN DDM EDDM HDDM_A HDDM_W KSWIN PageHinkley dummy \u00b6 Dummy estimators. This module is here for testing purposes, as well as providing baseline performances. NoChangeClassifier PriorClassifier StatisticRegressor ensemble \u00b6 Ensemble learning. This module includes ensemble methods. This kind of methods improve predictive performance by combining the prediction of their members. ADWINBaggingClassifier AdaBoostClassifier AdaptiveRandomForestClassifier AdaptiveRandomForestRegressor BaggingClassifier BaggingRegressor LeveragingBaggingClassifier SRPClassifier evaluate \u00b6 Model evaluation. This module provides utilities to evaluate an online model. The goal is to reproduce a real-world scenario with high fidelity. The core function of this module is progressive_val_score , which allows to evaluate a model via progressive validation. This module also exposes \"tracks\". A track is a predefined combination of a dataset and one or more metrics. This allows a principled manner to compare models with each other. For instance, the load_binary_clf_tracks returns tracks that are to be used to evaluate the performance of a binary classification model. The benchmarks directory at the root of the River repository uses these tracks. Classes Track Functions load_binary_clf_tracks progressive_val_score expert \u00b6 Expert learning. This module regroups a variety of methods that may be used for performing model selection. An expert learner is provided with a list of models, which are also called experts, and is tasked with performing at least as well as the best expert. Indeed, initially the best model is not known. The performance of each model becomes more apparent as time goes by. Different strategies are possible, each one offering a different tradeoff in terms of accuracy and computational performance. Expert learning can be used for tuning the hyperparameters of a model. This may be done by creating a copy of the model for each set of hyperparameters, and treating each copy as a separate model. The utils.expand_param_grid function can be used for this purpose. Note that this differs from the ensemble module in that methods from the latter are designed to improve the performance of a single model. Both modules may thus be used in conjunction with one another. EWARegressor EpsilonGreedyRegressor StackingClassifier SuccessiveHalvingClassifier SuccessiveHalvingRegressor UCBRegressor facto \u00b6 Factorization machines. FFMClassifier FFMRegressor FMClassifier FMRegressor FwFMClassifier FwFMRegressor HOFMClassifier HOFMRegressor feature_extraction \u00b6 Feature extraction. This module can be used to extract information from raw features. This includes encoding categorical data as well as looking at interactions between existing features. This differs from the processing module in that the latter's purpose is rather to clean the data so that it may be processed by a particular machine learning algorithm. Agg BagOfWords PolynomialExtender RBFSampler TFIDF TargetAgg feature_selection \u00b6 Feature selection. PoissonInclusion SelectKBest VarianceThreshold imblearn \u00b6 Sampling methods. HardSamplingClassifier HardSamplingRegressor RandomOverSampler RandomSampler RandomUnderSampler linear_model \u00b6 Linear models. ALMAClassifier LinearRegression LogisticRegression PAClassifier PARegressor Perceptron SoftmaxRegression meta \u00b6 Meta-models. BoxCoxRegressor PredClipper TransformedTargetRegressor metrics \u00b6 Evaluation metrics. All the metrics are updated one sample at a time. This way we can track performance of predictive methods over time. Accuracy AdjustedMutualInfo AdjustedRand BalancedAccuracy BinaryMetric ClassificationMetric ClassificationReport CohenKappa Completeness ConfusionMatrix CrossEntropy ExactMatch ExampleF1 ExampleFBeta ExamplePrecision ExampleRecall F1 FBeta FowlkesMallows GeometricMean Hamming HammingLoss Homogeneity Jaccard KappaM KappaT LogLoss MAE MCC MSE MacroF1 MacroFBeta MacroPrecision MacroRecall MatthewsCorrCoef Metric Metrics MicroF1 MicroFBeta MicroPrecision MicroRecall MultiClassMetric MultiFBeta MultiLabelConfusionMatrix MultiOutputClassificationMetric MultiOutputRegressionMetric MutualInfo NormalizedMutualInfo PairConfusionMatrix Precision PrevalenceThreshold Purity Q0 Q2 R2 RMSE RMSLE ROCAUC Rand Recall RegressionMetric RegressionMultiOutput Rolling SMAPE SorensenDice TimeRolling VBeta VariationInfo WeightedF1 WeightedFBeta WeightedPrecision WeightedRecall WrapperMetric cluster \u00b6 Internal clustering metrics This submodule includes all internal clustering metrics that are updated with one sample, its label and the current cluster centers at a time. Using this, we can track the performance of the clustering algorithm without having to store information of all previously passed points. BIC BallHall CalinskiHarabasz Cohesion DaviesBouldin GD43 GD53 Hartigan IIndex InternalMetric MSSTD PS R2 RMSSTD SD SSB SSW Separation Silhouette WB XieBeni Xu multiclass \u00b6 Multi-class classification. OneVsOneClassifier OneVsRestClassifier OutputCodeClassifier multioutput \u00b6 Multi-output models. ClassifierChain MonteCarloClassifierChain ProbabilisticClassifierChain RegressorChain naive_bayes \u00b6 Naive Bayes algorithms. BernoulliNB ComplementNB GaussianNB MultinomialNB neighbors \u00b6 Neighbors-based learning. Also known as lazy methods. In these methods, generalisation of the training data is delayed until a query is received. KNNADWINClassifier KNNClassifier KNNRegressor SAMKNNClassifier neural_net \u00b6 Neural networks. MLPRegressor activations \u00b6 Identity ReLU Sigmoid optim \u00b6 Stochastic optimization. AMSGrad AdaBound AdaDelta AdaGrad AdaMax Adam Averager FTRLProximal Momentum Nadam NesterovMomentum Optimizer RMSProp SGD initializers \u00b6 Weight initializers. Constant Normal Zeros losses \u00b6 Loss functions. Each loss function is intended to work with both single values as well as numpy vectors. Absolute BinaryFocalLoss BinaryLoss Cauchy CrossEntropy EpsilonInsensitiveHinge Hinge Log MultiClassLoss Poisson Quantile RegressionLoss Squared schedulers \u00b6 Learning rate schedulers. Constant InverseScaling Optimal Scheduler preprocessing \u00b6 Feature preprocessing. The purpose of this module is to modify an existing set of features so that they can be processed by a machine learning algorithm. This may be done by scaling numeric parts of the data or by one-hot encoding categorical features. The difference with the feature_extraction module is that the latter extracts new information from the data AdaptiveStandardScaler Binarizer FeatureHasher LDA MaxAbsScaler MinMaxScaler Normalizer OneHotEncoder PreviousImputer RobustScaler StandardScaler StatImputer proba \u00b6 Probability distributions. Gaussian Multinomial reco \u00b6 Recommender systems. Baseline BiasedMF FunkMF RandomNormal rules \u00b6 Decision rules-based algorithms. AMRules stats \u00b6 Running statistics AbsMax AutoCorr BayesianMean Bivariate Count Cov EWMean EWVar Entropy IQR Kurtosis Link Max Mean Min Mode NUnique PeakToPeak PearsonCorr Quantile RollingAbsMax RollingCov RollingIQR RollingMax RollingMean RollingMin RollingMode RollingPeakToPeak RollingPearsonCorr RollingQuantile RollingSEM RollingSum RollingVar SEM Shift Skew Sum Univariate Var stream \u00b6 Streaming utilities. The module includes tools to iterate over data streams. Classes Cache Functions iter_arff iter_array iter_csv iter_libsvm iter_pandas iter_sklearn_dataset iter_sql shuffle simulate_qa synth \u00b6 Synthetic datasets. Each synthetic dataset is a stream generator. The benefit of using a generator is that they do not store the data and each data sample is generated on the fly. Except for a couple of methods, the majority of these methods are infinite data generators. Agrawal AnomalySine ConceptDriftStream Friedman FriedmanDrift Hyperplane LED LEDDrift Logical Mixed Mv Planes2D RandomRBF RandomRBFDrift RandomTree SEA STAGGER Sine Waveform time_series \u00b6 Time series forecasting. Detrender GroupDetrender SNARIMAX tree \u00b6 This module implements incremental Decision Tree (iDT) algorithms for handling classification and regression tasks. Each family of iDT will be presented in a dedicated section. At any moment, iDT might face situations where an input feature previously used to make a split decision is missing in an incoming sample. In this case, the most traversed path is selected to pass down the instance. Moreover, in the case of nominal features, if a new category arises and the feature is used in a decision node, a new branch is created to accommodate the new value. 1. Hoeffding Trees This family of iDT algorithms use the Hoeffding Bound to determine whether or not the incrementally computed best split candidates would be equivalent to the ones obtained in a batch-processing fashion. All the available Hoeffding Tree (HT) implementation share some common functionalities: Set the maximum tree depth allowed ( max_depth ). Handle Active and Inactive nodes: Active learning nodes update their own internal state to improve predictions and monitor input features to perform split attempts. Inactive learning nodes do not update their internal state and only keep the predictors; they are used to save memory in the tree ( max_size ). Enable/disable memory management. Define strategies to sort leaves according to how likely they are going to be split. This enables deactivating non-promising leaves to save memory. Disabling \u2018poor\u2019 attributes to save memory and speed up tree construction. A poor attribute is an input feature whose split merit is much smaller than the current best candidate. Once a feature is disabled, the tree stops saving statistics necessary to split such a feature. Define properties to access leaf prediction strategies, split criteria, and other relevant characteristics. ExtremelyFastDecisionTreeClassifier HoeffdingAdaptiveTreeClassifier HoeffdingAdaptiveTreeRegressor HoeffdingTreeClassifier HoeffdingTreeRegressor LabelCombinationHoeffdingTreeClassifier iSOUPTreeRegressor splitter \u00b6 This module implements the Attribute Observers (AO) (or tree splitters) that are used by the iDTs. AOs are a core aspect of the iDT construction, and might represent one of the major bottlenecks when building the trees. The correct choice and setup of a splitter might result in significant differences in the running time and memory usage of the iDTs. Splitters for classification and regression trees can be differentiated by using the property is_target_class ( True for splitters designed to classification tasks). An error will be raised if one tries to use a classification splitter in a regression tree and vice-versa. EBSTSplitter ExhaustiveSplitter GaussianSplitter HistogramSplitter QOSplitter Splitter TEBSTSplitter utils \u00b6 Utility classes and functions. Classes Histogram SDFT Skyline SortedWindow VectorDict Window Functions check_estimator dict2numpy expand_param_grid numpy2dict math \u00b6 Mathematical utility functions (intended for internal purposes). A lot of this is experimental and has a high probability of changing in the future. argmax chain_dot clamp dot dotvecmat matmul2d minkowski_distance norm outer prod sherman_morrison sigmoid sign softmax pretty \u00b6 Helper functions for making things readable by humans. humanize_bytes print_table","title":"Overview"},{"location":"api/overview/#overview","text":"","title":"Overview"},{"location":"api/overview/#anomaly","text":"Anomaly detection. The estimators in the anomaly module have a slightly different API. Instead of a predict_one method, each anomaly detector has a score_one . The latter returns an anomaly score for a given set of features. High scores indicate anomalies, whereas low scores indicate normal observations. Note that the range of the scores is relative to each estimator. HalfSpaceTrees","title":"anomaly"},{"location":"api/overview/#base","text":"Base interfaces. Every estimator in river is a class, and as such inherits from at least one base interface. These are used to categorize, organize, and standardize the many estimators that river contains. This module contains mixin classes, which are all suffixed by Mixin . Their purpose is to provide additional functionality to an estimator, and thus need to be used in conjunction with a non-mixin base class. This module also contains utilities for type hinting and tagging estimators. AnomalyDetector Base Classifier Clusterer DriftDetector EnsembleMixin Estimator MiniBatchClassifier MiniBatchRegressor MultiOutputMixin Regressor SupervisedTransformer Transformer WrapperMixin","title":"base"},{"location":"api/overview/#cluster","text":"Unsupervised clustering. CluStream DBSTREAM DenStream KMeans STREAMKMeans","title":"cluster"},{"location":"api/overview/#compat","text":"Compatibility tools. This module contains adapters for making river estimators compatible with other libraries, and vice-versa whenever possible. The relevant adapters will only be usable if you have installed the necessary library. For instance, you have to install scikit-learn in order to use the compat.convert_sklearn_to_river function. Classes River2SKLClassifier River2SKLClusterer River2SKLRegressor River2SKLTransformer SKL2RiverClassifier SKL2RiverRegressor Functions convert_river_to_sklearn convert_sklearn_to_river","title":"compat"},{"location":"api/overview/#compose","text":"Model composition. This module contains utilities for merging multiple modeling steps into a single pipeline. Although pipelines are not the only way to process a stream of data, we highly encourage you to use them. Discard FuncTransformer Grouper Pipeline Renamer Select SelectType TransformerUnion","title":"compose"},{"location":"api/overview/#datasets","text":"Datasets. This module contains a collection of datasets for multiple tasks: classification, regression, etc. The data corresponds to popular datasets and are conveniently wrapped to easily iterate over the data in a stream fashion. All datasets have fixed size. Please refer to river.synth if you are interested in infinite synthetic data generators. AirlinePassengers Bananas Bikes ChickWeights CreditCard Elec2 HTTP Higgs ImageSegments Insects MaliciousURL MovieLens100K Music Phishing Restaurants SMSSpam SMTP SolarFlare TREC07 Taxis TrumpApproval","title":"datasets"},{"location":"api/overview/#drift","text":"Concept Drift Detection. This module contains concept drift detection methods. The purpose of a drift detector is to raise an alarm if the data distribution changes. A good drift detector method is the one that maximizes the true positives while keeping the number of false positives to a minimum. ADWIN DDM EDDM HDDM_A HDDM_W KSWIN PageHinkley","title":"drift"},{"location":"api/overview/#dummy","text":"Dummy estimators. This module is here for testing purposes, as well as providing baseline performances. NoChangeClassifier PriorClassifier StatisticRegressor","title":"dummy"},{"location":"api/overview/#ensemble","text":"Ensemble learning. This module includes ensemble methods. This kind of methods improve predictive performance by combining the prediction of their members. ADWINBaggingClassifier AdaBoostClassifier AdaptiveRandomForestClassifier AdaptiveRandomForestRegressor BaggingClassifier BaggingRegressor LeveragingBaggingClassifier SRPClassifier","title":"ensemble"},{"location":"api/overview/#evaluate","text":"Model evaluation. This module provides utilities to evaluate an online model. The goal is to reproduce a real-world scenario with high fidelity. The core function of this module is progressive_val_score , which allows to evaluate a model via progressive validation. This module also exposes \"tracks\". A track is a predefined combination of a dataset and one or more metrics. This allows a principled manner to compare models with each other. For instance, the load_binary_clf_tracks returns tracks that are to be used to evaluate the performance of a binary classification model. The benchmarks directory at the root of the River repository uses these tracks. Classes Track Functions load_binary_clf_tracks progressive_val_score","title":"evaluate"},{"location":"api/overview/#expert","text":"Expert learning. This module regroups a variety of methods that may be used for performing model selection. An expert learner is provided with a list of models, which are also called experts, and is tasked with performing at least as well as the best expert. Indeed, initially the best model is not known. The performance of each model becomes more apparent as time goes by. Different strategies are possible, each one offering a different tradeoff in terms of accuracy and computational performance. Expert learning can be used for tuning the hyperparameters of a model. This may be done by creating a copy of the model for each set of hyperparameters, and treating each copy as a separate model. The utils.expand_param_grid function can be used for this purpose. Note that this differs from the ensemble module in that methods from the latter are designed to improve the performance of a single model. Both modules may thus be used in conjunction with one another. EWARegressor EpsilonGreedyRegressor StackingClassifier SuccessiveHalvingClassifier SuccessiveHalvingRegressor UCBRegressor","title":"expert"},{"location":"api/overview/#facto","text":"Factorization machines. FFMClassifier FFMRegressor FMClassifier FMRegressor FwFMClassifier FwFMRegressor HOFMClassifier HOFMRegressor","title":"facto"},{"location":"api/overview/#feature_extraction","text":"Feature extraction. This module can be used to extract information from raw features. This includes encoding categorical data as well as looking at interactions between existing features. This differs from the processing module in that the latter's purpose is rather to clean the data so that it may be processed by a particular machine learning algorithm. Agg BagOfWords PolynomialExtender RBFSampler TFIDF TargetAgg","title":"feature_extraction"},{"location":"api/overview/#feature_selection","text":"Feature selection. PoissonInclusion SelectKBest VarianceThreshold","title":"feature_selection"},{"location":"api/overview/#imblearn","text":"Sampling methods. HardSamplingClassifier HardSamplingRegressor RandomOverSampler RandomSampler RandomUnderSampler","title":"imblearn"},{"location":"api/overview/#linear_model","text":"Linear models. ALMAClassifier LinearRegression LogisticRegression PAClassifier PARegressor Perceptron SoftmaxRegression","title":"linear_model"},{"location":"api/overview/#meta","text":"Meta-models. BoxCoxRegressor PredClipper TransformedTargetRegressor","title":"meta"},{"location":"api/overview/#metrics","text":"Evaluation metrics. All the metrics are updated one sample at a time. This way we can track performance of predictive methods over time. Accuracy AdjustedMutualInfo AdjustedRand BalancedAccuracy BinaryMetric ClassificationMetric ClassificationReport CohenKappa Completeness ConfusionMatrix CrossEntropy ExactMatch ExampleF1 ExampleFBeta ExamplePrecision ExampleRecall F1 FBeta FowlkesMallows GeometricMean Hamming HammingLoss Homogeneity Jaccard KappaM KappaT LogLoss MAE MCC MSE MacroF1 MacroFBeta MacroPrecision MacroRecall MatthewsCorrCoef Metric Metrics MicroF1 MicroFBeta MicroPrecision MicroRecall MultiClassMetric MultiFBeta MultiLabelConfusionMatrix MultiOutputClassificationMetric MultiOutputRegressionMetric MutualInfo NormalizedMutualInfo PairConfusionMatrix Precision PrevalenceThreshold Purity Q0 Q2 R2 RMSE RMSLE ROCAUC Rand Recall RegressionMetric RegressionMultiOutput Rolling SMAPE SorensenDice TimeRolling VBeta VariationInfo WeightedF1 WeightedFBeta WeightedPrecision WeightedRecall WrapperMetric","title":"metrics"},{"location":"api/overview/#cluster_1","text":"Internal clustering metrics This submodule includes all internal clustering metrics that are updated with one sample, its label and the current cluster centers at a time. Using this, we can track the performance of the clustering algorithm without having to store information of all previously passed points. BIC BallHall CalinskiHarabasz Cohesion DaviesBouldin GD43 GD53 Hartigan IIndex InternalMetric MSSTD PS R2 RMSSTD SD SSB SSW Separation Silhouette WB XieBeni Xu","title":"cluster"},{"location":"api/overview/#multiclass","text":"Multi-class classification. OneVsOneClassifier OneVsRestClassifier OutputCodeClassifier","title":"multiclass"},{"location":"api/overview/#multioutput","text":"Multi-output models. ClassifierChain MonteCarloClassifierChain ProbabilisticClassifierChain RegressorChain","title":"multioutput"},{"location":"api/overview/#naive_bayes","text":"Naive Bayes algorithms. BernoulliNB ComplementNB GaussianNB MultinomialNB","title":"naive_bayes"},{"location":"api/overview/#neighbors","text":"Neighbors-based learning. Also known as lazy methods. In these methods, generalisation of the training data is delayed until a query is received. KNNADWINClassifier KNNClassifier KNNRegressor SAMKNNClassifier","title":"neighbors"},{"location":"api/overview/#neural_net","text":"Neural networks. MLPRegressor","title":"neural_net"},{"location":"api/overview/#activations","text":"Identity ReLU Sigmoid","title":"activations"},{"location":"api/overview/#optim","text":"Stochastic optimization. AMSGrad AdaBound AdaDelta AdaGrad AdaMax Adam Averager FTRLProximal Momentum Nadam NesterovMomentum Optimizer RMSProp SGD","title":"optim"},{"location":"api/overview/#initializers","text":"Weight initializers. Constant Normal Zeros","title":"initializers"},{"location":"api/overview/#losses","text":"Loss functions. Each loss function is intended to work with both single values as well as numpy vectors. Absolute BinaryFocalLoss BinaryLoss Cauchy CrossEntropy EpsilonInsensitiveHinge Hinge Log MultiClassLoss Poisson Quantile RegressionLoss Squared","title":"losses"},{"location":"api/overview/#schedulers","text":"Learning rate schedulers. Constant InverseScaling Optimal Scheduler","title":"schedulers"},{"location":"api/overview/#preprocessing","text":"Feature preprocessing. The purpose of this module is to modify an existing set of features so that they can be processed by a machine learning algorithm. This may be done by scaling numeric parts of the data or by one-hot encoding categorical features. The difference with the feature_extraction module is that the latter extracts new information from the data AdaptiveStandardScaler Binarizer FeatureHasher LDA MaxAbsScaler MinMaxScaler Normalizer OneHotEncoder PreviousImputer RobustScaler StandardScaler StatImputer","title":"preprocessing"},{"location":"api/overview/#proba","text":"Probability distributions. Gaussian Multinomial","title":"proba"},{"location":"api/overview/#reco","text":"Recommender systems. Baseline BiasedMF FunkMF RandomNormal","title":"reco"},{"location":"api/overview/#rules","text":"Decision rules-based algorithms. AMRules","title":"rules"},{"location":"api/overview/#stats","text":"Running statistics AbsMax AutoCorr BayesianMean Bivariate Count Cov EWMean EWVar Entropy IQR Kurtosis Link Max Mean Min Mode NUnique PeakToPeak PearsonCorr Quantile RollingAbsMax RollingCov RollingIQR RollingMax RollingMean RollingMin RollingMode RollingPeakToPeak RollingPearsonCorr RollingQuantile RollingSEM RollingSum RollingVar SEM Shift Skew Sum Univariate Var","title":"stats"},{"location":"api/overview/#stream","text":"Streaming utilities. The module includes tools to iterate over data streams. Classes Cache Functions iter_arff iter_array iter_csv iter_libsvm iter_pandas iter_sklearn_dataset iter_sql shuffle simulate_qa","title":"stream"},{"location":"api/overview/#synth","text":"Synthetic datasets. Each synthetic dataset is a stream generator. The benefit of using a generator is that they do not store the data and each data sample is generated on the fly. Except for a couple of methods, the majority of these methods are infinite data generators. Agrawal AnomalySine ConceptDriftStream Friedman FriedmanDrift Hyperplane LED LEDDrift Logical Mixed Mv Planes2D RandomRBF RandomRBFDrift RandomTree SEA STAGGER Sine Waveform","title":"synth"},{"location":"api/overview/#time_series","text":"Time series forecasting. Detrender GroupDetrender SNARIMAX","title":"time_series"},{"location":"api/overview/#tree","text":"This module implements incremental Decision Tree (iDT) algorithms for handling classification and regression tasks. Each family of iDT will be presented in a dedicated section. At any moment, iDT might face situations where an input feature previously used to make a split decision is missing in an incoming sample. In this case, the most traversed path is selected to pass down the instance. Moreover, in the case of nominal features, if a new category arises and the feature is used in a decision node, a new branch is created to accommodate the new value. 1. Hoeffding Trees This family of iDT algorithms use the Hoeffding Bound to determine whether or not the incrementally computed best split candidates would be equivalent to the ones obtained in a batch-processing fashion. All the available Hoeffding Tree (HT) implementation share some common functionalities: Set the maximum tree depth allowed ( max_depth ). Handle Active and Inactive nodes: Active learning nodes update their own internal state to improve predictions and monitor input features to perform split attempts. Inactive learning nodes do not update their internal state and only keep the predictors; they are used to save memory in the tree ( max_size ). Enable/disable memory management. Define strategies to sort leaves according to how likely they are going to be split. This enables deactivating non-promising leaves to save memory. Disabling \u2018poor\u2019 attributes to save memory and speed up tree construction. A poor attribute is an input feature whose split merit is much smaller than the current best candidate. Once a feature is disabled, the tree stops saving statistics necessary to split such a feature. Define properties to access leaf prediction strategies, split criteria, and other relevant characteristics. ExtremelyFastDecisionTreeClassifier HoeffdingAdaptiveTreeClassifier HoeffdingAdaptiveTreeRegressor HoeffdingTreeClassifier HoeffdingTreeRegressor LabelCombinationHoeffdingTreeClassifier iSOUPTreeRegressor","title":"tree"},{"location":"api/overview/#splitter","text":"This module implements the Attribute Observers (AO) (or tree splitters) that are used by the iDTs. AOs are a core aspect of the iDT construction, and might represent one of the major bottlenecks when building the trees. The correct choice and setup of a splitter might result in significant differences in the running time and memory usage of the iDTs. Splitters for classification and regression trees can be differentiated by using the property is_target_class ( True for splitters designed to classification tasks). An error will be raised if one tries to use a classification splitter in a regression tree and vice-versa. EBSTSplitter ExhaustiveSplitter GaussianSplitter HistogramSplitter QOSplitter Splitter TEBSTSplitter","title":"splitter"},{"location":"api/overview/#utils","text":"Utility classes and functions. Classes Histogram SDFT Skyline SortedWindow VectorDict Window Functions check_estimator dict2numpy expand_param_grid numpy2dict","title":"utils"},{"location":"api/overview/#math","text":"Mathematical utility functions (intended for internal purposes). A lot of this is experimental and has a high probability of changing in the future. argmax chain_dot clamp dot dotvecmat matmul2d minkowski_distance norm outer prod sherman_morrison sigmoid sign softmax","title":"math"},{"location":"api/overview/#pretty","text":"Helper functions for making things readable by humans. humanize_bytes print_table","title":"pretty"},{"location":"api/anomaly/HalfSpaceTrees/","text":"HalfSpaceTrees \u00b6 Half-Space Trees (HST). Half-space trees are an online variant of isolation forests. They work well when anomalies are spread out. However, they do not work well if anomalies are packed together in windows. By default, this implementation assumes that each feature has values that are comprised between 0 and 1. If this isn't the case, then you can manually specify the limits via the limits argument. If you do not know the limits in advance, then you can use a preprocessing.MinMaxScaler as an initial preprocessing step. The current implementation builds the trees the first time the learn_one method is called. Therefore, the first learn_one call might be slow, whereas subsequent calls will be very fast in comparison. In general, the computation time of both learn_one and score_one scales linearly with the number of trees, and exponentially with the height of each tree. Note that high scores indicate anomalies, whereas low scores indicate normal observations. Parameters \u00b6 n_trees \u2013 defaults to 10 Number of trees to use. height \u2013 defaults to 8 Height of each tree. Note that a tree of height h is made up of h + 1 levels and therefore contains 2 ** (h + 1) - 1 nodes. window_size \u2013 defaults to 250 Number of observations to use for calculating the mass at each node in each tree. limits ( Dict[Hashable, Tuple[float, float]] ) \u2013 defaults to None Specifies the range of each feature. By default each feature is assumed to be in range [0, 1] . seed ( int ) \u2013 defaults to None Random number seed. Attributes \u00b6 size_limit This is the threshold under which the node search stops during the scoring phase. The value .1 is a magic constant indicated in the original paper. Examples \u00b6 >>> from river import anomaly >>> X = [ 0.5 , 0.45 , 0.43 , 0.44 , 0.445 , 0.45 , 0.0 ] >>> hst = anomaly . HalfSpaceTrees ( ... n_trees = 5 , ... height = 3 , ... window_size = 3 , ... seed = 42 ... ) >>> for x in X [: 3 ]: ... hst = hst . learn_one ({ 'x' : x }) # Warming up >>> for x in X : ... features = { 'x' : x } ... hst = hst . learn_one ( features ) ... print ( f 'Anomaly score for x= { x : .3f } : { hst . score_one ( features ) : .3f } ' ) Anomaly score for x = 0.500 : 0.107 Anomaly score for x = 0.450 : 0.071 Anomaly score for x = 0.430 : 0.107 Anomaly score for x = 0.440 : 0.107 Anomaly score for x = 0.445 : 0.107 Anomaly score for x = 0.450 : 0.071 Anomaly score for x = 0.000 : 0.853 The feature values are all comprised between 0 and 1. This is what is assumed by the model by default. In the following example, we construct a pipeline that scales the data online and ensures that the values of each feature are comprised between 0 and 1. >>> from river import compose >>> from river import datasets >>> from river import metrics >>> from river import preprocessing >>> model = compose . Pipeline ( ... preprocessing . MinMaxScaler (), ... anomaly . HalfSpaceTrees ( seed = 42 ) ... ) >>> auc = metrics . ROCAUC () >>> for x , y in datasets . CreditCard () . take ( 8000 ): ... score = model . score_one ( x ) ... model = model . learn_one ( x , y ) ... auc = auc . update ( y , score ) >>> auc ROCAUC : 0.940572 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model. Parameters x ( dict ) Returns AnomalyDetector : self score_one Return an outlier score. A high score is indicative of an anomaly. A low score corresponds a normal observation. Parameters x ( dict ) Returns float : An anomaly score. A high score is indicative of an anomaly. A low score corresponds a References \u00b6 Tan, S.C., Ting, K.M. and Liu, T.F., 2011, June. Fast anomaly detection for streaming data. In Twenty-Second International Joint Conference on Artificial Intelligence. \u21a9","title":"HalfSpaceTrees"},{"location":"api/anomaly/HalfSpaceTrees/#halfspacetrees","text":"Half-Space Trees (HST). Half-space trees are an online variant of isolation forests. They work well when anomalies are spread out. However, they do not work well if anomalies are packed together in windows. By default, this implementation assumes that each feature has values that are comprised between 0 and 1. If this isn't the case, then you can manually specify the limits via the limits argument. If you do not know the limits in advance, then you can use a preprocessing.MinMaxScaler as an initial preprocessing step. The current implementation builds the trees the first time the learn_one method is called. Therefore, the first learn_one call might be slow, whereas subsequent calls will be very fast in comparison. In general, the computation time of both learn_one and score_one scales linearly with the number of trees, and exponentially with the height of each tree. Note that high scores indicate anomalies, whereas low scores indicate normal observations.","title":"HalfSpaceTrees"},{"location":"api/anomaly/HalfSpaceTrees/#parameters","text":"n_trees \u2013 defaults to 10 Number of trees to use. height \u2013 defaults to 8 Height of each tree. Note that a tree of height h is made up of h + 1 levels and therefore contains 2 ** (h + 1) - 1 nodes. window_size \u2013 defaults to 250 Number of observations to use for calculating the mass at each node in each tree. limits ( Dict[Hashable, Tuple[float, float]] ) \u2013 defaults to None Specifies the range of each feature. By default each feature is assumed to be in range [0, 1] . seed ( int ) \u2013 defaults to None Random number seed.","title":"Parameters"},{"location":"api/anomaly/HalfSpaceTrees/#attributes","text":"size_limit This is the threshold under which the node search stops during the scoring phase. The value .1 is a magic constant indicated in the original paper.","title":"Attributes"},{"location":"api/anomaly/HalfSpaceTrees/#examples","text":">>> from river import anomaly >>> X = [ 0.5 , 0.45 , 0.43 , 0.44 , 0.445 , 0.45 , 0.0 ] >>> hst = anomaly . HalfSpaceTrees ( ... n_trees = 5 , ... height = 3 , ... window_size = 3 , ... seed = 42 ... ) >>> for x in X [: 3 ]: ... hst = hst . learn_one ({ 'x' : x }) # Warming up >>> for x in X : ... features = { 'x' : x } ... hst = hst . learn_one ( features ) ... print ( f 'Anomaly score for x= { x : .3f } : { hst . score_one ( features ) : .3f } ' ) Anomaly score for x = 0.500 : 0.107 Anomaly score for x = 0.450 : 0.071 Anomaly score for x = 0.430 : 0.107 Anomaly score for x = 0.440 : 0.107 Anomaly score for x = 0.445 : 0.107 Anomaly score for x = 0.450 : 0.071 Anomaly score for x = 0.000 : 0.853 The feature values are all comprised between 0 and 1. This is what is assumed by the model by default. In the following example, we construct a pipeline that scales the data online and ensures that the values of each feature are comprised between 0 and 1. >>> from river import compose >>> from river import datasets >>> from river import metrics >>> from river import preprocessing >>> model = compose . Pipeline ( ... preprocessing . MinMaxScaler (), ... anomaly . HalfSpaceTrees ( seed = 42 ) ... ) >>> auc = metrics . ROCAUC () >>> for x , y in datasets . CreditCard () . take ( 8000 ): ... score = model . score_one ( x ) ... model = model . learn_one ( x , y ) ... auc = auc . update ( y , score ) >>> auc ROCAUC : 0.940572","title":"Examples"},{"location":"api/anomaly/HalfSpaceTrees/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model. Parameters x ( dict ) Returns AnomalyDetector : self score_one Return an outlier score. A high score is indicative of an anomaly. A low score corresponds a normal observation. Parameters x ( dict ) Returns float : An anomaly score. A high score is indicative of an anomaly. A low score corresponds a","title":"Methods"},{"location":"api/anomaly/HalfSpaceTrees/#references","text":"Tan, S.C., Ting, K.M. and Liu, T.F., 2011, June. Fast anomaly detection for streaming data. In Twenty-Second International Joint Conference on Artificial Intelligence. \u21a9","title":"References"},{"location":"api/base/AnomalyDetector/","text":"AnomalyDetector \u00b6 An anomaly detector. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model. Parameters x ( dict ) Returns AnomalyDetector : self score_one Return an outlier score. A high score is indicative of an anomaly. A low score corresponds a normal observation. Parameters x ( dict ) Returns float : An anomaly score. A high score is indicative of an anomaly. A low score corresponds a","title":"AnomalyDetector"},{"location":"api/base/AnomalyDetector/#anomalydetector","text":"An anomaly detector.","title":"AnomalyDetector"},{"location":"api/base/AnomalyDetector/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model. Parameters x ( dict ) Returns AnomalyDetector : self score_one Return an outlier score. A high score is indicative of an anomaly. A low score corresponds a normal observation. Parameters x ( dict ) Returns float : An anomaly score. A high score is indicative of an anomaly. A low score corresponds a","title":"Methods"},{"location":"api/base/Base/","text":"Base \u00b6 Base class that is inherited by the majority of classes in River. This base class allows us to handle the following tasks in a uniform manner: Getting and setting parameters. - Displaying information. - Cloning. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Base"},{"location":"api/base/Base/#base","text":"Base class that is inherited by the majority of classes in River. This base class allows us to handle the following tasks in a uniform manner: Getting and setting parameters. - Displaying information. - Cloning.","title":"Base"},{"location":"api/base/Base/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Methods"},{"location":"api/base/Classifier/","text":"Classifier \u00b6 A classifier. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Classifier"},{"location":"api/base/Classifier/#classifier","text":"A classifier.","title":"Classifier"},{"location":"api/base/Classifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/base/Clusterer/","text":"Clusterer \u00b6 A clustering model. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number.","title":"Clusterer"},{"location":"api/base/Clusterer/#clusterer","text":"A clustering model.","title":"Clusterer"},{"location":"api/base/Clusterer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number.","title":"Methods"},{"location":"api/base/DriftDetector/","text":"DriftDetector \u00b6 A drift detector. Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"DriftDetector"},{"location":"api/base/DriftDetector/#driftdetector","text":"A drift detector.","title":"DriftDetector"},{"location":"api/base/DriftDetector/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/base/DriftDetector/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"Methods"},{"location":"api/base/EnsembleMixin/","text":"EnsembleMixin \u00b6 An ensemble model. Parameters \u00b6 models","title":"EnsembleMixin"},{"location":"api/base/EnsembleMixin/#ensemblemixin","text":"An ensemble model.","title":"EnsembleMixin"},{"location":"api/base/EnsembleMixin/#parameters","text":"models","title":"Parameters"},{"location":"api/base/Estimator/","text":"Estimator \u00b6 An estimator. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Estimator"},{"location":"api/base/Estimator/#estimator","text":"An estimator.","title":"Estimator"},{"location":"api/base/Estimator/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Methods"},{"location":"api/base/MiniBatchClassifier/","text":"MiniBatchClassifier \u00b6 A classifier that can can operate on mini-batches. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) kwargs Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"MiniBatchClassifier"},{"location":"api/base/MiniBatchClassifier/#minibatchclassifier","text":"A classifier that can can operate on mini-batches.","title":"MiniBatchClassifier"},{"location":"api/base/MiniBatchClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) kwargs Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/base/MiniBatchRegressor/","text":"MiniBatchRegressor \u00b6 A regressor that can operate on mini-batches. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) kwargs Returns MiniBatchRegressor : self learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) kwargs Returns Regressor : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted outcomes. predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"MiniBatchRegressor"},{"location":"api/base/MiniBatchRegressor/#minibatchregressor","text":"A regressor that can operate on mini-batches.","title":"MiniBatchRegressor"},{"location":"api/base/MiniBatchRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) kwargs Returns MiniBatchRegressor : self learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) kwargs Returns Regressor : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted outcomes. predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/base/MultiOutputMixin/","text":"MultiOutputMixin \u00b6 A multi-output estimator.","title":"MultiOutputMixin"},{"location":"api/base/MultiOutputMixin/#multioutputmixin","text":"A multi-output estimator.","title":"MultiOutputMixin"},{"location":"api/base/Regressor/","text":"Regressor \u00b6 A regressor. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) kwargs Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Regressor"},{"location":"api/base/Regressor/#regressor","text":"A regressor.","title":"Regressor"},{"location":"api/base/Regressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) kwargs Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/base/SupervisedTransformer/","text":"SupervisedTransformer \u00b6 A supervised transformer. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) kwargs Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"SupervisedTransformer"},{"location":"api/base/SupervisedTransformer/#supervisedtransformer","text":"A supervised transformer.","title":"SupervisedTransformer"},{"location":"api/base/SupervisedTransformer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) kwargs Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/base/Transformer/","text":"Transformer \u00b6 A transformer. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Transformer"},{"location":"api/base/Transformer/#transformer","text":"A transformer.","title":"Transformer"},{"location":"api/base/Transformer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/base/WrapperMixin/","text":"WrapperMixin \u00b6 A wrapper model.","title":"WrapperMixin"},{"location":"api/base/WrapperMixin/#wrappermixin","text":"A wrapper model.","title":"WrapperMixin"},{"location":"api/cluster/CluStream/","text":"CluStream \u00b6 CluStream The CluStream algorithm 1 maintains statistical information about the data using micro-clusters. These micro-clusters are temporal extensions of cluster feature vectors. The micro-clusters are stored at snapshots in time following a pyramidal pattern. This pattern allows to recall summary statistics from different time horizons. Training with a new point p is performed in two main tasks: Determinate closest micro-cluster to p Check whether p fits (memory) into the closest micro-cluster: if p fits, put into micro-cluster if p does not fit, free some space to insert a new micro-cluster. This is done in two ways, delete an old micro-cluster or merge the two micro-clusters closest to each other. Parameters \u00b6 seed ( int ) \u2013 defaults to None Random seed used for generating initial centroid positions. time_window ( int ) \u2013 defaults to 1000 If the current time is T and the time window is h , we only consider the data that arrived within the period (T-h,T) . max_micro_clusters ( int ) \u2013 defaults to 100 The maximum number of micro-clusters to use. micro_cluster_r_factor ( int ) \u2013 defaults to 2 Multiplier for the micro-cluster radius. When deciding to add a new data point to a micro-cluster, the maximum boundary is defined as a factor of the micro_cluster_r_factor of the RMS deviation of the data points in the micro-cluster from the centroid. n_macro_clusters ( int ) \u2013 defaults to 5 The number of clusters (k) for the k-means algorithm. kwargs Other parameters passed to the incremental kmeans at cluster.KMeans . Attributes \u00b6 centers ( dict ) Central positions of each cluster. Examples \u00b6 In the following example, max_micro_clusters and time_window are set relatively low due to the limited number of training points. Moreover, all points are learnt before any predictions are made. The halflife is set at 0.4, to show that you can pass cluster.KMeans parameters via keyword arguments. >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ] ... ] >>> clustream = cluster . CluStream ( time_window = 1 , ... max_micro_clusters = 3 , ... n_macro_clusters = 2 , ... seed = 0 , ... halflife = 0.4 ) >>> for i , ( x , _ ) in enumerate ( stream . iter_array ( X )): ... clustream = clustream . learn_one ( x ) >>> clustream . predict_one ({ 0 : 1 , 1 : 1 }) 1 >>> clustream . predict_one ({ 0 : 4 , 1 : 3 }) 0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number. References \u00b6 Aggarwal, C.C., Philip, S.Y., Han, J. and Wang, J., 2003, A framework for clustering evolving data streams. In Proceedings 2003 VLDB conference (pp. 81-92). Morgan Kaufmann. \u21a9","title":"CluStream"},{"location":"api/cluster/CluStream/#clustream","text":"CluStream The CluStream algorithm 1 maintains statistical information about the data using micro-clusters. These micro-clusters are temporal extensions of cluster feature vectors. The micro-clusters are stored at snapshots in time following a pyramidal pattern. This pattern allows to recall summary statistics from different time horizons. Training with a new point p is performed in two main tasks: Determinate closest micro-cluster to p Check whether p fits (memory) into the closest micro-cluster: if p fits, put into micro-cluster if p does not fit, free some space to insert a new micro-cluster. This is done in two ways, delete an old micro-cluster or merge the two micro-clusters closest to each other.","title":"CluStream"},{"location":"api/cluster/CluStream/#parameters","text":"seed ( int ) \u2013 defaults to None Random seed used for generating initial centroid positions. time_window ( int ) \u2013 defaults to 1000 If the current time is T and the time window is h , we only consider the data that arrived within the period (T-h,T) . max_micro_clusters ( int ) \u2013 defaults to 100 The maximum number of micro-clusters to use. micro_cluster_r_factor ( int ) \u2013 defaults to 2 Multiplier for the micro-cluster radius. When deciding to add a new data point to a micro-cluster, the maximum boundary is defined as a factor of the micro_cluster_r_factor of the RMS deviation of the data points in the micro-cluster from the centroid. n_macro_clusters ( int ) \u2013 defaults to 5 The number of clusters (k) for the k-means algorithm. kwargs Other parameters passed to the incremental kmeans at cluster.KMeans .","title":"Parameters"},{"location":"api/cluster/CluStream/#attributes","text":"centers ( dict ) Central positions of each cluster.","title":"Attributes"},{"location":"api/cluster/CluStream/#examples","text":"In the following example, max_micro_clusters and time_window are set relatively low due to the limited number of training points. Moreover, all points are learnt before any predictions are made. The halflife is set at 0.4, to show that you can pass cluster.KMeans parameters via keyword arguments. >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ] ... ] >>> clustream = cluster . CluStream ( time_window = 1 , ... max_micro_clusters = 3 , ... n_macro_clusters = 2 , ... seed = 0 , ... halflife = 0.4 ) >>> for i , ( x , _ ) in enumerate ( stream . iter_array ( X )): ... clustream = clustream . learn_one ( x ) >>> clustream . predict_one ({ 0 : 1 , 1 : 1 }) 1 >>> clustream . predict_one ({ 0 : 4 , 1 : 3 }) 0","title":"Examples"},{"location":"api/cluster/CluStream/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number.","title":"Methods"},{"location":"api/cluster/CluStream/#references","text":"Aggarwal, C.C., Philip, S.Y., Han, J. and Wang, J., 2003, A framework for clustering evolving data streams. In Proceedings 2003 VLDB conference (pp. 81-92). Morgan Kaufmann. \u21a9","title":"References"},{"location":"api/cluster/DBSTREAM/","text":"DBSTREAM \u00b6 DBSTREAM DBSTREAM 1 is a clustering algorithm for evolving data streams. It is the first micro-cluster-based online clustering component that explicitely captures the density between micro-clusters via a shared density graph. The density information in the graph is then exploited for reclustering based on actual density between adjacent micro clusters. The algorithm is divided into two parts: Online micro-cluster maintenance (learning) For a new point p : Find all micro clusters for which p falls within the fixed radius (clustering threshold). If no neighbor is found, a new micro cluster with a weight of 1 is created for p . If no neighbor is found, a new micro cluster with a weight of 1 is created for p . If one or more neighbors of p are found, we update the micro clusters by applying the appropriate fading, increasing their weight and then we try to move them closer to p using the Gaussian neighborhood function. Next, the shared density graph is updated. To prevent collapsing micro clusters, we will restrict the movement for micro clusters in case they come closer than \\(r\\) (clustering threshold) to each other. Finishing this process, the time stamp is also increased by 1. Finally, the cleanup will be processed. It is executed every t_gap time steps, removing weak micro clusters and weak entries in the shared density graph to recover memory and improve the clustering algorithm's processing speed. Offline generation of macro clusters (clustering) The offline generation of macro clusters is generated through the two following steps: The connectivity graph C is constructed using shared density entries between strong micro clusters. The edges in this connectivity graph with a connectivity value greater than the intersection threshold ( \\(\\alpha\\) ) are used to find connected components representing the final cluster. After the connectivity graph is generated, a variant of the DBSCAN algorithm proposed by Ester et al. is applied to form all macro clusters from \\(\\alpha\\) -connected micro clusters. Parameters \u00b6 clustering_threshold ( float ) \u2013 defaults to 1.0 DBStream represents each micro cluster by a leader (a data point defining the micro cluster's center) and the density in an area of a user-specified radius \\(r\\) ( clustering_threshold ) around the center. fading_factor ( float ) \u2013 defaults to 0.01 Parameter that controls the importance of historical data to current cluster. Note that fading_factor has to be different from 0 . cleanup_interval ( float ) \u2013 defaults to 2 The time interval between two consecutive time points when the cleanup process is conducted. intersection_factor ( float ) \u2013 defaults to 0.3 The intersection factor related to the area of the overlap of the micro clusters relative to the area cover by micro clusters. This parameter is used to determine whether a micro cluster or a shared density is weak. minimum_weight ( float ) \u2013 defaults to 1.0 The minimum weight for a cluster to be not \"noisy\". Attributes \u00b6 n_clusters Number of clusters generated by the algorithm. clusters A set of final clusters of type DBStreamMicroCluster . However, these are either micro clusters, or macro clusters that are generated by merging all \\(\\alpha\\) -connected micro clusters. This set is generated through the offline phase of the algorithm. centers Final clusters' centers. micro_clusters Micro clusters generated by the algorithm. Instead of updating directly the new instance points into a nearest micro cluster, through each iteration, the weight and center will be modified so that the clusters are closer to the new points, using the Gaussian neighborhood function. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 0.5 ], [ 1 , 0.625 ], [ 1 , 0.75 ], [ 1 , 1.125 ], [ 1 , 1.5 ], [ 1 , 1.75 ], ... [ 4 , 1.5 ], [ 4 , 2.25 ], [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ] ... ] >>> dbstream = cluster . DBSTREAM ( clustering_threshold = 1.5 , ... fading_factor = 0.05 , ... cleanup_interval = 4 , ... intersection_factor = 0.5 , ... minimum_weight = 1 ) >>> for x , _ in stream . iter_array ( X ): ... dbstream = dbstream . learn_one ( x ) >>> dbstream . predict_one ({ 0 : 1 , 1 : 2 }) 0 >>> dbstream . predict_one ({ 0 : 5 , 1 : 2 }) 1 >>> dbstream . n_clusters 2 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number. References \u00b6 Michael Hahsler and Matthew Bolanos (2016, pp 1449-1461). Clsutering Data Streams Based on Shared Density between Micro-Clusters, IEEE Transactions on Knowledge and Data Engineering 28(6) . In Proceedings of the Sixth SIAM International Conference on Data Mining, April 20\u201322, 2006, Bethesda, MD, USA. \u21a9 Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In KDD-96 Proceedings, AAAI. \u21a9","title":"DBSTREAM"},{"location":"api/cluster/DBSTREAM/#dbstream","text":"DBSTREAM DBSTREAM 1 is a clustering algorithm for evolving data streams. It is the first micro-cluster-based online clustering component that explicitely captures the density between micro-clusters via a shared density graph. The density information in the graph is then exploited for reclustering based on actual density between adjacent micro clusters. The algorithm is divided into two parts: Online micro-cluster maintenance (learning) For a new point p : Find all micro clusters for which p falls within the fixed radius (clustering threshold). If no neighbor is found, a new micro cluster with a weight of 1 is created for p . If no neighbor is found, a new micro cluster with a weight of 1 is created for p . If one or more neighbors of p are found, we update the micro clusters by applying the appropriate fading, increasing their weight and then we try to move them closer to p using the Gaussian neighborhood function. Next, the shared density graph is updated. To prevent collapsing micro clusters, we will restrict the movement for micro clusters in case they come closer than \\(r\\) (clustering threshold) to each other. Finishing this process, the time stamp is also increased by 1. Finally, the cleanup will be processed. It is executed every t_gap time steps, removing weak micro clusters and weak entries in the shared density graph to recover memory and improve the clustering algorithm's processing speed. Offline generation of macro clusters (clustering) The offline generation of macro clusters is generated through the two following steps: The connectivity graph C is constructed using shared density entries between strong micro clusters. The edges in this connectivity graph with a connectivity value greater than the intersection threshold ( \\(\\alpha\\) ) are used to find connected components representing the final cluster. After the connectivity graph is generated, a variant of the DBSCAN algorithm proposed by Ester et al. is applied to form all macro clusters from \\(\\alpha\\) -connected micro clusters.","title":"DBSTREAM"},{"location":"api/cluster/DBSTREAM/#parameters","text":"clustering_threshold ( float ) \u2013 defaults to 1.0 DBStream represents each micro cluster by a leader (a data point defining the micro cluster's center) and the density in an area of a user-specified radius \\(r\\) ( clustering_threshold ) around the center. fading_factor ( float ) \u2013 defaults to 0.01 Parameter that controls the importance of historical data to current cluster. Note that fading_factor has to be different from 0 . cleanup_interval ( float ) \u2013 defaults to 2 The time interval between two consecutive time points when the cleanup process is conducted. intersection_factor ( float ) \u2013 defaults to 0.3 The intersection factor related to the area of the overlap of the micro clusters relative to the area cover by micro clusters. This parameter is used to determine whether a micro cluster or a shared density is weak. minimum_weight ( float ) \u2013 defaults to 1.0 The minimum weight for a cluster to be not \"noisy\".","title":"Parameters"},{"location":"api/cluster/DBSTREAM/#attributes","text":"n_clusters Number of clusters generated by the algorithm. clusters A set of final clusters of type DBStreamMicroCluster . However, these are either micro clusters, or macro clusters that are generated by merging all \\(\\alpha\\) -connected micro clusters. This set is generated through the offline phase of the algorithm. centers Final clusters' centers. micro_clusters Micro clusters generated by the algorithm. Instead of updating directly the new instance points into a nearest micro cluster, through each iteration, the weight and center will be modified so that the clusters are closer to the new points, using the Gaussian neighborhood function.","title":"Attributes"},{"location":"api/cluster/DBSTREAM/#examples","text":">>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 0.5 ], [ 1 , 0.625 ], [ 1 , 0.75 ], [ 1 , 1.125 ], [ 1 , 1.5 ], [ 1 , 1.75 ], ... [ 4 , 1.5 ], [ 4 , 2.25 ], [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ] ... ] >>> dbstream = cluster . DBSTREAM ( clustering_threshold = 1.5 , ... fading_factor = 0.05 , ... cleanup_interval = 4 , ... intersection_factor = 0.5 , ... minimum_weight = 1 ) >>> for x , _ in stream . iter_array ( X ): ... dbstream = dbstream . learn_one ( x ) >>> dbstream . predict_one ({ 0 : 1 , 1 : 2 }) 0 >>> dbstream . predict_one ({ 0 : 5 , 1 : 2 }) 1 >>> dbstream . n_clusters 2","title":"Examples"},{"location":"api/cluster/DBSTREAM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number.","title":"Methods"},{"location":"api/cluster/DBSTREAM/#references","text":"Michael Hahsler and Matthew Bolanos (2016, pp 1449-1461). Clsutering Data Streams Based on Shared Density between Micro-Clusters, IEEE Transactions on Knowledge and Data Engineering 28(6) . In Proceedings of the Sixth SIAM International Conference on Data Mining, April 20\u201322, 2006, Bethesda, MD, USA. \u21a9 Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In KDD-96 Proceedings, AAAI. \u21a9","title":"References"},{"location":"api/cluster/DenStream/","text":"DenStream \u00b6 DenStream DenStream 1 is a clustering algorithm for evolving data streams. DenStream can discover clusters with arbitrary shape and is robust against noise (outliers). \"Dense\" micro-clusters (named core-micro-clusters) summarise the clusters of arbitrary shape. A pruning strategy based on the concepts of potential and outlier micro-clusters guarantees the precision of the weights of the micro-clusters with limited memory. The algorithm is divided into two parts: Online micro-cluster maintenance (learning) For a new point p : Try to merge p into either the nearest p-micro-cluster (potential), o-micro-cluster (outlier), or create a new o-micro-cluster and insert it into the outlier buffer. For each T_p iterations, consider the weights of all potential and outlier micro-clusters. If their weights are smaller than a certain threshold (different for each type of micro-clusters), the micro-cluster is deleted. Offline generation of clusters on-demand (clustering) A variant of the DBSCAN algorithm 2 is used, such that all density-connected p-micro-clusters determine the final clusters. Parameters \u00b6 decaying_factor ( float ) \u2013 defaults to 0.25 Parameter that controls the importance of historical data to current cluster. Note that decaying_factor has to be different from 0 . core_weight_threshold ( float ) \u2013 defaults to 5 Parameter to determine the threshold of outlier relative to core micro-clusters. Note that core_weight_threshold * tolerance_factor has to be greater than 1 or less than 0 . tolerance_factor ( float ) \u2013 defaults to 0.5 Parameter to determine the threshold of outliers relative to core micro-cluster. In a normal setting, this parameter is usuallly set within the range [0,1] . Once again, note that core_weight_threshold * tolerance_factor has to be greater than 1 or less than 0 . radius ( float ) \u2013 defaults to 2 This parameter is passed onto the DBSCAN offline algorithm as the \\(\\epsilon\\) parameter when a clustering request arrives. Attributes \u00b6 n_clusters Number of clusters generated by the algorithm. clusters A set of final clusters of type MicroCluster , which means that these cluster include all the required information, including number of points, creation time, weight, (weighted) linear sum, (weighted) square sum, center and radius. p_micro_clusters The p micro-clusters that are generated by the algorithm. When a generating cluster request arrives, these p-micro-clusters will go through a variant of DBSCAN algorithm to determine the final clusters. o_micro_clusters The outlier buffer, separating the processing of the potential core-micro-cluster and outlier-micro-clusters. Examples \u00b6 The following example uses the default parameters of the algorithm to test its functionality. It can easily be seen that the set of evolving points X are designed so that there can be a clear picture drawn on how the clusters can be generated. >>> from river import cluster >>> from river import stream >>> X = [ ... [ - 1 , - 0.5 ], [ - 1 , - 0.625 ], [ - 1 , - 0.75 ], [ - 1 , - 1 ], [ - 1 , - 1.125 ], [ - 1 , - 1.25 ], ... [ - 1.5 , - 0.5 ], [ - 1.5 , - 0.625 ], [ - 1.5 , - 0.75 ], [ - 1.5 , - 1 ], [ - 1.5 , - 1.125 ], [ - 1.5 , - 1.25 ], ... [ 1 , 1.5 ], [ 1 , 1.75 ], [ 1 , 2 ], [ 4 , 1.25 ], [ 4 , 1.5 ], [ 4 , 2.25 ], ... [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ], [ 4 , 3.75 ], [ 4 , 4 ], ... ] >>> denstream = cluster . DenStream ( decaying_factor = 0.01 , ... core_weight_threshold = 1.01 , ... tolerance_factor = 1.0005 , ... radius = 0.5 ) >>> for x , _ in stream . iter_array ( X ): ... denstream = denstream . learn_one ( x ) >>> denstream . predict_one ({ 0 : - 1 , 1 : - 2 }) 0 >>> denstream . predict_one ({ 0 : 5 , 1 : 4 }) 1 >>> denstream . n_clusters 2 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number. References \u00b6 Feng et al (2006, pp 328-339). Density-Based Clustering over an Evolving Data Stream with Noise. In Proceedings of the Sixth SIAM International Conference on Data Mining, April 20\u201322, 2006, Bethesda, MD, USA. \u21a9 Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In KDD-96 Proceedings, AAAI. \u21a9","title":"DenStream"},{"location":"api/cluster/DenStream/#denstream","text":"DenStream DenStream 1 is a clustering algorithm for evolving data streams. DenStream can discover clusters with arbitrary shape and is robust against noise (outliers). \"Dense\" micro-clusters (named core-micro-clusters) summarise the clusters of arbitrary shape. A pruning strategy based on the concepts of potential and outlier micro-clusters guarantees the precision of the weights of the micro-clusters with limited memory. The algorithm is divided into two parts: Online micro-cluster maintenance (learning) For a new point p : Try to merge p into either the nearest p-micro-cluster (potential), o-micro-cluster (outlier), or create a new o-micro-cluster and insert it into the outlier buffer. For each T_p iterations, consider the weights of all potential and outlier micro-clusters. If their weights are smaller than a certain threshold (different for each type of micro-clusters), the micro-cluster is deleted. Offline generation of clusters on-demand (clustering) A variant of the DBSCAN algorithm 2 is used, such that all density-connected p-micro-clusters determine the final clusters.","title":"DenStream"},{"location":"api/cluster/DenStream/#parameters","text":"decaying_factor ( float ) \u2013 defaults to 0.25 Parameter that controls the importance of historical data to current cluster. Note that decaying_factor has to be different from 0 . core_weight_threshold ( float ) \u2013 defaults to 5 Parameter to determine the threshold of outlier relative to core micro-clusters. Note that core_weight_threshold * tolerance_factor has to be greater than 1 or less than 0 . tolerance_factor ( float ) \u2013 defaults to 0.5 Parameter to determine the threshold of outliers relative to core micro-cluster. In a normal setting, this parameter is usuallly set within the range [0,1] . Once again, note that core_weight_threshold * tolerance_factor has to be greater than 1 or less than 0 . radius ( float ) \u2013 defaults to 2 This parameter is passed onto the DBSCAN offline algorithm as the \\(\\epsilon\\) parameter when a clustering request arrives.","title":"Parameters"},{"location":"api/cluster/DenStream/#attributes","text":"n_clusters Number of clusters generated by the algorithm. clusters A set of final clusters of type MicroCluster , which means that these cluster include all the required information, including number of points, creation time, weight, (weighted) linear sum, (weighted) square sum, center and radius. p_micro_clusters The p micro-clusters that are generated by the algorithm. When a generating cluster request arrives, these p-micro-clusters will go through a variant of DBSCAN algorithm to determine the final clusters. o_micro_clusters The outlier buffer, separating the processing of the potential core-micro-cluster and outlier-micro-clusters.","title":"Attributes"},{"location":"api/cluster/DenStream/#examples","text":"The following example uses the default parameters of the algorithm to test its functionality. It can easily be seen that the set of evolving points X are designed so that there can be a clear picture drawn on how the clusters can be generated. >>> from river import cluster >>> from river import stream >>> X = [ ... [ - 1 , - 0.5 ], [ - 1 , - 0.625 ], [ - 1 , - 0.75 ], [ - 1 , - 1 ], [ - 1 , - 1.125 ], [ - 1 , - 1.25 ], ... [ - 1.5 , - 0.5 ], [ - 1.5 , - 0.625 ], [ - 1.5 , - 0.75 ], [ - 1.5 , - 1 ], [ - 1.5 , - 1.125 ], [ - 1.5 , - 1.25 ], ... [ 1 , 1.5 ], [ 1 , 1.75 ], [ 1 , 2 ], [ 4 , 1.25 ], [ 4 , 1.5 ], [ 4 , 2.25 ], ... [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ], [ 4 , 3.75 ], [ 4 , 4 ], ... ] >>> denstream = cluster . DenStream ( decaying_factor = 0.01 , ... core_weight_threshold = 1.01 , ... tolerance_factor = 1.0005 , ... radius = 0.5 ) >>> for x , _ in stream . iter_array ( X ): ... denstream = denstream . learn_one ( x ) >>> denstream . predict_one ({ 0 : - 1 , 1 : - 2 }) 0 >>> denstream . predict_one ({ 0 : 5 , 1 : 4 }) 1 >>> denstream . n_clusters 2","title":"Examples"},{"location":"api/cluster/DenStream/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number.","title":"Methods"},{"location":"api/cluster/DenStream/#references","text":"Feng et al (2006, pp 328-339). Density-Based Clustering over an Evolving Data Stream with Noise. In Proceedings of the Sixth SIAM International Conference on Data Mining, April 20\u201322, 2006, Bethesda, MD, USA. \u21a9 Ester et al (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In KDD-96 Proceedings, AAAI. \u21a9","title":"References"},{"location":"api/cluster/KMeans/","text":"KMeans \u00b6 Incremental k-means. The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in assigning all the data points to a set of cluster centers and then moving the centers accordingly. This requires multiple passes over the data and thus isn't applicable in a streaming setting. In this implementation we start by finding the cluster that is closest to the current observation. We then move the cluster's central position towards the new observation. The halflife parameter determines by how much to move the cluster toward the new observation. You will get better results if you scale your data appropriately. Parameters \u00b6 n_clusters \u2013 defaults to 5 Maximum number of clusters to assign. halflife \u2013 defaults to 0.5 Amount by which to move the cluster centers, a reasonable value if between 0 and 1. mu \u2013 defaults to 0 Mean of the normal distribution used to instantiate cluster positions. sigma \u2013 defaults to 1 Standard deviation of the normal distribution used to instantiate cluster positions. p \u2013 defaults to 2 Power parameter for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. seed ( int ) \u2013 defaults to None Random seed used for generating initial centroid positions. Attributes \u00b6 centers ( dict ) Central positions of each cluster. Examples \u00b6 In the following example the cluster assignments are exactly the same as when using sklearn 's batch implementation. However changing the halflife parameter will produce different outputs. >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 2 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> for i , ( x , _ ) in enumerate ( stream . iter_array ( X )): ... k_means = k_means . learn_one ( x ) ... print ( f ' { X [ i ] } is assigned to cluster { k_means . predict_one ( x ) } ' ) [ 1 , 2 ] is assigned to cluster 1 [ 1 , 4 ] is assigned to cluster 1 [ 1 , 0 ] is assigned to cluster 0 [ 4 , 2 ] is assigned to cluster 0 [ 4 , 4 ] is assigned to cluster 0 [ 4 , 0 ] is assigned to cluster 0 >>> k_means . predict_one ({ 0 : 0 , 1 : 0 }) 1 >>> k_means . predict_one ({ 0 : 4 , 1 : 4 }) 0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) Returns Clusterer : self learn_predict_one Equivalent to k_means.learn_one(x).predict_one(x) , but faster. Parameters x predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number. References \u00b6 Sequential k-Means Clustering \u21a9 Sculley, D., 2010, April. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web (pp. 1177-1178 \u21a9","title":"KMeans"},{"location":"api/cluster/KMeans/#kmeans","text":"Incremental k-means. The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in assigning all the data points to a set of cluster centers and then moving the centers accordingly. This requires multiple passes over the data and thus isn't applicable in a streaming setting. In this implementation we start by finding the cluster that is closest to the current observation. We then move the cluster's central position towards the new observation. The halflife parameter determines by how much to move the cluster toward the new observation. You will get better results if you scale your data appropriately.","title":"KMeans"},{"location":"api/cluster/KMeans/#parameters","text":"n_clusters \u2013 defaults to 5 Maximum number of clusters to assign. halflife \u2013 defaults to 0.5 Amount by which to move the cluster centers, a reasonable value if between 0 and 1. mu \u2013 defaults to 0 Mean of the normal distribution used to instantiate cluster positions. sigma \u2013 defaults to 1 Standard deviation of the normal distribution used to instantiate cluster positions. p \u2013 defaults to 2 Power parameter for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. seed ( int ) \u2013 defaults to None Random seed used for generating initial centroid positions.","title":"Parameters"},{"location":"api/cluster/KMeans/#attributes","text":"centers ( dict ) Central positions of each cluster.","title":"Attributes"},{"location":"api/cluster/KMeans/#examples","text":"In the following example the cluster assignments are exactly the same as when using sklearn 's batch implementation. However changing the halflife parameter will produce different outputs. >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 2 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> for i , ( x , _ ) in enumerate ( stream . iter_array ( X )): ... k_means = k_means . learn_one ( x ) ... print ( f ' { X [ i ] } is assigned to cluster { k_means . predict_one ( x ) } ' ) [ 1 , 2 ] is assigned to cluster 1 [ 1 , 4 ] is assigned to cluster 1 [ 1 , 0 ] is assigned to cluster 0 [ 4 , 2 ] is assigned to cluster 0 [ 4 , 4 ] is assigned to cluster 0 [ 4 , 0 ] is assigned to cluster 0 >>> k_means . predict_one ({ 0 : 0 , 1 : 0 }) 1 >>> k_means . predict_one ({ 0 : 4 , 1 : 4 }) 0","title":"Examples"},{"location":"api/cluster/KMeans/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) Returns Clusterer : self learn_predict_one Equivalent to k_means.learn_one(x).predict_one(x) , but faster. Parameters x predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) Returns int : A cluster number.","title":"Methods"},{"location":"api/cluster/KMeans/#references","text":"Sequential k-Means Clustering \u21a9 Sculley, D., 2010, April. Web-scale k-means clustering. In Proceedings of the 19th international conference on World wide web (pp. 1177-1178 \u21a9","title":"References"},{"location":"api/cluster/STREAMKMeans/","text":"STREAMKMeans \u00b6 STREAMKMeans STREAMKMeans is an alternative version of the original algorithm STREAMLSEARCH proposed by O'Callaghan et al. 1 by replacing the k-Medians using LSEARCH by the classical KMeans algorithm. However, instead of using the traditional KMeans that requires a total reclustering after each time the temporary chunk of data points is full, the implementation of this algorithm in River uses the increamental KMeans . This allows the algorithm to update KMeans without the need of re-initialization, saving a substantial amount of computing resources. The algorithm is constructed as follows. To begin, the algorithm will be initialized with an incremental KMeans algorithm with the same number of centers as required. For a new point p : If the size of chunk is less than the maximum size allowed, add the new point to the temporary chunk. When the size of chunk reaches the maximum value size allowed A new incremental KMeans algorithm will be initiated. This algorithm will run through all points in the temporary chunk. The centers of this new algorithm will be passed through the originally initialized KMeans to update the centers of the algorithm All points will be deleted from the temporary chunk to continue adding new points later. When a prediction request arrives, the centers of the algorithm will be exactly the same as the centers of the original KMeans at the time of retrieval. Parameters \u00b6 chunk_size \u2013 defaults to 10 Maximum size allowed for the temporary data chunk. n_clusters \u2013 defaults to 2 Number of clusters generated by the algorithm. kwargs Other parameters passed to the incremental kmeans at cluster.KMeans . Attributes \u00b6 centers Cluster centers generated from running the incremental KMeans algorithm through centers of each chunk. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 0.5 ], [ 1 , 0.625 ], [ 1 , 0.75 ], [ 1 , 1.125 ], [ 1 , 1.5 ], [ 1 , 1.75 ], ... [ 4 , 1.5 ], [ 4 , 2.25 ], [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ] ... ] >>> streamkmeans = cluster . STREAMKMeans ( chunk_size = 3 , n_clusters = 2 , halflife = 0.5 , sigma = 1.5 , seed = 0 ) >>> for x , _ in stream . iter_array ( X ): ... streamkmeans = streamkmeans . learn_one ( x ) >>> streamkmeans . predict_one ({ 0 : 1 , 1 : 0 }) 1 >>> streamkmeans . predict_one ({ 0 : 5 , 1 : 2 }) 0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number. References \u00b6 O'Callaghan et al. (2002). Streaming-data algorithms for high-quality clustering. In Proceedings 18th International Conference on Data Engineering, Feb 26 - March 1, San Jose, CA, USA. DOI: 10.1109/ICDE.2002.994785. \u21a9","title":"STREAMKMeans"},{"location":"api/cluster/STREAMKMeans/#streamkmeans","text":"STREAMKMeans STREAMKMeans is an alternative version of the original algorithm STREAMLSEARCH proposed by O'Callaghan et al. 1 by replacing the k-Medians using LSEARCH by the classical KMeans algorithm. However, instead of using the traditional KMeans that requires a total reclustering after each time the temporary chunk of data points is full, the implementation of this algorithm in River uses the increamental KMeans . This allows the algorithm to update KMeans without the need of re-initialization, saving a substantial amount of computing resources. The algorithm is constructed as follows. To begin, the algorithm will be initialized with an incremental KMeans algorithm with the same number of centers as required. For a new point p : If the size of chunk is less than the maximum size allowed, add the new point to the temporary chunk. When the size of chunk reaches the maximum value size allowed A new incremental KMeans algorithm will be initiated. This algorithm will run through all points in the temporary chunk. The centers of this new algorithm will be passed through the originally initialized KMeans to update the centers of the algorithm All points will be deleted from the temporary chunk to continue adding new points later. When a prediction request arrives, the centers of the algorithm will be exactly the same as the centers of the original KMeans at the time of retrieval.","title":"STREAMKMeans"},{"location":"api/cluster/STREAMKMeans/#parameters","text":"chunk_size \u2013 defaults to 10 Maximum size allowed for the temporary data chunk. n_clusters \u2013 defaults to 2 Number of clusters generated by the algorithm. kwargs Other parameters passed to the incremental kmeans at cluster.KMeans .","title":"Parameters"},{"location":"api/cluster/STREAMKMeans/#attributes","text":"centers Cluster centers generated from running the incremental KMeans algorithm through centers of each chunk.","title":"Attributes"},{"location":"api/cluster/STREAMKMeans/#examples","text":">>> from river import cluster >>> from river import stream >>> X = [ ... [ 1 , 0.5 ], [ 1 , 0.625 ], [ 1 , 0.75 ], [ 1 , 1.125 ], [ 1 , 1.5 ], [ 1 , 1.75 ], ... [ 4 , 1.5 ], [ 4 , 2.25 ], [ 4 , 2.5 ], [ 4 , 3 ], [ 4 , 3.25 ], [ 4 , 3.5 ] ... ] >>> streamkmeans = cluster . STREAMKMeans ( chunk_size = 3 , n_clusters = 2 , halflife = 0.5 , sigma = 1.5 , seed = 0 ) >>> for x , _ in stream . iter_array ( X ): ... streamkmeans = streamkmeans . learn_one ( x ) >>> streamkmeans . predict_one ({ 0 : 1 , 1 : 0 }) 1 >>> streamkmeans . predict_one ({ 0 : 5 , 1 : 2 }) 0","title":"Examples"},{"location":"api/cluster/STREAMKMeans/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x . Parameters x ( dict ) sample_weight ( int ) \u2013 defaults to None Returns Clusterer : self predict_one Predicts the cluster number for a set of features x . Parameters x ( dict ) sample_weight \u2013 defaults to None Returns int : A cluster number.","title":"Methods"},{"location":"api/cluster/STREAMKMeans/#references","text":"O'Callaghan et al. (2002). Streaming-data algorithms for high-quality clustering. In Proceedings 18th International Conference on Data Engineering, Feb 26 - March 1, San Jose, CA, USA. DOI: 10.1109/ICDE.2002.994785. \u21a9","title":"References"},{"location":"api/compat/River2SKLClassifier/","text":"River2SKLClassifier \u00b6 Compatibility layer from River to scikit-learn for classification. Parameters \u00b6 river_estimator ( base.Classifier ) Methods \u00b6 fit Fits to an entire dataset contained in memory. Parameters X y Returns self get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y classes \u2013 defaults to None Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns Predicted target values for each row of X . predict_proba Predicts the target probability of an entire dataset contained in memory. Parameters X Returns Predicted target values for each row of X . score Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X y sample_weight \u2013 defaults to None Returns float set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"River2SKLClassifier"},{"location":"api/compat/River2SKLClassifier/#river2sklclassifier","text":"Compatibility layer from River to scikit-learn for classification.","title":"River2SKLClassifier"},{"location":"api/compat/River2SKLClassifier/#parameters","text":"river_estimator ( base.Classifier )","title":"Parameters"},{"location":"api/compat/River2SKLClassifier/#methods","text":"fit Fits to an entire dataset contained in memory. Parameters X y Returns self get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y classes \u2013 defaults to None Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns Predicted target values for each row of X . predict_proba Predicts the target probability of an entire dataset contained in memory. Parameters X Returns Predicted target values for each row of X . score Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X y sample_weight \u2013 defaults to None Returns float set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"Methods"},{"location":"api/compat/River2SKLClusterer/","text":"River2SKLClusterer \u00b6 Compatibility layer from River to scikit-learn for clustering. Parameters \u00b6 river_estimator ( base.Clusterer ) Methods \u00b6 fit Fits to an entire dataset contained in memory. Parameters X y \u2013 defaults to None Returns self fit_predict Perform clustering on X and returns cluster labels. Parameters X y \u2013 defaults to None Returns ndarray of shape (n_samples,), dtype=np.int64 get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns Transformed output. set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"River2SKLClusterer"},{"location":"api/compat/River2SKLClusterer/#river2sklclusterer","text":"Compatibility layer from River to scikit-learn for clustering.","title":"River2SKLClusterer"},{"location":"api/compat/River2SKLClusterer/#parameters","text":"river_estimator ( base.Clusterer )","title":"Parameters"},{"location":"api/compat/River2SKLClusterer/#methods","text":"fit Fits to an entire dataset contained in memory. Parameters X y \u2013 defaults to None Returns self fit_predict Perform clustering on X and returns cluster labels. Parameters X y \u2013 defaults to None Returns ndarray of shape (n_samples,), dtype=np.int64 get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns Transformed output. set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"Methods"},{"location":"api/compat/River2SKLRegressor/","text":"River2SKLRegressor \u00b6 Compatibility layer from River to scikit-learn for regression. Parameters \u00b6 river_estimator ( base.Regressor ) Methods \u00b6 fit Fits to an entire dataset contained in memory. Parameters X y Returns self get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns ndarray : Predicted target values for each row of X . score Return the coefficient of determination :math: R^2 of the prediction. The coefficient :math: R^2 is defined as :math: (1 - \\frac{u}{v}) , where :math: u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X y sample_weight \u2013 defaults to None Returns float set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"River2SKLRegressor"},{"location":"api/compat/River2SKLRegressor/#river2sklregressor","text":"Compatibility layer from River to scikit-learn for regression.","title":"River2SKLRegressor"},{"location":"api/compat/River2SKLRegressor/#parameters","text":"river_estimator ( base.Regressor )","title":"Parameters"},{"location":"api/compat/River2SKLRegressor/#methods","text":"fit Fits to an entire dataset contained in memory. Parameters X y Returns self get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y Returns self predict Predicts the target of an entire dataset contained in memory. Parameters X Returns ndarray : Predicted target values for each row of X . score Return the coefficient of determination :math: R^2 of the prediction. The coefficient :math: R^2 is defined as :math: (1 - \\frac{u}{v}) , where :math: u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X y sample_weight \u2013 defaults to None Returns float set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance","title":"Methods"},{"location":"api/compat/River2SKLTransformer/","text":"River2SKLTransformer \u00b6 Compatibility layer from River to scikit-learn for transformation. Parameters \u00b6 river_estimator ( base.Transformer ) Methods \u00b6 fit Fits to an entire dataset contained in memory. Parameters X y \u2013 defaults to None Returns self fit_transform Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters X y \u2013 defaults to None fit_params Returns ndarray array of shape (n_samples, n_features_new) get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y \u2013 defaults to None Returns self set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance transform Predicts the target of an entire dataset contained in memory. Parameters X Returns Transformed output.","title":"River2SKLTransformer"},{"location":"api/compat/River2SKLTransformer/#river2skltransformer","text":"Compatibility layer from River to scikit-learn for transformation.","title":"River2SKLTransformer"},{"location":"api/compat/River2SKLTransformer/#parameters","text":"river_estimator ( base.Transformer )","title":"Parameters"},{"location":"api/compat/River2SKLTransformer/#methods","text":"fit Fits to an entire dataset contained in memory. Parameters X y \u2013 defaults to None Returns self fit_transform Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters X y \u2013 defaults to None fit_params Returns ndarray array of shape (n_samples, n_features_new) get_params Get parameters for this estimator. Parameters deep \u2013 defaults to True Returns dict partial_fit Fits incrementally on a portion of a dataset. Parameters X y \u2013 defaults to None Returns self set_params Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters params Returns estimator instance transform Predicts the target of an entire dataset contained in memory. Parameters X Returns Transformed output.","title":"Methods"},{"location":"api/compat/SKL2RiverClassifier/","text":"SKL2RiverClassifier \u00b6 Compatibility layer from scikit-learn to River for classification. Parameters \u00b6 estimator ( sklearn.base.ClassifierMixin ) A scikit-learn regressor which has a partial_fit method. classes ( list ) Examples \u00b6 >>> from river import compat >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import stream >>> from sklearn import linear_model >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_breast_cancer (), ... shuffle = True , ... seed = 42 ... ) >>> model = preprocessing . StandardScaler () >>> model |= compat . convert_sklearn_to_river ( ... estimator = linear_model . SGDClassifier ( ... loss = 'log' , ... eta0 = 0.01 , ... learning_rate = 'constant' ... ), ... classes = [ False , True ] ... ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.199554 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X Returns Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X Returns DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"SKL2RiverClassifier"},{"location":"api/compat/SKL2RiverClassifier/#skl2riverclassifier","text":"Compatibility layer from scikit-learn to River for classification.","title":"SKL2RiverClassifier"},{"location":"api/compat/SKL2RiverClassifier/#parameters","text":"estimator ( sklearn.base.ClassifierMixin ) A scikit-learn regressor which has a partial_fit method. classes ( list )","title":"Parameters"},{"location":"api/compat/SKL2RiverClassifier/#examples","text":">>> from river import compat >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import stream >>> from sklearn import linear_model >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_breast_cancer (), ... shuffle = True , ... seed = 42 ... ) >>> model = preprocessing . StandardScaler () >>> model |= compat . convert_sklearn_to_river ( ... estimator = linear_model . SGDClassifier ( ... loss = 'log' , ... eta0 = 0.01 , ... learning_rate = 'constant' ... ), ... classes = [ False , True ] ... ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.199554","title":"Examples"},{"location":"api/compat/SKL2RiverClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X Returns Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X Returns DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/compat/SKL2RiverRegressor/","text":"SKL2RiverRegressor \u00b6 Compatibility layer from scikit-learn to River for regression. Parameters \u00b6 estimator ( sklearn.base.BaseEstimator ) A scikit-learn transformer which has a partial_fit method. Examples \u00b6 >>> from river import compat >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import stream >>> from sklearn import linear_model >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_boston (), ... shuffle = True , ... seed = 42 ... ) >>> scaler = preprocessing . StandardScaler () >>> sgd_reg = compat . convert_sklearn_to_river ( linear_model . SGDRegressor ()) >>> model = scaler | sgd_reg >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 11.004415 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self predict_many predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"SKL2RiverRegressor"},{"location":"api/compat/SKL2RiverRegressor/#skl2riverregressor","text":"Compatibility layer from scikit-learn to River for regression.","title":"SKL2RiverRegressor"},{"location":"api/compat/SKL2RiverRegressor/#parameters","text":"estimator ( sklearn.base.BaseEstimator ) A scikit-learn transformer which has a partial_fit method.","title":"Parameters"},{"location":"api/compat/SKL2RiverRegressor/#examples","text":">>> from river import compat >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import stream >>> from sklearn import linear_model >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_boston (), ... shuffle = True , ... seed = 42 ... ) >>> scaler = preprocessing . StandardScaler () >>> sgd_reg = compat . convert_sklearn_to_river ( linear_model . SGDRegressor ()) >>> model = scaler | sgd_reg >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 11.004415","title":"Examples"},{"location":"api/compat/SKL2RiverRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self predict_many predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/compat/convert-river-to-sklearn/","text":"convert_river_to_sklearn \u00b6 Wraps a river estimator to make it compatible with scikit-learn. Parameters \u00b6 estimator ( base.Estimator )","title":"convert_river_to_sklearn"},{"location":"api/compat/convert-river-to-sklearn/#convert_river_to_sklearn","text":"Wraps a river estimator to make it compatible with scikit-learn.","title":"convert_river_to_sklearn"},{"location":"api/compat/convert-river-to-sklearn/#parameters","text":"estimator ( base.Estimator )","title":"Parameters"},{"location":"api/compat/convert-sklearn-to-river/","text":"convert_sklearn_to_river \u00b6 Wraps a scikit-learn estimator to make it compatible with river. Parameters \u00b6 estimator ( sklearn.base.BaseEstimator ) classes ( list ) \u2013 defaults to None Class names necessary for classifiers.","title":"convert_sklearn_to_river"},{"location":"api/compat/convert-sklearn-to-river/#convert_sklearn_to_river","text":"Wraps a scikit-learn estimator to make it compatible with river.","title":"convert_sklearn_to_river"},{"location":"api/compat/convert-sklearn-to-river/#parameters","text":"estimator ( sklearn.base.BaseEstimator ) classes ( list ) \u2013 defaults to None Class names necessary for classifiers.","title":"Parameters"},{"location":"api/compose/Discard/","text":"Discard \u00b6 Removes features according to a blacklist. This can be used in a pipeline when you want to remove certain features. The transform_one method is pure, and therefore returns a fresh new dictionary instead of removing the specified keys from the input. Parameters \u00b6 blacklist ( Tuple[Hashable] ) Key(s) to discard. Examples \u00b6 >>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 , 'c' : 13 } >>> compose . Discard ( 'a' , 'b' ) . transform_one ( x ) { 'c' : 13 } You can chain a discarder with any estimator in order to apply said estimator to the desired features. >>> from river import feature_extraction as fx >>> x = { 'sales' : 10 , 'shop' : 'Ikea' , 'country' : 'Sweden' } >>> pipeline = ( ... compose . Discard ( 'shop' , 'country' ) | ... fx . PolynomialExtender () ... ) >>> pipeline . transform_one ( x ) { 'sales' : 10 , 'sales*sales' : 100 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Discard"},{"location":"api/compose/Discard/#discard","text":"Removes features according to a blacklist. This can be used in a pipeline when you want to remove certain features. The transform_one method is pure, and therefore returns a fresh new dictionary instead of removing the specified keys from the input.","title":"Discard"},{"location":"api/compose/Discard/#parameters","text":"blacklist ( Tuple[Hashable] ) Key(s) to discard.","title":"Parameters"},{"location":"api/compose/Discard/#examples","text":">>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 , 'c' : 13 } >>> compose . Discard ( 'a' , 'b' ) . transform_one ( x ) { 'c' : 13 } You can chain a discarder with any estimator in order to apply said estimator to the desired features. >>> from river import feature_extraction as fx >>> x = { 'sales' : 10 , 'shop' : 'Ikea' , 'country' : 'Sweden' } >>> pipeline = ( ... compose . Discard ( 'shop' , 'country' ) | ... fx . PolynomialExtender () ... ) >>> pipeline . transform_one ( x ) { 'sales' : 10 , 'sales*sales' : 100 }","title":"Examples"},{"location":"api/compose/Discard/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/FuncTransformer/","text":"FuncTransformer \u00b6 Wraps a function to make it usable in a pipeline. There is often a need to apply an arbitrary transformation to a set of features. For instance, this could involve parsing a date and then extracting the hour from said date. If you're processing a stream of data, then you can do this yourself by calling the necessary code at your leisure. On the other hand, if you want to do this as part of a pipeline, then you need to follow a simple convention. To use a function as part of a pipeline, take as input a dict of features and output a dict . Once you have initialized this class with your function, then you can use it like you would use any other (unsupervised) transformer. It is up to you if you want your function to be pure or not. By pure we refer to a function that doesn't modify its input. However, we recommend writing pure functions because this reduces the chances of inserting bugs into your pipeline. Parameters \u00b6 func ( Callable[[dict], dict] ) A function that takes as input a dict and outputs a dict . Examples \u00b6 >>> from pprint import pprint >>> import datetime as dt >>> from river import compose >>> x = { 'date' : '2019-02-14' } >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... x [ 'is_weekend' ] = date . day in ( 5 , 6 ) ... x [ 'hour' ] = date . hour ... return x >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'date' : '2019-02-14' , 'hour' : 0 , 'is_weekend' : False } The above example is not pure because it modifies the input. The following example is pure and produces the same output: >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... return { 'is_weekend' : date . day in ( 5 , 6 ), 'hour' : date . hour } >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'hour' : 0 , 'is_weekend' : False } The previous example doesn't include the date feature because it returns a new dict . However, a common usecase is to add a feature to an existing set of features. You can do this in a pure way by unpacking the input dict into the output dict : >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... return { 'is_weekend' : date . day in ( 5 , 6 ), 'hour' : date . hour , ** x } >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'date' : '2019-02-14' , 'hour' : 0 , 'is_weekend' : False } You can add FuncTransformer to a pipeline just like you would with any other transformer. >>> from river import naive_bayes >>> pipeline = compose . FuncTransformer ( parse_date ) | naive_bayes . MultinomialNB () >>> pipeline Pipeline ( FuncTransformer ( func = \"parse_date\" ), MultinomialNB ( alpha = 1. ) ) If you provide a function with wrapping it, then the pipeline will do it for you: >>> pipeline = parse_date | naive_bayes . MultinomialNB () Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"FuncTransformer"},{"location":"api/compose/FuncTransformer/#functransformer","text":"Wraps a function to make it usable in a pipeline. There is often a need to apply an arbitrary transformation to a set of features. For instance, this could involve parsing a date and then extracting the hour from said date. If you're processing a stream of data, then you can do this yourself by calling the necessary code at your leisure. On the other hand, if you want to do this as part of a pipeline, then you need to follow a simple convention. To use a function as part of a pipeline, take as input a dict of features and output a dict . Once you have initialized this class with your function, then you can use it like you would use any other (unsupervised) transformer. It is up to you if you want your function to be pure or not. By pure we refer to a function that doesn't modify its input. However, we recommend writing pure functions because this reduces the chances of inserting bugs into your pipeline.","title":"FuncTransformer"},{"location":"api/compose/FuncTransformer/#parameters","text":"func ( Callable[[dict], dict] ) A function that takes as input a dict and outputs a dict .","title":"Parameters"},{"location":"api/compose/FuncTransformer/#examples","text":">>> from pprint import pprint >>> import datetime as dt >>> from river import compose >>> x = { 'date' : '2019-02-14' } >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... x [ 'is_weekend' ] = date . day in ( 5 , 6 ) ... x [ 'hour' ] = date . hour ... return x >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'date' : '2019-02-14' , 'hour' : 0 , 'is_weekend' : False } The above example is not pure because it modifies the input. The following example is pure and produces the same output: >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... return { 'is_weekend' : date . day in ( 5 , 6 ), 'hour' : date . hour } >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'hour' : 0 , 'is_weekend' : False } The previous example doesn't include the date feature because it returns a new dict . However, a common usecase is to add a feature to an existing set of features. You can do this in a pure way by unpacking the input dict into the output dict : >>> def parse_date ( x ): ... date = dt . datetime . strptime ( x [ 'date' ], '%Y-%m- %d ' ) ... return { 'is_weekend' : date . day in ( 5 , 6 ), 'hour' : date . hour , ** x } >>> t = compose . FuncTransformer ( parse_date ) >>> pprint ( t . transform_one ( x )) { 'date' : '2019-02-14' , 'hour' : 0 , 'is_weekend' : False } You can add FuncTransformer to a pipeline just like you would with any other transformer. >>> from river import naive_bayes >>> pipeline = compose . FuncTransformer ( parse_date ) | naive_bayes . MultinomialNB () >>> pipeline Pipeline ( FuncTransformer ( func = \"parse_date\" ), MultinomialNB ( alpha = 1. ) ) If you provide a function with wrapping it, then the pipeline will do it for you: >>> pipeline = parse_date | naive_bayes . MultinomialNB ()","title":"Examples"},{"location":"api/compose/FuncTransformer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/Grouper/","text":"Grouper \u00b6 Applies a transformer within different groups. This transformer allows you to split your data into groups and apply a transformer within each group. This happens in a streaming manner, which means that the groups are discovered online. A separate copy of the provided transformer is made whenever a new group appears. The groups are defined according to one or more keys. Parameters \u00b6 transformer ( base.Transformer ) by ( Union[Hashable, List[Hashable]] ) The field on which to group the data. This can either by a single value, or a list of values. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Grouper"},{"location":"api/compose/Grouper/#grouper","text":"Applies a transformer within different groups. This transformer allows you to split your data into groups and apply a transformer within each group. This happens in a streaming manner, which means that the groups are discovered online. A separate copy of the provided transformer is made whenever a new group appears. The groups are defined according to one or more keys.","title":"Grouper"},{"location":"api/compose/Grouper/#parameters","text":"transformer ( base.Transformer ) by ( Union[Hashable, List[Hashable]] ) The field on which to group the data. This can either by a single value, or a list of values.","title":"Parameters"},{"location":"api/compose/Grouper/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/Pipeline/","text":"Pipeline \u00b6 A pipeline of estimators. Pipelines allow you to chain different steps into a sequence. Typically, when doing supervised learning, a pipeline contains one ore more transformation steps, whilst it's is a regressor or a classifier. It is highly recommended to use pipelines with river . Indeed, in an online learning setting, it is very practical to have a model defined as a single object. Take a look at the user guide for further information and practical examples. One special thing to take notice to is the way transformers are handled. In a typical scenario, it is usual to predict something for a sample and wait for the ground truth to arrive. In such a case, the features are seen before the ground truth arrives. Therefore, the unsupervised parts of the pipeline are updated when predict_one and predict_proba_one are called. Usually the unsupervised parts of the pipeline are all the steps that precede the final step, which is a supervised model. However, some transformers are supervised and are therefore obtained during calls to learn_one . Parameters \u00b6 steps Ideally, a list of (name, estimator) tuples. A name is automatically inferred if none is provided. Examples \u00b6 The recommended way to declare a pipeline is to use the | operator. The latter allows you to chain estimators in a very terse manner: >>> from river import linear_model >>> from river import preprocessing >>> scaler = preprocessing . StandardScaler () >>> log_reg = linear_model . LinearRegression () >>> model = scaler | log_reg This results in a pipeline that stores each step inside a dictionary. >>> model Pipeline ( StandardScaler (), LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.01 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) ) You can access parts of a pipeline in the same manner as a dictionary: >>> model [ 'LinearRegression' ] LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.01 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) Note that you can also declare a pipeline by using the compose.Pipeline constructor method, which is slightly more verbose: >>> from river import compose >>> model = compose . Pipeline ( scaler , log_reg ) By using a compose.TransformerUnion , you can define complex pipelines that apply different steps to different parts of the data. For instance, we can extract word counts from text data, and extract polynomial features from numeric data. >>> from river import feature_extraction as fx >>> tfidf = fx . TFIDF ( 'text' ) >>> counts = fx . BagOfWords ( 'text' ) >>> text_part = compose . Select ( 'text' ) | ( tfidf + counts ) >>> num_part = compose . Select ( 'a' , 'b' ) | fx . PolynomialExtender () >>> model = text_part + num_part >>> model |= preprocessing . StandardScaler () >>> model |= linear_model . LinearRegression () The following shows an example of using debug_one to visualize how the information flows and changes throughout the pipeline. >>> from river import compose >>> from river import naive_bayes >>> dataset = [ ... ( 'A positive comment' , True ), ... ( 'A negative comment' , False ), ... ( 'A happy comment' , True ), ... ( 'A lovely comment' , True ), ... ( 'A harsh comment' , False ) ... ] >>> tfidf = fx . TFIDF () | compose . Renamer ( prefix = 'tfidf_' ) >>> counts = fx . BagOfWords () | compose . Renamer ( prefix = 'count_' ) >>> mnb = naive_bayes . MultinomialNB () >>> model = ( tfidf + counts ) | mnb >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> x = dataset [ 0 ][ 0 ] >>> report = model . debug_one ( dataset [ 0 ][ 0 ]) >>> print ( report ) 0. Input -------- A positive comment < BLANKLINE > 1. Transformer union -------------------- 1.0 TFIDF | Renamer ------------------- tfidf_comment : 0.47606 ( float ) tfidf_positive : 0.87942 ( float ) < BLANKLINE > 1.1 BagOfWords | Renamer ------------------------ count_comment : 1 ( int ) count_positive : 1 ( int ) < BLANKLINE > count_comment : 1 ( int ) count_positive : 1 ( int ) tfidf_comment : 0.50854 ( float ) tfidf_positive : 0.86104 ( float ) < BLANKLINE > 2. MultinomialNB ---------------- False : 0.19313 True : 0.80687 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Displays the state of a set of features as it goes through the pipeline. Parameters x ( dict ) show_types \u2013 defaults to True n_decimals \u2013 defaults to 5 forecast Return a forecast. Only works if each estimator has a transform_one method and the final estimator has a forecast method. This is the case of time series models from the time_series module. Parameters horizon ( int ) xs ( List[dict] ) \u2013 defaults to None learn_many Fit to a mini-batch. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) \u2013 defaults to None learn_unsupervised \u2013 defaults to False params learn_one Fit to a single instance. Parameters x ( dict ) y \u2013 defaults to None learn_unsupervised \u2013 defaults to False params predict_many predict_one Call transform_one on the first steps and predict_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True predict_proba_many predict_proba_one Call transform_one on the first steps and predict_proba_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True score_one Call transform_one on the first steps and score_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True transform_many Apply each transformer in the pipeline to some features. The final step in the pipeline will be applied if it is a transformer. If not, then it will be ignored and the output from the penultimate step will be returned. Note that the steps that precede the final step are assumed to all be transformers. Parameters X ( pandas.core.frame.DataFrame ) transform_one Apply each transformer in the pipeline to some features. The final step in the pipeline will be applied if it is a transformer. If not, then it will be ignored and the output from the penultimate step will be returned. Note that the steps that precede the final step are assumed to all be transformers. Parameters x ( dict )","title":"Pipeline"},{"location":"api/compose/Pipeline/#pipeline","text":"A pipeline of estimators. Pipelines allow you to chain different steps into a sequence. Typically, when doing supervised learning, a pipeline contains one ore more transformation steps, whilst it's is a regressor or a classifier. It is highly recommended to use pipelines with river . Indeed, in an online learning setting, it is very practical to have a model defined as a single object. Take a look at the user guide for further information and practical examples. One special thing to take notice to is the way transformers are handled. In a typical scenario, it is usual to predict something for a sample and wait for the ground truth to arrive. In such a case, the features are seen before the ground truth arrives. Therefore, the unsupervised parts of the pipeline are updated when predict_one and predict_proba_one are called. Usually the unsupervised parts of the pipeline are all the steps that precede the final step, which is a supervised model. However, some transformers are supervised and are therefore obtained during calls to learn_one .","title":"Pipeline"},{"location":"api/compose/Pipeline/#parameters","text":"steps Ideally, a list of (name, estimator) tuples. A name is automatically inferred if none is provided.","title":"Parameters"},{"location":"api/compose/Pipeline/#examples","text":"The recommended way to declare a pipeline is to use the | operator. The latter allows you to chain estimators in a very terse manner: >>> from river import linear_model >>> from river import preprocessing >>> scaler = preprocessing . StandardScaler () >>> log_reg = linear_model . LinearRegression () >>> model = scaler | log_reg This results in a pipeline that stores each step inside a dictionary. >>> model Pipeline ( StandardScaler (), LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.01 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) ) You can access parts of a pipeline in the same manner as a dictionary: >>> model [ 'LinearRegression' ] LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.01 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) Note that you can also declare a pipeline by using the compose.Pipeline constructor method, which is slightly more verbose: >>> from river import compose >>> model = compose . Pipeline ( scaler , log_reg ) By using a compose.TransformerUnion , you can define complex pipelines that apply different steps to different parts of the data. For instance, we can extract word counts from text data, and extract polynomial features from numeric data. >>> from river import feature_extraction as fx >>> tfidf = fx . TFIDF ( 'text' ) >>> counts = fx . BagOfWords ( 'text' ) >>> text_part = compose . Select ( 'text' ) | ( tfidf + counts ) >>> num_part = compose . Select ( 'a' , 'b' ) | fx . PolynomialExtender () >>> model = text_part + num_part >>> model |= preprocessing . StandardScaler () >>> model |= linear_model . LinearRegression () The following shows an example of using debug_one to visualize how the information flows and changes throughout the pipeline. >>> from river import compose >>> from river import naive_bayes >>> dataset = [ ... ( 'A positive comment' , True ), ... ( 'A negative comment' , False ), ... ( 'A happy comment' , True ), ... ( 'A lovely comment' , True ), ... ( 'A harsh comment' , False ) ... ] >>> tfidf = fx . TFIDF () | compose . Renamer ( prefix = 'tfidf_' ) >>> counts = fx . BagOfWords () | compose . Renamer ( prefix = 'count_' ) >>> mnb = naive_bayes . MultinomialNB () >>> model = ( tfidf + counts ) | mnb >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> x = dataset [ 0 ][ 0 ] >>> report = model . debug_one ( dataset [ 0 ][ 0 ]) >>> print ( report ) 0. Input -------- A positive comment < BLANKLINE > 1. Transformer union -------------------- 1.0 TFIDF | Renamer ------------------- tfidf_comment : 0.47606 ( float ) tfidf_positive : 0.87942 ( float ) < BLANKLINE > 1.1 BagOfWords | Renamer ------------------------ count_comment : 1 ( int ) count_positive : 1 ( int ) < BLANKLINE > count_comment : 1 ( int ) count_positive : 1 ( int ) tfidf_comment : 0.50854 ( float ) tfidf_positive : 0.86104 ( float ) < BLANKLINE > 2. MultinomialNB ---------------- False : 0.19313 True : 0.80687","title":"Examples"},{"location":"api/compose/Pipeline/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Displays the state of a set of features as it goes through the pipeline. Parameters x ( dict ) show_types \u2013 defaults to True n_decimals \u2013 defaults to 5 forecast Return a forecast. Only works if each estimator has a transform_one method and the final estimator has a forecast method. This is the case of time series models from the time_series module. Parameters horizon ( int ) xs ( List[dict] ) \u2013 defaults to None learn_many Fit to a mini-batch. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) \u2013 defaults to None learn_unsupervised \u2013 defaults to False params learn_one Fit to a single instance. Parameters x ( dict ) y \u2013 defaults to None learn_unsupervised \u2013 defaults to False params predict_many predict_one Call transform_one on the first steps and predict_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True predict_proba_many predict_proba_one Call transform_one on the first steps and predict_proba_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True score_one Call transform_one on the first steps and score_one on the last step. Parameters x ( dict ) learn_unsupervised \u2013 defaults to True transform_many Apply each transformer in the pipeline to some features. The final step in the pipeline will be applied if it is a transformer. If not, then it will be ignored and the output from the penultimate step will be returned. Note that the steps that precede the final step are assumed to all be transformers. Parameters X ( pandas.core.frame.DataFrame ) transform_one Apply each transformer in the pipeline to some features. The final step in the pipeline will be applied if it is a transformer. If not, then it will be ignored and the output from the penultimate step will be returned. Note that the steps that precede the final step are assumed to all be transformers. Parameters x ( dict )","title":"Methods"},{"location":"api/compose/Renamer/","text":"Renamer \u00b6 Renames keys based on given parameters. Parameters \u00b6 prefix \u2013 defaults to None suffix \u2013 defaults to None Examples \u00b6 >>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 } >>> compose . Renamer ( prefix = 'prefix_' , suffix = '_suffix' ) . transform_one ( x ) { 'prefix_a_suffix' : 42 , 'prefix_b_suffix' : 12 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Renamer"},{"location":"api/compose/Renamer/#renamer","text":"Renames keys based on given parameters.","title":"Renamer"},{"location":"api/compose/Renamer/#parameters","text":"prefix \u2013 defaults to None suffix \u2013 defaults to None","title":"Parameters"},{"location":"api/compose/Renamer/#examples","text":">>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 } >>> compose . Renamer ( prefix = 'prefix_' , suffix = '_suffix' ) . transform_one ( x ) { 'prefix_a_suffix' : 42 , 'prefix_b_suffix' : 12 }","title":"Examples"},{"location":"api/compose/Renamer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/Select/","text":"Select \u00b6 Selects features according to a whitelist. This can be used in a pipeline when you want to remove certain features. The transform_one method is pure, and therefore returns a fresh new dictionary instead of removing the specified keys from the input. Parameters \u00b6 whitelist ( Tuple[Hashable] ) Key(s) to keep. Examples \u00b6 >>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 , 'c' : 13 } >>> compose . Select ( 'c' ) . transform_one ( x ) { 'c' : 13 } You can chain a selector with any estimator in order to apply said estimator to the desired features. >>> from river import feature_extraction as fx >>> x = { 'sales' : 10 , 'shop' : 'Ikea' , 'country' : 'Sweden' } >>> pipeline = ( ... compose . Select ( 'sales' ) | ... fx . PolynomialExtender () ... ) >>> pipeline . transform_one ( x ) { 'sales' : 10 , 'sales*sales' : 100 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Select"},{"location":"api/compose/Select/#select","text":"Selects features according to a whitelist. This can be used in a pipeline when you want to remove certain features. The transform_one method is pure, and therefore returns a fresh new dictionary instead of removing the specified keys from the input.","title":"Select"},{"location":"api/compose/Select/#parameters","text":"whitelist ( Tuple[Hashable] ) Key(s) to keep.","title":"Parameters"},{"location":"api/compose/Select/#examples","text":">>> from river import compose >>> x = { 'a' : 42 , 'b' : 12 , 'c' : 13 } >>> compose . Select ( 'c' ) . transform_one ( x ) { 'c' : 13 } You can chain a selector with any estimator in order to apply said estimator to the desired features. >>> from river import feature_extraction as fx >>> x = { 'sales' : 10 , 'shop' : 'Ikea' , 'country' : 'Sweden' } >>> pipeline = ( ... compose . Select ( 'sales' ) | ... fx . PolynomialExtender () ... ) >>> pipeline . transform_one ( x ) { 'sales' : 10 , 'sales*sales' : 100 }","title":"Examples"},{"location":"api/compose/Select/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/SelectType/","text":"SelectType \u00b6 Selects features based on their type. This is practical when you want to apply different preprocessing steps to different kinds of features. For instance, a common usecase is to apply a preprocessing.StandardScaler to numeric features and a preprocessing.OneHotEncoder to categorical features. Parameters \u00b6 types ( Tuple[type] ) Python types which you want to select. Under the hood, the isinstance method will be used to check if a value is of a given type. Examples \u00b6 >>> import numbers >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> num = compose . SelectType ( numbers . Number ) | preprocessing . StandardScaler () >>> cat = compose . SelectType ( str ) | preprocessing . OneHotEncoder () >>> model = ( num + cat ) | linear_model . LogisticRegression () Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"SelectType"},{"location":"api/compose/SelectType/#selecttype","text":"Selects features based on their type. This is practical when you want to apply different preprocessing steps to different kinds of features. For instance, a common usecase is to apply a preprocessing.StandardScaler to numeric features and a preprocessing.OneHotEncoder to categorical features.","title":"SelectType"},{"location":"api/compose/SelectType/#parameters","text":"types ( Tuple[type] ) Python types which you want to select. Under the hood, the isinstance method will be used to check if a value is of a given type.","title":"Parameters"},{"location":"api/compose/SelectType/#examples","text":">>> import numbers >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> num = compose . SelectType ( numbers . Number ) | preprocessing . StandardScaler () >>> cat = compose . SelectType ( str ) | preprocessing . OneHotEncoder () >>> model = ( num + cat ) | linear_model . LogisticRegression ()","title":"Examples"},{"location":"api/compose/SelectType/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/compose/TransformerUnion/","text":"TransformerUnion \u00b6 Packs multiple transformers into a single one. Pipelines allow you to apply steps sequentially. Therefore, the output of a step becomes the input of the next one. In many cases, you may want to pass the output of a step to multiple steps. This simple transformer allows you to do so. In other words, it enables you to apply particular steps to different parts of an input. A typical example is when you want to scale numeric features and one-hot encode categorical features. This transformer is essentially a list of transformers. Whenever it is updated, it loops through each transformer and updates them. Meanwhile, calling transform_one collects the output of each transformer and merges them into a single dictionary. Parameters \u00b6 transformers Ideally, a list of (name, estimator) tuples. A name is automatically inferred if none is provided. Examples \u00b6 Take the following dataset: >>> X = [ ... { 'place' : 'Taco Bell' , 'revenue' : 42 }, ... { 'place' : 'Burger King' , 'revenue' : 16 }, ... { 'place' : 'Burger King' , 'revenue' : 24 }, ... { 'place' : 'Taco Bell' , 'revenue' : 58 }, ... { 'place' : 'Burger King' , 'revenue' : 20 }, ... { 'place' : 'Taco Bell' , 'revenue' : 50 } ... ] As an example, let's assume we want to compute two aggregates of a dataset. We therefore define two feature_extraction.Agg s and initialize a TransformerUnion with them: >>> from river import compose >>> from river import feature_extraction >>> from river import stats >>> mean = feature_extraction . Agg ( ... on = 'revenue' , by = 'place' , ... how = stats . Mean () ... ) >>> count = feature_extraction . Agg ( ... on = 'revenue' , by = 'place' , ... how = stats . Count () ... ) >>> agg = compose . TransformerUnion ( mean , count ) We can now update each transformer and obtain their output with a single function call: >>> from pprint import pprint >>> for x in X : ... agg = agg . learn_one ( x ) ... pprint ( agg . transform_one ( x )) { 'revenue_count_by_place' : 1 , 'revenue_mean_by_place' : 42.0 } { 'revenue_count_by_place' : 1 , 'revenue_mean_by_place' : 16.0 } { 'revenue_count_by_place' : 2 , 'revenue_mean_by_place' : 20.0 } { 'revenue_count_by_place' : 2 , 'revenue_mean_by_place' : 50.0 } { 'revenue_count_by_place' : 3 , 'revenue_mean_by_place' : 20.0 } { 'revenue_count_by_place' : 3 , 'revenue_mean_by_place' : 50.0 } Note that you can use the + operator as a shorthand notation: agg = mean + count This allows you to build complex pipelines in a very terse manner. For instance, we can create a pipeline that scales each feature and fits a logistic regression as so: >>> from river import linear_model as lm >>> from river import preprocessing as pp >>> model = ( ... ( mean + count ) | ... pp . StandardScaler () | ... lm . LogisticRegression () ... ) Whice is equivalent to the following code: >>> model = compose . Pipeline ( ... compose . TransformerUnion ( mean , count ), ... pp . StandardScaler (), ... lm . LogisticRegression () ... ) Note that you access any part of a TransformerUnion by name: >>> model [ 'TransformerUnion' ][ 'Agg' ] Agg ( on = \"revenue\" by = [ 'place' ] how = Mean () ) >>> model [ 'TransformerUnion' ][ 'Agg1' ] Agg ( on = \"revenue\" by = [ 'place' ] how = Count () ) You can also manually provide a name for each step: >>> agg = compose . TransformerUnion ( ... ( 'Mean revenue by place' , mean ), ... ( '# by place' , count ) ... ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update each transformer. Parameters x ( dict ) y \u2013 defaults to None transform_one Passes the data through each transformer and packs the results together. Parameters x ( dict )","title":"TransformerUnion"},{"location":"api/compose/TransformerUnion/#transformerunion","text":"Packs multiple transformers into a single one. Pipelines allow you to apply steps sequentially. Therefore, the output of a step becomes the input of the next one. In many cases, you may want to pass the output of a step to multiple steps. This simple transformer allows you to do so. In other words, it enables you to apply particular steps to different parts of an input. A typical example is when you want to scale numeric features and one-hot encode categorical features. This transformer is essentially a list of transformers. Whenever it is updated, it loops through each transformer and updates them. Meanwhile, calling transform_one collects the output of each transformer and merges them into a single dictionary.","title":"TransformerUnion"},{"location":"api/compose/TransformerUnion/#parameters","text":"transformers Ideally, a list of (name, estimator) tuples. A name is automatically inferred if none is provided.","title":"Parameters"},{"location":"api/compose/TransformerUnion/#examples","text":"Take the following dataset: >>> X = [ ... { 'place' : 'Taco Bell' , 'revenue' : 42 }, ... { 'place' : 'Burger King' , 'revenue' : 16 }, ... { 'place' : 'Burger King' , 'revenue' : 24 }, ... { 'place' : 'Taco Bell' , 'revenue' : 58 }, ... { 'place' : 'Burger King' , 'revenue' : 20 }, ... { 'place' : 'Taco Bell' , 'revenue' : 50 } ... ] As an example, let's assume we want to compute two aggregates of a dataset. We therefore define two feature_extraction.Agg s and initialize a TransformerUnion with them: >>> from river import compose >>> from river import feature_extraction >>> from river import stats >>> mean = feature_extraction . Agg ( ... on = 'revenue' , by = 'place' , ... how = stats . Mean () ... ) >>> count = feature_extraction . Agg ( ... on = 'revenue' , by = 'place' , ... how = stats . Count () ... ) >>> agg = compose . TransformerUnion ( mean , count ) We can now update each transformer and obtain their output with a single function call: >>> from pprint import pprint >>> for x in X : ... agg = agg . learn_one ( x ) ... pprint ( agg . transform_one ( x )) { 'revenue_count_by_place' : 1 , 'revenue_mean_by_place' : 42.0 } { 'revenue_count_by_place' : 1 , 'revenue_mean_by_place' : 16.0 } { 'revenue_count_by_place' : 2 , 'revenue_mean_by_place' : 20.0 } { 'revenue_count_by_place' : 2 , 'revenue_mean_by_place' : 50.0 } { 'revenue_count_by_place' : 3 , 'revenue_mean_by_place' : 20.0 } { 'revenue_count_by_place' : 3 , 'revenue_mean_by_place' : 50.0 } Note that you can use the + operator as a shorthand notation: agg = mean + count This allows you to build complex pipelines in a very terse manner. For instance, we can create a pipeline that scales each feature and fits a logistic regression as so: >>> from river import linear_model as lm >>> from river import preprocessing as pp >>> model = ( ... ( mean + count ) | ... pp . StandardScaler () | ... lm . LogisticRegression () ... ) Whice is equivalent to the following code: >>> model = compose . Pipeline ( ... compose . TransformerUnion ( mean , count ), ... pp . StandardScaler (), ... lm . LogisticRegression () ... ) Note that you access any part of a TransformerUnion by name: >>> model [ 'TransformerUnion' ][ 'Agg' ] Agg ( on = \"revenue\" by = [ 'place' ] how = Mean () ) >>> model [ 'TransformerUnion' ][ 'Agg1' ] Agg ( on = \"revenue\" by = [ 'place' ] how = Count () ) You can also manually provide a name for each step: >>> agg = compose . TransformerUnion ( ... ( 'Mean revenue by place' , mean ), ... ( '# by place' , count ) ... )","title":"Examples"},{"location":"api/compose/TransformerUnion/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update each transformer. Parameters x ( dict ) y \u2013 defaults to None transform_one Passes the data through each transformer and packs the results together. Parameters x ( dict )","title":"Methods"},{"location":"api/datasets/AirlinePassengers/","text":"AirlinePassengers \u00b6 Monthly number of international airline passengers. The stream contains 144 items and only one single feature, which is the month. The goal is to predict the number of passengers each month by capturing the trend and the seasonality of the data. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 International airline passengers: monthly totals in thousands. Jan 49 \u2013 Dec 60 \u21a9","title":"AirlinePassengers"},{"location":"api/datasets/AirlinePassengers/#airlinepassengers","text":"Monthly number of international airline passengers. The stream contains 144 items and only one single feature, which is the month. The goal is to predict the number of passengers each month by capturing the trend and the seasonality of the data.","title":"AirlinePassengers"},{"location":"api/datasets/AirlinePassengers/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/AirlinePassengers/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/AirlinePassengers/#references","text":"International airline passengers: monthly totals in thousands. Jan 49 \u2013 Dec 60 \u21a9","title":"References"},{"location":"api/datasets/Bananas/","text":"Bananas \u00b6 Bananas dataset. An artificial dataset where instances belongs to several clusters with a banana shape. There are two attributes that correspond to the x and y axis, respectively. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 OpenML page \u21a9","title":"Bananas"},{"location":"api/datasets/Bananas/#bananas","text":"Bananas dataset. An artificial dataset where instances belongs to several clusters with a banana shape. There are two attributes that correspond to the x and y axis, respectively.","title":"Bananas"},{"location":"api/datasets/Bananas/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/Bananas/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Bananas/#references","text":"OpenML page \u21a9","title":"References"},{"location":"api/datasets/Bikes/","text":"Bikes \u00b6 Bike sharing station information from the city of Toulouse. The goal is to predict the number of bikes in 5 different bike stations from the city of Toulouse. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 A short introduction and conclusion to the OpenBikes 2016 Challenge \u21a9","title":"Bikes"},{"location":"api/datasets/Bikes/#bikes","text":"Bike sharing station information from the city of Toulouse. The goal is to predict the number of bikes in 5 different bike stations from the city of Toulouse.","title":"Bikes"},{"location":"api/datasets/Bikes/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Bikes/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Bikes/#references","text":"A short introduction and conclusion to the OpenBikes 2016 Challenge \u21a9","title":"References"},{"location":"api/datasets/ChickWeights/","text":"ChickWeights \u00b6 Chick weights along time. The stream contains 578 items and 3 features. The goal is to predict the weight of each chick along time, according to the diet the chick is on. The data is ordered by time and then by chick. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Chick weight dataset overview \u21a9","title":"ChickWeights"},{"location":"api/datasets/ChickWeights/#chickweights","text":"Chick weights along time. The stream contains 578 items and 3 features. The goal is to predict the weight of each chick along time, according to the diet the chick is on. The data is ordered by time and then by chick.","title":"ChickWeights"},{"location":"api/datasets/ChickWeights/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/ChickWeights/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/ChickWeights/#references","text":"Chick weight dataset overview \u21a9","title":"References"},{"location":"api/datasets/CreditCard/","text":"CreditCard \u00b6 Credit card frauds. The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 \u21a9 Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon \u21a9 Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE \u21a9 Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) \u21a9 Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier \u21a9 Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing \u21a9 Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 \u21a9 Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019 \u21a9","title":"CreditCard"},{"location":"api/datasets/CreditCard/#creditcard","text":"Credit card frauds. The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","title":"CreditCard"},{"location":"api/datasets/CreditCard/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/CreditCard/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/CreditCard/#references","text":"Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 \u21a9 Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon \u21a9 Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE \u21a9 Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) \u21a9 Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier \u21a9 Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing \u21a9 Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 \u21a9 Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019 \u21a9","title":"References"},{"location":"api/datasets/Elec2/","text":"Elec2 \u00b6 Electricity prices in New South Wales. This is a binary classification task, where the goal is to predict if the price of electricity will go up or down. This data was collected from the Australian New South Wales Electricity Market. In this market, prices are not fixed and are affected by demand and supply of the market. They are set every five minutes. Electricity transfers to/from the neighboring state of Victoria were done to alleviate fluctuations. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 SPLICE-2 Comparative Evaluation: Electricity Pricing \u21a9 DataHub description \u21a9","title":"Elec2"},{"location":"api/datasets/Elec2/#elec2","text":"Electricity prices in New South Wales. This is a binary classification task, where the goal is to predict if the price of electricity will go up or down. This data was collected from the Australian New South Wales Electricity Market. In this market, prices are not fixed and are affected by demand and supply of the market. They are set every five minutes. Electricity transfers to/from the neighboring state of Victoria were done to alleviate fluctuations.","title":"Elec2"},{"location":"api/datasets/Elec2/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Elec2/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Elec2/#references","text":"SPLICE-2 Comparative Evaluation: Electricity Pricing \u21a9 DataHub description \u21a9","title":"References"},{"location":"api/datasets/HTTP/","text":"HTTP \u00b6 HTTP dataset of the KDD 1999 cup. The goal is to predict whether or not an HTTP connection is anomalous or not. The dataset only contains 2,211 (0.4%) positive labels. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 HTTP (KDDCUP99) dataset \u21a9","title":"HTTP"},{"location":"api/datasets/HTTP/#http","text":"HTTP dataset of the KDD 1999 cup. The goal is to predict whether or not an HTTP connection is anomalous or not. The dataset only contains 2,211 (0.4%) positive labels.","title":"HTTP"},{"location":"api/datasets/HTTP/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/HTTP/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/HTTP/#references","text":"HTTP (KDDCUP99) dataset \u21a9","title":"References"},{"location":"api/datasets/Higgs/","text":"Higgs \u00b6 Higgs dataset. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 UCI page \u21a9","title":"Higgs"},{"location":"api/datasets/Higgs/#higgs","text":"Higgs dataset. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.","title":"Higgs"},{"location":"api/datasets/Higgs/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Higgs/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Higgs/#references","text":"UCI page \u21a9","title":"References"},{"location":"api/datasets/ImageSegments/","text":"ImageSegments \u00b6 Image segments classification. This dataset contains features that describe image segments into 7 classes: brickface, sky, foliage, cement, window, path, and grass. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 UCI page \u21a9","title":"ImageSegments"},{"location":"api/datasets/ImageSegments/#imagesegments","text":"Image segments classification. This dataset contains features that describe image segments into 7 classes: brickface, sky, foliage, cement, window, path, and grass.","title":"ImageSegments"},{"location":"api/datasets/ImageSegments/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/ImageSegments/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/ImageSegments/#references","text":"UCI page \u21a9","title":"References"},{"location":"api/datasets/Insects/","text":"Insects \u00b6 Insects dataset. This dataset has different variants, which are: abrupt_balanced - abrupt_imbalanced - gradual_balanced - gradual_imbalanced - incremental-abrupt_balanced - incremental-abrupt_imbalanced - incremental-reoccurring_balanced - incremental-reoccurring_imbalanced - incremental_balanced - incremental_imbalanced - out-of-control The number of samples and the difficulty change from one variant to another. The number of classes is always the same (6), except for the last variant (24). Parameters \u00b6 variant \u2013 defaults to abrupt_balanced Indicates which variant of the dataset to load. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 USP DS repository \u21a9 Souza, V., Reis, D.M.D., Maletzke, A.G. and Batista, G.E., 2020. Challenges in Benchmarking Stream Learning Algorithms with Real-world Data. arXiv preprint arXiv:2005.00113. \u21a9","title":"Insects"},{"location":"api/datasets/Insects/#insects","text":"Insects dataset. This dataset has different variants, which are: abrupt_balanced - abrupt_imbalanced - gradual_balanced - gradual_imbalanced - incremental-abrupt_balanced - incremental-abrupt_imbalanced - incremental-reoccurring_balanced - incremental-reoccurring_imbalanced - incremental_balanced - incremental_imbalanced - out-of-control The number of samples and the difficulty change from one variant to another. The number of classes is always the same (6), except for the last variant (24).","title":"Insects"},{"location":"api/datasets/Insects/#parameters","text":"variant \u2013 defaults to abrupt_balanced Indicates which variant of the dataset to load.","title":"Parameters"},{"location":"api/datasets/Insects/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Insects/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Insects/#references","text":"USP DS repository \u21a9 Souza, V., Reis, D.M.D., Maletzke, A.G. and Batista, G.E., 2020. Challenges in Benchmarking Stream Learning Algorithms with Real-world Data. arXiv preprint arXiv:2005.00113. \u21a9","title":"References"},{"location":"api/datasets/MaliciousURL/","text":"MaliciousURL \u00b6 Malicious URLs dataset. This dataset contains features about URLs that are classified as malicious or not. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 Detecting Malicious URLs \u21a9 Identifying Suspicious URLs: An Application of Large-Scale Online Learning \u21a9","title":"MaliciousURL"},{"location":"api/datasets/MaliciousURL/#maliciousurl","text":"Malicious URLs dataset. This dataset contains features about URLs that are classified as malicious or not.","title":"MaliciousURL"},{"location":"api/datasets/MaliciousURL/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/MaliciousURL/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/MaliciousURL/#references","text":"Detecting Malicious URLs \u21a9 Identifying Suspicious URLs: An Application of Large-Scale Online Learning \u21a9","title":"References"},{"location":"api/datasets/MovieLens100K/","text":"MovieLens100K \u00b6 MovieLens 100K dataset. MovieLens datasets were collected by the GroupLens Research Project at the University of Minnesota. This dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. User and movie information are provided. The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 The MovieLens Datasets: History and Context \u21a9","title":"MovieLens100K"},{"location":"api/datasets/MovieLens100K/#movielens100k","text":"MovieLens 100K dataset. MovieLens datasets were collected by the GroupLens Research Project at the University of Minnesota. This dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. User and movie information are provided. The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998.","title":"MovieLens100K"},{"location":"api/datasets/MovieLens100K/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/MovieLens100K/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/MovieLens100K/#references","text":"The MovieLens Datasets: History and Context \u21a9","title":"References"},{"location":"api/datasets/Music/","text":"Music \u00b6 Bike sharing station information from the city of Toulouse. The goal is to predict to which kinds of moods a song pertains to. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 Read, J., Reutemann, P., Pfahringer, B. and Holmes, G., 2016. MEKA: a multi-label/multi-target extension to WEKA. The Journal of Machine Learning Research, 17(1), pp.667-671. \u21a9","title":"Music"},{"location":"api/datasets/Music/#music","text":"Bike sharing station information from the city of Toulouse. The goal is to predict to which kinds of moods a song pertains to.","title":"Music"},{"location":"api/datasets/Music/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Music/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Music/#references","text":"Read, J., Reutemann, P., Pfahringer, B. and Holmes, G., 2016. MEKA: a multi-label/multi-target extension to WEKA. The Journal of Machine Learning Research, 17(1), pp.667-671. \u21a9","title":"References"},{"location":"api/datasets/Phishing/","text":"Phishing \u00b6 Phishing websites. This dataset contains features from web pages that are classified as phishing or not. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 UCI page \u21a9","title":"Phishing"},{"location":"api/datasets/Phishing/#phishing","text":"Phishing websites. This dataset contains features from web pages that are classified as phishing or not.","title":"Phishing"},{"location":"api/datasets/Phishing/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/Phishing/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Phishing/#references","text":"UCI page \u21a9","title":"References"},{"location":"api/datasets/Restaurants/","text":"Restaurants \u00b6 Data from the Kaggle Recruit Restaurants challenge. The goal is to predict the number of visitors in each of 829 Japanese restaurants over a priod of roughly 16 weeks. The data is ordered by date and then by restaurant ID. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 Recruit Restaurant Visitor Forecasting \u21a9","title":"Restaurants"},{"location":"api/datasets/Restaurants/#restaurants","text":"Data from the Kaggle Recruit Restaurants challenge. The goal is to predict the number of visitors in each of 829 Japanese restaurants over a priod of roughly 16 weeks. The data is ordered by date and then by restaurant ID.","title":"Restaurants"},{"location":"api/datasets/Restaurants/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Restaurants/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Restaurants/#references","text":"Recruit Restaurant Visitor Forecasting \u21a9","title":"References"},{"location":"api/datasets/SMSSpam/","text":"SMSSpam \u00b6 SMS Spam Collection dataset. The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent 13.4% of the dataset. The goal is to predict whether an SMS is a spam or not. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 Almeida, T.A., Hidalgo, J.M.G. and Yamakami, A., 2011, September. Contributions to the study of SMS spam filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document engineering (pp. 259-262). \u21a9","title":"SMSSpam"},{"location":"api/datasets/SMSSpam/#smsspam","text":"SMS Spam Collection dataset. The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent 13.4% of the dataset. The goal is to predict whether an SMS is a spam or not.","title":"SMSSpam"},{"location":"api/datasets/SMSSpam/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/SMSSpam/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/SMSSpam/#references","text":"Almeida, T.A., Hidalgo, J.M.G. and Yamakami, A., 2011, September. Contributions to the study of SMS spam filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document engineering (pp. 259-262). \u21a9","title":"References"},{"location":"api/datasets/SMTP/","text":"SMTP \u00b6 SMTP dataset from the KDD 1999 cup. The goal is to predict whether or not an SMTP connection is anomalous or not. The dataset only contains 2,211 (0.4%) positive labels. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 SMTP (KDDCUP99) dataset \u21a9","title":"SMTP"},{"location":"api/datasets/SMTP/#smtp","text":"SMTP dataset from the KDD 1999 cup. The goal is to predict whether or not an SMTP connection is anomalous or not. The dataset only contains 2,211 (0.4%) positive labels.","title":"SMTP"},{"location":"api/datasets/SMTP/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/SMTP/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/SMTP/#references","text":"SMTP (KDDCUP99) dataset \u21a9","title":"References"},{"location":"api/datasets/SolarFlare/","text":"SolarFlare \u00b6 Solar flare multi-output regression. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 UCI page \u21a9","title":"SolarFlare"},{"location":"api/datasets/SolarFlare/#solarflare","text":"Solar flare multi-output regression.","title":"SolarFlare"},{"location":"api/datasets/SolarFlare/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/SolarFlare/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/SolarFlare/#references","text":"UCI page \u21a9","title":"References"},{"location":"api/datasets/TREC07/","text":"TREC07 \u00b6 TREC's 2007 Spam Track dataset. The data contains 75,419 chronologically ordered items, i.e. 3 months of emails delivered to a particular server in 2007. Spam messages represent 66.6% of the dataset. The goal is to predict whether an email is a spam or not. The available raw features are: sender, recipients, date, subject, body. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 TREC 2007 Spam Track Overview \u21a9 Code ran to parse the dataset \u21a9","title":"TREC07"},{"location":"api/datasets/TREC07/#trec07","text":"TREC's 2007 Spam Track dataset. The data contains 75,419 chronologically ordered items, i.e. 3 months of emails delivered to a particular server in 2007. Spam messages represent 66.6% of the dataset. The goal is to predict whether an email is a spam or not. The available raw features are: sender, recipients, date, subject, body.","title":"TREC07"},{"location":"api/datasets/TREC07/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/TREC07/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/TREC07/#references","text":"TREC 2007 Spam Track Overview \u21a9 Code ran to parse the dataset \u21a9","title":"References"},{"location":"api/datasets/Taxis/","text":"Taxis \u00b6 Taxi ride durations in New York City. The goal is to predict the duration of taxi rides in New York City. Attributes \u00b6 desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path Methods \u00b6 download take Iterate over the k samples. Parameters k ( int ) References \u00b6 New York City Taxi Trip Duration competition on Kaggle \u21a9","title":"Taxis"},{"location":"api/datasets/Taxis/#taxis","text":"Taxi ride durations in New York City. The goal is to predict the duration of taxi rides in New York City.","title":"Taxis"},{"location":"api/datasets/Taxis/#attributes","text":"desc Return the description from the docstring. is_downloaded Indicate whether or the data has been correctly downloaded. path","title":"Attributes"},{"location":"api/datasets/Taxis/#methods","text":"download take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/Taxis/#references","text":"New York City Taxi Trip Duration competition on Kaggle \u21a9","title":"References"},{"location":"api/datasets/TrumpApproval/","text":"TrumpApproval \u00b6 Donald Trump approval ratings. This dataset was obtained by reshaping the data used by FiveThirtyEight for analyzing Donald Trump's approval ratings. It contains 5 features, which are approval ratings collected by 5 polling agencies. The target is the approval rating from FiveThirtyEight's model. The goal of this task is to see if we can reproduce FiveThirtyEight's model. Attributes \u00b6 desc Return the description from the docstring. path Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Trump Approval Ratings \u21a9","title":"TrumpApproval"},{"location":"api/datasets/TrumpApproval/#trumpapproval","text":"Donald Trump approval ratings. This dataset was obtained by reshaping the data used by FiveThirtyEight for analyzing Donald Trump's approval ratings. It contains 5 features, which are approval ratings collected by 5 polling agencies. The target is the approval rating from FiveThirtyEight's model. The goal of this task is to see if we can reproduce FiveThirtyEight's model.","title":"TrumpApproval"},{"location":"api/datasets/TrumpApproval/#attributes","text":"desc Return the description from the docstring. path","title":"Attributes"},{"location":"api/datasets/TrumpApproval/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/datasets/TrumpApproval/#references","text":"Trump Approval Ratings \u21a9","title":"References"},{"location":"api/drift/ADWIN/","text":"ADWIN \u00b6 Adaptive Windowing method for concept drift detection. ADWIN (ADaptive WINdowing) is a popular drift detection method with mathematical guarantees. ADWIN efficiently keeps a variable-length window of recent items; such that it holds that there has no been change in the data distribution. This window is further divided into two sub-windows \\((W_0, W_1)\\) used to determine if a change has happened. ADWIN compares the average of \\(W_0\\) and \\(W_1\\) to confirm that they correspond to the same distribution. Concept drift is detected if the distribution equality no longer holds. Upon detecting a drift, \\(W_0\\) is replaced by \\(W_1\\) and a new \\(W_1\\) is initialized. ADWIN uses a confidence value \\(\\delta=\\in(0,1)\\) to determine if the two sub-windows correspond to the same distribution. Input : value can be any numeric value related to the definition of concept change for the data analyzed. For example, using 0's or 1's to track drift in a classifier's performance as follows: 0: Means the learners prediction was wrong 1: Means the learners prediction was correct Parameters \u00b6 delta \u2013 defaults to 0.002 Confidence value. Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. delta estimation Error estimation n_detections total variance warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. width Window size Examples \u00b6 >>> import numpy as np >>> from river.drift import ADWIN >>> np . random . seed ( 12345 ) >>> adwin = ADWIN () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = adwin . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1023 , input value : 5 Change detected at index 1055 , input value : 7 Change detected at index 1087 , input value : 5 Change detected at index 1119 , input value : 7 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Apart from adding the element value to the window, by inserting it in the correct bucket, it will also update the relevant statistics, in this case the total sum of all values, the window width and the total variance. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : tuple References \u00b6 Albert Bifet and Ricard Gavalda. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"ADWIN"},{"location":"api/drift/ADWIN/#adwin","text":"Adaptive Windowing method for concept drift detection. ADWIN (ADaptive WINdowing) is a popular drift detection method with mathematical guarantees. ADWIN efficiently keeps a variable-length window of recent items; such that it holds that there has no been change in the data distribution. This window is further divided into two sub-windows \\((W_0, W_1)\\) used to determine if a change has happened. ADWIN compares the average of \\(W_0\\) and \\(W_1\\) to confirm that they correspond to the same distribution. Concept drift is detected if the distribution equality no longer holds. Upon detecting a drift, \\(W_0\\) is replaced by \\(W_1\\) and a new \\(W_1\\) is initialized. ADWIN uses a confidence value \\(\\delta=\\in(0,1)\\) to determine if the two sub-windows correspond to the same distribution. Input : value can be any numeric value related to the definition of concept change for the data analyzed. For example, using 0's or 1's to track drift in a classifier's performance as follows: 0: Means the learners prediction was wrong 1: Means the learners prediction was correct","title":"ADWIN"},{"location":"api/drift/ADWIN/#parameters","text":"delta \u2013 defaults to 0.002 Confidence value.","title":"Parameters"},{"location":"api/drift/ADWIN/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. delta estimation Error estimation n_detections total variance warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. width Window size","title":"Attributes"},{"location":"api/drift/ADWIN/#examples","text":">>> import numpy as np >>> from river.drift import ADWIN >>> np . random . seed ( 12345 ) >>> adwin = ADWIN () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = adwin . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1023 , input value : 5 Change detected at index 1055 , input value : 7 Change detected at index 1087 , input value : 5 Change detected at index 1119 , input value : 7","title":"Examples"},{"location":"api/drift/ADWIN/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Apart from adding the element value to the window, by inserting it in the correct bucket, it will also update the relevant statistics, in this case the total sum of all values, the window width and the total variance. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : tuple","title":"Methods"},{"location":"api/drift/ADWIN/#references","text":"Albert Bifet and Ricard Gavalda. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"References"},{"location":"api/drift/DDM/","text":"DDM \u00b6 Drift Detection Method. DDM (Drift Detection Method) is a concept change detection method based on the PAC learning model premise, that the learner's error rate will decrease as the number of analysed samples increase, as long as the data distribution is stationary. If the algorithm detects an increase in the error rate, that surpasses a calculated threshold, either change is detected or the algorithm will warn the user that change may occur in the near future, which is called the warning zone. The detection threshold is calculated in function of two statistics, obtained when \\((p_i + s_i)\\) is minimum: \\(p_{min}\\) : The minimum recorded error rate. \\(s_{min}\\) : The minimum recorded standard deviation. At instant \\(i\\) , the detection algorithm uses: \\(p_i\\) : The error rate at instant \\(i\\) . \\(s_i\\) : The standard deviation at instant \\(i\\) . The conditions for entering the warning zone and detecting change are as follows [see implementation note bellow]: if \\(p_i + s_i \\geq p_{min} + 2 * s_{min}\\) -> Warning zone if \\(p_i + s_i \\geq p_{min} + 3 * s_{min}\\) -> Change detected Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Parameters \u00b6 min_num_instances \u2013 defaults to 30 The minimum required number of analyzed samples so change can be detected. This is used to avoid false detections during the early moments of the detector, when the weight of one sample is important. warning_level \u2013 defaults to 2.0 Warning level. out_control_level \u2013 defaults to 3.0 Out-control level. Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import DDM >>> np . random . seed ( 12345 ) >>> ddm = DDM () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 1000 : 1200 ] = [ np . random . binomial ( 1 , .8 ) for _ in range ( 200 )] >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = ddm . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1157 , input value : 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Notes \u00b6 In this implementation, the conditions to signal drift and warning are \\(p_i + s_i > thershold\\) instead of \\(p_i + s_i \\geq thershold\\) . This is to avoid a corner case when a classifier is consistently wrong ( value=1 ) that results in DDM indicating a drift every min_num_instances . This modification is consistent with the implementation in MOA. References \u00b6 Jo\u00e3o Gama, Pedro Medas, Gladys Castillo, Pedro Pereira Rodrigues: Learning with Drift Detection. SBIA 2004: 286-295 \u21a9","title":"DDM"},{"location":"api/drift/DDM/#ddm","text":"Drift Detection Method. DDM (Drift Detection Method) is a concept change detection method based on the PAC learning model premise, that the learner's error rate will decrease as the number of analysed samples increase, as long as the data distribution is stationary. If the algorithm detects an increase in the error rate, that surpasses a calculated threshold, either change is detected or the algorithm will warn the user that change may occur in the near future, which is called the warning zone. The detection threshold is calculated in function of two statistics, obtained when \\((p_i + s_i)\\) is minimum: \\(p_{min}\\) : The minimum recorded error rate. \\(s_{min}\\) : The minimum recorded standard deviation. At instant \\(i\\) , the detection algorithm uses: \\(p_i\\) : The error rate at instant \\(i\\) . \\(s_i\\) : The standard deviation at instant \\(i\\) . The conditions for entering the warning zone and detecting change are as follows [see implementation note bellow]: if \\(p_i + s_i \\geq p_{min} + 2 * s_{min}\\) -> Warning zone if \\(p_i + s_i \\geq p_{min} + 3 * s_{min}\\) -> Change detected Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\)","title":"DDM"},{"location":"api/drift/DDM/#parameters","text":"min_num_instances \u2013 defaults to 30 The minimum required number of analyzed samples so change can be detected. This is used to avoid false detections during the early moments of the detector, when the weight of one sample is important. warning_level \u2013 defaults to 2.0 Warning level. out_control_level \u2013 defaults to 3.0 Out-control level.","title":"Parameters"},{"location":"api/drift/DDM/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/DDM/#examples","text":">>> import numpy as np >>> from river.drift import DDM >>> np . random . seed ( 12345 ) >>> ddm = DDM () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 1000 : 1200 ] = [ np . random . binomial ( 1 , .8 ) for _ in range ( 200 )] >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = ddm . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1157 , input value : 1","title":"Examples"},{"location":"api/drift/DDM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number )","title":"Methods"},{"location":"api/drift/DDM/#notes","text":"In this implementation, the conditions to signal drift and warning are \\(p_i + s_i > thershold\\) instead of \\(p_i + s_i \\geq thershold\\) . This is to avoid a corner case when a classifier is consistently wrong ( value=1 ) that results in DDM indicating a drift every min_num_instances . This modification is consistent with the implementation in MOA.","title":"Notes"},{"location":"api/drift/DDM/#references","text":"Jo\u00e3o Gama, Pedro Medas, Gladys Castillo, Pedro Pereira Rodrigues: Learning with Drift Detection. SBIA 2004: 286-295 \u21a9","title":"References"},{"location":"api/drift/EDDM/","text":"EDDM \u00b6 Early Drift Detection Method. EDDM (Early Drift Detection Method) aims to improve the detection rate of gradual concept drift in DDM, while keeping a good performance against abrupt concept drift. This method works by keeping track of the average distance between two errors instead of only the error rate. For this, it is necessary to keep track of the running average distance and the running standard deviation, as well as the maximum distance and the maximum standard deviation. The algorithm works similarly to the DDM algorithm, by keeping track of statistics only. It works with the running average distance ( \\(p_i'\\) ) and the running standard deviation ( \\(s_i'\\) ), as well as \\(p'_{max}\\) and \\(s'_{max}\\) , which are the values of \\(p_i'\\) and \\(s_i'\\) when \\((p_i' + 2 * s_i')\\) reaches its maximum. Like DDM, there are two threshold values that define the borderline between no change, warning zone, and drift detected. These are as follows: if \\((p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \\alpha\\) -> Warning zone if \\((p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \\beta\\) -> Change detected \\(\\alpha\\) and \\(\\beta\\) are set to 0.95 and 0.9, respectively. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import EDDM >>> np . random . seed ( 12345 ) >>> eddm = EDDM () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = eddm . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 53 , input value : 1 Change detected at index 121 , input value : 1 Change detected at index 185 , input value : 1 Change detected at index 272 , input value : 1 Change detected at index 336 , input value : 1 Change detected at index 391 , input value : 1 Change detected at index 571 , input value : 1 Change detected at index 627 , input value : 1 Change detected at index 686 , input value : 1 Change detected at index 754 , input value : 1 Change detected at index 1033 , input value : 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected. References \u00b6 Early Drift Detection Method. Manuel Baena-Garcia, Jose Del Campo-Avila, Ra\u00fal Fidalgo, Albert Bifet, Ricard Gavalda, Rafael Morales-Bueno. In Fourth International Workshop on Knowledge Discovery from Data Streams, 2006. \u21a9","title":"EDDM"},{"location":"api/drift/EDDM/#eddm","text":"Early Drift Detection Method. EDDM (Early Drift Detection Method) aims to improve the detection rate of gradual concept drift in DDM, while keeping a good performance against abrupt concept drift. This method works by keeping track of the average distance between two errors instead of only the error rate. For this, it is necessary to keep track of the running average distance and the running standard deviation, as well as the maximum distance and the maximum standard deviation. The algorithm works similarly to the DDM algorithm, by keeping track of statistics only. It works with the running average distance ( \\(p_i'\\) ) and the running standard deviation ( \\(s_i'\\) ), as well as \\(p'_{max}\\) and \\(s'_{max}\\) , which are the values of \\(p_i'\\) and \\(s_i'\\) when \\((p_i' + 2 * s_i')\\) reaches its maximum. Like DDM, there are two threshold values that define the borderline between no change, warning zone, and drift detected. These are as follows: if \\((p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \\alpha\\) -> Warning zone if \\((p_i' + 2 * s_i')/(p'_{max} + 2 * s'_{max}) < \\beta\\) -> Change detected \\(\\alpha\\) and \\(\\beta\\) are set to 0.95 and 0.9, respectively. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\)","title":"EDDM"},{"location":"api/drift/EDDM/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/EDDM/#examples","text":">>> import numpy as np >>> from river.drift import EDDM >>> np . random . seed ( 12345 ) >>> eddm = EDDM () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = eddm . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 53 , input value : 1 Change detected at index 121 , input value : 1 Change detected at index 185 , input value : 1 Change detected at index 272 , input value : 1 Change detected at index 336 , input value : 1 Change detected at index 391 , input value : 1 Change detected at index 571 , input value : 1 Change detected at index 627 , input value : 1 Change detected at index 686 , input value : 1 Change detected at index 754 , input value : 1 Change detected at index 1033 , input value : 1","title":"Examples"},{"location":"api/drift/EDDM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"Methods"},{"location":"api/drift/EDDM/#references","text":"Early Drift Detection Method. Manuel Baena-Garcia, Jose Del Campo-Avila, Ra\u00fal Fidalgo, Albert Bifet, Ricard Gavalda, Rafael Morales-Bueno. In Fourth International Workshop on Knowledge Discovery from Data Streams, 2006. \u21a9","title":"References"},{"location":"api/drift/HDDM-A/","text":"HDDM_A \u00b6 Drift Detection Method based on Hoeffding\u2019s bounds with moving average-test. HDDM_A is a drift detection method based on the Hoeffding\u2019s inequality. HDDM_A uses the average as estimator. It receives as input a stream of real values and returns the estimated status of the stream: STABLE, WARNING or DRIFT. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Implementation based on MOA. Parameters \u00b6 drift_confidence \u2013 defaults to 0.001 Confidence to the drift warning_confidence \u2013 defaults to 0.005 Confidence to the warning two_sided_test \u2013 defaults to False If True , will monitor error increments and decrements (two-sided). By default will only monitor increments (one-sided). Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import HDDM_A >>> np . random . seed ( 12345 ) >>> hddm_a = HDDM_A () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = hddm_a . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1013 , input value : 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected. References \u00b6 Fr\u00edas-Blanco I, del Campo-\u00c1vila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding\u2019s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"HDDM_A"},{"location":"api/drift/HDDM-A/#hddm_a","text":"Drift Detection Method based on Hoeffding\u2019s bounds with moving average-test. HDDM_A is a drift detection method based on the Hoeffding\u2019s inequality. HDDM_A uses the average as estimator. It receives as input a stream of real values and returns the estimated status of the stream: STABLE, WARNING or DRIFT. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Implementation based on MOA.","title":"HDDM_A"},{"location":"api/drift/HDDM-A/#parameters","text":"drift_confidence \u2013 defaults to 0.001 Confidence to the drift warning_confidence \u2013 defaults to 0.005 Confidence to the warning two_sided_test \u2013 defaults to False If True , will monitor error increments and decrements (two-sided). By default will only monitor increments (one-sided).","title":"Parameters"},{"location":"api/drift/HDDM-A/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/HDDM-A/#examples","text":">>> import numpy as np >>> from river.drift import HDDM_A >>> np . random . seed ( 12345 ) >>> hddm_a = HDDM_A () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = hddm_a . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1013 , input value : 1","title":"Examples"},{"location":"api/drift/HDDM-A/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"Methods"},{"location":"api/drift/HDDM-A/#references","text":"Fr\u00edas-Blanco I, del Campo-\u00c1vila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding\u2019s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"References"},{"location":"api/drift/HDDM-W/","text":"HDDM_W \u00b6 Drift Detection Method based on Hoeffding\u2019s bounds with moving weighted average-test. HDDM_W is an online drift detection method based on McDiarmid's bounds. HDDM_W uses the Exponentially Weighted Moving Average (EWMA) statistic as estimator. It receives as input a stream of real predictions and returns the estimated status of the stream: STABLE, WARNING or DRIFT. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Implementation based on MOA. Parameters \u00b6 drift_confidence \u2013 defaults to 0.001 Confidence to the drift warning_confidence \u2013 defaults to 0.005 Confidence to the warning lambda_option \u2013 defaults to 0.05 The weight given to recent data. Smaller values mean less weight given to recent data. two_sided_test \u2013 defaults to False If True, will monitor error increments and decrements (two-sided). By default will only monitor increments (one-sided). Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import HDDM_W >>> np . random . seed ( 12345 ) >>> hddm_w = HDDM_W () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = hddm_w . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1011 , input value : 1 Methods \u00b6 SampleInfo clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : tuple References \u00b6 Fr\u00edas-Blanco I, del Campo-\u00c1vila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding\u2019s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"HDDM_W"},{"location":"api/drift/HDDM-W/#hddm_w","text":"Drift Detection Method based on Hoeffding\u2019s bounds with moving weighted average-test. HDDM_W is an online drift detection method based on McDiarmid's bounds. HDDM_W uses the Exponentially Weighted Moving Average (EWMA) statistic as estimator. It receives as input a stream of real predictions and returns the estimated status of the stream: STABLE, WARNING or DRIFT. Input: value must be a binary signal, where 0 indicates error. For example, if a classifier's prediction \\(y'\\) is right or wrong w.r.t the true target label \\(y\\) : 0: Correct, \\(y=y'\\) 1: Error, \\(y \\neq y'\\) Implementation based on MOA.","title":"HDDM_W"},{"location":"api/drift/HDDM-W/#parameters","text":"drift_confidence \u2013 defaults to 0.001 Confidence to the drift warning_confidence \u2013 defaults to 0.005 Confidence to the warning lambda_option \u2013 defaults to 0.05 The weight given to recent data. Smaller values mean less weight given to recent data. two_sided_test \u2013 defaults to False If True, will monitor error increments and decrements (two-sided). By default will only monitor increments (one-sided).","title":"Parameters"},{"location":"api/drift/HDDM-W/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/HDDM-W/#examples","text":">>> import numpy as np >>> from river.drift import HDDM_W >>> np . random . seed ( 12345 ) >>> hddm_w = HDDM_W () >>> # Simulate a data stream as a normal distribution of 1's and 0's >>> data_stream = np . random . randint ( 2 , size = 2000 ) >>> # Change the data distribution from index 999 to 1500, simulating an >>> # increase in error rate (1 indicates error) >>> data_stream [ 999 : 1500 ] = 1 >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = hddm_w . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1011 , input value : 1","title":"Examples"},{"location":"api/drift/HDDM-W/#methods","text":"SampleInfo clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns typing.Tuple[bool, bool] : tuple","title":"Methods"},{"location":"api/drift/HDDM-W/#references","text":"Fr\u00edas-Blanco I, del Campo-\u00c1vila J, Ramos-Jimenez G, et al. Online and non-parametric drift detection methods based on Hoeffding\u2019s bounds. IEEE Transactions on Knowledge and Data Engineering, 2014, 27(3): 810-823. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"References"},{"location":"api/drift/KSWIN/","text":"KSWIN \u00b6 Kolmogorov-Smirnov Windowing method for concept drift detection. Parameters \u00b6 alpha \u2013 defaults to 0.005 Probability for the test statistic of the Kolmogorov-Smirnov-Test. The alpha parameter is very sensitive, therefore should be set below 0.01. window_size \u2013 defaults to 100 Size of the sliding window. stat_size \u2013 defaults to 30 Size of the statistic window. window \u2013 defaults to None Already collected data to avoid cold start. Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import KSWIN >>> np . random . seed ( 12345 ) >>> kswin = KSWIN () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = kswin . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1014 , input value : 5 Change detected at index 1828 , input value : 6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Adds an element on top of the sliding window and removes the oldest one from the window. Afterwards, the KS-test is performed. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected. Notes \u00b6 KSWIN (Kolmogorov-Smirnov Windowing) is a concept change detection method based on the Kolmogorov-Smirnov (KS) statistical test. KS-test is a statistical test with no assumption of underlying data distribution. KSWIN can monitor data or performance distributions. Note that the detector accepts one dimensional input as array. KSWIN maintains a sliding window \\(\\Psi\\) of fixed size \\(n\\) (window_size). The last \\(r\\) (stat_size) samples of \\(\\Psi\\) are assumed to represent the last concept considered as \\(R\\) . From the first \\(n-r\\) samples of \\(\\Psi\\) , \\(r\\) samples are uniformly drawn, representing an approximated last concept \\(W\\) . The KS-test is performed on the windows \\(R\\) and \\(W\\) of the same size. KS -test compares the distance of the empirical cumulative data distribution \\(dist(R,W)\\) . A concept drift is detected by KSWIN if: \\[ dist(R,W) > \\sqrt{-\\frac{ln\\alpha}{r}} \\] The difference in empirical data distributions between the windows \\(R\\) and \\(W\\) is too large since \\(R\\) and \\(W\\) come from the same distribution. References \u00b6 Christoph Raab, Moritz Heusinger, Frank-Michael Schleif, Reactive Soft Prototype Computing for Concept Drift Streams, Neurocomputing, 2020, \u21a9","title":"KSWIN"},{"location":"api/drift/KSWIN/#kswin","text":"Kolmogorov-Smirnov Windowing method for concept drift detection.","title":"KSWIN"},{"location":"api/drift/KSWIN/#parameters","text":"alpha \u2013 defaults to 0.005 Probability for the test statistic of the Kolmogorov-Smirnov-Test. The alpha parameter is very sensitive, therefore should be set below 0.01. window_size \u2013 defaults to 100 Size of the sliding window. stat_size \u2013 defaults to 30 Size of the statistic window. window \u2013 defaults to None Already collected data to avoid cold start.","title":"Parameters"},{"location":"api/drift/KSWIN/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/KSWIN/#examples","text":">>> import numpy as np >>> from river.drift import KSWIN >>> np . random . seed ( 12345 ) >>> kswin = KSWIN () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = kswin . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1014 , input value : 5 Change detected at index 1828 , input value : 6","title":"Examples"},{"location":"api/drift/KSWIN/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Adds an element on top of the sliding window and removes the oldest one from the window. Afterwards, the KS-test is performed. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"Methods"},{"location":"api/drift/KSWIN/#notes","text":"KSWIN (Kolmogorov-Smirnov Windowing) is a concept change detection method based on the Kolmogorov-Smirnov (KS) statistical test. KS-test is a statistical test with no assumption of underlying data distribution. KSWIN can monitor data or performance distributions. Note that the detector accepts one dimensional input as array. KSWIN maintains a sliding window \\(\\Psi\\) of fixed size \\(n\\) (window_size). The last \\(r\\) (stat_size) samples of \\(\\Psi\\) are assumed to represent the last concept considered as \\(R\\) . From the first \\(n-r\\) samples of \\(\\Psi\\) , \\(r\\) samples are uniformly drawn, representing an approximated last concept \\(W\\) . The KS-test is performed on the windows \\(R\\) and \\(W\\) of the same size. KS -test compares the distance of the empirical cumulative data distribution \\(dist(R,W)\\) . A concept drift is detected by KSWIN if: \\[ dist(R,W) > \\sqrt{-\\frac{ln\\alpha}{r}} \\] The difference in empirical data distributions between the windows \\(R\\) and \\(W\\) is too large since \\(R\\) and \\(W\\) come from the same distribution.","title":"Notes"},{"location":"api/drift/KSWIN/#references","text":"Christoph Raab, Moritz Heusinger, Frank-Michael Schleif, Reactive Soft Prototype Computing for Concept Drift Streams, Neurocomputing, 2020, \u21a9","title":"References"},{"location":"api/drift/PageHinkley/","text":"PageHinkley \u00b6 Page-Hinkley method for concept drift detection. This change detection method works by computing the observed values and their mean up to the current moment. Page-Hinkley does not signal warning zones, only change detections. The method works by means of the Page-Hinkley test. In general lines it will detect a concept drift if the observed mean at some instant is greater then a threshold value lambda. Parameters \u00b6 min_instances \u2013 defaults to 30 The minimum number of instances before detecting change. delta \u2013 defaults to 0.005 The delta factor for the Page Hinkley test. threshold \u2013 defaults to 50 The change detection threshold (lambda). alpha \u2013 defaults to 0.9999 The forgetting factor, used to weight the observed value and the mean. Attributes \u00b6 change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone. Examples \u00b6 >>> import numpy as np >>> from river.drift import PageHinkley >>> np . random . seed ( 12345 ) >>> ph = PageHinkley () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = ph . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1009 , input value : 5 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected. References \u00b6 E. S. Page. 1954. Continuous Inspection Schemes. Biometrika 41, 1/2 (1954), 100\u2013115. \u21a9","title":"PageHinkley"},{"location":"api/drift/PageHinkley/#pagehinkley","text":"Page-Hinkley method for concept drift detection. This change detection method works by computing the observed values and their mean up to the current moment. Page-Hinkley does not signal warning zones, only change detections. The method works by means of the Page-Hinkley test. In general lines it will detect a concept drift if the observed mean at some instant is greater then a threshold value lambda.","title":"PageHinkley"},{"location":"api/drift/PageHinkley/#parameters","text":"min_instances \u2013 defaults to 30 The minimum number of instances before detecting change. delta \u2013 defaults to 0.005 The delta factor for the Page Hinkley test. threshold \u2013 defaults to 50 The change detection threshold (lambda). alpha \u2013 defaults to 0.9999 The forgetting factor, used to weight the observed value and the mean.","title":"Parameters"},{"location":"api/drift/PageHinkley/#attributes","text":"change_detected Concept drift alarm. True if concept drift is detected. warning_detected Warning zone alarm. Indicates if the drift detector is in the warning zone. Applicability depends on each drift detector implementation. True if the change detector is in the warning zone.","title":"Attributes"},{"location":"api/drift/PageHinkley/#examples","text":">>> import numpy as np >>> from river.drift import PageHinkley >>> np . random . seed ( 12345 ) >>> ph = PageHinkley () >>> # Simulate a data stream composed by two data distributions >>> data_stream = np . concatenate (( np . random . randint ( 2 , size = 1000 ), ... np . random . randint ( 4 , high = 8 , size = 1000 ))) >>> # Update drift detector and verify if change is detected >>> for i , val in enumerate ( data_stream ): ... in_drift , in_warning = ph . update ( val ) ... if in_drift : ... print ( f \"Change detected at index { i } , input value: { val } \" ) Change detected at index 1009 , input value : 5","title":"Examples"},{"location":"api/drift/PageHinkley/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. reset Reset the change detector. update Update the change detector with a single data point. Parameters value ( numbers.Number ) Returns tuple : A tuple (drift, warning) where its elements indicate if a drift or a warning is detected.","title":"Methods"},{"location":"api/drift/PageHinkley/#references","text":"E. S. Page. 1954. Continuous Inspection Schemes. Biometrika 41, 1/2 (1954), 100\u2013115. \u21a9","title":"References"},{"location":"api/dummy/NoChangeClassifier/","text":"NoChangeClassifier \u00b6 Dummy classifier which returns the last class seen. The predict_one method will output the last class seen whilst predict_proba_one will return 1 for the last class seen and 0 for the others. Attributes \u00b6 last_class The last class seen. classes The set of classes seen. Examples \u00b6 Taken from example 2.1 from this page . >>> import pprint >>> from river import dummy >>> sentences = [ ... ( 'glad happy glad' , '+' ), ... ( 'glad glad joyful' , '+' ), ... ( 'glad pleasant' , '+' ), ... ( 'miserable sad glad' , '\u2212' ) ... ] >>> model = dummy . NoChangeClassifier () >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) '\u2212' >>> pprint . pprint ( model . predict_proba_one ( new_sentence )) { '+' : 0 , '\u2212' : 1 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"NoChangeClassifier"},{"location":"api/dummy/NoChangeClassifier/#nochangeclassifier","text":"Dummy classifier which returns the last class seen. The predict_one method will output the last class seen whilst predict_proba_one will return 1 for the last class seen and 0 for the others.","title":"NoChangeClassifier"},{"location":"api/dummy/NoChangeClassifier/#attributes","text":"last_class The last class seen. classes The set of classes seen.","title":"Attributes"},{"location":"api/dummy/NoChangeClassifier/#examples","text":"Taken from example 2.1 from this page . >>> import pprint >>> from river import dummy >>> sentences = [ ... ( 'glad happy glad' , '+' ), ... ( 'glad glad joyful' , '+' ), ... ( 'glad pleasant' , '+' ), ... ( 'miserable sad glad' , '\u2212' ) ... ] >>> model = dummy . NoChangeClassifier () >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) '\u2212' >>> pprint . pprint ( model . predict_proba_one ( new_sentence )) { '+' : 0 , '\u2212' : 1 }","title":"Examples"},{"location":"api/dummy/NoChangeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/dummy/PriorClassifier/","text":"PriorClassifier \u00b6 Dummy classifier which uses the prior distribution. The predict_one method will output the most common class whilst predict_proba_one will return the normalized class counts. Attributes \u00b6 counts ( collections.Counter ) Class counts. n ( int ) Total number of seen instances. Examples \u00b6 Taken from example 2.1 from this page >>> from river import dummy >>> sentences = [ ... ( 'glad happy glad' , '+' ), ... ( 'glad glad joyful' , '+' ), ... ( 'glad pleasant' , '+' ), ... ( 'miserable sad glad' , '\u2212' ) ... ] >>> model = dummy . PriorClassifier () >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) '+' >>> model . predict_proba_one ( new_sentence ) { '+' : 0.75 , '\u2212' : 0.25 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"PriorClassifier"},{"location":"api/dummy/PriorClassifier/#priorclassifier","text":"Dummy classifier which uses the prior distribution. The predict_one method will output the most common class whilst predict_proba_one will return the normalized class counts.","title":"PriorClassifier"},{"location":"api/dummy/PriorClassifier/#attributes","text":"counts ( collections.Counter ) Class counts. n ( int ) Total number of seen instances.","title":"Attributes"},{"location":"api/dummy/PriorClassifier/#examples","text":"Taken from example 2.1 from this page >>> from river import dummy >>> sentences = [ ... ( 'glad happy glad' , '+' ), ... ( 'glad glad joyful' , '+' ), ... ( 'glad pleasant' , '+' ), ... ( 'miserable sad glad' , '\u2212' ) ... ] >>> model = dummy . PriorClassifier () >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) '+' >>> model . predict_proba_one ( new_sentence ) { '+' : 0.75 , '\u2212' : 0.25 }","title":"Examples"},{"location":"api/dummy/PriorClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/dummy/StatisticRegressor/","text":"StatisticRegressor \u00b6 Dummy regressor that uses a univariate statistic to make predictions. Parameters \u00b6 statistic ( river.stats.base.Univariate ) Examples \u00b6 >>> from pprint import pprint >>> from river import dummy >>> from river import stats >>> sentences = [ ... ( 'glad happy glad' , 3 ), ... ( 'glad glad joyful' , 3 ), ... ( 'glad pleasant' , 2 ), ... ( 'miserable sad glad' , - 3 ) ... ] >>> model = dummy . StatisticRegressor ( stats . Mean ()) >>> for sentence , score in sentences : ... model = model . learn_one ( sentence , score ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) 1.25 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"StatisticRegressor"},{"location":"api/dummy/StatisticRegressor/#statisticregressor","text":"Dummy regressor that uses a univariate statistic to make predictions.","title":"StatisticRegressor"},{"location":"api/dummy/StatisticRegressor/#parameters","text":"statistic ( river.stats.base.Univariate )","title":"Parameters"},{"location":"api/dummy/StatisticRegressor/#examples","text":">>> from pprint import pprint >>> from river import dummy >>> from river import stats >>> sentences = [ ... ( 'glad happy glad' , 3 ), ... ( 'glad glad joyful' , 3 ), ... ( 'glad pleasant' , 2 ), ... ( 'miserable sad glad' , - 3 ) ... ] >>> model = dummy . StatisticRegressor ( stats . Mean ()) >>> for sentence , score in sentences : ... model = model . learn_one ( sentence , score ) >>> new_sentence = 'glad sad miserable pleasant glad' >>> model . predict_one ( new_sentence ) 1.25","title":"Examples"},{"location":"api/dummy/StatisticRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/ensemble/ADWINBaggingClassifier/","text":"ADWINBaggingClassifier \u00b6 ADWIN Bagging classifier. ADWIN Bagging 1 is the online bagging method of Oza and Russell 2 with the addition of the ADWIN algorithm as a change detector. If concept drift is detected, the worst member of the ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier. Parameters \u00b6 model ( base.Classifier ) The classifier to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . ADWINBaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878788 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x References \u00b6 Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Richard Kirkby, and Ricard Gavald\u00e0. \"New ensemble methods for evolving data streams.\" In 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009. \u21a9 Oza, N., Russell, S. \"Online bagging and boosting.\" In: Artificial Intelligence and Statistics 2001, pp. 105\u2013112. Morgan Kaufmann, 2001. \u21a9","title":"ADWINBaggingClassifier"},{"location":"api/ensemble/ADWINBaggingClassifier/#adwinbaggingclassifier","text":"ADWIN Bagging classifier. ADWIN Bagging 1 is the online bagging method of Oza and Russell 2 with the addition of the ADWIN algorithm as a change detector. If concept drift is detected, the worst member of the ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier.","title":"ADWINBaggingClassifier"},{"location":"api/ensemble/ADWINBaggingClassifier/#parameters","text":"model ( base.Classifier ) The classifier to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/ensemble/ADWINBaggingClassifier/#examples","text":">>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . ADWINBaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878788","title":"Examples"},{"location":"api/ensemble/ADWINBaggingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x","title":"Methods"},{"location":"api/ensemble/ADWINBaggingClassifier/#references","text":"Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Richard Kirkby, and Ricard Gavald\u00e0. \"New ensemble methods for evolving data streams.\" In 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009. \u21a9 Oza, N., Russell, S. \"Online bagging and boosting.\" In: Artificial Intelligence and Statistics 2001, pp. 105\u2013112. Morgan Kaufmann, 2001. \u21a9","title":"References"},{"location":"api/ensemble/AdaBoostClassifier/","text":"AdaBoostClassifier \u00b6 Boosting for classification For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter lambda. The lambda parameter is updated when the weaks learners fit successively the same observation. Parameters \u00b6 model ( base.Classifier ) The classifier to boost. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Attributes \u00b6 wrong_weight ( collections.defaultdict ) Number of times a model has made a mistake when making predictions. correct_weight ( collections.defaultdict ) Number of times a model has predicted the right label when making predictions. Examples \u00b6 In the following example three tree classifiers are boosted together. The performance is slightly better than when using a single tree. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> dataset = datasets . Phishing () >>> metric = metrics . LogLoss () >>> model = ensemble . AdaBoostClassifier ( ... model = ( ... tree . HoeffdingTreeClassifier ( ... split_criterion = 'gini' , ... split_confidence = 1e-5 , ... grace_period = 2000 ... ) ... ), ... n_models = 5 , ... seed = 42 ... ) >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.364558 >>> print ( model ) AdaBoostClassifier ( HoeffdingTreeClassifier ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"AdaBoostClassifier"},{"location":"api/ensemble/AdaBoostClassifier/#adaboostclassifier","text":"Boosting for classification For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter lambda. The lambda parameter is updated when the weaks learners fit successively the same observation.","title":"AdaBoostClassifier"},{"location":"api/ensemble/AdaBoostClassifier/#parameters","text":"model ( base.Classifier ) The classifier to boost. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/ensemble/AdaBoostClassifier/#attributes","text":"wrong_weight ( collections.defaultdict ) Number of times a model has made a mistake when making predictions. correct_weight ( collections.defaultdict ) Number of times a model has predicted the right label when making predictions.","title":"Attributes"},{"location":"api/ensemble/AdaBoostClassifier/#examples","text":"In the following example three tree classifiers are boosted together. The performance is slightly better than when using a single tree. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> dataset = datasets . Phishing () >>> metric = metrics . LogLoss () >>> model = ensemble . AdaBoostClassifier ( ... model = ( ... tree . HoeffdingTreeClassifier ( ... split_criterion = 'gini' , ... split_confidence = 1e-5 , ... grace_period = 2000 ... ) ... ), ... n_models = 5 , ... seed = 42 ... ) >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.364558 >>> print ( model ) AdaBoostClassifier ( HoeffdingTreeClassifier )","title":"Examples"},{"location":"api/ensemble/AdaBoostClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/ensemble/AdaBoostClassifier/#references","text":"Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"References"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/","text":"AdaptiveRandomForestClassifier \u00b6 Adaptive Random Forest classifier. The 3 most important aspects of Adaptive Random Forest 1 are: inducing diversity through re-sampling inducing diversity through randomly selecting subsets of features for node splits drift detectors per base tree, which cause selective resets in response to drifts It also allows training background trees, which start training if a warning is detected and replace the active tree if the warning escalates to a drift. Parameters \u00b6 n_models ( int ) \u2013 defaults to 10 Number of trees in the ensemble. max_features ( Union[bool, str, int] ) \u2013 defaults to sqrt Max number of attributes for each node split. - If int , then consider max_features at each split. - If float , then max_features is a percentage and int(max_features * n_features) features are considered per split. - If \"sqrt\", then max_features=sqrt(n_features) . - If \"log2\", then max_features=log2(n_features) . - If None, then max_features=n_features . lambda_value ( int ) \u2013 defaults to 6 The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging). metric ( river.metrics.base.MultiClassMetric ) \u2013 defaults to Accuracy: 0.00% Metric used to track trees performance within the ensemble. disable_weighted_vote \u2013 defaults to False If True , disables the weighted vote prediction. drift_detector ( Union[ base.DriftDetector , NoneType] ) \u2013 defaults to ADWIN Drift Detection method. Set to None to disable Drift detection. warning_detector ( Union[ base.DriftDetector , NoneType] ) \u2013 defaults to ADWIN Warning Detection method. Set to None to disable warning detection. grace_period ( int ) \u2013 defaults to 50 [ Tree parameter ] Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None [ Tree parameter ] The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain [ Tree parameter ] Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Hellinger Distance split_confidence ( float ) \u2013 defaults to 0.01 [ Tree parameter ] Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 [ Tree parameter ] Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba [ Tree parameter ] Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 [ Tree parameter ] Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None [ Tree parameter ] List of Nominal attributes. If empty, then assume that all attributes are numerical. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None [ Tree parameter ] The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False [ Tree parameter ] If True, only allow binary splits. max_size ( int ) \u2013 defaults to 32 [ Tree parameter ] Maximum memory (MB) consumed by the tree. memory_estimate_period ( int ) \u2013 defaults to 2000000 [ Tree parameter ] Number of instances between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False [ Tree parameter ] If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False [ Tree parameter ] If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True [ Tree parameter ] If True, enable merit-based tree pre-pruning. seed ( int ) \u2013 defaults to None If int , seed is used to seed the random number generator; If RandomState , seed is the random number generator; If None , the random number generator is the RandomState instance used by np.random . Examples \u00b6 >>> from river import synth >>> from river import ensemble >>> from river import evaluate >>> from river import metrics >>> dataset = synth . ConceptDriftStream ( seed = 42 , position = 500 , ... width = 40 ) . take ( 1000 ) >>> model = ensemble . AdaptiveRandomForestClassifier ( ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 70.47 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. reset Reset the forest. References \u00b6 Heitor Murilo Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal, Fabricio Enembreck, Bernhard Pfharinger, Geoff Holmes, Talel Abdessalem. Adaptive random forests for evolving data stream classification. In Machine Learning, DOI: 10.1007/s10994-017-5642-8, Springer, 2017. \u21a9","title":"AdaptiveRandomForestClassifier"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/#adaptiverandomforestclassifier","text":"Adaptive Random Forest classifier. The 3 most important aspects of Adaptive Random Forest 1 are: inducing diversity through re-sampling inducing diversity through randomly selecting subsets of features for node splits drift detectors per base tree, which cause selective resets in response to drifts It also allows training background trees, which start training if a warning is detected and replace the active tree if the warning escalates to a drift.","title":"AdaptiveRandomForestClassifier"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/#parameters","text":"n_models ( int ) \u2013 defaults to 10 Number of trees in the ensemble. max_features ( Union[bool, str, int] ) \u2013 defaults to sqrt Max number of attributes for each node split. - If int , then consider max_features at each split. - If float , then max_features is a percentage and int(max_features * n_features) features are considered per split. - If \"sqrt\", then max_features=sqrt(n_features) . - If \"log2\", then max_features=log2(n_features) . - If None, then max_features=n_features . lambda_value ( int ) \u2013 defaults to 6 The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging). metric ( river.metrics.base.MultiClassMetric ) \u2013 defaults to Accuracy: 0.00% Metric used to track trees performance within the ensemble. disable_weighted_vote \u2013 defaults to False If True , disables the weighted vote prediction. drift_detector ( Union[ base.DriftDetector , NoneType] ) \u2013 defaults to ADWIN Drift Detection method. Set to None to disable Drift detection. warning_detector ( Union[ base.DriftDetector , NoneType] ) \u2013 defaults to ADWIN Warning Detection method. Set to None to disable warning detection. grace_period ( int ) \u2013 defaults to 50 [ Tree parameter ] Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None [ Tree parameter ] The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain [ Tree parameter ] Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Hellinger Distance split_confidence ( float ) \u2013 defaults to 0.01 [ Tree parameter ] Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 [ Tree parameter ] Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba [ Tree parameter ] Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 [ Tree parameter ] Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None [ Tree parameter ] List of Nominal attributes. If empty, then assume that all attributes are numerical. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None [ Tree parameter ] The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False [ Tree parameter ] If True, only allow binary splits. max_size ( int ) \u2013 defaults to 32 [ Tree parameter ] Maximum memory (MB) consumed by the tree. memory_estimate_period ( int ) \u2013 defaults to 2000000 [ Tree parameter ] Number of instances between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False [ Tree parameter ] If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False [ Tree parameter ] If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True [ Tree parameter ] If True, enable merit-based tree pre-pruning. seed ( int ) \u2013 defaults to None If int , seed is used to seed the random number generator; If RandomState , seed is the random number generator; If None , the random number generator is the RandomState instance used by np.random .","title":"Parameters"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/#examples","text":">>> from river import synth >>> from river import ensemble >>> from river import evaluate >>> from river import metrics >>> dataset = synth . ConceptDriftStream ( seed = 42 , position = 500 , ... width = 40 ) . take ( 1000 ) >>> model = ensemble . AdaptiveRandomForestClassifier ( ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 70.47 %","title":"Examples"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. reset Reset the forest.","title":"Methods"},{"location":"api/ensemble/AdaptiveRandomForestClassifier/#references","text":"Heitor Murilo Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal, Fabricio Enembreck, Bernhard Pfharinger, Geoff Holmes, Talel Abdessalem. Adaptive random forests for evolving data stream classification. In Machine Learning, DOI: 10.1007/s10994-017-5642-8, Springer, 2017. \u21a9","title":"References"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/","text":"AdaptiveRandomForestRegressor \u00b6 Adaptive Random Forest regressor. The 3 most important aspects of Adaptive Random Forest 1 are: inducing diversity through re-sampling inducing diversity through randomly selecting subsets of features for node splits drift detectors per base tree, which cause selective resets in response to drifts Notice that this implementation is slightly different from the original algorithm proposed in 2 . The HoeffdingTreeRegressor is used as base learner, instead of FIMT-DD . It also adds a new strategy to monitor the predictions and check for concept drifts. The deviations of the predictions to the target are monitored and normalized in the [0, 1] range to fulfill ADWIN's requirements. We assume that the data subjected to the normalization follows a normal distribution, and thus, lies within the interval of the mean \\(\\pm3\\sigma\\) . Parameters \u00b6 n_models ( int ) \u2013 defaults to 10 Number of trees in the ensemble. max_features \u2013 defaults to sqrt Max number of attributes for each node split. - If int , then consider max_features at each split. - If float , then max_features is a percentage and int(max_features * n_features) features are considered per split. - If \"sqrt\", then max_features=sqrt(n_features) . - If \"log2\", then max_features=log2(n_features) . - If None, then max_features=n_features . aggregation_method ( str ) \u2013 defaults to median The method to use to aggregate predictions in the ensemble. - 'mean' - 'median' - If selected will disable the weighted vote. lambda_value ( int ) \u2013 defaults to 6 The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging). metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to MSE: 0. Metric used to track trees performance within the ensemble. Depending, on the configuration, this metric is also used to weight predictions from the members of the ensemble. disable_weighted_vote \u2013 defaults to True If True , disables the weighted vote prediction, i.e. does not assign weights to individual tree's predictions and uses the arithmetic mean instead. Otherwise will use the metric value to weight predictions. drift_detector ( base.DriftDetector ) \u2013 defaults to ADWIN Drift Detection method. Set to None to disable Drift detection. warning_detector ( base.DriftDetector ) \u2013 defaults to ADWIN Warning Detection method. Set to None to disable warning detection. grace_period ( int ) \u2013 defaults to 50 [ Tree parameter ] Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None [ Tree parameter ] The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 0.01 [ Tree parameter ] Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 [ Tree parameter ] Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model [ Tree parameter ] Prediction mechanism used at leaves. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None [ Tree parameter ] The regression model used to provide responses if leaf_prediction='model' . If not provided, an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 [ Tree parameter ] The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None [ Tree parameter ] List of Nominal attributes. If empty, then assume that all attributes are numerical. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None [ Tree parameter ] The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters.By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 [ Tree parameter ] The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False [ Tree parameter ] If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 [ Tree parameter ] Maximum memory (MB) consumed by the tree. memory_estimate_period ( int ) \u2013 defaults to 2000000 [ Tree parameter ] Number of instances between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False [ Tree parameter ] If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False [ Tree parameter ] If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True [ Tree parameter ] If True, enable merit-based tree pre-pruning. seed ( int ) \u2013 defaults to None If int , seed is used to seed the random number generator; If RandomState , seed is the random number generator; If None , the random number generator is the RandomState instance used by np.random . Attributes \u00b6 valid_aggregation_method Valid aggregation_method values. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import ensemble >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... ensemble . AdaptiveRandomForestRegressor ( n_models = 3 , seed = 42 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.870913 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. reset Reset the forest. References \u00b6 Gomes, H.M., Bifet, A., Read, J., Barddal, J.P., Enembreck, F., Pfharinger, B., Holmes, G. and Abdessalem, T., 2017. Adaptive random forests for evolving data stream classification. Machine Learning, 106(9-10), pp.1469-1495. \u21a9 Gomes, H.M., Barddal, J.P., Boiko, L.E., Bifet, A., 2018. Adaptive random forests for data stream regression. ESANN 2018. \u21a9","title":"AdaptiveRandomForestRegressor"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#adaptiverandomforestregressor","text":"Adaptive Random Forest regressor. The 3 most important aspects of Adaptive Random Forest 1 are: inducing diversity through re-sampling inducing diversity through randomly selecting subsets of features for node splits drift detectors per base tree, which cause selective resets in response to drifts Notice that this implementation is slightly different from the original algorithm proposed in 2 . The HoeffdingTreeRegressor is used as base learner, instead of FIMT-DD . It also adds a new strategy to monitor the predictions and check for concept drifts. The deviations of the predictions to the target are monitored and normalized in the [0, 1] range to fulfill ADWIN's requirements. We assume that the data subjected to the normalization follows a normal distribution, and thus, lies within the interval of the mean \\(\\pm3\\sigma\\) .","title":"AdaptiveRandomForestRegressor"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#parameters","text":"n_models ( int ) \u2013 defaults to 10 Number of trees in the ensemble. max_features \u2013 defaults to sqrt Max number of attributes for each node split. - If int , then consider max_features at each split. - If float , then max_features is a percentage and int(max_features * n_features) features are considered per split. - If \"sqrt\", then max_features=sqrt(n_features) . - If \"log2\", then max_features=log2(n_features) . - If None, then max_features=n_features . aggregation_method ( str ) \u2013 defaults to median The method to use to aggregate predictions in the ensemble. - 'mean' - 'median' - If selected will disable the weighted vote. lambda_value ( int ) \u2013 defaults to 6 The lambda value for bagging (lambda=6 corresponds to Leveraging Bagging). metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to MSE: 0. Metric used to track trees performance within the ensemble. Depending, on the configuration, this metric is also used to weight predictions from the members of the ensemble. disable_weighted_vote \u2013 defaults to True If True , disables the weighted vote prediction, i.e. does not assign weights to individual tree's predictions and uses the arithmetic mean instead. Otherwise will use the metric value to weight predictions. drift_detector ( base.DriftDetector ) \u2013 defaults to ADWIN Drift Detection method. Set to None to disable Drift detection. warning_detector ( base.DriftDetector ) \u2013 defaults to ADWIN Warning Detection method. Set to None to disable warning detection. grace_period ( int ) \u2013 defaults to 50 [ Tree parameter ] Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None [ Tree parameter ] The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 0.01 [ Tree parameter ] Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 [ Tree parameter ] Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model [ Tree parameter ] Prediction mechanism used at leaves. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None [ Tree parameter ] The regression model used to provide responses if leaf_prediction='model' . If not provided, an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 [ Tree parameter ] The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None [ Tree parameter ] List of Nominal attributes. If empty, then assume that all attributes are numerical. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None [ Tree parameter ] The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters.By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 [ Tree parameter ] The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False [ Tree parameter ] If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 [ Tree parameter ] Maximum memory (MB) consumed by the tree. memory_estimate_period ( int ) \u2013 defaults to 2000000 [ Tree parameter ] Number of instances between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False [ Tree parameter ] If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False [ Tree parameter ] If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True [ Tree parameter ] If True, enable merit-based tree pre-pruning. seed ( int ) \u2013 defaults to None If int , seed is used to seed the random number generator; If RandomState , seed is the random number generator; If None , the random number generator is the RandomState instance used by np.random .","title":"Parameters"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#attributes","text":"valid_aggregation_method Valid aggregation_method values.","title":"Attributes"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import ensemble >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... ensemble . AdaptiveRandomForestRegressor ( n_models = 3 , seed = 42 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.870913","title":"Examples"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. reset Reset the forest.","title":"Methods"},{"location":"api/ensemble/AdaptiveRandomForestRegressor/#references","text":"Gomes, H.M., Bifet, A., Read, J., Barddal, J.P., Enembreck, F., Pfharinger, B., Holmes, G. and Abdessalem, T., 2017. Adaptive random forests for evolving data stream classification. Machine Learning, 106(9-10), pp.1469-1495. \u21a9 Gomes, H.M., Barddal, J.P., Boiko, L.E., Bifet, A., 2018. Adaptive random forests for data stream regression. ESANN 2018. \u21a9","title":"References"},{"location":"api/ensemble/BaggingClassifier/","text":"BaggingClassifier \u00b6 Online bootstrap aggregation for classification. For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter 1. k thus has a 36% chance of being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6% chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do scipy.stats.poisson(1).pmf(k) to obtain more detailed values. Parameters \u00b6 model ( base.Classifier ) The classifier to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Examples \u00b6 In the following example three logistic regressions are bagged together. The performance is slightly better than when using a single logistic regression. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . BaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.877788 >>> print ( model ) BaggingClassifier ( StandardScaler | LogisticRegression ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x References \u00b6 Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"BaggingClassifier"},{"location":"api/ensemble/BaggingClassifier/#baggingclassifier","text":"Online bootstrap aggregation for classification. For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter 1. k thus has a 36% chance of being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6% chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do scipy.stats.poisson(1).pmf(k) to obtain more detailed values.","title":"BaggingClassifier"},{"location":"api/ensemble/BaggingClassifier/#parameters","text":"model ( base.Classifier ) The classifier to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/ensemble/BaggingClassifier/#examples","text":"In the following example three logistic regressions are bagged together. The performance is slightly better than when using a single logistic regression. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . BaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.877788 >>> print ( model ) BaggingClassifier ( StandardScaler | LogisticRegression )","title":"Examples"},{"location":"api/ensemble/BaggingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x","title":"Methods"},{"location":"api/ensemble/BaggingClassifier/#references","text":"Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"References"},{"location":"api/ensemble/BaggingRegressor/","text":"BaggingRegressor \u00b6 Online bootstrap aggregation for regression. For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter 1. k thus has a 36% chance of being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6% chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do scipy.stats.poisson(1).pmf(k) for more detailed values. Parameters \u00b6 model ( base.Regressor ) The regressor to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Examples \u00b6 In the following example three logistic regressions are bagged together. The performance is slightly better than when using a single logistic regression. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = preprocessing . StandardScaler () >>> model |= ensemble . BaggingRegressor ( ... model = linear_model . LinearRegression ( intercept_lr = 0.1 ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.641799 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one Averages the predictions of each regressor. Parameters x References \u00b6 Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"BaggingRegressor"},{"location":"api/ensemble/BaggingRegressor/#baggingregressor","text":"Online bootstrap aggregation for regression. For each incoming observation, each model's learn_one method is called k times where k is sampled from a Poisson distribution of parameter 1. k thus has a 36% chance of being equal to 0, a 36% chance of being equal to 1, an 18% chance of being equal to 2, a 6% chance of being equal to 3, a 1% chance of being equal to 4, etc. You can do scipy.stats.poisson(1).pmf(k) for more detailed values.","title":"BaggingRegressor"},{"location":"api/ensemble/BaggingRegressor/#parameters","text":"model ( base.Regressor ) The regressor to bag. n_models \u2013 defaults to 10 The number of models in the ensemble. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/ensemble/BaggingRegressor/#examples","text":"In the following example three logistic regressions are bagged together. The performance is slightly better than when using a single logistic regression. >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = preprocessing . StandardScaler () >>> model |= ensemble . BaggingRegressor ( ... model = linear_model . LinearRegression ( intercept_lr = 0.1 ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.641799","title":"Examples"},{"location":"api/ensemble/BaggingRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one Averages the predictions of each regressor. Parameters x","title":"Methods"},{"location":"api/ensemble/BaggingRegressor/#references","text":"Oza, N.C., 2005, October. Online bagging and boosting. In 2005 IEEE international conference on systems, man and cybernetics (Vol. 3, pp. 2340-2345). Ieee. \u21a9","title":"References"},{"location":"api/ensemble/LeveragingBaggingClassifier/","text":"LeveragingBaggingClassifier \u00b6 Leveraging Bagging ensemble classifier. Leveraging Bagging [^1] is an improvement over the Oza Bagging algorithm. The bagging performance is leveraged by increasing the re-sampling. It uses a poisson distribution to simulate the re-sampling process. To increase re-sampling it uses a higher w value of the Poisson distribution (agerage number of events), 6 by default, increasing the input space diversity, by attributing a different range of weights to the data samples. To deal with concept drift, Leveraging Bagging uses the ADWIN algorithm to monitor the performance of each member of the enemble If concept drift is detected, the worst member of the ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier. Parameters \u00b6 model ( base.Classifier ) The classifier to bag. n_models ( int ) \u2013 defaults to 10 The number of models in the ensemble. w ( float ) \u2013 defaults to 6 Indicates the average number of events. This is the lambda parameter of the Poisson distribution used to compute the re-sampling weight. adwin_delta ( float ) \u2013 defaults to 0.002 The delta parameter for the ADWIN change detector. bagging_method ( str ) \u2013 defaults to bag The bagging method to use. Can be one of the following: * 'bag' - Leveraging Bagging using ADWIN. * 'me' - Assigns \\(weight=1\\) if sample is misclassified, otherwise \\(weight=error/(1-error)\\) . * 'half' - Use resampling without replacement for half of the instances. * 'wt' - Resample without taking out all instances. * 'subag' - Resampling without replacement. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Attributes \u00b6 bagging_methods Valid bagging_method options. Examples \u00b6 >>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . LeveragingBaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.886282 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x","title":"LeveragingBaggingClassifier"},{"location":"api/ensemble/LeveragingBaggingClassifier/#leveragingbaggingclassifier","text":"Leveraging Bagging ensemble classifier. Leveraging Bagging [^1] is an improvement over the Oza Bagging algorithm. The bagging performance is leveraged by increasing the re-sampling. It uses a poisson distribution to simulate the re-sampling process. To increase re-sampling it uses a higher w value of the Poisson distribution (agerage number of events), 6 by default, increasing the input space diversity, by attributing a different range of weights to the data samples. To deal with concept drift, Leveraging Bagging uses the ADWIN algorithm to monitor the performance of each member of the enemble If concept drift is detected, the worst member of the ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier.","title":"LeveragingBaggingClassifier"},{"location":"api/ensemble/LeveragingBaggingClassifier/#parameters","text":"model ( base.Classifier ) The classifier to bag. n_models ( int ) \u2013 defaults to 10 The number of models in the ensemble. w ( float ) \u2013 defaults to 6 Indicates the average number of events. This is the lambda parameter of the Poisson distribution used to compute the re-sampling weight. adwin_delta ( float ) \u2013 defaults to 0.002 The delta parameter for the ADWIN change detector. bagging_method ( str ) \u2013 defaults to bag The bagging method to use. Can be one of the following: * 'bag' - Leveraging Bagging using ADWIN. * 'me' - Assigns \\(weight=1\\) if sample is misclassified, otherwise \\(weight=error/(1-error)\\) . * 'half' - Use resampling without replacement for half of the instances. * 'wt' - Resample without taking out all instances. * 'subag' - Resampling without replacement. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/ensemble/LeveragingBaggingClassifier/#attributes","text":"bagging_methods Valid bagging_method options.","title":"Attributes"},{"location":"api/ensemble/LeveragingBaggingClassifier/#examples","text":">>> from river import datasets >>> from river import ensemble >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ensemble . LeveragingBaggingClassifier ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... n_models = 3 , ... seed = 42 ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.886282","title":"Examples"},{"location":"api/ensemble/LeveragingBaggingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Averages the predictions of each classifier. Parameters x","title":"Methods"},{"location":"api/ensemble/SRPClassifier/","text":"SRPClassifier \u00b6 Streaming Random Patches ensemble classifier. The Streaming Random Patches (SRP) 1 is an ensemble method that simulates bagging or random subspaces. The default algorithm uses both bagging and random subspaces, namely Random Patches. The default base estimator is a Hoeffding Tree, but other base estimators can be used (differently from random forest variations). Parameters \u00b6 model ( base.Classifier ) \u2013 defaults to None The base estimator. n_models ( int ) \u2013 defaults to 100 Number of members in the ensemble. subspace_size ( Union[int, float, str] ) \u2013 defaults to 0.6 Number of features per subset for each classifier where M is the total number of features. A negative value means M - subspace_size . Only applies when using random subspaces or random patches. * If int indicates the number of features to use. Valid range [2, M]. * If float indicates the percentage of features to use, Valid range (0., 1.]. * 'sqrt' - sqrt(M)+1 * 'rmsqrt' - Residual from M-(sqrt(M)+1) training_method ( str ) \u2013 defaults to patches The training method to use. * 'subspaces' - Random subspaces. * 'resampling' - Resampling. * 'patches' - Random patches. lam ( float ) \u2013 defaults to 6.0 Lambda value for resampling. drift_detector ( base.DriftDetector ) \u2013 defaults to None Drift detector. warning_detector ( base.DriftDetector ) \u2013 defaults to None Warning detector. disable_detector ( str ) \u2013 defaults to off Option to disable drift detectors: * If 'off' , detectors are enabled. * If 'drift' , disables concept drift detection and the background learner. * If 'warning' , disables the background learner and ensemble members are reset if drift is detected. disable_weighted_vote ( bool ) \u2013 defaults to False If True, disables weighted voting. nominal_attributes \u2013 defaults to None List of Nominal attributes. If empty, then assumes that all attributes are numerical. Note: Only applies if the base model allows to define the nominal attributes. seed \u2013 defaults to None Random number generator seed for reproducibility. metric ( river.metrics.base.MultiClassMetric ) \u2013 defaults to None Metric to track members performance within the ensemble. Examples \u00b6 >>> from river import synth >>> from river import ensemble >>> from river import tree >>> from river import evaluate >>> from river import metrics >>> dataset = synth . ConceptDriftStream ( seed = 42 , position = 500 , ... width = 50 ) . take ( 1000 ) >>> base_model = tree . HoeffdingTreeClassifier ( ... grace_period = 50 , split_confidence = 0.01 , ... nominal_attributes = [ 'age' , 'car' , 'zipcode' ] ... ) >>> model = ensemble . SRPClassifier ( ... model = base_model , n_models = 3 , seed = 42 , ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) # doctest: +SKIP Accuracy : 70.97 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. reset References \u00b6 Heitor Murilo Gomes, Jesse Read, Albert Bifet. Streaming Random Patches for Evolving Data Stream Classification. IEEE International Conference on Data Mining (ICDM), 2019. \u21a9","title":"SRPClassifier"},{"location":"api/ensemble/SRPClassifier/#srpclassifier","text":"Streaming Random Patches ensemble classifier. The Streaming Random Patches (SRP) 1 is an ensemble method that simulates bagging or random subspaces. The default algorithm uses both bagging and random subspaces, namely Random Patches. The default base estimator is a Hoeffding Tree, but other base estimators can be used (differently from random forest variations).","title":"SRPClassifier"},{"location":"api/ensemble/SRPClassifier/#parameters","text":"model ( base.Classifier ) \u2013 defaults to None The base estimator. n_models ( int ) \u2013 defaults to 100 Number of members in the ensemble. subspace_size ( Union[int, float, str] ) \u2013 defaults to 0.6 Number of features per subset for each classifier where M is the total number of features. A negative value means M - subspace_size . Only applies when using random subspaces or random patches. * If int indicates the number of features to use. Valid range [2, M]. * If float indicates the percentage of features to use, Valid range (0., 1.]. * 'sqrt' - sqrt(M)+1 * 'rmsqrt' - Residual from M-(sqrt(M)+1) training_method ( str ) \u2013 defaults to patches The training method to use. * 'subspaces' - Random subspaces. * 'resampling' - Resampling. * 'patches' - Random patches. lam ( float ) \u2013 defaults to 6.0 Lambda value for resampling. drift_detector ( base.DriftDetector ) \u2013 defaults to None Drift detector. warning_detector ( base.DriftDetector ) \u2013 defaults to None Warning detector. disable_detector ( str ) \u2013 defaults to off Option to disable drift detectors: * If 'off' , detectors are enabled. * If 'drift' , disables concept drift detection and the background learner. * If 'warning' , disables the background learner and ensemble members are reset if drift is detected. disable_weighted_vote ( bool ) \u2013 defaults to False If True, disables weighted voting. nominal_attributes \u2013 defaults to None List of Nominal attributes. If empty, then assumes that all attributes are numerical. Note: Only applies if the base model allows to define the nominal attributes. seed \u2013 defaults to None Random number generator seed for reproducibility. metric ( river.metrics.base.MultiClassMetric ) \u2013 defaults to None Metric to track members performance within the ensemble.","title":"Parameters"},{"location":"api/ensemble/SRPClassifier/#examples","text":">>> from river import synth >>> from river import ensemble >>> from river import tree >>> from river import evaluate >>> from river import metrics >>> dataset = synth . ConceptDriftStream ( seed = 42 , position = 500 , ... width = 50 ) . take ( 1000 ) >>> base_model = tree . HoeffdingTreeClassifier ( ... grace_period = 50 , split_confidence = 0.01 , ... nominal_attributes = [ 'age' , 'car' , 'zipcode' ] ... ) >>> model = ensemble . SRPClassifier ( ... model = base_model , n_models = 3 , seed = 42 , ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) # doctest: +SKIP Accuracy : 70.97 %","title":"Examples"},{"location":"api/ensemble/SRPClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) kwargs Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. reset","title":"Methods"},{"location":"api/ensemble/SRPClassifier/#references","text":"Heitor Murilo Gomes, Jesse Read, Albert Bifet. Streaming Random Patches for Evolving Data Stream Classification. IEEE International Conference on Data Mining (ICDM), 2019. \u21a9","title":"References"},{"location":"api/evaluate/Track/","text":"Track \u00b6 A track evaluate a model's performance. The following metrics are recorded: - FLOPS: floating point operations per second. - Time, which should be interpreted with wisdom. Indeed time can depend on the architecture and local resource situations. Comparison via FLOPS should be preferred. - The model's memory footprint. - The model's predictive performance on the track's dataset. \u00b6 Parameters \u00b6 name ( str ) The name of the track. dataset The dataset from which samples will be retrieved. A slice must be used if the dataset is a data generator. metric The metric(s) used to track performance. n_samples ( int ) \u2013 defaults to None The number of samples that are going to be processed by the track. Methods \u00b6 run","title":"Track"},{"location":"api/evaluate/Track/#track","text":"A track evaluate a model's performance. The following metrics are recorded:","title":"Track"},{"location":"api/evaluate/Track/#-flops-floating-point-operations-per-second-time-which-should-be-interpreted-with-wisdom-indeed-time-can-depend-on-the-architecture-and-local-resource-situations-comparison-via-flops-should-be-preferred-the-models-memory-footprint-the-models-predictive-performance-on-the-tracks-dataset","text":"","title":"- FLOPS: floating point operations per second. - Time, which should be interpreted with wisdom. Indeed time can depend on the architecture     and local resource situations. Comparison via FLOPS should be preferred. - The model's memory footprint. - The model's predictive performance on the track's dataset."},{"location":"api/evaluate/Track/#parameters","text":"name ( str ) The name of the track. dataset The dataset from which samples will be retrieved. A slice must be used if the dataset is a data generator. metric The metric(s) used to track performance. n_samples ( int ) \u2013 defaults to None The number of samples that are going to be processed by the track.","title":"Parameters"},{"location":"api/evaluate/Track/#methods","text":"run","title":"Methods"},{"location":"api/evaluate/load-binary-clf-tracks/","text":"load_binary_clf_tracks \u00b6 Return binary classification tracks.","title":"load_binary_clf_tracks"},{"location":"api/evaluate/load-binary-clf-tracks/#load_binary_clf_tracks","text":"Return binary classification tracks.","title":"load_binary_clf_tracks"},{"location":"api/evaluate/progressive-val-score/","text":"progressive_val_score \u00b6 Evaluates the performance of a model on a streaming dataset. This method is the canonical way to evaluate a model's performance. When used correctly, it allows you to exactly assess how a model would have performed in a production scenario. dataset is converted into a stream of questions and answers. At each step the model is either asked to predict an observation, or is either updated. The target is only revealed to the model after a certain amount of time, which is determined by the delay parameter. Note that under the hood this uses the stream.simulate_qa function to go through the data in arrival order. By default, there is no delay, which means that the samples are processed one after the other. When there is no delay, this function essentially performs progressive validation. When there is a delay, then we refer to it as delayed progressive validation. It is recommended to use this method when you want to determine a model's performance on a dataset. In particular, it is advised to use the delay parameter in order to get a reliable assessment. Indeed, in a production scenario, it is often the case that ground truths are made available after a certain amount of time. By using this method, you can reproduce this scenario and therefore truthfully assess what would have been the performance of a model on a given dataset. Parameters \u00b6 dataset ( Iterator[Tuple[dict, Any]] ) The stream of observations against which the model will be evaluated. model The model to evaluate. metric ( river.metrics.base.Metric ) The metric used to evaluate the model's predictions. moment ( Union[str, Callable] ) \u2013 defaults to None The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitly timestamped in the order in which they arrive. delay ( Union[str, int, datetime.timedelta, Callable] ) \u2013 defaults to None The amount to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. print_every \u2013 defaults to 0 Iteration number at which to print the current metric. This only takes into account the predictions, and not the training steps. show_time \u2013 defaults to False Whether or not to display the elapsed time. show_memory \u2013 defaults to False Whether or not to display the memory usage of the model. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress. Examples \u00b6 Take the following model: >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) We can evaluate it on the Phishing dataset as so: >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 ... ) [ 200 ] ROCAUC : 0.897995 [ 400 ] ROCAUC : 0.920896 [ 600 ] ROCAUC : 0.931339 [ 800 ] ROCAUC : 0.939909 [ 1 , 000 ] ROCAUC : 0.947417 [ 1 , 200 ] ROCAUC : 0.950304 ROCAUC : 0.950363 We haven't specified a delay, therefore this is strictly equivalent to the following piece of code: >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) >>> metric = metrics . ROCAUC () >>> for x , y in datasets . Phishing (): ... y_pred = model . predict_proba_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric ROCAUC : 0.950363 When print_every is specified, the current state is printed at regular intervals. Under the hood, Python's print method is being used. You can pass extra keyword arguments to modify its behavior. For instance, you may use the file argument if you want to log the progress to a file of your choice. >>> with open ( 'progress.log' , 'w' ) as f : ... metric = evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 , ... file = f ... ) >>> with open ( 'progress.log' ) as f : ... for line in f . read () . splitlines (): ... print ( line ) [ 200 ] ROCAUC : 0.94 [ 400 ] ROCAUC : 0.946969 [ 600 ] ROCAUC : 0.9517 [ 800 ] ROCAUC : 0.954238 [ 1 , 000 ] ROCAUC : 0.958207 [ 1 , 200 ] ROCAUC : 0.96002 Note that the performance is slightly better than above because we haven't used a fresh copy of the model. Instead, we've reused the existing model which has already done a full pass on the data. >>> import os ; os . remove ( 'progress.log' ) References \u00b6 Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation \u21a9 Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30 \u21a9","title":"progressive_val_score"},{"location":"api/evaluate/progressive-val-score/#progressive_val_score","text":"Evaluates the performance of a model on a streaming dataset. This method is the canonical way to evaluate a model's performance. When used correctly, it allows you to exactly assess how a model would have performed in a production scenario. dataset is converted into a stream of questions and answers. At each step the model is either asked to predict an observation, or is either updated. The target is only revealed to the model after a certain amount of time, which is determined by the delay parameter. Note that under the hood this uses the stream.simulate_qa function to go through the data in arrival order. By default, there is no delay, which means that the samples are processed one after the other. When there is no delay, this function essentially performs progressive validation. When there is a delay, then we refer to it as delayed progressive validation. It is recommended to use this method when you want to determine a model's performance on a dataset. In particular, it is advised to use the delay parameter in order to get a reliable assessment. Indeed, in a production scenario, it is often the case that ground truths are made available after a certain amount of time. By using this method, you can reproduce this scenario and therefore truthfully assess what would have been the performance of a model on a given dataset.","title":"progressive_val_score"},{"location":"api/evaluate/progressive-val-score/#parameters","text":"dataset ( Iterator[Tuple[dict, Any]] ) The stream of observations against which the model will be evaluated. model The model to evaluate. metric ( river.metrics.base.Metric ) The metric used to evaluate the model's predictions. moment ( Union[str, Callable] ) \u2013 defaults to None The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitly timestamped in the order in which they arrive. delay ( Union[str, int, datetime.timedelta, Callable] ) \u2013 defaults to None The amount to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. print_every \u2013 defaults to 0 Iteration number at which to print the current metric. This only takes into account the predictions, and not the training steps. show_time \u2013 defaults to False Whether or not to display the elapsed time. show_memory \u2013 defaults to False Whether or not to display the memory usage of the model. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress.","title":"Parameters"},{"location":"api/evaluate/progressive-val-score/#examples","text":"Take the following model: >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) We can evaluate it on the Phishing dataset as so: >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 ... ) [ 200 ] ROCAUC : 0.897995 [ 400 ] ROCAUC : 0.920896 [ 600 ] ROCAUC : 0.931339 [ 800 ] ROCAUC : 0.939909 [ 1 , 000 ] ROCAUC : 0.947417 [ 1 , 200 ] ROCAUC : 0.950304 ROCAUC : 0.950363 We haven't specified a delay, therefore this is strictly equivalent to the following piece of code: >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) >>> metric = metrics . ROCAUC () >>> for x , y in datasets . Phishing (): ... y_pred = model . predict_proba_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric ROCAUC : 0.950363 When print_every is specified, the current state is printed at regular intervals. Under the hood, Python's print method is being used. You can pass extra keyword arguments to modify its behavior. For instance, you may use the file argument if you want to log the progress to a file of your choice. >>> with open ( 'progress.log' , 'w' ) as f : ... metric = evaluate . progressive_val_score ( ... model = model , ... dataset = datasets . Phishing (), ... metric = metrics . ROCAUC (), ... print_every = 200 , ... file = f ... ) >>> with open ( 'progress.log' ) as f : ... for line in f . read () . splitlines (): ... print ( line ) [ 200 ] ROCAUC : 0.94 [ 400 ] ROCAUC : 0.946969 [ 600 ] ROCAUC : 0.9517 [ 800 ] ROCAUC : 0.954238 [ 1 , 000 ] ROCAUC : 0.958207 [ 1 , 200 ] ROCAUC : 0.96002 Note that the performance is slightly better than above because we haven't used a fresh copy of the model. Instead, we've reused the existing model which has already done a full pass on the data. >>> import os ; os . remove ( 'progress.log' )","title":"Examples"},{"location":"api/evaluate/progressive-val-score/#references","text":"Beating the Hold-Out: Bounds for K-fold and Progressive Cross-Validation \u21a9 Grzenda, M., Gomes, H.M. and Bifet, A., 2019. Delayed labelling evaluation for data streams. Data Mining and Knowledge Discovery, pp.1-30 \u21a9","title":"References"},{"location":"api/expert/EWARegressor/","text":"EWARegressor \u00b6 Exponentially Weighted Average regressor. Parameters \u00b6 regressors ( List[ base.Regressor ] ) The regressors to hedge. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function that has to be minimized. Defaults to optim.losses.Squared . learning_rate \u2013 defaults to 0.5 The learning rate by which the model weights are multiplied at each iteration. Attributes \u00b6 regressors Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import expert >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> from river import stream >>> optimizers = [ ... optim . SGD ( 0.01 ), ... optim . RMSProp (), ... optim . AdaGrad () ... ] >>> for optimizer in optimizers : ... ... dataset = datasets . TrumpApproval () ... metric = metrics . MAE () ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( ... optimizer = optimizer , ... intercept_lr = .1 ... ) ... ) ... ... print ( optimizer , evaluate . progressive_val_score ( dataset , model , metric )) SGD MAE : 0.555971 RMSProp MAE : 0.528284 AdaGrad MAE : 0.481461 >>> dataset = datasets . TrumpApproval () >>> metric = metrics . MAE () >>> hedge = ( ... preprocessing . StandardScaler () | ... expert . EWARegressor ( ... regressors = [ ... linear_model . LinearRegression ( optimizer = o , intercept_lr = .1 ) ... for o in optimizers ... ], ... learning_rate = 0.005 ... ) ... ) >>> evaluate . progressive_val_score ( dataset , hedge , metric ) MAE : 0.494832 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self learn_predict_one predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Online Learning from Experts: Weighed Majority and Hedge \u21a9 Wikipedia page on the multiplicative weight update method \u21a9 Kivinen, J. and Warmuth, M.K., 1997. Exponentiated gradient versus gradient descent for linear predictors. information and computation, 132(1), pp.1-63. \u21a9","title":"EWARegressor"},{"location":"api/expert/EWARegressor/#ewaregressor","text":"Exponentially Weighted Average regressor.","title":"EWARegressor"},{"location":"api/expert/EWARegressor/#parameters","text":"regressors ( List[ base.Regressor ] ) The regressors to hedge. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function that has to be minimized. Defaults to optim.losses.Squared . learning_rate \u2013 defaults to 0.5 The learning rate by which the model weights are multiplied at each iteration.","title":"Parameters"},{"location":"api/expert/EWARegressor/#attributes","text":"regressors","title":"Attributes"},{"location":"api/expert/EWARegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import expert >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> from river import stream >>> optimizers = [ ... optim . SGD ( 0.01 ), ... optim . RMSProp (), ... optim . AdaGrad () ... ] >>> for optimizer in optimizers : ... ... dataset = datasets . TrumpApproval () ... metric = metrics . MAE () ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( ... optimizer = optimizer , ... intercept_lr = .1 ... ) ... ) ... ... print ( optimizer , evaluate . progressive_val_score ( dataset , model , metric )) SGD MAE : 0.555971 RMSProp MAE : 0.528284 AdaGrad MAE : 0.481461 >>> dataset = datasets . TrumpApproval () >>> metric = metrics . MAE () >>> hedge = ( ... preprocessing . StandardScaler () | ... expert . EWARegressor ( ... regressors = [ ... linear_model . LinearRegression ( optimizer = o , intercept_lr = .1 ) ... for o in optimizers ... ], ... learning_rate = 0.005 ... ) ... ) >>> evaluate . progressive_val_score ( dataset , hedge , metric ) MAE : 0.494832","title":"Examples"},{"location":"api/expert/EWARegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self learn_predict_one predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/expert/EWARegressor/#references","text":"Online Learning from Experts: Weighed Majority and Hedge \u21a9 Wikipedia page on the multiplicative weight update method \u21a9 Kivinen, J. and Warmuth, M.K., 1997. Exponentiated gradient versus gradient descent for linear predictors. information and computation, 132(1), pp.1-63. \u21a9","title":"References"},{"location":"api/expert/EpsilonGreedyRegressor/","text":"EpsilonGreedyRegressor \u00b6 Epsilon-greedy bandit algorithm for regression. This bandit selects the best arm (defined as the one with the highest average reward) with probability \\((1 - \\epsilon)\\) and draws a random arm with probability \\(\\epsilon\\) . It is also called Follow-The-Leader (FTL) algorithm. Parameters \u00b6 models ( List[ base.Estimator ] ) The models to compare. metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to None Metric used for comparing models with. epsilon ( float ) \u2013 defaults to 0.1 Exploration parameter (default : 0.1). epsilon_decay ( float ) \u2013 defaults to None Exponential decay factor applied to epsilon. explore_each_arm ( int ) \u2013 defaults to 3 The number of times each arm should explored first. start_after ( int ) \u2013 defaults to 20 The number of iteration after which the bandit mechanism should begin. seed ( int ) \u2013 defaults to None The seed for the algorithm (since not deterministic). Attributes \u00b6 best_model Returns the best model, defined as the one who maximises average reward. percentage_pulled Returns the number of times (in %) each arm has been pulled. Examples \u00b6 Let's use UCBRegressor to select the best learning rate for a linear regression model. First, we define the grid of models: >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> from river import optim >>> models = [ ... compose . Pipeline ( ... preprocessing . StandardScaler (), ... linear_model . LinearRegression ( optimizer = optim . SGD ( lr = lr )) ... ) ... for lr in [ 1e-4 , 1e-3 , 1e-2 , 1e-1 ] ... ] We decide to use TrumpApproval dataset: >>> from river import datasets >>> dataset = datasets . TrumpApproval () The chosen bandit is epsilon-greedy: >>> from river.expert import EpsilonGreedyRegressor >>> bandit = EpsilonGreedyRegressor ( models = models , seed = 1 ) The models in the bandit can then be trained in an online fashion. >>> for x , y in dataset : ... bandit = bandit . learn_one ( x = x , y = y ) We can inspect the number of times (in percentage) each arm has been pulled. >>> for model , pct in zip ( bandit . models , bandit . percentage_pulled ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { pct : .2% } \" ) 1.0e-04 \u2014 3.47 % 1.0e-03 \u2014 2.85 % 1.0e-02 \u2014 44.75 % 1.0e-01 \u2014 48.93 % The average reward of each model is also available: >>> for model , avg in zip ( bandit . models , bandit . average_reward ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { avg : .2f } \" ) 1.0e-04 \u2014 0.00 1.0e-03 \u2014 0.00 1.0e-02 \u2014 0.56 1.0e-01 \u2014 0.01 We can also select the best model (the one with the highest average reward). >>> best_model = bandit . best_model The learning rate chosen by the bandit is: >>> best_model [ \"LinearRegression\" ] . intercept_lr . learning_rate 0.01 Methods \u00b6 add_models clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Updates the chosen model and the arm internals (the actual implementation is in Bandit._learn_one). Parameters x y predict_one Return the prediction of the best model (defined as the one who maximises average reward). Parameters x References \u00b6 Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press. \u21a9 Lattimore, T., & Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press. \u21a9","title":"EpsilonGreedyRegressor"},{"location":"api/expert/EpsilonGreedyRegressor/#epsilongreedyregressor","text":"Epsilon-greedy bandit algorithm for regression. This bandit selects the best arm (defined as the one with the highest average reward) with probability \\((1 - \\epsilon)\\) and draws a random arm with probability \\(\\epsilon\\) . It is also called Follow-The-Leader (FTL) algorithm.","title":"EpsilonGreedyRegressor"},{"location":"api/expert/EpsilonGreedyRegressor/#parameters","text":"models ( List[ base.Estimator ] ) The models to compare. metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to None Metric used for comparing models with. epsilon ( float ) \u2013 defaults to 0.1 Exploration parameter (default : 0.1). epsilon_decay ( float ) \u2013 defaults to None Exponential decay factor applied to epsilon. explore_each_arm ( int ) \u2013 defaults to 3 The number of times each arm should explored first. start_after ( int ) \u2013 defaults to 20 The number of iteration after which the bandit mechanism should begin. seed ( int ) \u2013 defaults to None The seed for the algorithm (since not deterministic).","title":"Parameters"},{"location":"api/expert/EpsilonGreedyRegressor/#attributes","text":"best_model Returns the best model, defined as the one who maximises average reward. percentage_pulled Returns the number of times (in %) each arm has been pulled.","title":"Attributes"},{"location":"api/expert/EpsilonGreedyRegressor/#examples","text":"Let's use UCBRegressor to select the best learning rate for a linear regression model. First, we define the grid of models: >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> from river import optim >>> models = [ ... compose . Pipeline ( ... preprocessing . StandardScaler (), ... linear_model . LinearRegression ( optimizer = optim . SGD ( lr = lr )) ... ) ... for lr in [ 1e-4 , 1e-3 , 1e-2 , 1e-1 ] ... ] We decide to use TrumpApproval dataset: >>> from river import datasets >>> dataset = datasets . TrumpApproval () The chosen bandit is epsilon-greedy: >>> from river.expert import EpsilonGreedyRegressor >>> bandit = EpsilonGreedyRegressor ( models = models , seed = 1 ) The models in the bandit can then be trained in an online fashion. >>> for x , y in dataset : ... bandit = bandit . learn_one ( x = x , y = y ) We can inspect the number of times (in percentage) each arm has been pulled. >>> for model , pct in zip ( bandit . models , bandit . percentage_pulled ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { pct : .2% } \" ) 1.0e-04 \u2014 3.47 % 1.0e-03 \u2014 2.85 % 1.0e-02 \u2014 44.75 % 1.0e-01 \u2014 48.93 % The average reward of each model is also available: >>> for model , avg in zip ( bandit . models , bandit . average_reward ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { avg : .2f } \" ) 1.0e-04 \u2014 0.00 1.0e-03 \u2014 0.00 1.0e-02 \u2014 0.56 1.0e-01 \u2014 0.01 We can also select the best model (the one with the highest average reward). >>> best_model = bandit . best_model The learning rate chosen by the bandit is: >>> best_model [ \"LinearRegression\" ] . intercept_lr . learning_rate 0.01","title":"Examples"},{"location":"api/expert/EpsilonGreedyRegressor/#methods","text":"add_models clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Updates the chosen model and the arm internals (the actual implementation is in Bandit._learn_one). Parameters x y predict_one Return the prediction of the best model (defined as the one who maximises average reward). Parameters x","title":"Methods"},{"location":"api/expert/EpsilonGreedyRegressor/#references","text":"Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press. \u21a9 Lattimore, T., & Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press. \u21a9","title":"References"},{"location":"api/expert/StackingClassifier/","text":"StackingClassifier \u00b6 Stacking for binary classification. Parameters \u00b6 classifiers ( List[ base.Classifier ] ) meta_classifier ( base.Classifier ) include_features \u2013 defaults to True Indicates whether or not the original features should be provided to the meta-model along with the predictions from each model. Examples \u00b6 >>> from river import compose >>> from river import datasets >>> from river import evaluate >>> from river import expert >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = compose . Pipeline ( ... ( 'scale' , pp . StandardScaler ()), ... ( 'stack' , expert . StackingClassifier ( ... classifiers = [ ... lm . LogisticRegression (), ... lm . PAClassifier ( mode = 1 , C = 0.01 ), ... lm . PAClassifier ( mode = 2 , C = 0.01 ) ... ], ... meta_classifier = lm . LogisticRegression () ... )) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.881387 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 A Kaggler's Guide to Model Stacking in Practice \u21a9","title":"StackingClassifier"},{"location":"api/expert/StackingClassifier/#stackingclassifier","text":"Stacking for binary classification.","title":"StackingClassifier"},{"location":"api/expert/StackingClassifier/#parameters","text":"classifiers ( List[ base.Classifier ] ) meta_classifier ( base.Classifier ) include_features \u2013 defaults to True Indicates whether or not the original features should be provided to the meta-model along with the predictions from each model.","title":"Parameters"},{"location":"api/expert/StackingClassifier/#examples","text":">>> from river import compose >>> from river import datasets >>> from river import evaluate >>> from river import expert >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = compose . Pipeline ( ... ( 'scale' , pp . StandardScaler ()), ... ( 'stack' , expert . StackingClassifier ( ... classifiers = [ ... lm . LogisticRegression (), ... lm . PAClassifier ( mode = 1 , C = 0.01 ), ... lm . PAClassifier ( mode = 2 , C = 0.01 ) ... ], ... meta_classifier = lm . LogisticRegression () ... )) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.881387","title":"Examples"},{"location":"api/expert/StackingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/expert/StackingClassifier/#references","text":"A Kaggler's Guide to Model Stacking in Practice \u21a9","title":"References"},{"location":"api/expert/SuccessiveHalvingClassifier/","text":"SuccessiveHalvingClassifier \u00b6 Successive halving algorithm for classification. Successive halving is a method for performing model selection without having to train each model on all the dataset. At certain points in time (called \"rungs\"), the worst performing will be discarded and the best ones will keep competing between each other. The rung values are designed so that at most budget model updates will be performed in total. If you have k combinations of hyperparameters and that your dataset contains n observations, then the maximal budget you can allocate is: \\[\\frac{2kn}{eta}\\] It is recommended that you check this beforehand. This bound can't be checked by the function because the size of the dataset is not known. In fact it is potentially infinite, in which case the algorithm will terminate once all the budget has been spent. If you have a budget of B , and that your dataset contains n observations, then the number of hyperparameter combinations that will spend all the budget and go through all the data is: \\[\\ceil(\\floor(\\frac{B}{(2n)}) \\times eta)\\] Parameters \u00b6 models The models to compare. metric ( river.metrics.base.Metric ) Metric used for comparing models with. budget ( int ) Total number of model updates you wish to allocate. eta \u2013 defaults to 2 Rate of elimination. At every rung, math.ceil(k / eta) models are kept, where k is the number of models that have reached the rung. A higher eta value will focus on less models but will allocate more iterations to the best models. verbose \u2013 defaults to False Whether to display progress or not. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress. Attributes \u00b6 best_model The current best model. Examples \u00b6 As an example, let's use successive halving to tune the optimizer of a logistic regression. We'll first define the model. >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) Let's now define a grid of parameters which we would like to compare. We'll try different optimizers with various learning rates. >>> from river import utils >>> from river import optim >>> models = utils . expand_param_grid ( model , { ... 'LogisticRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .005 ]}), ... ( optim . Adam , { 'beta_1' : [ .01 , .001 ], 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'beta_1' : [ .1 ], 'lr' : [ .001 ]}), ... ] ... } ... }) We can check how many models we've created. >>> len ( models ) 10 We can now pass these models to a SuccessiveHalvingClassifier . We also need to pick a metric to compare the models, and a budget which indicates how many iterations to run before picking the best model and discarding the rest. >>> from river import expert >>> sh = expert . SuccessiveHalvingClassifier ( ... models = models , ... metric = metrics . Accuracy (), ... budget = 2000 , ... eta = 2 , ... verbose = True ... ) A SuccessiveHalvingClassifier is also a classifier with a learn_one and a predict_proba_one method. We can therefore evaluate it like any other classifier with evaluate.progressive_val_score . >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... dataset = datasets . Phishing (), ... model = sh , ... metric = metrics . ROCAUC () ... ) [ 1 ] 5 removed 5 left 50 iterations budget used : 500 budget left : 1500 best Accuracy : 80.00 % [ 2 ] 2 removed 3 left 100 iterations budget used : 1000 budget left : 1000 best Accuracy : 84.00 % [ 3 ] 1 removed 2 left 166 iterations budget used : 1498 budget left : 502 best Accuracy : 86.14 % [ 4 ] 1 removed 1 left 250 iterations budget used : 1998 budget left : 2 best Accuracy : 84.80 % ROCAUC : 0.952889 We can now view the best model. >>> sh . best_model Pipeline ( StandardScaler (), LogisticRegression ( optimizer = Adam ( lr = Constant ( learning_rate = 0.01 ) beta_1 = 0.01 beta_2 = 0.999 eps = 1e-08 ) loss = Log ( weight_pos = 1. weight_neg = 1. ) l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248). \u21a9 Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. \u21a9 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816. \u21a9","title":"SuccessiveHalvingClassifier"},{"location":"api/expert/SuccessiveHalvingClassifier/#successivehalvingclassifier","text":"Successive halving algorithm for classification. Successive halving is a method for performing model selection without having to train each model on all the dataset. At certain points in time (called \"rungs\"), the worst performing will be discarded and the best ones will keep competing between each other. The rung values are designed so that at most budget model updates will be performed in total. If you have k combinations of hyperparameters and that your dataset contains n observations, then the maximal budget you can allocate is: \\[\\frac{2kn}{eta}\\] It is recommended that you check this beforehand. This bound can't be checked by the function because the size of the dataset is not known. In fact it is potentially infinite, in which case the algorithm will terminate once all the budget has been spent. If you have a budget of B , and that your dataset contains n observations, then the number of hyperparameter combinations that will spend all the budget and go through all the data is: \\[\\ceil(\\floor(\\frac{B}{(2n)}) \\times eta)\\]","title":"SuccessiveHalvingClassifier"},{"location":"api/expert/SuccessiveHalvingClassifier/#parameters","text":"models The models to compare. metric ( river.metrics.base.Metric ) Metric used for comparing models with. budget ( int ) Total number of model updates you wish to allocate. eta \u2013 defaults to 2 Rate of elimination. At every rung, math.ceil(k / eta) models are kept, where k is the number of models that have reached the rung. A higher eta value will focus on less models but will allocate more iterations to the best models. verbose \u2013 defaults to False Whether to display progress or not. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress.","title":"Parameters"},{"location":"api/expert/SuccessiveHalvingClassifier/#attributes","text":"best_model The current best model.","title":"Attributes"},{"location":"api/expert/SuccessiveHalvingClassifier/#examples","text":"As an example, let's use successive halving to tune the optimizer of a logistic regression. We'll first define the model. >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ) Let's now define a grid of parameters which we would like to compare. We'll try different optimizers with various learning rates. >>> from river import utils >>> from river import optim >>> models = utils . expand_param_grid ( model , { ... 'LogisticRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .005 ]}), ... ( optim . Adam , { 'beta_1' : [ .01 , .001 ], 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'beta_1' : [ .1 ], 'lr' : [ .001 ]}), ... ] ... } ... }) We can check how many models we've created. >>> len ( models ) 10 We can now pass these models to a SuccessiveHalvingClassifier . We also need to pick a metric to compare the models, and a budget which indicates how many iterations to run before picking the best model and discarding the rest. >>> from river import expert >>> sh = expert . SuccessiveHalvingClassifier ( ... models = models , ... metric = metrics . Accuracy (), ... budget = 2000 , ... eta = 2 , ... verbose = True ... ) A SuccessiveHalvingClassifier is also a classifier with a learn_one and a predict_proba_one method. We can therefore evaluate it like any other classifier with evaluate.progressive_val_score . >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... dataset = datasets . Phishing (), ... model = sh , ... metric = metrics . ROCAUC () ... ) [ 1 ] 5 removed 5 left 50 iterations budget used : 500 budget left : 1500 best Accuracy : 80.00 % [ 2 ] 2 removed 3 left 100 iterations budget used : 1000 budget left : 1000 best Accuracy : 84.00 % [ 3 ] 1 removed 2 left 166 iterations budget used : 1498 budget left : 502 best Accuracy : 86.14 % [ 4 ] 1 removed 1 left 250 iterations budget used : 1998 budget left : 2 best Accuracy : 84.80 % ROCAUC : 0.952889 We can now view the best model. >>> sh . best_model Pipeline ( StandardScaler (), LogisticRegression ( optimizer = Adam ( lr = Constant ( learning_rate = 0.01 ) beta_1 = 0.01 beta_2 = 0.999 eps = 1e-08 ) loss = Log ( weight_pos = 1. weight_neg = 1. ) l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) )","title":"Examples"},{"location":"api/expert/SuccessiveHalvingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/expert/SuccessiveHalvingClassifier/#references","text":"Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248). \u21a9 Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. \u21a9 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816. \u21a9","title":"References"},{"location":"api/expert/SuccessiveHalvingRegressor/","text":"SuccessiveHalvingRegressor \u00b6 Successive halving algorithm for regression. Successive halving is a method for performing model selection without having to train each model on all the dataset. At certain points in time (called \"rungs\"), the worst performing will be discarded and the best ones will keep competing between each other. The rung values are designed so that at most budget model updates will be performed in total. If you have k combinations of hyperparameters and that your dataset contains n observations, then the maximal budget you can allocate is: \\[\\frac{2kn}{eta}\\] It is recommended that you check this beforehand. This bound can't be checked by the function because the size of the dataset is not known. In fact it is potentially infinite, in which case the algorithm will terminate once all the budget has been spent. If you have a budget of B , and that your dataset contains n observations, then the number of hyperparameter combinations that will spend all the budget and go through all the data is: \\[\\ceil(\\floor(\\frac{B}{2n}) \\times eta)\\] Parameters \u00b6 models The models to compare. metric ( river.metrics.base.Metric ) Metric used for comparing models with. budget ( int ) Total number of model updates you wish to allocate. eta \u2013 defaults to 2 Rate of elimination. At every rung, math.ceil(k / eta) models are kept, where k is the number of models that have reached the rung. A higher eta value will focus on less models but will allocate more iterations to the best models. verbose \u2013 defaults to False Whether to display progress or not. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress. Attributes \u00b6 best_model The current best model. Examples \u00b6 As an example, let's use successive halving to tune the optimizer of a linear regression. We'll first define the model. >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = .1 ) ... ) Let's now define a grid of parameters which we would like to compare. We'll try different optimizers with various learning rates. >>> from river import optim >>> from river import utils >>> models = utils . expand_param_grid ( model , { ... 'LinearRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .005 ]}), ... ( optim . Adam , { 'beta_1' : [ .01 , .001 ], 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'beta_1' : [ .1 ], 'lr' : [ .001 ]}), ... ] ... } ... }) We can check how many models we've created. >>> len ( models ) 10 We can now pass these models to a SuccessiveHalvingRegressor . We also need to pick a metric to compare the models, and a budget which indicates how many iterations to run before picking the best model and discarding the rest. >>> from river import expert >>> sh = expert . SuccessiveHalvingRegressor ( ... models = models , ... metric = metrics . MAE (), ... budget = 2000 , ... eta = 2 , ... verbose = True ... ) A SuccessiveHalvingRegressor is also a regressor with a learn_one and a predict_one method. We can therefore evaluate it like any other classifier with evaluate.progressive_val_score . >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... dataset = datasets . TrumpApproval (), ... model = sh , ... metric = metrics . MAE () ... ) [ 1 ] 5 removed 5 left 50 iterations budget used : 500 budget left : 1500 best MAE : 4.540491 [ 2 ] 2 removed 3 left 100 iterations budget used : 1000 budget left : 1000 best MAE : 2.458765 [ 3 ] 1 removed 2 left 166 iterations budget used : 1498 budget left : 502 best MAE : 1.583751 [ 4 ] 1 removed 1 left 250 iterations budget used : 1998 budget left : 2 best MAE : 1.147296 MAE : 0.488387 We can now view the best model. >>> sh . best_model Pipeline ( StandardScaler (), LinearRegression ( optimizer = Adam ( lr = Constant ( learning_rate = 0.1 ) beta_1 = 0.01 beta_2 = 0.999 eps = 1e-08 ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.1 ) clip_gradient = 1e+12 initializer = Zeros () ) ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248). \u21a9 Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. \u21a9 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816. \u21a9","title":"SuccessiveHalvingRegressor"},{"location":"api/expert/SuccessiveHalvingRegressor/#successivehalvingregressor","text":"Successive halving algorithm for regression. Successive halving is a method for performing model selection without having to train each model on all the dataset. At certain points in time (called \"rungs\"), the worst performing will be discarded and the best ones will keep competing between each other. The rung values are designed so that at most budget model updates will be performed in total. If you have k combinations of hyperparameters and that your dataset contains n observations, then the maximal budget you can allocate is: \\[\\frac{2kn}{eta}\\] It is recommended that you check this beforehand. This bound can't be checked by the function because the size of the dataset is not known. In fact it is potentially infinite, in which case the algorithm will terminate once all the budget has been spent. If you have a budget of B , and that your dataset contains n observations, then the number of hyperparameter combinations that will spend all the budget and go through all the data is: \\[\\ceil(\\floor(\\frac{B}{2n}) \\times eta)\\]","title":"SuccessiveHalvingRegressor"},{"location":"api/expert/SuccessiveHalvingRegressor/#parameters","text":"models The models to compare. metric ( river.metrics.base.Metric ) Metric used for comparing models with. budget ( int ) Total number of model updates you wish to allocate. eta \u2013 defaults to 2 Rate of elimination. At every rung, math.ceil(k / eta) models are kept, where k is the number of models that have reached the rung. A higher eta value will focus on less models but will allocate more iterations to the best models. verbose \u2013 defaults to False Whether to display progress or not. print_kwargs Extra keyword arguments are passed to the print function. For instance, this allows providing a file argument, which indicates where to output progress.","title":"Parameters"},{"location":"api/expert/SuccessiveHalvingRegressor/#attributes","text":"best_model The current best model.","title":"Attributes"},{"location":"api/expert/SuccessiveHalvingRegressor/#examples","text":"As an example, let's use successive halving to tune the optimizer of a linear regression. We'll first define the model. >>> from river import linear_model >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = .1 ) ... ) Let's now define a grid of parameters which we would like to compare. We'll try different optimizers with various learning rates. >>> from river import optim >>> from river import utils >>> models = utils . expand_param_grid ( model , { ... 'LinearRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .005 ]}), ... ( optim . Adam , { 'beta_1' : [ .01 , .001 ], 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'beta_1' : [ .1 ], 'lr' : [ .001 ]}), ... ] ... } ... }) We can check how many models we've created. >>> len ( models ) 10 We can now pass these models to a SuccessiveHalvingRegressor . We also need to pick a metric to compare the models, and a budget which indicates how many iterations to run before picking the best model and discarding the rest. >>> from river import expert >>> sh = expert . SuccessiveHalvingRegressor ( ... models = models , ... metric = metrics . MAE (), ... budget = 2000 , ... eta = 2 , ... verbose = True ... ) A SuccessiveHalvingRegressor is also a regressor with a learn_one and a predict_one method. We can therefore evaluate it like any other classifier with evaluate.progressive_val_score . >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> evaluate . progressive_val_score ( ... dataset = datasets . TrumpApproval (), ... model = sh , ... metric = metrics . MAE () ... ) [ 1 ] 5 removed 5 left 50 iterations budget used : 500 budget left : 1500 best MAE : 4.540491 [ 2 ] 2 removed 3 left 100 iterations budget used : 1000 budget left : 1000 best MAE : 2.458765 [ 3 ] 1 removed 2 left 166 iterations budget used : 1498 budget left : 502 best MAE : 1.583751 [ 4 ] 1 removed 1 left 250 iterations budget used : 1998 budget left : 2 best MAE : 1.147296 MAE : 0.488387 We can now view the best model. >>> sh . best_model Pipeline ( StandardScaler (), LinearRegression ( optimizer = Adam ( lr = Constant ( learning_rate = 0.1 ) beta_1 = 0.01 beta_2 = 0.999 eps = 1e-08 ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.1 ) clip_gradient = 1e+12 initializer = Zeros () ) )","title":"Examples"},{"location":"api/expert/SuccessiveHalvingRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/expert/SuccessiveHalvingRegressor/#references","text":"Jamieson, K. and Talwalkar, A., 2016, May. Non-stochastic best arm identification and hyperparameter optimization. In Artificial Intelligence and Statistics (pp. 240-248). \u21a9 Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Hardt, M., Recht, B. and Talwalkar, A., 2018. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934. \u21a9 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1), pp.6765-6816. \u21a9","title":"References"},{"location":"api/expert/UCBRegressor/","text":"UCBRegressor \u00b6 Upper Confidence Bound bandit for regression. The class offers 2 implementations of UCB: UCB1 from 1 , when the parameter delta has value None - UCB(delta) from 2 , when the parameter delta is in (0, 1) For this bandit, rewards are supposed to be 1-subgaussian (see Lattimore and Szepesv\u00e1ri, chapter 6, p. 91) hence the use of the StandardScaler and MaxAbsScaler as reward_scaler . Parameters \u00b6 models ( List[ base.Estimator ] ) The models to compare. metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to None Metric used for comparing models with. delta ( float ) \u2013 defaults to None For UCB(delta) implementation. Lower value means more exploration. explore_each_arm ( int ) \u2013 defaults to 1 The number of times each arm should explored first. start_after ( int ) \u2013 defaults to 20 The number of iteration after which the bandit mechanism should begin. seed ( int ) \u2013 defaults to None The seed for the algorithm (since not deterministic). Attributes \u00b6 best_model Returns the best model, defined as the one who maximises average reward. percentage_pulled Returns the number of times (in %) each arm has been pulled. Examples \u00b6 Let's use UCBRegressor to select the best learning rate for a linear regression model. First, we define the grid of models: >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> from river import optim >>> models = [ ... compose . Pipeline ( ... preprocessing . StandardScaler (), ... linear_model . LinearRegression ( optimizer = optim . SGD ( lr = lr )) ... ) ... for lr in [ 1e-4 , 1e-3 , 1e-2 , 1e-1 ] ... ] We decide to use TrumpApproval dataset: >>> from river import datasets >>> dataset = datasets . TrumpApproval () We use the UCB bandit: >>> from river.expert import UCBRegressor >>> bandit = UCBRegressor ( models = models , seed = 1 ) The models in the bandit can be trained in an online fashion. >>> for x , y in dataset : ... bandit = bandit . learn_one ( x = x , y = y ) We can inspect the number of times (in percentage) each arm has been pulled. >>> for model , pct in zip ( bandit . models , bandit . percentage_pulled ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { pct : .2% } \" ) 1.0e-04 \u2014 2.45 % 1.0e-03 \u2014 2.45 % 1.0e-02 \u2014 92.25 % 1.0e-01 \u2014 2.85 % The average reward of each model is also available: >>> for model , avg in zip ( bandit . models , bandit . average_reward ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { avg : .2f } \" ) 1.0e-04 \u2014 0.00 1.0e-03 \u2014 0.00 1.0e-02 \u2014 0.74 1.0e-01 \u2014 0.05 We can also select the best model (the one with the highest average reward). >>> best_model = bandit . best_model The learning rate chosen by the bandit is: >>> best_model [ \"LinearRegression\" ] . intercept_lr . learning_rate 0.01 Methods \u00b6 add_models clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Updates the chosen model and the arm internals (the actual implementation is in Bandit._learn_one). Parameters x y predict_one Return the prediction of the best model (defined as the one who maximises average reward). Parameters x References \u00b6 Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3), 235-256. \u21a9 Lattimore, T., & Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press. \u21a9","title":"UCBRegressor"},{"location":"api/expert/UCBRegressor/#ucbregressor","text":"Upper Confidence Bound bandit for regression. The class offers 2 implementations of UCB: UCB1 from 1 , when the parameter delta has value None - UCB(delta) from 2 , when the parameter delta is in (0, 1) For this bandit, rewards are supposed to be 1-subgaussian (see Lattimore and Szepesv\u00e1ri, chapter 6, p. 91) hence the use of the StandardScaler and MaxAbsScaler as reward_scaler .","title":"UCBRegressor"},{"location":"api/expert/UCBRegressor/#parameters","text":"models ( List[ base.Estimator ] ) The models to compare. metric ( river.metrics.base.RegressionMetric ) \u2013 defaults to None Metric used for comparing models with. delta ( float ) \u2013 defaults to None For UCB(delta) implementation. Lower value means more exploration. explore_each_arm ( int ) \u2013 defaults to 1 The number of times each arm should explored first. start_after ( int ) \u2013 defaults to 20 The number of iteration after which the bandit mechanism should begin. seed ( int ) \u2013 defaults to None The seed for the algorithm (since not deterministic).","title":"Parameters"},{"location":"api/expert/UCBRegressor/#attributes","text":"best_model Returns the best model, defined as the one who maximises average reward. percentage_pulled Returns the number of times (in %) each arm has been pulled.","title":"Attributes"},{"location":"api/expert/UCBRegressor/#examples","text":"Let's use UCBRegressor to select the best learning rate for a linear regression model. First, we define the grid of models: >>> from river import compose >>> from river import linear_model >>> from river import preprocessing >>> from river import optim >>> models = [ ... compose . Pipeline ( ... preprocessing . StandardScaler (), ... linear_model . LinearRegression ( optimizer = optim . SGD ( lr = lr )) ... ) ... for lr in [ 1e-4 , 1e-3 , 1e-2 , 1e-1 ] ... ] We decide to use TrumpApproval dataset: >>> from river import datasets >>> dataset = datasets . TrumpApproval () We use the UCB bandit: >>> from river.expert import UCBRegressor >>> bandit = UCBRegressor ( models = models , seed = 1 ) The models in the bandit can be trained in an online fashion. >>> for x , y in dataset : ... bandit = bandit . learn_one ( x = x , y = y ) We can inspect the number of times (in percentage) each arm has been pulled. >>> for model , pct in zip ( bandit . models , bandit . percentage_pulled ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { pct : .2% } \" ) 1.0e-04 \u2014 2.45 % 1.0e-03 \u2014 2.45 % 1.0e-02 \u2014 92.25 % 1.0e-01 \u2014 2.85 % The average reward of each model is also available: >>> for model , avg in zip ( bandit . models , bandit . average_reward ): ... lr = model [ \"LinearRegression\" ] . optimizer . learning_rate ... print ( f \" { lr : .1e } \u2014 { avg : .2f } \" ) 1.0e-04 \u2014 0.00 1.0e-03 \u2014 0.00 1.0e-02 \u2014 0.74 1.0e-01 \u2014 0.05 We can also select the best model (the one with the highest average reward). >>> best_model = bandit . best_model The learning rate chosen by the bandit is: >>> best_model [ \"LinearRegression\" ] . intercept_lr . learning_rate 0.01","title":"Examples"},{"location":"api/expert/UCBRegressor/#methods","text":"add_models clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Updates the chosen model and the arm internals (the actual implementation is in Bandit._learn_one). Parameters x y predict_one Return the prediction of the best model (defined as the one who maximises average reward). Parameters x","title":"Methods"},{"location":"api/expert/UCBRegressor/#references","text":"Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3), 235-256. \u21a9 Lattimore, T., & Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press. \u21a9","title":"References"},{"location":"api/facto/FFMClassifier/","text":"FFMClassifier \u00b6 Field-aware Factorization Machine for binary classification. The model equation is defined by: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}\\] Where \\mathbf{v} {j, f {j'}} is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] . Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, False ) ... ) >>> model = facto . FFMClassifier ( ... n_factors = 10 , ... intercept = .5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) True Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).","title":"FFMClassifier"},{"location":"api/facto/FFMClassifier/#ffmclassifier","text":"Field-aware Factorization Machine for binary classification. The model equation is defined by: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}\\] Where \\mathbf{v} {j, f {j'}} is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] .","title":"FFMClassifier"},{"location":"api/facto/FFMClassifier/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FFMClassifier/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/FFMClassifier/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, False ) ... ) >>> model = facto . FFMClassifier ( ... n_factors = 10 , ... intercept = .5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) True","title":"Examples"},{"location":"api/facto/FFMClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/facto/FFMClassifier/#references","text":"Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50).","title":"References"},{"location":"api/facto/FFMRegressor/","text":"FFMRegressor \u00b6 Field-aware Factorization Machine for regression. The model equation is defined by: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}\\] Where \\mathbf{v} {j, f {j'}} is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] . Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, 2 ) ... ) >>> model = facto . FFMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) 5.319945 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). \u21a9","title":"FFMRegressor"},{"location":"api/facto/FFMRegressor/#ffmregressor","text":"Field-aware Factorization Machine for regression. The model equation is defined by: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'}\\] Where \\mathbf{v} {j, f {j'}} is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\mathbf{v}_{j', f_j} is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] .","title":"FFMRegressor"},{"location":"api/facto/FFMRegressor/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FFMRegressor/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/FFMRegressor/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, 2 ) ... ) >>> model = facto . FFMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) 5.319945","title":"Examples"},{"location":"api/facto/FFMRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/facto/FFMRegressor/#references","text":"Juan, Y., Zhuang, Y., Chin, W.S. and Lin, C.J., 2016, September. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems (pp. 43-50). \u21a9","title":"References"},{"location":"api/facto/FMClassifier/","text":"FMClassifier \u00b6 Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, False ) ... ) >>> model = facto . FMClassifier ( ... n_factors = 10 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) True Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9 Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages. \u21a9","title":"FMClassifier"},{"location":"api/facto/FMClassifier/#fmclassifier","text":"Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables.","title":"FMClassifier"},{"location":"api/facto/FMClassifier/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FMClassifier/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/FMClassifier/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, False ) ... ) >>> model = facto . FMClassifier ( ... n_factors = 10 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) True","title":"Examples"},{"location":"api/facto/FMClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/facto/FMClassifier/#references","text":"Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9 Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages. \u21a9","title":"References"},{"location":"api/facto/FMRegressor/","text":"FMRegressor \u00b6 Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = facto . FMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) 5.236504 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9 Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages. \u21a9","title":"FMRegressor"},{"location":"api/facto/FMRegressor/#fmregressor","text":"Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables.","title":"FMRegressor"},{"location":"api/facto/FMRegressor/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FMRegressor/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/FMRegressor/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = facto . FMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) 5.236504","title":"Examples"},{"location":"api/facto/FMRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/facto/FMRegressor/#references","text":"Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9 Rendle, S., 2012, May. Factorization Machines with libFM. In ACM Transactions on Intelligent Systems and Technology 3, 3, Article 57, 22 pages. \u21a9","title":"References"},{"location":"api/facto/FwFMClassifier/","text":"FwFMClassifier \u00b6 Field-weighted Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(f_j\\) and \\(f_{j'}\\) are \\(j\\) and \\(j'\\) fields, respectively, and \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] . Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. int_weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the field pairs interaction weights. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. interaction_weights The current interaction strengths of field pairs. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, False ) ... ) >>> model = facto . FwFMClassifier ( ... n_factors = 10 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) True Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349\u20131357). \u21a9","title":"FwFMClassifier"},{"location":"api/facto/FwFMClassifier/#fwfmclassifier","text":"Field-weighted Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(f_j\\) and \\(f_{j'}\\) are \\(j\\) and \\(j'\\) fields, respectively, and \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] .","title":"FwFMClassifier"},{"location":"api/facto/FwFMClassifier/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. int_weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the field pairs interaction weights. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FwFMClassifier/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features. interaction_weights The current interaction strengths of field pairs.","title":"Attributes"},{"location":"api/facto/FwFMClassifier/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, False ) ... ) >>> model = facto . FwFMClassifier ( ... n_factors = 10 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) True","title":"Examples"},{"location":"api/facto/FwFMClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/facto/FwFMClassifier/#references","text":"Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349\u20131357). \u21a9","title":"References"},{"location":"api/facto/FwFMRegressor/","text":"FwFMRegressor \u00b6 Field-weighted Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(f_j\\) and \\(f_{j'}\\) are \\(j\\) and \\(j'\\) fields, respectively, and \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] . Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. int_weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the field pairs interaction weights. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. interaction_weights The current interaction strengths of field pairs. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = facto . FwFMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) 5.236501 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349\u20131357). \u21a9","title":"FwFMRegressor"},{"location":"api/facto/FwFMRegressor/#fwfmregressor","text":"Field-weighted Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'}\\] Where \\(f_j\\) and \\(f_{j'}\\) are \\(j\\) and \\(j'\\) fields, respectively, and \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors, respectively. For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Field names are inferred from feature names by taking everything before the first underscore: feature_name.split('_')[0] .","title":"FwFMRegressor"},{"location":"api/facto/FwFMRegressor/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. int_weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the field pairs interaction weights. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/FwFMRegressor/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features. interaction_weights The current interaction strengths of field pairs.","title":"Attributes"},{"location":"api/facto/FwFMRegressor/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = facto . FwFMRegressor ( ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... model = model . learn_one ( x , y ) >>> model . predict_one ({ 'Bob' : 1 , 'Harry Potter' : 1 }) 5.236501","title":"Examples"},{"location":"api/facto/FwFMRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/facto/FwFMRegressor/#references","text":"Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu, 2018, April. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, (pp. 1349\u20131357). \u21a9","title":"References"},{"location":"api/facto/HOFMClassifier/","text":"HOFMClassifier \u00b6 Higher-Order Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)\\] For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Parameters \u00b6 degree \u2013 defaults to 3 Polynomial degree or model order. n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, False ) ... ) >>> model = facto . HOFMClassifier ( ... degree = 3 , ... n_factors = 10 , ... intercept = .5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) True Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9","title":"HOFMClassifier"},{"location":"api/facto/HOFMClassifier/#hofmclassifier","text":"Higher-Order Factorization Machine for binary classification. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)\\] For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables.","title":"HOFMClassifier"},{"location":"api/facto/HOFMClassifier/#parameters","text":"degree \u2013 defaults to 3 Polynomial degree or model order. n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note that the intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/HOFMClassifier/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/HOFMClassifier/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, True ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, False ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, True ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, False ) ... ) >>> model = facto . HOFMClassifier ( ... degree = 3 , ... n_factors = 10 , ... intercept = .5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) True","title":"Examples"},{"location":"api/facto/HOFMClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) sample_weight \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/facto/HOFMClassifier/#references","text":"Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9","title":"References"},{"location":"api/facto/HOFMRegressor/","text":"HOFMRegressor \u00b6 Higher-Order Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)\\] For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables. Parameters \u00b6 degree \u2013 defaults to 3 Polynomial degree or model order. n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note thatthe intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 weights The current weights assigned to the features. latents The current latent weights assigned to the features. Examples \u00b6 >>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, 2 ) ... ) >>> model = facto . HOFMRegressor ( ... degree = 3 , ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) 5.311745 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9","title":"HOFMRegressor"},{"location":"api/facto/HOFMRegressor/#hofmregressor","text":"Higher-Order Factorization Machine for regression. The model equation is defined as: \\[\\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right)\\] For more efficiency, this model automatically one-hot encodes strings features considering them as categorical variables.","title":"HOFMRegressor"},{"location":"api/facto/HOFMRegressor/#parameters","text":"degree \u2013 defaults to 3 Polynomial degree or model order. n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. weight_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the feature weights. Note thatthe intercept is handled separately. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. sample_normalization \u2013 defaults to False Whether to divide each element of x by x 's L2-norm. l1_weight \u2013 defaults to 0.0 Amount of L1 regularization used to push weights towards 0. l2_weight \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. l1_latent \u2013 defaults to 0.0 Amount of L1 regularization used to push latent weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. intercept \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. An instance of optim.schedulers.Constant is used if a float is passed. No intercept will be used if this is set to 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Defaults to optim.initializers.Zeros() . latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. Defaults to optim.initializers.Normal(mu=.0, sigma=.1, random_state=self.random_state) . clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/facto/HOFMRegressor/#attributes","text":"weights The current weights assigned to the features. latents The current latent weights assigned to the features.","title":"Attributes"},{"location":"api/facto/HOFMRegressor/#examples","text":">>> from river import facto >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' , 'time' : .12 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' , 'time' : .13 }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' , 'time' : .14 }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' , 'time' : .15 }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter ' , 'time' : .16 }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' , 'time' : .13 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' , 'time' : .12 }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' , 'time' : .16 }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' , 'time' : .10 }, 2 ) ... ) >>> model = facto . HOFMRegressor ( ... degree = 3 , ... n_factors = 10 , ... intercept = 5 , ... seed = 42 , ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' , 'time' : .14 }) 5.311745","title":"Examples"},{"location":"api/facto/HOFMRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) sample_weight \u2013 defaults to 1.0 Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/facto/HOFMRegressor/#references","text":"Rendle, S., 2010, December. Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE. \u21a9","title":"References"},{"location":"api/feature-extraction/Agg/","text":"Agg \u00b6 Computes a streaming aggregate. This transformer allows to compute an aggregate statistic, very much like the groupby method from pandas , but on a streaming dataset. This makes use of the streaming statistics from the stats module. When learn_one is called, the running statistic how of group by is updated with the value of on . Meanwhile, the output of transform_one is a single-element dictionary, where the key is the name of the aggregate and the value is the current value of the statistic for the relevant group. The key is automatically inferred from the parameters. Note that you can use a compose.TransformerUnion to extract many aggregate statistics in a concise manner. Parameters \u00b6 on ( str ) The feature on which to compute the aggregate statistic. by ( Union[str, List[str]] ) The feature by which to group the data. how ( river.stats.base.Univariate ) The statistic to compute. Attributes \u00b6 groups ( collections.defaultdict ) Maps group keys to univariate statistics. feature_name ( str ) The name of the feature used in the output. Examples \u00b6 Consider the following dataset: >>> X = [ ... { 'country' : 'France' , 'place' : 'Taco Bell' , 'revenue' : 42 }, ... { 'country' : 'Sweden' , 'place' : 'Burger King' , 'revenue' : 16 }, ... { 'country' : 'France' , 'place' : 'Burger King' , 'revenue' : 24 }, ... { 'country' : 'Sweden' , 'place' : 'Taco Bell' , 'revenue' : 58 }, ... { 'country' : 'Sweden' , 'place' : 'Burger King' , 'revenue' : 20 }, ... { 'country' : 'France' , 'place' : 'Taco Bell' , 'revenue' : 50 }, ... { 'country' : 'France' , 'place' : 'Burger King' , 'revenue' : 10 }, ... { 'country' : 'Sweden' , 'place' : 'Taco Bell' , 'revenue' : 80 } ... ] As an example, we can calculate the average (how) revenue (on) for each place (by): >>> from river import feature_extraction as fx >>> from river import stats >>> agg = fx . Agg ( ... on = 'revenue' , ... by = 'place' , ... how = stats . Mean () ... ) >>> for x in X : ... agg = agg . learn_one ( x ) ... print ( agg . transform_one ( x )) { 'revenue_mean_by_place' : 42.0 } { 'revenue_mean_by_place' : 16.0 } { 'revenue_mean_by_place' : 20.0 } { 'revenue_mean_by_place' : 50.0 } { 'revenue_mean_by_place' : 20.0 } { 'revenue_mean_by_place' : 50.0 } { 'revenue_mean_by_place' : 17.5 } { 'revenue_mean_by_place' : 57.5 } You can compute an aggregate over multiple keys by passing a tuple to the by argument. For instance, we can compute the maximum (how) revenue (on) per place as well as per day (by): >>> agg = fx . Agg ( ... on = 'revenue' , ... by = [ 'place' , 'country' ], ... how = stats . Max () ... ) >>> for x in X : ... agg = agg . learn_one ( x ) ... print ( agg . transform_one ( x )) { 'revenue_max_by_place_and_country' : 42 } { 'revenue_max_by_place_and_country' : 16 } { 'revenue_max_by_place_and_country' : 24 } { 'revenue_max_by_place_and_country' : 58 } { 'revenue_max_by_place_and_country' : 20 } { 'revenue_max_by_place_and_country' : 50 } { 'revenue_max_by_place_and_country' : 24 } { 'revenue_max_by_place_and_country' : 80 } You can use a compose.TransformerUnion in order to calculate multiple aggregates in one go. The latter can be constructed by using the + operator: >>> agg = ( ... fx . Agg ( on = 'revenue' , by = 'place' , how = stats . Mean ()) + ... fx . Agg ( on = 'revenue' , by = [ 'place' , 'country' ], how = stats . Max ()) ... ) >>> import pprint >>> for x in X : ... agg = agg . learn_one ( x ) ... pprint . pprint ( agg . transform_one ( x )) { 'revenue_max_by_place_and_country' : 42 , 'revenue_mean_by_place' : 42.0 } { 'revenue_max_by_place_and_country' : 16 , 'revenue_mean_by_place' : 16.0 } { 'revenue_max_by_place_and_country' : 24 , 'revenue_mean_by_place' : 20.0 } { 'revenue_max_by_place_and_country' : 58 , 'revenue_mean_by_place' : 50.0 } { 'revenue_max_by_place_and_country' : 20 , 'revenue_mean_by_place' : 20.0 } { 'revenue_max_by_place_and_country' : 50 , 'revenue_mean_by_place' : 50.0 } { 'revenue_max_by_place_and_country' : 24 , 'revenue_mean_by_place' : 17.5 } { 'revenue_max_by_place_and_country' : 80 , 'revenue_mean_by_place' : 57.5 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 Streaming groupbys in pandas for big datasets \u21a9","title":"Agg"},{"location":"api/feature-extraction/Agg/#agg","text":"Computes a streaming aggregate. This transformer allows to compute an aggregate statistic, very much like the groupby method from pandas , but on a streaming dataset. This makes use of the streaming statistics from the stats module. When learn_one is called, the running statistic how of group by is updated with the value of on . Meanwhile, the output of transform_one is a single-element dictionary, where the key is the name of the aggregate and the value is the current value of the statistic for the relevant group. The key is automatically inferred from the parameters. Note that you can use a compose.TransformerUnion to extract many aggregate statistics in a concise manner.","title":"Agg"},{"location":"api/feature-extraction/Agg/#parameters","text":"on ( str ) The feature on which to compute the aggregate statistic. by ( Union[str, List[str]] ) The feature by which to group the data. how ( river.stats.base.Univariate ) The statistic to compute.","title":"Parameters"},{"location":"api/feature-extraction/Agg/#attributes","text":"groups ( collections.defaultdict ) Maps group keys to univariate statistics. feature_name ( str ) The name of the feature used in the output.","title":"Attributes"},{"location":"api/feature-extraction/Agg/#examples","text":"Consider the following dataset: >>> X = [ ... { 'country' : 'France' , 'place' : 'Taco Bell' , 'revenue' : 42 }, ... { 'country' : 'Sweden' , 'place' : 'Burger King' , 'revenue' : 16 }, ... { 'country' : 'France' , 'place' : 'Burger King' , 'revenue' : 24 }, ... { 'country' : 'Sweden' , 'place' : 'Taco Bell' , 'revenue' : 58 }, ... { 'country' : 'Sweden' , 'place' : 'Burger King' , 'revenue' : 20 }, ... { 'country' : 'France' , 'place' : 'Taco Bell' , 'revenue' : 50 }, ... { 'country' : 'France' , 'place' : 'Burger King' , 'revenue' : 10 }, ... { 'country' : 'Sweden' , 'place' : 'Taco Bell' , 'revenue' : 80 } ... ] As an example, we can calculate the average (how) revenue (on) for each place (by): >>> from river import feature_extraction as fx >>> from river import stats >>> agg = fx . Agg ( ... on = 'revenue' , ... by = 'place' , ... how = stats . Mean () ... ) >>> for x in X : ... agg = agg . learn_one ( x ) ... print ( agg . transform_one ( x )) { 'revenue_mean_by_place' : 42.0 } { 'revenue_mean_by_place' : 16.0 } { 'revenue_mean_by_place' : 20.0 } { 'revenue_mean_by_place' : 50.0 } { 'revenue_mean_by_place' : 20.0 } { 'revenue_mean_by_place' : 50.0 } { 'revenue_mean_by_place' : 17.5 } { 'revenue_mean_by_place' : 57.5 } You can compute an aggregate over multiple keys by passing a tuple to the by argument. For instance, we can compute the maximum (how) revenue (on) per place as well as per day (by): >>> agg = fx . Agg ( ... on = 'revenue' , ... by = [ 'place' , 'country' ], ... how = stats . Max () ... ) >>> for x in X : ... agg = agg . learn_one ( x ) ... print ( agg . transform_one ( x )) { 'revenue_max_by_place_and_country' : 42 } { 'revenue_max_by_place_and_country' : 16 } { 'revenue_max_by_place_and_country' : 24 } { 'revenue_max_by_place_and_country' : 58 } { 'revenue_max_by_place_and_country' : 20 } { 'revenue_max_by_place_and_country' : 50 } { 'revenue_max_by_place_and_country' : 24 } { 'revenue_max_by_place_and_country' : 80 } You can use a compose.TransformerUnion in order to calculate multiple aggregates in one go. The latter can be constructed by using the + operator: >>> agg = ( ... fx . Agg ( on = 'revenue' , by = 'place' , how = stats . Mean ()) + ... fx . Agg ( on = 'revenue' , by = [ 'place' , 'country' ], how = stats . Max ()) ... ) >>> import pprint >>> for x in X : ... agg = agg . learn_one ( x ) ... pprint . pprint ( agg . transform_one ( x )) { 'revenue_max_by_place_and_country' : 42 , 'revenue_mean_by_place' : 42.0 } { 'revenue_max_by_place_and_country' : 16 , 'revenue_mean_by_place' : 16.0 } { 'revenue_max_by_place_and_country' : 24 , 'revenue_mean_by_place' : 20.0 } { 'revenue_max_by_place_and_country' : 58 , 'revenue_mean_by_place' : 50.0 } { 'revenue_max_by_place_and_country' : 20 , 'revenue_mean_by_place' : 20.0 } { 'revenue_max_by_place_and_country' : 50 , 'revenue_mean_by_place' : 50.0 } { 'revenue_max_by_place_and_country' : 24 , 'revenue_mean_by_place' : 17.5 } { 'revenue_max_by_place_and_country' : 80 , 'revenue_mean_by_place' : 57.5 }","title":"Examples"},{"location":"api/feature-extraction/Agg/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/Agg/#references","text":"Streaming groupbys in pandas for big datasets \u21a9","title":"References"},{"location":"api/feature-extraction/BagOfWords/","text":"BagOfWords \u00b6 Counts tokens in sentences. This transformer can be used to counts tokens in a given piece of text. It takes care of normalizing the text before tokenizing it. In mini-batch settings, this transformers allows to convert a series of pandas of text into sparse dataframe. Note that the parameters are identical to those of feature_extraction.TFIDF . Parameters \u00b6 on ( str ) \u2013 defaults to None The name of the feature that contains the text to vectorize. If None , then each learn_one and transform_one will assume that each x that is provided is a str , andnot a dict . strip_accents \u2013 defaults to True Whether or not to strip accent characters. lowercase \u2013 defaults to True Whether or not to convert all characters to lowercase. preprocessor ( Callable ) \u2013 defaults to None Override the preprocessing step while preserving the tokenizing and n-grams generation steps. tokenizer ( Callable ) \u2013 defaults to None A function used to convert preprocessed text into a dict of tokens. By default, a regex formula that works well in most cases is used. ngram_range \u2013 defaults to (1, 1) The lower and upper boundary of the range n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Examples \u00b6 By default, BagOfWords will take as input a sentence, preprocess it, tokenize the preprocessed text, and then return a collections.Counter containing the number of occurrences of each token. >>> from river import feature_extraction as fx >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> bow = fx . BagOfWords () >>> for sentence in corpus : ... print ( bow . transform_one ( sentence )) Counter ({ 'this' : 1 , 'is' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Counter ({ 'document' : 2 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'second' : 1 }) Counter ({ 'and' : 1 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'third' : 1 , 'one' : 1 }) Counter ({ 'is' : 1 , 'this' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Note that learn_one does not have to be called because BagOfWords is stateless. You can call it but it won't do anything. In the above example, a string is passed to transform_one . You can also indicate which field to access if the string is stored in a dictionary: >>> bow = fx . BagOfWords ( on = 'sentence' ) >>> for sentence in corpus : ... x = { 'sentence' : sentence } ... print ( bow . transform_one ( x )) Counter ({ 'this' : 1 , 'is' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Counter ({ 'document' : 2 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'second' : 1 }) Counter ({ 'and' : 1 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'third' : 1 , 'one' : 1 }) Counter ({ 'is' : 1 , 'this' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) The ngram_range parameter can be used to extract n-grams (including unigrams): >>> ngrammer = fx . BagOfWords ( ngram_range = ( 1 , 2 )) >>> ngrams = ngrammer . transform_one ( 'I love the smell of napalm in the morning' ) >>> for ngram , count in ngrams . items (): ... print ( ngram , count ) love 1 the 2 smell 1 of 1 napalm 1 in 1 morning 1 ( 'love' , 'the' ) 1 ( 'the' , 'smell' ) 1 ( 'smell' , 'of' ) 1 ( 'of' , 'napalm' ) 1 ( 'napalm' , 'in' ) 1 ( 'in' , 'the' ) 1 ( 'the' , 'morning' ) 1 BagOfWord allows to build a term-frequency pandas sparse dataframe with the transform_many method. >>> import pandas as pd >>> X = pd . Series ([ 'Hello world' , 'Hello River' ], index = [ 'river' , 'rocks' ]) >>> bow = fx . BagOfWords () >>> bow . transform_many ( X = X ) hello world river river 1 1 0 rocks 1 0 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self process_text transform_many Transform pandas series of string into term-frequency pandas sparse dataframe. Parameters X ( pandas.core.series.Series ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"BagOfWords"},{"location":"api/feature-extraction/BagOfWords/#bagofwords","text":"Counts tokens in sentences. This transformer can be used to counts tokens in a given piece of text. It takes care of normalizing the text before tokenizing it. In mini-batch settings, this transformers allows to convert a series of pandas of text into sparse dataframe. Note that the parameters are identical to those of feature_extraction.TFIDF .","title":"BagOfWords"},{"location":"api/feature-extraction/BagOfWords/#parameters","text":"on ( str ) \u2013 defaults to None The name of the feature that contains the text to vectorize. If None , then each learn_one and transform_one will assume that each x that is provided is a str , andnot a dict . strip_accents \u2013 defaults to True Whether or not to strip accent characters. lowercase \u2013 defaults to True Whether or not to convert all characters to lowercase. preprocessor ( Callable ) \u2013 defaults to None Override the preprocessing step while preserving the tokenizing and n-grams generation steps. tokenizer ( Callable ) \u2013 defaults to None A function used to convert preprocessed text into a dict of tokens. By default, a regex formula that works well in most cases is used. ngram_range \u2013 defaults to (1, 1) The lower and upper boundary of the range n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.","title":"Parameters"},{"location":"api/feature-extraction/BagOfWords/#examples","text":"By default, BagOfWords will take as input a sentence, preprocess it, tokenize the preprocessed text, and then return a collections.Counter containing the number of occurrences of each token. >>> from river import feature_extraction as fx >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> bow = fx . BagOfWords () >>> for sentence in corpus : ... print ( bow . transform_one ( sentence )) Counter ({ 'this' : 1 , 'is' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Counter ({ 'document' : 2 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'second' : 1 }) Counter ({ 'and' : 1 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'third' : 1 , 'one' : 1 }) Counter ({ 'is' : 1 , 'this' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Note that learn_one does not have to be called because BagOfWords is stateless. You can call it but it won't do anything. In the above example, a string is passed to transform_one . You can also indicate which field to access if the string is stored in a dictionary: >>> bow = fx . BagOfWords ( on = 'sentence' ) >>> for sentence in corpus : ... x = { 'sentence' : sentence } ... print ( bow . transform_one ( x )) Counter ({ 'this' : 1 , 'is' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) Counter ({ 'document' : 2 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'second' : 1 }) Counter ({ 'and' : 1 , 'this' : 1 , 'is' : 1 , 'the' : 1 , 'third' : 1 , 'one' : 1 }) Counter ({ 'is' : 1 , 'this' : 1 , 'the' : 1 , 'first' : 1 , 'document' : 1 }) The ngram_range parameter can be used to extract n-grams (including unigrams): >>> ngrammer = fx . BagOfWords ( ngram_range = ( 1 , 2 )) >>> ngrams = ngrammer . transform_one ( 'I love the smell of napalm in the morning' ) >>> for ngram , count in ngrams . items (): ... print ( ngram , count ) love 1 the 2 smell 1 of 1 napalm 1 in 1 morning 1 ( 'love' , 'the' ) 1 ( 'the' , 'smell' ) 1 ( 'smell' , 'of' ) 1 ( 'of' , 'napalm' ) 1 ( 'napalm' , 'in' ) 1 ( 'in' , 'the' ) 1 ( 'the' , 'morning' ) 1 BagOfWord allows to build a term-frequency pandas sparse dataframe with the transform_many method. >>> import pandas as pd >>> X = pd . Series ([ 'Hello world' , 'Hello River' ], index = [ 'river' , 'rocks' ]) >>> bow = fx . BagOfWords () >>> bow . transform_many ( X = X ) hello world river river 1 1 0 rocks 1 0 1","title":"Examples"},{"location":"api/feature-extraction/BagOfWords/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self process_text transform_many Transform pandas series of string into term-frequency pandas sparse dataframe. Parameters X ( pandas.core.series.Series ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/PolynomialExtender/","text":"PolynomialExtender \u00b6 Polynomial feature extender. Generate features consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. Be aware that the number of outputted features scales polynomially in the number of input features and exponentially in the degree. High degrees can cause overfitting. Parameters \u00b6 degree \u2013 defaults to 2 The maximum degree of the polynomial features. interaction_only \u2013 defaults to False If True then only combinations that include an element at most once will be computed. include_bias \u2013 defaults to False Whether or not to include a dummy feature which is always equal to 1. bias_name \u2013 defaults to bias Name to give to the bias feature. Examples \u00b6 >>> from river import feature_extraction as fx >>> X = [ ... { 'x' : 0 , 'y' : 1 }, ... { 'x' : 2 , 'y' : 3 }, ... { 'x' : 4 , 'y' : 5 } ... ] >>> poly = fx . PolynomialExtender ( degree = 2 , include_bias = True ) >>> for x in X : ... print ( poly . transform_one ( x )) { 'x' : 0 , 'y' : 1 , 'x*x' : 0 , 'x*y' : 0 , 'y*y' : 1 , 'bias' : 1 } { 'x' : 2 , 'y' : 3 , 'x*x' : 4 , 'x*y' : 6 , 'y*y' : 9 , 'bias' : 1 } { 'x' : 4 , 'y' : 5 , 'x*x' : 16 , 'x*y' : 20 , 'y*y' : 25 , 'bias' : 1 } >>> X = [ ... { 'x' : 0 , 'y' : 1 , 'z' : 2 }, ... { 'x' : 2 , 'y' : 3 , 'z' : 2 }, ... { 'x' : 4 , 'y' : 5 , 'z' : 2 } ... ] >>> poly = fx . PolynomialExtender ( degree = 3 , interaction_only = True ) >>> for x in X : ... print ( poly . transform_one ( x )) { 'x' : 0 , 'y' : 1 , 'z' : 2 , 'x*y' : 0 , 'x*z' : 0 , 'y*z' : 2 , 'x*y*z' : 0 } { 'x' : 2 , 'y' : 3 , 'z' : 2 , 'x*y' : 6 , 'x*z' : 4 , 'y*z' : 6 , 'x*y*z' : 12 } { 'x' : 4 , 'y' : 5 , 'z' : 2 , 'x*y' : 20 , 'x*z' : 8 , 'y*z' : 10 , 'x*y*z' : 40 } Polynomial features are typically used for a linear model to capture interactions between features. This may done by setting up a pipeline, as so: >>> from river import datasets >>> from river import evaluate >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = ( ... fx . PolynomialExtender () | ... pp . StandardScaler () | ... lm . LogisticRegression () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.88 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"PolynomialExtender"},{"location":"api/feature-extraction/PolynomialExtender/#polynomialextender","text":"Polynomial feature extender. Generate features consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. Be aware that the number of outputted features scales polynomially in the number of input features and exponentially in the degree. High degrees can cause overfitting.","title":"PolynomialExtender"},{"location":"api/feature-extraction/PolynomialExtender/#parameters","text":"degree \u2013 defaults to 2 The maximum degree of the polynomial features. interaction_only \u2013 defaults to False If True then only combinations that include an element at most once will be computed. include_bias \u2013 defaults to False Whether or not to include a dummy feature which is always equal to 1. bias_name \u2013 defaults to bias Name to give to the bias feature.","title":"Parameters"},{"location":"api/feature-extraction/PolynomialExtender/#examples","text":">>> from river import feature_extraction as fx >>> X = [ ... { 'x' : 0 , 'y' : 1 }, ... { 'x' : 2 , 'y' : 3 }, ... { 'x' : 4 , 'y' : 5 } ... ] >>> poly = fx . PolynomialExtender ( degree = 2 , include_bias = True ) >>> for x in X : ... print ( poly . transform_one ( x )) { 'x' : 0 , 'y' : 1 , 'x*x' : 0 , 'x*y' : 0 , 'y*y' : 1 , 'bias' : 1 } { 'x' : 2 , 'y' : 3 , 'x*x' : 4 , 'x*y' : 6 , 'y*y' : 9 , 'bias' : 1 } { 'x' : 4 , 'y' : 5 , 'x*x' : 16 , 'x*y' : 20 , 'y*y' : 25 , 'bias' : 1 } >>> X = [ ... { 'x' : 0 , 'y' : 1 , 'z' : 2 }, ... { 'x' : 2 , 'y' : 3 , 'z' : 2 }, ... { 'x' : 4 , 'y' : 5 , 'z' : 2 } ... ] >>> poly = fx . PolynomialExtender ( degree = 3 , interaction_only = True ) >>> for x in X : ... print ( poly . transform_one ( x )) { 'x' : 0 , 'y' : 1 , 'z' : 2 , 'x*y' : 0 , 'x*z' : 0 , 'y*z' : 2 , 'x*y*z' : 0 } { 'x' : 2 , 'y' : 3 , 'z' : 2 , 'x*y' : 6 , 'x*z' : 4 , 'y*z' : 6 , 'x*y*z' : 12 } { 'x' : 4 , 'y' : 5 , 'z' : 2 , 'x*y' : 20 , 'x*z' : 8 , 'y*z' : 10 , 'x*y*z' : 40 } Polynomial features are typically used for a linear model to capture interactions between features. This may done by setting up a pipeline, as so: >>> from river import datasets >>> from river import evaluate >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = ( ... fx . PolynomialExtender () | ... pp . StandardScaler () | ... lm . LogisticRegression () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.88 %","title":"Examples"},{"location":"api/feature-extraction/PolynomialExtender/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/RBFSampler/","text":"RBFSampler \u00b6 Extracts random features which approximate an RBF kernel. This is a powerful way to give non-linear capacity to linear classifiers. This method is also called \"random Fourier features\" in the literature. Parameters \u00b6 gamma \u2013 defaults to 1.0 RBF kernel parameter in (-gamma * x^2) . n_components \u2013 defaults to 100 Number of samples per original feature. Equals the dimensionality of the computed feature space. seed ( int ) \u2013 defaults to None Random number seed. Examples \u00b6 >>> from river import feature_extraction as fx >>> from river import linear_model as lm >>> from river import optim >>> from river import stream >>> # XOR function >>> X = [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 0 ], [ 0 , 1 ]] >>> Y = [ 0 , 0 , 1 , 1 ] >>> model = lm . LogisticRegression ( optimizer = optim . SGD ( .1 )) >>> for x , y in stream . iter_array ( X , Y ): ... model = model . learn_one ( x , y ) ... y_pred = model . predict_one ( x ) ... print ( y , int ( y_pred )) 0 0 0 0 1 0 1 1 >>> model = ( ... fx . RBFSampler ( seed = 3 ) | ... lm . LogisticRegression ( optimizer = optim . SGD ( .1 )) ... ) >>> for x , y in stream . iter_array ( X , Y ): ... model = model . learn_one ( x , y ) ... y_pred = model . predict_one ( x ) ... print ( y , int ( y_pred )) 0 0 0 0 1 1 1 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) y \u2013 defaults to None Returns dict : The transformed values. References \u00b6 Rahimi, A. and Recht, B., 2008. Random features for large-scale kernel machines. In Advances in neural information processing systems (pp. 1177-1184 \u21a9","title":"RBFSampler"},{"location":"api/feature-extraction/RBFSampler/#rbfsampler","text":"Extracts random features which approximate an RBF kernel. This is a powerful way to give non-linear capacity to linear classifiers. This method is also called \"random Fourier features\" in the literature.","title":"RBFSampler"},{"location":"api/feature-extraction/RBFSampler/#parameters","text":"gamma \u2013 defaults to 1.0 RBF kernel parameter in (-gamma * x^2) . n_components \u2013 defaults to 100 Number of samples per original feature. Equals the dimensionality of the computed feature space. seed ( int ) \u2013 defaults to None Random number seed.","title":"Parameters"},{"location":"api/feature-extraction/RBFSampler/#examples","text":">>> from river import feature_extraction as fx >>> from river import linear_model as lm >>> from river import optim >>> from river import stream >>> # XOR function >>> X = [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 0 ], [ 0 , 1 ]] >>> Y = [ 0 , 0 , 1 , 1 ] >>> model = lm . LogisticRegression ( optimizer = optim . SGD ( .1 )) >>> for x , y in stream . iter_array ( X , Y ): ... model = model . learn_one ( x , y ) ... y_pred = model . predict_one ( x ) ... print ( y , int ( y_pred )) 0 0 0 0 1 0 1 1 >>> model = ( ... fx . RBFSampler ( seed = 3 ) | ... lm . LogisticRegression ( optimizer = optim . SGD ( .1 )) ... ) >>> for x , y in stream . iter_array ( X , Y ): ... model = model . learn_one ( x , y ) ... y_pred = model . predict_one ( x ) ... print ( y , int ( y_pred )) 0 0 0 0 1 1 1 1","title":"Examples"},{"location":"api/feature-extraction/RBFSampler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) y \u2013 defaults to None Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/RBFSampler/#references","text":"Rahimi, A. and Recht, B., 2008. Random features for large-scale kernel machines. In Advances in neural information processing systems (pp. 1177-1184 \u21a9","title":"References"},{"location":"api/feature-extraction/TFIDF/","text":"TFIDF \u00b6 Computes TF-IDF values from sentences. The TF-IDF formula is the same one as scikit-learn. The only difference is the fact that the document frequencies are determined online, whereas in a batch setting they can be determined by performing an initial pass through the data. Note that the parameters are identical to those of feature_extraction.BagOfWords . Parameters \u00b6 normalize \u2013 defaults to True Whether or not the TF-IDF values by their L2 norm. on ( str ) \u2013 defaults to None The name of the feature that contains the text to vectorize. If None , then the input is treated as a document instead of a set of features. strip_accents \u2013 defaults to True Whether or not to strip accent characters. lowercase \u2013 defaults to True Whether or not to convert all characters to lowercase. preprocessor ( Callable ) \u2013 defaults to None Override the preprocessing step while preserving the tokenizing and n-grams generation steps. tokenizer ( Callable ) \u2013 defaults to None A function used to convert preprocessed text into a dict of tokens. By default, a regex formula that works well in most cases is used. ngram_range \u2013 defaults to (1, 1) The lower and upper boundary of the range n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only works if tokenizer is not set to False . Attributes \u00b6 dfs ( collections.defaultdict) ) Document counts. n ( int ) Number of scanned documents. Examples \u00b6 >>> from river import feature_extraction >>> tfidf = feature_extraction . TFIDF () >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> for sentence in corpus : ... tfidf = tfidf . learn_one ( sentence ) ... print ( tfidf . transform_one ( sentence )) { 'this' : 0.447 , 'is' : 0.447 , 'the' : 0.447 , 'first' : 0.447 , 'document' : 0.447 } { 'this' : 0.333 , 'document' : 0.667 , 'is' : 0.333 , 'the' : 0.333 , 'second' : 0.469 } { 'and' : 0.497 , 'this' : 0.293 , 'is' : 0.293 , 'the' : 0.293 , 'third' : 0.497 , 'one' : 0.497 } { 'is' : 0.384 , 'this' : 0.384 , 'the' : 0.384 , 'first' : 0.580 , 'document' : 0.469 } In the above example, a string is passed to transform_one . You can also indicate which field to access if the string is stored in a dictionary: >>> tfidf = feature_extraction . TFIDF ( on = 'sentence' ) >>> for sentence in corpus : ... x = { 'sentence' : sentence } ... tfidf = tfidf . learn_one ( x ) ... print ( tfidf . transform_one ( x )) { 'this' : 0.447 , 'is' : 0.447 , 'the' : 0.447 , 'first' : 0.447 , 'document' : 0.447 } { 'this' : 0.333 , 'document' : 0.667 , 'is' : 0.333 , 'the' : 0.333 , 'second' : 0.469 } { 'and' : 0.497 , 'this' : 0.293 , 'is' : 0.293 , 'the' : 0.293 , 'third' : 0.497 , 'one' : 0.497 } { 'is' : 0.384 , 'this' : 0.384 , 'the' : 0.384 , 'first' : 0.580 , 'document' : 0.469 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self process_text transform_many Transform pandas series of string into term-frequency pandas sparse dataframe. Parameters X ( pandas.core.series.Series ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"TFIDF"},{"location":"api/feature-extraction/TFIDF/#tfidf","text":"Computes TF-IDF values from sentences. The TF-IDF formula is the same one as scikit-learn. The only difference is the fact that the document frequencies are determined online, whereas in a batch setting they can be determined by performing an initial pass through the data. Note that the parameters are identical to those of feature_extraction.BagOfWords .","title":"TFIDF"},{"location":"api/feature-extraction/TFIDF/#parameters","text":"normalize \u2013 defaults to True Whether or not the TF-IDF values by their L2 norm. on ( str ) \u2013 defaults to None The name of the feature that contains the text to vectorize. If None , then the input is treated as a document instead of a set of features. strip_accents \u2013 defaults to True Whether or not to strip accent characters. lowercase \u2013 defaults to True Whether or not to convert all characters to lowercase. preprocessor ( Callable ) \u2013 defaults to None Override the preprocessing step while preserving the tokenizing and n-grams generation steps. tokenizer ( Callable ) \u2013 defaults to None A function used to convert preprocessed text into a dict of tokens. By default, a regex formula that works well in most cases is used. ngram_range \u2013 defaults to (1, 1) The lower and upper boundary of the range n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only works if tokenizer is not set to False .","title":"Parameters"},{"location":"api/feature-extraction/TFIDF/#attributes","text":"dfs ( collections.defaultdict) ) Document counts. n ( int ) Number of scanned documents.","title":"Attributes"},{"location":"api/feature-extraction/TFIDF/#examples","text":">>> from river import feature_extraction >>> tfidf = feature_extraction . TFIDF () >>> corpus = [ ... 'This is the first document.' , ... 'This document is the second document.' , ... 'And this is the third one.' , ... 'Is this the first document?' , ... ] >>> for sentence in corpus : ... tfidf = tfidf . learn_one ( sentence ) ... print ( tfidf . transform_one ( sentence )) { 'this' : 0.447 , 'is' : 0.447 , 'the' : 0.447 , 'first' : 0.447 , 'document' : 0.447 } { 'this' : 0.333 , 'document' : 0.667 , 'is' : 0.333 , 'the' : 0.333 , 'second' : 0.469 } { 'and' : 0.497 , 'this' : 0.293 , 'is' : 0.293 , 'the' : 0.293 , 'third' : 0.497 , 'one' : 0.497 } { 'is' : 0.384 , 'this' : 0.384 , 'the' : 0.384 , 'first' : 0.580 , 'document' : 0.469 } In the above example, a string is passed to transform_one . You can also indicate which field to access if the string is stored in a dictionary: >>> tfidf = feature_extraction . TFIDF ( on = 'sentence' ) >>> for sentence in corpus : ... x = { 'sentence' : sentence } ... tfidf = tfidf . learn_one ( x ) ... print ( tfidf . transform_one ( x )) { 'this' : 0.447 , 'is' : 0.447 , 'the' : 0.447 , 'first' : 0.447 , 'document' : 0.447 } { 'this' : 0.333 , 'document' : 0.667 , 'is' : 0.333 , 'the' : 0.333 , 'second' : 0.469 } { 'and' : 0.497 , 'this' : 0.293 , 'is' : 0.293 , 'the' : 0.293 , 'third' : 0.497 , 'one' : 0.497 } { 'is' : 0.384 , 'this' : 0.384 , 'the' : 0.384 , 'first' : 0.580 , 'document' : 0.469 }","title":"Examples"},{"location":"api/feature-extraction/TFIDF/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self process_text transform_many Transform pandas series of string into term-frequency pandas sparse dataframe. Parameters X ( pandas.core.series.Series ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/TargetAgg/","text":"TargetAgg \u00b6 Computes a streaming aggregate of the target values. This transformer is identical to feature_extraction.Agg , the only difference is that it operates on the target rather than on a feature. At each step, the running statistic how of target values in group by is updated with the target. It is therefore a supervised transformer. Parameters \u00b6 by ( Union[str, List[str]] ) The feature by which to group the target values. how ( river.stats.base.Univariate ) The statistic to compute. target_name \u2013 defaults to target The target name which is used in the result. Attributes \u00b6 groups Maps group keys to univariate statistics. feature_name The name of the feature in the output. Examples \u00b6 Consider the following dataset, where the second value of each value is the target: >>> dataset = [ ... ({ 'country' : 'France' , 'place' : 'Taco Bell' }, 42 ), ... ({ 'country' : 'Sweden' , 'place' : 'Burger King' }, 16 ), ... ({ 'country' : 'France' , 'place' : 'Burger King' }, 24 ), ... ({ 'country' : 'Sweden' , 'place' : 'Taco Bell' }, 58 ), ... ({ 'country' : 'Sweden' , 'place' : 'Burger King' }, 20 ), ... ({ 'country' : 'France' , 'place' : 'Taco Bell' }, 50 ), ... ({ 'country' : 'France' , 'place' : 'Burger King' }, 10 ), ... ({ 'country' : 'Sweden' , 'place' : 'Taco Bell' }, 80 ) ... ] As an example, let's perform a target encoding of the place feature. Instead of simply updating a running average, we use a stats.BayesianMean which allows us to incorporate some prior knowledge. This makes subsequent models less prone to overfitting. Indeed, it dampens the fact that too few samples might have been seen within a group. >>> from river import feature_extraction >>> from river import stats >>> agg = feature_extraction . TargetAgg ( ... by = 'place' , ... how = stats . BayesianMean ( ... prior = 3 , ... prior_weight = 1 ... ) ... ) >>> for x , y in dataset : ... print ( agg . transform_one ( x )) ... agg = agg . learn_one ( x , y ) { 'target_bayes_mean_by_place' : 3.0 } { 'target_bayes_mean_by_place' : 3.0 } { 'target_bayes_mean_by_place' : 9.5 } { 'target_bayes_mean_by_place' : 22.5 } { 'target_bayes_mean_by_place' : 14.333 } { 'target_bayes_mean_by_place' : 34.333 } { 'target_bayes_mean_by_place' : 15.75 } { 'target_bayes_mean_by_place' : 38.25 } Just like with feature_extraction.Agg , we can specify multiple features on which to group the data: >>> agg = feature_extraction . TargetAgg ( ... by = [ 'place' , 'country' ], ... how = stats . BayesianMean ( ... prior = 3 , ... prior_weight = 1 ... ) ... ) >>> for x , y in dataset : ... print ( agg . transform_one ( x )) ... agg = agg . learn_one ( x , y ) { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 9.5 } { 'target_bayes_mean_by_place_and_country' : 22.5 } { 'target_bayes_mean_by_place_and_country' : 13.5 } { 'target_bayes_mean_by_place_and_country' : 30.5 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 Streaming groupbys in pandas for big datasets","title":"TargetAgg"},{"location":"api/feature-extraction/TargetAgg/#targetagg","text":"Computes a streaming aggregate of the target values. This transformer is identical to feature_extraction.Agg , the only difference is that it operates on the target rather than on a feature. At each step, the running statistic how of target values in group by is updated with the target. It is therefore a supervised transformer.","title":"TargetAgg"},{"location":"api/feature-extraction/TargetAgg/#parameters","text":"by ( Union[str, List[str]] ) The feature by which to group the target values. how ( river.stats.base.Univariate ) The statistic to compute. target_name \u2013 defaults to target The target name which is used in the result.","title":"Parameters"},{"location":"api/feature-extraction/TargetAgg/#attributes","text":"groups Maps group keys to univariate statistics. feature_name The name of the feature in the output.","title":"Attributes"},{"location":"api/feature-extraction/TargetAgg/#examples","text":"Consider the following dataset, where the second value of each value is the target: >>> dataset = [ ... ({ 'country' : 'France' , 'place' : 'Taco Bell' }, 42 ), ... ({ 'country' : 'Sweden' , 'place' : 'Burger King' }, 16 ), ... ({ 'country' : 'France' , 'place' : 'Burger King' }, 24 ), ... ({ 'country' : 'Sweden' , 'place' : 'Taco Bell' }, 58 ), ... ({ 'country' : 'Sweden' , 'place' : 'Burger King' }, 20 ), ... ({ 'country' : 'France' , 'place' : 'Taco Bell' }, 50 ), ... ({ 'country' : 'France' , 'place' : 'Burger King' }, 10 ), ... ({ 'country' : 'Sweden' , 'place' : 'Taco Bell' }, 80 ) ... ] As an example, let's perform a target encoding of the place feature. Instead of simply updating a running average, we use a stats.BayesianMean which allows us to incorporate some prior knowledge. This makes subsequent models less prone to overfitting. Indeed, it dampens the fact that too few samples might have been seen within a group. >>> from river import feature_extraction >>> from river import stats >>> agg = feature_extraction . TargetAgg ( ... by = 'place' , ... how = stats . BayesianMean ( ... prior = 3 , ... prior_weight = 1 ... ) ... ) >>> for x , y in dataset : ... print ( agg . transform_one ( x )) ... agg = agg . learn_one ( x , y ) { 'target_bayes_mean_by_place' : 3.0 } { 'target_bayes_mean_by_place' : 3.0 } { 'target_bayes_mean_by_place' : 9.5 } { 'target_bayes_mean_by_place' : 22.5 } { 'target_bayes_mean_by_place' : 14.333 } { 'target_bayes_mean_by_place' : 34.333 } { 'target_bayes_mean_by_place' : 15.75 } { 'target_bayes_mean_by_place' : 38.25 } Just like with feature_extraction.Agg , we can specify multiple features on which to group the data: >>> agg = feature_extraction . TargetAgg ( ... by = [ 'place' , 'country' ], ... how = stats . BayesianMean ( ... prior = 3 , ... prior_weight = 1 ... ) ... ) >>> for x , y in dataset : ... print ( agg . transform_one ( x )) ... agg = agg . learn_one ( x , y ) { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 3.0 } { 'target_bayes_mean_by_place_and_country' : 9.5 } { 'target_bayes_mean_by_place_and_country' : 22.5 } { 'target_bayes_mean_by_place_and_country' : 13.5 } { 'target_bayes_mean_by_place_and_country' : 30.5 }","title":"Examples"},{"location":"api/feature-extraction/TargetAgg/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-extraction/TargetAgg/#references","text":"Streaming groupbys in pandas for big datasets","title":"References"},{"location":"api/feature-selection/PoissonInclusion/","text":"PoissonInclusion \u00b6 Randomly selects features with an inclusion trial. When a new feature is encountered, it is selected with probability p . The number of times a feature needs to beseen before it is added to the model follows a geometric distribution with expected value 1 / p . This feature selection method is meant to be used when you have a very large amount of sparse features. Parameters \u00b6 p ( float ) Probability of including a feature the first time it is encountered. seed ( int ) \u2013 defaults to None Random seed value used for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import feature_selection >>> from river import stream >>> selector = feature_selection . PoissonInclusion ( p = 0.1 , seed = 42 ) >>> dataset = iter ( datasets . TrumpApproval ()) >>> feature_names = next ( dataset )[ 0 ] . keys () >>> n = 0 >>> while True : ... x , y = next ( dataset ) ... xt = selector . transform_one ( x ) ... if xt . keys () == feature_names : ... break ... n += 1 >>> n 12 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230) \u21a9","title":"PoissonInclusion"},{"location":"api/feature-selection/PoissonInclusion/#poissoninclusion","text":"Randomly selects features with an inclusion trial. When a new feature is encountered, it is selected with probability p . The number of times a feature needs to beseen before it is added to the model follows a geometric distribution with expected value 1 / p . This feature selection method is meant to be used when you have a very large amount of sparse features.","title":"PoissonInclusion"},{"location":"api/feature-selection/PoissonInclusion/#parameters","text":"p ( float ) Probability of including a feature the first time it is encountered. seed ( int ) \u2013 defaults to None Random seed value used for reproducibility.","title":"Parameters"},{"location":"api/feature-selection/PoissonInclusion/#examples","text":">>> from river import datasets >>> from river import feature_selection >>> from river import stream >>> selector = feature_selection . PoissonInclusion ( p = 0.1 , seed = 42 ) >>> dataset = iter ( datasets . TrumpApproval ()) >>> feature_names = next ( dataset )[ 0 ] . keys () >>> n = 0 >>> while True : ... x , y = next ( dataset ) ... xt = selector . transform_one ( x ) ... if xt . keys () == feature_names : ... break ... n += 1 >>> n 12","title":"Examples"},{"location":"api/feature-selection/PoissonInclusion/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-selection/PoissonInclusion/#references","text":"McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230) \u21a9","title":"References"},{"location":"api/feature-selection/SelectKBest/","text":"SelectKBest \u00b6 Removes all but the \\(k\\) highest scoring features. Parameters \u00b6 similarity ( river.stats.base.Bivariate ) k \u2013 defaults to 10 The number of features to keep. Attributes \u00b6 similarities ( dict ) The similarity instances used for each feature. leaderboard ( dict ) The actual similarity measures. Examples \u00b6 >>> from pprint import pprint >>> from river import feature_selection >>> from river import stats >>> from river import stream >>> from sklearn import datasets >>> X , y = datasets . make_regression ( ... n_samples = 100 , ... n_features = 10 , ... n_informative = 2 , ... random_state = 42 ... ) >>> selector = feature_selection . SelectKBest ( ... similarity = stats . PearsonCorr (), ... k = 2 ... ) >>> for xi , yi , in stream . iter_array ( X , y ): ... selector = selector . learn_one ( xi , yi ) >>> pprint ( selector . leaderboard ) Counter ({ 9 : 0.7898 , 7 : 0.5444 , 8 : 0.1062 , 2 : 0.0638 , 4 : 0.0538 , 5 : 0.0271 , 1 : - 0.0312 , 6 : - 0.0657 , 3 : - 0.1501 , 0 : - 0.1895 }) >>> selector . transform_one ( xi ) { 7 : - 1.2795 , 9 : - 1.8408 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"SelectKBest"},{"location":"api/feature-selection/SelectKBest/#selectkbest","text":"Removes all but the \\(k\\) highest scoring features.","title":"SelectKBest"},{"location":"api/feature-selection/SelectKBest/#parameters","text":"similarity ( river.stats.base.Bivariate ) k \u2013 defaults to 10 The number of features to keep.","title":"Parameters"},{"location":"api/feature-selection/SelectKBest/#attributes","text":"similarities ( dict ) The similarity instances used for each feature. leaderboard ( dict ) The actual similarity measures.","title":"Attributes"},{"location":"api/feature-selection/SelectKBest/#examples","text":">>> from pprint import pprint >>> from river import feature_selection >>> from river import stats >>> from river import stream >>> from sklearn import datasets >>> X , y = datasets . make_regression ( ... n_samples = 100 , ... n_features = 10 , ... n_informative = 2 , ... random_state = 42 ... ) >>> selector = feature_selection . SelectKBest ( ... similarity = stats . PearsonCorr (), ... k = 2 ... ) >>> for xi , yi , in stream . iter_array ( X , y ): ... selector = selector . learn_one ( xi , yi ) >>> pprint ( selector . leaderboard ) Counter ({ 9 : 0.7898 , 7 : 0.5444 , 8 : 0.1062 , 2 : 0.0638 , 4 : 0.0538 , 5 : 0.0271 , 1 : - 0.0312 , 6 : - 0.0657 , 3 : - 0.1501 , 0 : - 0.1895 }) >>> selector . transform_one ( xi ) { 7 : - 1.2795 , 9 : - 1.8408 }","title":"Examples"},{"location":"api/feature-selection/SelectKBest/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x and a target y . Parameters x ( dict ) y ( Union[bool, str, int, numbers.Number] ) Returns SupervisedTransformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/feature-selection/VarianceThreshold/","text":"VarianceThreshold \u00b6 Removes low-variance features. Parameters \u00b6 threshold \u2013 defaults to 0 Only features with a variance above the threshold will be kept. min_samples \u2013 defaults to 2 The minimum number of samples required to perform selection. Attributes \u00b6 variances ( dict ) The variance of each feature. Examples \u00b6 >>> from river import feature_selection >>> from river import stream >>> X = [ ... [ 0 , 2 , 0 , 3 ], ... [ 0 , 1 , 4 , 3 ], ... [ 0 , 1 , 1 , 3 ] ... ] >>> selector = feature_selection . VarianceThreshold () >>> for x , _ in stream . iter_array ( X ): ... print ( selector . learn_one ( x ) . transform_one ( x )) { 0 : 0 , 1 : 2 , 2 : 0 , 3 : 3 } { 1 : 1 , 2 : 4 } { 1 : 1 , 2 : 1 } Methods \u00b6 check_feature clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"VarianceThreshold"},{"location":"api/feature-selection/VarianceThreshold/#variancethreshold","text":"Removes low-variance features.","title":"VarianceThreshold"},{"location":"api/feature-selection/VarianceThreshold/#parameters","text":"threshold \u2013 defaults to 0 Only features with a variance above the threshold will be kept. min_samples \u2013 defaults to 2 The minimum number of samples required to perform selection.","title":"Parameters"},{"location":"api/feature-selection/VarianceThreshold/#attributes","text":"variances ( dict ) The variance of each feature.","title":"Attributes"},{"location":"api/feature-selection/VarianceThreshold/#examples","text":">>> from river import feature_selection >>> from river import stream >>> X = [ ... [ 0 , 2 , 0 , 3 ], ... [ 0 , 1 , 4 , 3 ], ... [ 0 , 1 , 1 , 3 ] ... ] >>> selector = feature_selection . VarianceThreshold () >>> for x , _ in stream . iter_array ( X ): ... print ( selector . learn_one ( x ) . transform_one ( x )) { 0 : 0 , 1 : 2 , 2 : 0 , 3 : 3 } { 1 : 1 , 2 : 4 } { 1 : 1 , 2 : 1 }","title":"Examples"},{"location":"api/feature-selection/VarianceThreshold/#methods","text":"check_feature clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/imblearn/HardSamplingClassifier/","text":"HardSamplingClassifier \u00b6 Hard sampling classifier. This wrapper enables a model to retrain on past samples who's output was hard to predict. This works by storing the hardest samples in a buffer of a fixed size. When a new sample arrives, the wrapped model is either trained on one of the buffered samples with a probability p or on the new sample with a probability (1 - p). The hardness of an observation is evaluated with a loss function that compares the sample's ground truth with the wrapped model's prediction. If the buffer is not full, then the sample is added to the buffer. If the buffer is full and the new sample has a bigger loss than the lowest loss in the buffer, then the sample takes it's place. Parameters \u00b6 classifier ( base.Classifier ) size ( int ) Size of the buffer. p ( float ) Probability of updating the model with a sample from the buffer instead of a new incoming sample. loss ( Union[ optim.losses.BinaryLoss , optim.losses.MultiClassLoss ] ) \u2013 defaults to None Criterion used to evaluate the hardness of a sample. seed ( int ) \u2013 defaults to None Random seed. Attributes \u00b6 classifier Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... imblearn . HardSamplingClassifier ( ... classifier = linear_model . LogisticRegression (), ... p = 0.1 , ... size = 40 , ... seed = 42 , ... ) ... ) >>> evaluate . progressive_val_score ( ... dataset = datasets . Phishing (), ... model = model , ... metric = metrics . ROCAUC (), ... print_every = 500 , ... ) [ 500 ] ROCAUC : 0.927112 [ 1 , 000 ] ROCAUC : 0.947515 ROCAUC : 0.950541 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"HardSamplingClassifier"},{"location":"api/imblearn/HardSamplingClassifier/#hardsamplingclassifier","text":"Hard sampling classifier. This wrapper enables a model to retrain on past samples who's output was hard to predict. This works by storing the hardest samples in a buffer of a fixed size. When a new sample arrives, the wrapped model is either trained on one of the buffered samples with a probability p or on the new sample with a probability (1 - p). The hardness of an observation is evaluated with a loss function that compares the sample's ground truth with the wrapped model's prediction. If the buffer is not full, then the sample is added to the buffer. If the buffer is full and the new sample has a bigger loss than the lowest loss in the buffer, then the sample takes it's place.","title":"HardSamplingClassifier"},{"location":"api/imblearn/HardSamplingClassifier/#parameters","text":"classifier ( base.Classifier ) size ( int ) Size of the buffer. p ( float ) Probability of updating the model with a sample from the buffer instead of a new incoming sample. loss ( Union[ optim.losses.BinaryLoss , optim.losses.MultiClassLoss ] ) \u2013 defaults to None Criterion used to evaluate the hardness of a sample. seed ( int ) \u2013 defaults to None Random seed.","title":"Parameters"},{"location":"api/imblearn/HardSamplingClassifier/#attributes","text":"classifier","title":"Attributes"},{"location":"api/imblearn/HardSamplingClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... imblearn . HardSamplingClassifier ( ... classifier = linear_model . LogisticRegression (), ... p = 0.1 , ... size = 40 , ... seed = 42 , ... ) ... ) >>> evaluate . progressive_val_score ( ... dataset = datasets . Phishing (), ... model = model , ... metric = metrics . ROCAUC (), ... print_every = 500 , ... ) [ 500 ] ROCAUC : 0.927112 [ 1 , 000 ] ROCAUC : 0.947515 ROCAUC : 0.950541","title":"Examples"},{"location":"api/imblearn/HardSamplingClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/imblearn/HardSamplingRegressor/","text":"HardSamplingRegressor \u00b6 Hard sampling regressor. This wrapper enables a model to retrain on past samples who's output was hard to predict. This works by storing the hardest samples in a buffer of a fixed size. When a new sample arrives, the wrapped model is either trained on one of the buffered samples with a probability p or on the new sample with a probability (1 - p). The hardness of an observation is evaluated with a loss function that compares the sample's ground truth with the wrapped model's prediction. If the buffer is not full, then the sample is added to the buffer. If the buffer is full and the new sample has a bigger loss than the lowest loss in the buffer, then the sample takes it's place. Parameters \u00b6 regressor ( base.Regressor ) size ( int ) Size of the buffer. p ( float ) Probability of updating the model with a sample from the buffer instead of a new incoming sample. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None Criterion used to evaluate the hardness of a sample. seed ( int ) \u2013 defaults to None Random seed. Attributes \u00b6 regressor Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... imblearn . HardSamplingRegressor ( ... regressor = linear_model . LinearRegression (), ... p = .2 , ... size = 30 , ... seed = 42 , ... ) ... ) >>> evaluate . progressive_val_score ( ... datasets . TrumpApproval (), ... model , ... metrics . MAE (), ... print_every = 500 ... ) [ 500 ] MAE : 2.292501 [ 1 , 000 ] MAE : 1.395797 MAE : 1.394693 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one","title":"HardSamplingRegressor"},{"location":"api/imblearn/HardSamplingRegressor/#hardsamplingregressor","text":"Hard sampling regressor. This wrapper enables a model to retrain on past samples who's output was hard to predict. This works by storing the hardest samples in a buffer of a fixed size. When a new sample arrives, the wrapped model is either trained on one of the buffered samples with a probability p or on the new sample with a probability (1 - p). The hardness of an observation is evaluated with a loss function that compares the sample's ground truth with the wrapped model's prediction. If the buffer is not full, then the sample is added to the buffer. If the buffer is full and the new sample has a bigger loss than the lowest loss in the buffer, then the sample takes it's place.","title":"HardSamplingRegressor"},{"location":"api/imblearn/HardSamplingRegressor/#parameters","text":"regressor ( base.Regressor ) size ( int ) Size of the buffer. p ( float ) Probability of updating the model with a sample from the buffer instead of a new incoming sample. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None Criterion used to evaluate the hardness of a sample. seed ( int ) \u2013 defaults to None Random seed.","title":"Parameters"},{"location":"api/imblearn/HardSamplingRegressor/#attributes","text":"regressor","title":"Attributes"},{"location":"api/imblearn/HardSamplingRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> model = ( ... preprocessing . StandardScaler () | ... imblearn . HardSamplingRegressor ( ... regressor = linear_model . LinearRegression (), ... p = .2 , ... size = 30 , ... seed = 42 , ... ) ... ) >>> evaluate . progressive_val_score ( ... datasets . TrumpApproval (), ... model , ... metrics . MAE (), ... print_every = 500 ... ) [ 500 ] MAE : 2.292501 [ 1 , 000 ] MAE : 1.395797 MAE : 1.394693","title":"Examples"},{"location":"api/imblearn/HardSamplingRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one predict_one","title":"Methods"},{"location":"api/imblearn/RandomOverSampler/","text":"RandomOverSampler \u00b6 Random over-sampling. This is a wrapper for classifiers. It will train the provided classifier by over-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. The implementation is a discrete version of reverse rejection sampling. See Working with imbalanced data for example usage. Parameters \u00b6 classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. seed ( int ) \u2013 defaults to None Random seed for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomOverSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.05421 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"RandomOverSampler"},{"location":"api/imblearn/RandomOverSampler/#randomoversampler","text":"Random over-sampling. This is a wrapper for classifiers. It will train the provided classifier by over-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. The implementation is a discrete version of reverse rejection sampling. See Working with imbalanced data for example usage.","title":"RandomOverSampler"},{"location":"api/imblearn/RandomOverSampler/#parameters","text":"classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. seed ( int ) \u2013 defaults to None Random seed for reproducibility.","title":"Parameters"},{"location":"api/imblearn/RandomOverSampler/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomOverSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.05421","title":"Examples"},{"location":"api/imblearn/RandomOverSampler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/imblearn/RandomSampler/","text":"RandomSampler \u00b6 Random sampling by mixing under-sampling and over-sampling. This is a wrapper for classifiers. It will train the provided classifier by both under-sampling and over-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. See Working with imbalanced data for example usage. Parameters \u00b6 classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. If set to None , then the observations will be sampled uniformly at random, which is stricly equivalent to using ensemble.BaggingClassifier . sampling_rate \u2013 defaults to 1.0 The desired ratio of data to sample. seed ( int ) \u2013 defaults to None Random seed for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... sampling_rate = 0.8 , ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.130906 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"RandomSampler"},{"location":"api/imblearn/RandomSampler/#randomsampler","text":"Random sampling by mixing under-sampling and over-sampling. This is a wrapper for classifiers. It will train the provided classifier by both under-sampling and over-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. See Working with imbalanced data for example usage.","title":"RandomSampler"},{"location":"api/imblearn/RandomSampler/#parameters","text":"classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. If set to None , then the observations will be sampled uniformly at random, which is stricly equivalent to using ensemble.BaggingClassifier . sampling_rate \u2013 defaults to 1.0 The desired ratio of data to sample. seed ( int ) \u2013 defaults to None Random seed for reproducibility.","title":"Parameters"},{"location":"api/imblearn/RandomSampler/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... sampling_rate = 0.8 , ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.130906","title":"Examples"},{"location":"api/imblearn/RandomSampler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/imblearn/RandomUnderSampler/","text":"RandomUnderSampler \u00b6 Random under-sampling. This is a wrapper for classifiers. It will train the provided classifier by under-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. The implementation is a discrete version of rejection sampling. See Working with imbalanced data for example usage. Parameters \u00b6 classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. seed ( int ) \u2013 defaults to None Random seed for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomUnderSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.07292 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Under-sampling a dataset with desired ratios \u21a9 Wikipedia article on rejection sampling \u21a9","title":"RandomUnderSampler"},{"location":"api/imblearn/RandomUnderSampler/#randomundersampler","text":"Random under-sampling. This is a wrapper for classifiers. It will train the provided classifier by under-sampling the stream of given observations so that the class distribution seen by the classifier follows a given desired distribution. The implementation is a discrete version of rejection sampling. See Working with imbalanced data for example usage.","title":"RandomUnderSampler"},{"location":"api/imblearn/RandomUnderSampler/#parameters","text":"classifier ( base.Classifier ) desired_dist ( dict ) The desired class distribution. The keys are the classes whilst the values are the desired class percentages. The values must sum up to 1. seed ( int ) \u2013 defaults to None Random seed for reproducibility.","title":"Parameters"},{"location":"api/imblearn/RandomUnderSampler/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import imblearn >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> model = imblearn . RandomUnderSampler ( ... ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression () ... ), ... desired_dist = { False : 0.4 , True : 0.6 }, ... seed = 42 ... ) >>> dataset = datasets . CreditCard () . take ( 3000 ) >>> metric = metrics . LogLoss () >>> evaluate . progressive_val_score ( dataset , model , metric ) LogLoss : 0.07292","title":"Examples"},{"location":"api/imblearn/RandomUnderSampler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/imblearn/RandomUnderSampler/#references","text":"Under-sampling a dataset with desired ratios \u21a9 Wikipedia article on rejection sampling \u21a9","title":"References"},{"location":"api/linear-model/ALMAClassifier/","text":"ALMAClassifier \u00b6 Approximate Large Margin Algorithm (ALMA). Parameters \u00b6 p \u2013 defaults to 2 alpha \u2013 defaults to 0.9 B \u2013 defaults to 1.1111111111111112 C \u2013 defaults to 1.4142135623730951 Attributes \u00b6 w ( collections.defaultdict ) The current weights. k ( int ) The number of instances seen during training. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . ALMAClassifier () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 82.64 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. References \u00b6 Gentile, Claudio. \"A new approximate maximal margin classification algorithm.\" Journal of Machine Learning Research 2.Dec (2001): 213-242 \u21a9","title":"ALMAClassifier"},{"location":"api/linear-model/ALMAClassifier/#almaclassifier","text":"Approximate Large Margin Algorithm (ALMA).","title":"ALMAClassifier"},{"location":"api/linear-model/ALMAClassifier/#parameters","text":"p \u2013 defaults to 2 alpha \u2013 defaults to 0.9 B \u2013 defaults to 1.1111111111111112 C \u2013 defaults to 1.4142135623730951","title":"Parameters"},{"location":"api/linear-model/ALMAClassifier/#attributes","text":"w ( collections.defaultdict ) The current weights. k ( int ) The number of instances seen during training.","title":"Attributes"},{"location":"api/linear-model/ALMAClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . ALMAClassifier () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 82.64 %","title":"Examples"},{"location":"api/linear-model/ALMAClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/linear-model/ALMAClassifier/#references","text":"Gentile, Claudio. \"A new approximate maximal margin classification algorithm.\" Journal of Machine Learning Research 2.Dec (2001): 213-242 \u21a9","title":"References"},{"location":"api/linear-model/LinearRegression/","text":"LinearRegression \u00b6 Linear regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler . Parameters \u00b6 optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept updates are handled separately. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept_init \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Attributes \u00b6 weights ( dict ) The current weights. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = .1 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.555971 >>> model [ 'LinearRegression' ] . intercept 35.617670 You can call the debug_one method to break down a prediction. This works even if the linear regression is part of a pipeline. >>> x , y = next ( iter ( dataset )) >>> report = model . debug_one ( x ) >>> print ( report ) 0. Input -------- gallup : 43.84321 ( float ) ipsos : 46.19925 ( float ) morning_consult : 48.31875 ( float ) ordinal_date : 736389 ( int ) rasmussen : 44.10469 ( float ) you_gov : 43.63691 ( float ) < BLANKLINE > 1. StandardScaler ----------------- gallup : 1.18810 ( float ) ipsos : 2.10348 ( float ) morning_consult : 2.73545 ( float ) ordinal_date : - 1.73032 ( float ) rasmussen : 1.26872 ( float ) you_gov : 1.48391 ( float ) < BLANKLINE > 2. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 35.61767 35.61767 ipsos 2.10348 0.62689 1.31866 morning_consult 2.73545 0.24180 0.66144 gallup 1.18810 0.43568 0.51764 rasmussen 1.26872 0.28118 0.35674 you_gov 1.48391 0.03123 0.04634 ordinal_date - 1.73032 3.45162 - 5.97242 < BLANKLINE > Prediction : 32.54607 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Debugs the output of the linear regression. Parameters x ( dict ) decimals \u2013 defaults to 5 Returns str : A table which explains the output. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchRegressor : self learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) w \u2013 defaults to 1.0 Returns Regressor : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X Returns The predicted outcomes. predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"LinearRegression"},{"location":"api/linear-model/LinearRegression/#linearregression","text":"Linear regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler .","title":"LinearRegression"},{"location":"api/linear-model/LinearRegression/#parameters","text":"optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept updates are handled separately. loss ( optim.losses.RegressionLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept_init \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[ optim.schedulers.Scheduler , float] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme.","title":"Parameters"},{"location":"api/linear-model/LinearRegression/#attributes","text":"weights ( dict ) The current weights.","title":"Attributes"},{"location":"api/linear-model/LinearRegression/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = .1 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.555971 >>> model [ 'LinearRegression' ] . intercept 35.617670 You can call the debug_one method to break down a prediction. This works even if the linear regression is part of a pipeline. >>> x , y = next ( iter ( dataset )) >>> report = model . debug_one ( x ) >>> print ( report ) 0. Input -------- gallup : 43.84321 ( float ) ipsos : 46.19925 ( float ) morning_consult : 48.31875 ( float ) ordinal_date : 736389 ( int ) rasmussen : 44.10469 ( float ) you_gov : 43.63691 ( float ) < BLANKLINE > 1. StandardScaler ----------------- gallup : 1.18810 ( float ) ipsos : 2.10348 ( float ) morning_consult : 2.73545 ( float ) ordinal_date : - 1.73032 ( float ) rasmussen : 1.26872 ( float ) you_gov : 1.48391 ( float ) < BLANKLINE > 2. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 35.61767 35.61767 ipsos 2.10348 0.62689 1.31866 morning_consult 2.73545 0.24180 0.66144 gallup 1.18810 0.43568 0.51764 rasmussen 1.26872 0.28118 0.35674 you_gov 1.48391 0.03123 0.04634 ordinal_date - 1.73032 3.45162 - 5.97242 < BLANKLINE > Prediction : 32.54607","title":"Examples"},{"location":"api/linear-model/LinearRegression/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Debugs the output of the linear regression. Parameters x ( dict ) decimals \u2013 defaults to 5 Returns str : A table which explains the output. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchRegressor : self learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) w \u2013 defaults to 1.0 Returns Regressor : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X Returns The predicted outcomes. predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/linear-model/LogisticRegression/","text":"LogisticRegression \u00b6 Logistic regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler . Parameters \u00b6 optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept is handled separately. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. Defaults to optim.losses.Log . l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept_init \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[float, optim.schedulers.Scheduler ] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Attributes \u00b6 weights The current weights. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer = optim . SGD ( .1 )) ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.96 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) w \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"LogisticRegression"},{"location":"api/linear-model/LogisticRegression/#logisticregression","text":"Logistic regression. This estimator supports learning with mini-batches. On top of the single instance methods, it provides the following methods: learn_many , predict_many , predict_proba_many . Each method takes as input a pandas.DataFrame where each column represents a feature. It is generally a good idea to scale the data beforehand in order for the optimizer to converge. You can do this online with a preprocessing.StandardScaler .","title":"LogisticRegression"},{"location":"api/linear-model/LogisticRegression/#parameters","text":"optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. Note that the intercept is handled separately. loss ( optim.losses.BinaryLoss ) \u2013 defaults to None The loss function to optimize for. Defaults to optim.losses.Log . l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. intercept_init \u2013 defaults to 0.0 Initial intercept value. intercept_lr ( Union[float, optim.schedulers.Scheduler ] ) \u2013 defaults to 0.01 Learning rate scheduler used for updating the intercept. A optim.schedulers.Constant is used if a float is provided. The intercept is not updated when this is set to 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme.","title":"Parameters"},{"location":"api/linear-model/LogisticRegression/#attributes","text":"weights The current weights.","title":"Attributes"},{"location":"api/linear-model/LogisticRegression/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer = optim . SGD ( .1 )) ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.96 %","title":"Examples"},{"location":"api/linear-model/LogisticRegression/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) w \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/linear-model/PAClassifier/","text":"PAClassifier \u00b6 Passive-aggressive learning for classification. Parameters \u00b6 C \u2013 defaults to 1.0 mode \u2013 defaults to 1 learn_intercept \u2013 defaults to True Examples \u00b6 The following example is taken from this blog post . >>> from river import linear_model >>> from river import metrics >>> from river import stream >>> import numpy as np >>> from sklearn import datasets >>> from sklearn import model_selection >>> np . random . seed ( 1000 ) >>> X , y = datasets . make_classification ( ... n_samples = 5000 , ... n_features = 4 , ... n_informative = 2 , ... n_redundant = 0 , ... n_repeated = 0 , ... n_classes = 2 , ... n_clusters_per_class = 2 ... ) >>> X_train , X_test , y_train , y_test = model_selection . train_test_split ( ... X , ... y , ... test_size = 0.35 , ... random_state = 1000 ... ) >>> model = linear_model . PAClassifier ( ... C = 0.01 , ... mode = 1 ... ) >>> for xi , yi in stream . iter_array ( X_train , y_train ): ... y_pred = model . learn_one ( xi , yi ) >>> metric = metrics . Accuracy () + metrics . LogLoss () >>> for xi , yi in stream . iter_array ( X_test , y_test ): ... metric = metric . update ( yi , model . predict_proba_one ( xi )) >>> print ( metric ) Accuracy : 88.46 % , LogLoss : 0.325727 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. References \u00b6 Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S. and Singer, Y., 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar), pp.551-585 \u21a9","title":"PAClassifier"},{"location":"api/linear-model/PAClassifier/#paclassifier","text":"Passive-aggressive learning for classification.","title":"PAClassifier"},{"location":"api/linear-model/PAClassifier/#parameters","text":"C \u2013 defaults to 1.0 mode \u2013 defaults to 1 learn_intercept \u2013 defaults to True","title":"Parameters"},{"location":"api/linear-model/PAClassifier/#examples","text":"The following example is taken from this blog post . >>> from river import linear_model >>> from river import metrics >>> from river import stream >>> import numpy as np >>> from sklearn import datasets >>> from sklearn import model_selection >>> np . random . seed ( 1000 ) >>> X , y = datasets . make_classification ( ... n_samples = 5000 , ... n_features = 4 , ... n_informative = 2 , ... n_redundant = 0 , ... n_repeated = 0 , ... n_classes = 2 , ... n_clusters_per_class = 2 ... ) >>> X_train , X_test , y_train , y_test = model_selection . train_test_split ( ... X , ... y , ... test_size = 0.35 , ... random_state = 1000 ... ) >>> model = linear_model . PAClassifier ( ... C = 0.01 , ... mode = 1 ... ) >>> for xi , yi in stream . iter_array ( X_train , y_train ): ... y_pred = model . learn_one ( xi , yi ) >>> metric = metrics . Accuracy () + metrics . LogLoss () >>> for xi , yi in stream . iter_array ( X_test , y_test ): ... metric = metric . update ( yi , model . predict_proba_one ( xi )) >>> print ( metric ) Accuracy : 88.46 % , LogLoss : 0.325727","title":"Examples"},{"location":"api/linear-model/PAClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/linear-model/PAClassifier/#references","text":"Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S. and Singer, Y., 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar), pp.551-585 \u21a9","title":"References"},{"location":"api/linear-model/PARegressor/","text":"PARegressor \u00b6 Passive-aggressive learning for regression. Parameters \u00b6 C \u2013 defaults to 1.0 mode \u2013 defaults to 1 eps \u2013 defaults to 0.1 learn_intercept \u2013 defaults to True Examples \u00b6 The following example is taken from this blog post . >>> from river import linear_model >>> from river import metrics >>> from river import stream >>> import numpy as np >>> from sklearn import datasets >>> np . random . seed ( 1000 ) >>> X , y = datasets . make_regression ( n_samples = 500 , n_features = 4 ) >>> model = linear_model . PARegressor ( ... C = 0.01 , ... mode = 2 , ... eps = 0.1 , ... learn_intercept = False ... ) >>> metric = metrics . MAE () + metrics . MSE () >>> for xi , yi in stream . iter_array ( X , y ): ... y_pred = model . predict_one ( xi ) ... model = model . learn_one ( xi , yi ) ... metric = metric . update ( yi , y_pred ) >>> print ( metric ) MAE : 9.809402 , MSE : 472.393532 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. References \u00b6 Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S. and Singer, Y., 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar), pp.551-585. \u21a9","title":"PARegressor"},{"location":"api/linear-model/PARegressor/#paregressor","text":"Passive-aggressive learning for regression.","title":"PARegressor"},{"location":"api/linear-model/PARegressor/#parameters","text":"C \u2013 defaults to 1.0 mode \u2013 defaults to 1 eps \u2013 defaults to 0.1 learn_intercept \u2013 defaults to True","title":"Parameters"},{"location":"api/linear-model/PARegressor/#examples","text":"The following example is taken from this blog post . >>> from river import linear_model >>> from river import metrics >>> from river import stream >>> import numpy as np >>> from sklearn import datasets >>> np . random . seed ( 1000 ) >>> X , y = datasets . make_regression ( n_samples = 500 , n_features = 4 ) >>> model = linear_model . PARegressor ( ... C = 0.01 , ... mode = 2 , ... eps = 0.1 , ... learn_intercept = False ... ) >>> metric = metrics . MAE () + metrics . MSE () >>> for xi , yi in stream . iter_array ( X , y ): ... y_pred = model . predict_one ( xi ) ... model = model . learn_one ( xi , yi ) ... metric = metric . update ( yi , y_pred ) >>> print ( metric ) MAE : 9.809402 , MSE : 472.393532","title":"Examples"},{"location":"api/linear-model/PARegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction.","title":"Methods"},{"location":"api/linear-model/PARegressor/#references","text":"Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S. and Singer, Y., 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7(Mar), pp.551-585. \u21a9","title":"References"},{"location":"api/linear-model/Perceptron/","text":"Perceptron \u00b6 Perceptron classifier. In this implementation, the Perceptron is viewed as a special case of the logistic regression. The loss function that is used is the Hinge loss with a threshold set to 0, whilst the learning rate of the stochastic gradient descent procedure is set to 1 for both the weights and the intercept. Parameters \u00b6 l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. Attributes \u00b6 weights The current weights. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = pp . StandardScaler () | lm . Perceptron () >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 85.84 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) w \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Perceptron"},{"location":"api/linear-model/Perceptron/#perceptron","text":"Perceptron classifier. In this implementation, the Perceptron is viewed as a special case of the logistic regression. The loss function that is used is the Hinge loss with a threshold set to 0, whilst the learning rate of the stochastic gradient descent procedure is set to 1 for both the weights and the intercept.","title":"Perceptron"},{"location":"api/linear-model/Perceptron/#parameters","text":"l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme.","title":"Parameters"},{"location":"api/linear-model/Perceptron/#attributes","text":"weights The current weights.","title":"Attributes"},{"location":"api/linear-model/Perceptron/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model as lm >>> from river import metrics >>> from river import preprocessing as pp >>> dataset = datasets . Phishing () >>> model = pp . StandardScaler () | lm . Perceptron () >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 85.84 %","title":"Examples"},{"location":"api/linear-model/Perceptron/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update the model with a mini-batch of features X and boolean targets y . Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) w ( Union[float, pandas.core.series.Series] ) \u2013 defaults to 1 Returns MiniBatchClassifier : self learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) w \u2013 defaults to 1.0 Returns Classifier : self predict_many Predict the outcome for each given sample. Parameters --------- X A dataframe of features. Parameters X ( pandas.core.frame.DataFrame ) Returns Series : The predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the outcome probabilities for each given sample. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : A dataframe with probabilities of True and False for each sample. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/linear-model/SoftmaxRegression/","text":"SoftmaxRegression \u00b6 Softmax regression is a generalization of logistic regression to multiple classes. Softmax regression is also known as \"multinomial logistic regression\". There are a set weights for each class, hence the weights attribute is a nested collections.defaultdict . The main advantage of using this instead of a one-vs-all logistic regression is that the probabilities will be calibrated. Moreover softmax regression is more robust to outliers. Parameters \u00b6 optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used to tune the weights. loss ( optim.losses.MultiClassLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0 Amount of L2 regularization used to push weights towards 0. Attributes \u00b6 weights ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> model = preprocessing . StandardScaler () >>> model |= linear_model . SoftmaxRegression () >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.818765 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. References \u00b6 Course on classification stochastic gradient descent \u21a9 Binary vs. Multi-Class Logistic Regression \u21a9","title":"SoftmaxRegression"},{"location":"api/linear-model/SoftmaxRegression/#softmaxregression","text":"Softmax regression is a generalization of logistic regression to multiple classes. Softmax regression is also known as \"multinomial logistic regression\". There are a set weights for each class, hence the weights attribute is a nested collections.defaultdict . The main advantage of using this instead of a one-vs-all logistic regression is that the probabilities will be calibrated. Moreover softmax regression is more robust to outliers.","title":"SoftmaxRegression"},{"location":"api/linear-model/SoftmaxRegression/#parameters","text":"optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used to tune the weights. loss ( optim.losses.MultiClassLoss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0 Amount of L2 regularization used to push weights towards 0.","title":"Parameters"},{"location":"api/linear-model/SoftmaxRegression/#attributes","text":"weights ( collections.defaultdict )","title":"Attributes"},{"location":"api/linear-model/SoftmaxRegression/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> model = preprocessing . StandardScaler () >>> model |= linear_model . SoftmaxRegression () >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.818765","title":"Examples"},{"location":"api/linear-model/SoftmaxRegression/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/linear-model/SoftmaxRegression/#references","text":"Course on classification stochastic gradient descent \u21a9 Binary vs. Multi-Class Logistic Regression \u21a9","title":"References"},{"location":"api/meta/BoxCoxRegressor/","text":"BoxCoxRegressor \u00b6 Applies the Box-Cox transform to the target before training. Box-Cox transform is useful when the target variable is heteroscedastic (i.e. there are sub-populations that have different variabilities from others) allowing to transform it towards normality. The power parameter is denoted \u03bb in the literature. If power is equal to 0 than the Box-Cox transform will be equivalent to a log transform. Parameters \u00b6 regressor ( base.Regressor ) Regression model to wrap. power \u2013 defaults to 1.0 power value to do the transformation. Examples \u00b6 >>> import math >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import meta >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... meta . BoxCoxRegressor ( ... regressor = linear_model . LinearRegression ( intercept_lr = 0.2 ), ... power = 0.05 ... ) ... ) >>> metric = metrics . MSE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MSE : 5.898196 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"BoxCoxRegressor"},{"location":"api/meta/BoxCoxRegressor/#boxcoxregressor","text":"Applies the Box-Cox transform to the target before training. Box-Cox transform is useful when the target variable is heteroscedastic (i.e. there are sub-populations that have different variabilities from others) allowing to transform it towards normality. The power parameter is denoted \u03bb in the literature. If power is equal to 0 than the Box-Cox transform will be equivalent to a log transform.","title":"BoxCoxRegressor"},{"location":"api/meta/BoxCoxRegressor/#parameters","text":"regressor ( base.Regressor ) Regression model to wrap. power \u2013 defaults to 1.0 power value to do the transformation.","title":"Parameters"},{"location":"api/meta/BoxCoxRegressor/#examples","text":">>> import math >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import meta >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... meta . BoxCoxRegressor ( ... regressor = linear_model . LinearRegression ( intercept_lr = 0.2 ), ... power = 0.05 ... ) ... ) >>> metric = metrics . MSE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MSE : 5.898196","title":"Examples"},{"location":"api/meta/BoxCoxRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/meta/PredClipper/","text":"PredClipper \u00b6 Clips the target after predicting. Parameters \u00b6 regressor ( base.Regressor ) Regressor model for which to clip the predictions. y_min ( float ) minimum value. y_max ( float ) maximum value. Examples \u00b6 >>> from river import linear_model >>> from river import meta >>> dataset = ( ... ({ 'a' : 2 , 'b' : 4 }, 80 ), ... ({ 'a' : 3 , 'b' : 5 }, 100 ), ... ({ 'a' : 4 , 'b' : 6 }, 120 ) ... ) >>> model = meta . PredClipper ( ... regressor = linear_model . LinearRegression (), ... y_min = 0 , ... y_max = 200 ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'a' : - 100 , 'b' : - 200 }) 0 >>> model . predict_one ({ 'a' : 50 , 'b' : 60 }) 200 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"PredClipper"},{"location":"api/meta/PredClipper/#predclipper","text":"Clips the target after predicting.","title":"PredClipper"},{"location":"api/meta/PredClipper/#parameters","text":"regressor ( base.Regressor ) Regressor model for which to clip the predictions. y_min ( float ) minimum value. y_max ( float ) maximum value.","title":"Parameters"},{"location":"api/meta/PredClipper/#examples","text":">>> from river import linear_model >>> from river import meta >>> dataset = ( ... ({ 'a' : 2 , 'b' : 4 }, 80 ), ... ({ 'a' : 3 , 'b' : 5 }, 100 ), ... ({ 'a' : 4 , 'b' : 6 }, 120 ) ... ) >>> model = meta . PredClipper ( ... regressor = linear_model . LinearRegression (), ... y_min = 0 , ... y_max = 200 ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'a' : - 100 , 'b' : - 200 }) 0 >>> model . predict_one ({ 'a' : 50 , 'b' : 60 }) 200","title":"Examples"},{"location":"api/meta/PredClipper/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/meta/TransformedTargetRegressor/","text":"TransformedTargetRegressor \u00b6 Modifies the target before training. The user is expected to check that func and inverse_func are coherent with each other. Parameters \u00b6 regressor ( base.Regressor ) Regression model to wrap. func ( ) A function modifying the target before training. inverse_func ( ) A function to return to the target's original space. Examples \u00b6 >>> import math >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import meta >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... meta . TransformedTargetRegressor ( ... regressor = linear_model . LinearRegression ( intercept_lr = 0.15 ), ... func = math . log , ... inverse_func = math . exp ... ) ... ) >>> metric = metrics . MSE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MSE : 8.759624 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"TransformedTargetRegressor"},{"location":"api/meta/TransformedTargetRegressor/#transformedtargetregressor","text":"Modifies the target before training. The user is expected to check that func and inverse_func are coherent with each other.","title":"TransformedTargetRegressor"},{"location":"api/meta/TransformedTargetRegressor/#parameters","text":"regressor ( base.Regressor ) Regression model to wrap. func ( ) A function modifying the target before training. inverse_func ( ) A function to return to the target's original space.","title":"Parameters"},{"location":"api/meta/TransformedTargetRegressor/#examples","text":">>> import math >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import meta >>> from river import metrics >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... meta . TransformedTargetRegressor ( ... regressor = linear_model . LinearRegression ( intercept_lr = 0.15 ), ... func = math . log , ... inverse_func = math . exp ... ) ... ) >>> metric = metrics . MSE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MSE : 8.759624","title":"Examples"},{"location":"api/meta/TransformedTargetRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/metrics/Accuracy/","text":"Accuracy \u00b6 Accuracy score, which is the percentage of exact matches. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Accuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric Accuracy : 60.00 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#accuracy","text":"Accuracy score, which is the percentage of exact matches.","title":"Accuracy"},{"location":"api/metrics/Accuracy/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Accuracy/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Accuracy/#examples","text":">>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Accuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric Accuracy : 60.00 %","title":"Examples"},{"location":"api/metrics/Accuracy/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/AdjustedMutualInfo/","text":"AdjustedMutualInfo \u00b6 Adjusted Mutual Information between two clusterings. Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information score that accounts for chance. It corrects the effect of agreement solely due to chance between clusterings, similar to the way the Adjusted Rand Index corrects the Rand Index. It is closely related to variation of information. The adjusted measure, however, is no longer metrical. For two clusterings \\(U\\) and \\(V\\) , the Adjusted Mutual Information is calculated as: \\[ AMI(U, V) = \\frac{MI(U, V) - E(MI(U, V))}{avg(H(U), H(V)) - E(MI(U, V))} \\] This metric is independent of the permutation of the class or cluster label values; furthermore, it is also symmetric. This can be useful to measure the agreement of two label assignments strategies on the same dataset, regardless of the ground truth. However, due to the complexity of the Expected Mutual Info Score, the computation of this metric is an order of magnitude slower than most other metrics, in general. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. average_method \u2013 defaults to arithmetic This parameter defines how to compute the normalizer in the denominator. Possible options include min , max , arithmetic and geometric . Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . AdjustedMutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.0 0.105891 0.298792 >>> metric AdjustedMutualInfo : 0.298792 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"AdjustedMutualInfo"},{"location":"api/metrics/AdjustedMutualInfo/#adjustedmutualinfo","text":"Adjusted Mutual Information between two clusterings. Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information score that accounts for chance. It corrects the effect of agreement solely due to chance between clusterings, similar to the way the Adjusted Rand Index corrects the Rand Index. It is closely related to variation of information. The adjusted measure, however, is no longer metrical. For two clusterings \\(U\\) and \\(V\\) , the Adjusted Mutual Information is calculated as: \\[ AMI(U, V) = \\frac{MI(U, V) - E(MI(U, V))}{avg(H(U), H(V)) - E(MI(U, V))} \\] This metric is independent of the permutation of the class or cluster label values; furthermore, it is also symmetric. This can be useful to measure the agreement of two label assignments strategies on the same dataset, regardless of the ground truth. However, due to the complexity of the Expected Mutual Info Score, the computation of this metric is an order of magnitude slower than most other metrics, in general.","title":"AdjustedMutualInfo"},{"location":"api/metrics/AdjustedMutualInfo/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. average_method \u2013 defaults to arithmetic This parameter defines how to compute the normalizer in the denominator. Possible options include min , max , arithmetic and geometric .","title":"Parameters"},{"location":"api/metrics/AdjustedMutualInfo/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/AdjustedMutualInfo/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . AdjustedMutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.0 0.105891 0.298792 >>> metric AdjustedMutualInfo : 0.298792","title":"Examples"},{"location":"api/metrics/AdjustedMutualInfo/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/AdjustedMutualInfo/#references","text":"Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"References"},{"location":"api/metrics/AdjustedRand/","text":"AdjustedRand \u00b6 Adjusted Rand Index. The Adjusted Rand Index is the corrected-for-chance version of the Rand Index 1 2 . Such a correction for chance establishes a baseline by using the expected similarity of all pair-wise comparisions between clusterings specified by a random model. Traditionally, the Rand Index was corrected using the Permutation Model for Clustering. However, the premises of the permutation model are frequently violated; in many clustering scenarios, either the number of clusters or the size distribution of those clusters vary drastically. Variations of the adjusted Rand Index account for different models of random clusterings. Though the Rand Index may only yield a value between 0 and 1, the Adjusted Rand index can yield negative values if the index is less than the expected index. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . AdjustedRand () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.0 0.09090909090909091 0.24242424242424243 >>> metric AdjustedRand : 0.242424 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, January 13). Rand index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Rand_index&oldid=1000098911 \u21a9 W. M. Rand (1971). \"Objective criteria for the evaluation of clustering methods\". Journal of the American Statistical Association. American Statistical Association. 66 (336): 846\u2013850. arXiv:1704.01036. doi:10.2307/2284239. JSTOR 2284239. \u21a9","title":"AdjustedRand"},{"location":"api/metrics/AdjustedRand/#adjustedrand","text":"Adjusted Rand Index. The Adjusted Rand Index is the corrected-for-chance version of the Rand Index 1 2 . Such a correction for chance establishes a baseline by using the expected similarity of all pair-wise comparisions between clusterings specified by a random model. Traditionally, the Rand Index was corrected using the Permutation Model for Clustering. However, the premises of the permutation model are frequently violated; in many clustering scenarios, either the number of clusters or the size distribution of those clusters vary drastically. Variations of the adjusted Rand Index account for different models of random clusterings. Though the Rand Index may only yield a value between 0 and 1, the Adjusted Rand index can yield negative values if the index is less than the expected index.","title":"AdjustedRand"},{"location":"api/metrics/AdjustedRand/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/AdjustedRand/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/AdjustedRand/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . AdjustedRand () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.0 0.09090909090909091 0.24242424242424243 >>> metric AdjustedRand : 0.242424","title":"Examples"},{"location":"api/metrics/AdjustedRand/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/AdjustedRand/#references","text":"Wikipedia contributors. (2021, January 13). Rand index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Rand_index&oldid=1000098911 \u21a9 W. M. Rand (1971). \"Objective criteria for the evaluation of clustering methods\". Journal of the American Statistical Association. American Statistical Association. 66 (336): 846\u2013850. arXiv:1704.01036. doi:10.2307/2284239. JSTOR 2284239. \u21a9","title":"References"},{"location":"api/metrics/BalancedAccuracy/","text":"BalancedAccuracy \u00b6 Balanced accuracy. Balanced accuracy is the average of recall obtained on each class. It is used to deal with imbalanced datasets in binary and multi-class classification problems. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , False , True , True , False , True ] >>> y_pred = [ True , False , True , True , True , False ] >>> metric = metrics . BalancedAccuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric BalancedAccuracy : 62.50 % >>> y_true = [ 0 , 1 , 0 , 0 , 1 , 0 ] >>> y_pred = [ 0 , 1 , 0 , 0 , 0 , 1 ] >>> metric = metrics . BalancedAccuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric BalancedAccuracy : 62.50 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"BalancedAccuracy"},{"location":"api/metrics/BalancedAccuracy/#balancedaccuracy","text":"Balanced accuracy. Balanced accuracy is the average of recall obtained on each class. It is used to deal with imbalanced datasets in binary and multi-class classification problems.","title":"BalancedAccuracy"},{"location":"api/metrics/BalancedAccuracy/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/BalancedAccuracy/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/BalancedAccuracy/#examples","text":">>> from river import metrics >>> y_true = [ True , False , True , True , False , True ] >>> y_pred = [ True , False , True , True , True , False ] >>> metric = metrics . BalancedAccuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric BalancedAccuracy : 62.50 % >>> y_true = [ 0 , 1 , 0 , 0 , 1 , 0 ] >>> y_pred = [ 0 , 1 , 0 , 0 , 0 , 1 ] >>> metric = metrics . BalancedAccuracy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric BalancedAccuracy : 62.50 %","title":"Examples"},{"location":"api/metrics/BalancedAccuracy/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/BinaryMetric/","text":"BinaryMetric \u00b6 Mother class for all binary classification metrics. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"BinaryMetric"},{"location":"api/metrics/BinaryMetric/#binarymetric","text":"Mother class for all binary classification metrics.","title":"BinaryMetric"},{"location":"api/metrics/BinaryMetric/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/BinaryMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/BinaryMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ClassificationMetric/","text":"ClassificationMetric \u00b6 Mother class for all classification metrics. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ClassificationMetric"},{"location":"api/metrics/ClassificationMetric/#classificationmetric","text":"Mother class for all classification metrics.","title":"ClassificationMetric"},{"location":"api/metrics/ClassificationMetric/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ClassificationMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ClassificationMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ClassificationReport/","text":"ClassificationReport \u00b6 A report for monitoring a classifier. This class maintains a set of metrics and updates each of them every time update is called. You can print this class at any time during a model's lifetime to get a tabular visualization of various metrics. You can wrap a metrics.ClassificationReport with metrics.Rolling in order to obtain a classification report over a window of observations. You can also wrap it with metrics.TimeRolling to obtain a report over a period of time. Parameters \u00b6 decimals \u2013 defaults to 3 The number of decimals to display in each cell. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 'pear' , 'apple' , 'banana' , 'banana' , 'banana' ] >>> y_pred = [ 'apple' , 'pear' , 'banana' , 'banana' , 'apple' ] >>> report = metrics . ClassificationReport () >>> for yt , yp in zip ( y_true , y_pred ): ... report = report . update ( yt , yp ) >>> print ( report ) Precision Recall F1 Support < BLANKLINE > apple 0.000 0.000 0.000 1 banana 1.000 0.667 0.800 3 pear 0.000 0.000 0.000 1 < BLANKLINE > Macro 0.333 0.222 0.267 Micro 0.400 0.400 0.400 Weighted 0.600 0.400 0.480 < BLANKLINE > 40.0 % accuracy Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ClassificationReport"},{"location":"api/metrics/ClassificationReport/#classificationreport","text":"A report for monitoring a classifier. This class maintains a set of metrics and updates each of them every time update is called. You can print this class at any time during a model's lifetime to get a tabular visualization of various metrics. You can wrap a metrics.ClassificationReport with metrics.Rolling in order to obtain a classification report over a window of observations. You can also wrap it with metrics.TimeRolling to obtain a report over a period of time.","title":"ClassificationReport"},{"location":"api/metrics/ClassificationReport/#parameters","text":"decimals \u2013 defaults to 3 The number of decimals to display in each cell. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ClassificationReport/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ClassificationReport/#examples","text":">>> from river import metrics >>> y_true = [ 'pear' , 'apple' , 'banana' , 'banana' , 'banana' ] >>> y_pred = [ 'apple' , 'pear' , 'banana' , 'banana' , 'apple' ] >>> report = metrics . ClassificationReport () >>> for yt , yp in zip ( y_true , y_pred ): ... report = report . update ( yt , yp ) >>> print ( report ) Precision Recall F1 Support < BLANKLINE > apple 0.000 0.000 0.000 1 banana 1.000 0.667 0.800 3 pear 0.000 0.000 0.000 1 < BLANKLINE > Macro 0.333 0.222 0.267 Micro 0.400 0.400 0.400 Weighted 0.600 0.400 0.480 < BLANKLINE > 40.0 % accuracy","title":"Examples"},{"location":"api/metrics/ClassificationReport/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/CohenKappa/","text":"CohenKappa \u00b6 Cohen's Kappa score. Cohen's Kappa expresses the level of agreement between two annotators on a classification problem. It is defined as \\[ \\kappa = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the expected agreement when both annotators assign labels randomly. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> metric = metrics . CohenKappa () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric CohenKappa : 0.428571 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 J. Cohen (1960). \"A coefficient of agreement for nominal scales\". Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104. \u21a9","title":"CohenKappa"},{"location":"api/metrics/CohenKappa/#cohenkappa","text":"Cohen's Kappa score. Cohen's Kappa expresses the level of agreement between two annotators on a classification problem. It is defined as \\[ \\kappa = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the expected agreement when both annotators assign labels randomly.","title":"CohenKappa"},{"location":"api/metrics/CohenKappa/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/CohenKappa/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/CohenKappa/#examples","text":">>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> metric = metrics . CohenKappa () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric CohenKappa : 0.428571","title":"Examples"},{"location":"api/metrics/CohenKappa/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/CohenKappa/#references","text":"J. Cohen (1960). \"A coefficient of agreement for nominal scales\". Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104. \u21a9","title":"References"},{"location":"api/metrics/Completeness/","text":"Completeness \u00b6 Completeness Score. Completeness 1 is symmetrical to homogeneity. In order to satisfy the completeness criteria, a clustering must assign all of those datapoints that are members of a single class to a single cluster. To evaluate completeness, we examine the distribution cluster assignments within each class. In a perfectly complete clustering solution, each of these distributions will be completely skewed to a single cluster. We can evaluate this degree of skew by calculating the conditional entropy of the proposed cluster distribution given the class of the component data points. However, in the worst case scenario, each class is represented by every cluster with a distribution equal to the distribution of cluster sizes. Therefore, symmetric to the claculation above, we define completeness as: \\[ c = \\begin{cases} 1 if H(K) = 0, \\\\ 1 - \\frac{H(K|C)}{H(K)} otherwise. \\end{cases}. \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Completeness () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 1.0 0.3836885465963443 0.5880325916843805 0.6666666666666667 >>> metric Completeness : 0.666667 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"Completeness"},{"location":"api/metrics/Completeness/#completeness","text":"Completeness Score. Completeness 1 is symmetrical to homogeneity. In order to satisfy the completeness criteria, a clustering must assign all of those datapoints that are members of a single class to a single cluster. To evaluate completeness, we examine the distribution cluster assignments within each class. In a perfectly complete clustering solution, each of these distributions will be completely skewed to a single cluster. We can evaluate this degree of skew by calculating the conditional entropy of the proposed cluster distribution given the class of the component data points. However, in the worst case scenario, each class is represented by every cluster with a distribution equal to the distribution of cluster sizes. Therefore, symmetric to the claculation above, we define completeness as: \\[ c = \\begin{cases} 1 if H(K) = 0, \\\\ 1 - \\frac{H(K|C)}{H(K)} otherwise. \\end{cases}. \\]","title":"Completeness"},{"location":"api/metrics/Completeness/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Completeness/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Completeness/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Completeness () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 1.0 0.3836885465963443 0.5880325916843805 0.6666666666666667 >>> metric Completeness : 0.666667","title":"Examples"},{"location":"api/metrics/Completeness/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Completeness/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"References"},{"location":"api/metrics/ConfusionMatrix/","text":"ConfusionMatrix \u00b6 ConfusionMatrix(classes=None) Confusion Matrix for binary-class and multi-class classification. Attributes \u00b6 classes data last_y_pred last_y_true majority_class n_classes n_samples sample_correction shape sum_col sum_diag sum_row total_weight weight_majority_classifier weight_no_change_classifier Examples \u00b6 >>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> cm = metrics . ConfusionMatrix () >>> for yt , yp in zip ( y_true , y_pred ): ... cm = cm . update ( yt , yp ) >>> cm ant bird cat ant 2 0 0 bird 0 0 1 cat 1 0 2 >>> cm [ 'bird' ][ 'cat' ] 1.0 Methods \u00b6 false_negatives Parameters label false_positives Parameters label reset revert Parameters y_true y_pred sample_weight correction true_negatives Parameters label true_positives Parameters label update Parameters y_true y_pred sample_weight Notes \u00b6 This confusion matrix is a 2D matrix of shape `(n_classes, n_classes)`, corresponding to a single-target (binary and multi-class) classification task. Each row represents `true` (actual) class-labels, while each column corresponds to the `predicted` class-labels. For example, an entry in position `[1, 2]` means that the true class-label is 1, and the predicted class-label is 2 (incorrect prediction). This structure is used to keep updated statistics about a single-output classifier's performance and to compute multiple evaluation metrics.","title":"ConfusionMatrix"},{"location":"api/metrics/ConfusionMatrix/#confusionmatrix","text":"ConfusionMatrix(classes=None) Confusion Matrix for binary-class and multi-class classification.","title":"ConfusionMatrix"},{"location":"api/metrics/ConfusionMatrix/#attributes","text":"classes data last_y_pred last_y_true majority_class n_classes n_samples sample_correction shape sum_col sum_diag sum_row total_weight weight_majority_classifier weight_no_change_classifier","title":"Attributes"},{"location":"api/metrics/ConfusionMatrix/#examples","text":">>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> cm = metrics . ConfusionMatrix () >>> for yt , yp in zip ( y_true , y_pred ): ... cm = cm . update ( yt , yp ) >>> cm ant bird cat ant 2 0 0 bird 0 0 1 cat 1 0 2 >>> cm [ 'bird' ][ 'cat' ] 1.0","title":"Examples"},{"location":"api/metrics/ConfusionMatrix/#methods","text":"false_negatives Parameters label false_positives Parameters label reset revert Parameters y_true y_pred sample_weight correction true_negatives Parameters label true_positives Parameters label update Parameters y_true y_pred sample_weight","title":"Methods"},{"location":"api/metrics/ConfusionMatrix/#notes","text":"This confusion matrix is a 2D matrix of shape `(n_classes, n_classes)`, corresponding to a single-target (binary and multi-class) classification task. Each row represents `true` (actual) class-labels, while each column corresponds to the `predicted` class-labels. For example, an entry in position `[1, 2]` means that the true class-label is 1, and the predicted class-label is 2 (incorrect prediction). This structure is used to keep updated statistics about a single-output classifier's performance and to compute multiple evaluation metrics.","title":"Notes"},{"location":"api/metrics/CrossEntropy/","text":"CrossEntropy \u00b6 Multiclass generalization of the logarithmic loss. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 ] >>> y_pred = [ ... { 0 : 0.29450637 , 1 : 0.34216758 , 2 : 0.36332605 }, ... { 0 : 0.21290077 , 1 : 0.32728332 , 2 : 0.45981591 }, ... { 0 : 0.42860913 , 1 : 0.33380113 , 2 : 0.23758974 }, ... { 0 : 0.44941979 , 1 : 0.32962558 , 2 : 0.22095463 } ... ] >>> metric = metrics . CrossEntropy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) ... print ( metric . get ()) 1.222454 1.169691 1.258864 1.321597 >>> metric CrossEntropy : 1.321598 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"CrossEntropy"},{"location":"api/metrics/CrossEntropy/#crossentropy","text":"Multiclass generalization of the logarithmic loss.","title":"CrossEntropy"},{"location":"api/metrics/CrossEntropy/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/CrossEntropy/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 ] >>> y_pred = [ ... { 0 : 0.29450637 , 1 : 0.34216758 , 2 : 0.36332605 }, ... { 0 : 0.21290077 , 1 : 0.32728332 , 2 : 0.45981591 }, ... { 0 : 0.42860913 , 1 : 0.33380113 , 2 : 0.23758974 }, ... { 0 : 0.44941979 , 1 : 0.32962558 , 2 : 0.22095463 } ... ] >>> metric = metrics . CrossEntropy () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) ... print ( metric . get ()) 1.222454 1.169691 1.258864 1.321597 >>> metric CrossEntropy : 1.321598","title":"Examples"},{"location":"api/metrics/CrossEntropy/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/ExactMatch/","text":"ExactMatch \u00b6 Exact match score. This is the most strict multi-label metric, defined as the number of samples that have all their labels correctly classified, divided by the total number of samples. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExactMatch () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExactMatch : 0.333333 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ExactMatch"},{"location":"api/metrics/ExactMatch/#exactmatch","text":"Exact match score. This is the most strict multi-label metric, defined as the number of samples that have all their labels correctly classified, divided by the total number of samples.","title":"ExactMatch"},{"location":"api/metrics/ExactMatch/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ExactMatch/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ExactMatch/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExactMatch () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExactMatch : 0.333333","title":"Examples"},{"location":"api/metrics/ExactMatch/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ExampleF1/","text":"ExampleF1 \u00b6 Example-based F1 score for multilabel classification. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleF1 : 0.860215 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ExampleF1"},{"location":"api/metrics/ExampleF1/#examplef1","text":"Example-based F1 score for multilabel classification.","title":"ExampleF1"},{"location":"api/metrics/ExampleF1/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ExampleF1/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ExampleF1/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleF1 : 0.860215","title":"Examples"},{"location":"api/metrics/ExampleF1/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ExampleFBeta/","text":"ExampleFBeta \u00b6 Example-based F-Beta score. Parameters \u00b6 beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 precision ( metrics.ExamplePrecision ) recall ( metrics.ExampleRecall ) Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleFBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleFBeta : 0.843882 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ExampleFBeta"},{"location":"api/metrics/ExampleFBeta/#examplefbeta","text":"Example-based F-Beta score.","title":"ExampleFBeta"},{"location":"api/metrics/ExampleFBeta/#parameters","text":"beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ExampleFBeta/#attributes","text":"precision ( metrics.ExamplePrecision ) recall ( metrics.ExampleRecall )","title":"Attributes"},{"location":"api/metrics/ExampleFBeta/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleFBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleFBeta : 0.843882","title":"Examples"},{"location":"api/metrics/ExampleFBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ExamplePrecision/","text":"ExamplePrecision \u00b6 Example-based precision score for multilabel classification. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExamplePrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExamplePrecision : 0.888889 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ExamplePrecision"},{"location":"api/metrics/ExamplePrecision/#exampleprecision","text":"Example-based precision score for multilabel classification.","title":"ExamplePrecision"},{"location":"api/metrics/ExamplePrecision/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ExamplePrecision/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ExamplePrecision/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExamplePrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExamplePrecision : 0.888889","title":"Examples"},{"location":"api/metrics/ExamplePrecision/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/ExampleRecall/","text":"ExampleRecall \u00b6 Example-based recall score for multilabel classification. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleRecall : 0.833333 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ExampleRecall"},{"location":"api/metrics/ExampleRecall/#examplerecall","text":"Example-based recall score for multilabel classification.","title":"ExampleRecall"},{"location":"api/metrics/ExampleRecall/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/ExampleRecall/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ExampleRecall/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . ExampleRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ExampleRecall : 0.833333","title":"Examples"},{"location":"api/metrics/ExampleRecall/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/F1/","text":"F1 \u00b6 Binary F1 score. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , False ] >>> metric = metrics . F1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric F1 : 0.4 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"F1"},{"location":"api/metrics/F1/#f1","text":"Binary F1 score.","title":"F1"},{"location":"api/metrics/F1/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/F1/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/F1/#examples","text":">>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , False ] >>> metric = metrics . F1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric F1 : 0.4","title":"Examples"},{"location":"api/metrics/F1/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/FBeta/","text":"FBeta \u00b6 Binary F-Beta score. The FBeta score is a weighted harmonic mean between precision and recall. The higher the beta value, the higher the recall will be taken into account. When beta equals 1, precision and recall and equivalently weighted, which results in the F1 score (see metrics.F1 ). Parameters \u00b6 beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 precision ( metrics.Precision ) recall ( metrics.Recall ) Examples \u00b6 >>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , False ] >>> metric = metrics . FBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric FBeta : 0.357143 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"FBeta"},{"location":"api/metrics/FBeta/#fbeta","text":"Binary F-Beta score. The FBeta score is a weighted harmonic mean between precision and recall. The higher the beta value, the higher the recall will be taken into account. When beta equals 1, precision and recall and equivalently weighted, which results in the F1 score (see metrics.F1 ).","title":"FBeta"},{"location":"api/metrics/FBeta/#parameters","text":"beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/FBeta/#attributes","text":"precision ( metrics.Precision ) recall ( metrics.Recall )","title":"Attributes"},{"location":"api/metrics/FBeta/#examples","text":">>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , False ] >>> metric = metrics . FBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric FBeta : 0.357143","title":"Examples"},{"location":"api/metrics/FBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/FowlkesMallows/","text":"FowlkesMallows \u00b6 Fowlkes-Mallows Index. The Fowlkes-Mallows Index 1 2 is an external evaluation method that is used to determine the similarity between two clusterings, and also a mmetric to measure confusion matrices. The measure of similarity could be either between two hierarchical clusterings or a clustering and a benchmark classification. A higher value for the Fowlkes-Mallows index indicates a greater similarity between the clusters and the benchmark classifications. The Fowlkes-Mallows Index, for two cluster algorithms, is defined as: \\[ FM = \\sqrt{PPV \\times TPR} = \\sqrt{\\frac{TP}{TP+FP} \\times \\frac{TP}{TP+FN}} \\] where TP, FP, FN are respectively the number of true positives, false positives and false negatives; TPR is the True Positive Rate (or Sensitivity/Recall), PPV is the Positive Predictive Rate (or Precision). Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . FowlkesMallows () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 1.0 0.5773502691896257 0.408248290463863 0.3535533905932738 0.4714045207910317 >>> metric FowlkesMallows : 0.471405 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2020, December 22). Fowlkes\u2013Mallows index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Fowlkes%E2%80%93Mallows_index&oldid=995714222 \u21a9 E. B. Fowkles and C. L. Mallows (1983). \u201cA method for comparing two hierarchical clusterings\u201d. Journal of the American Statistical Association \u21a9","title":"FowlkesMallows"},{"location":"api/metrics/FowlkesMallows/#fowlkesmallows","text":"Fowlkes-Mallows Index. The Fowlkes-Mallows Index 1 2 is an external evaluation method that is used to determine the similarity between two clusterings, and also a mmetric to measure confusion matrices. The measure of similarity could be either between two hierarchical clusterings or a clustering and a benchmark classification. A higher value for the Fowlkes-Mallows index indicates a greater similarity between the clusters and the benchmark classifications. The Fowlkes-Mallows Index, for two cluster algorithms, is defined as: \\[ FM = \\sqrt{PPV \\times TPR} = \\sqrt{\\frac{TP}{TP+FP} \\times \\frac{TP}{TP+FN}} \\] where TP, FP, FN are respectively the number of true positives, false positives and false negatives; TPR is the True Positive Rate (or Sensitivity/Recall), PPV is the Positive Predictive Rate (or Precision).","title":"FowlkesMallows"},{"location":"api/metrics/FowlkesMallows/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/FowlkesMallows/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/FowlkesMallows/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . FowlkesMallows () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 1.0 0.5773502691896257 0.408248290463863 0.3535533905932738 0.4714045207910317 >>> metric FowlkesMallows : 0.471405","title":"Examples"},{"location":"api/metrics/FowlkesMallows/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/FowlkesMallows/#references","text":"Wikipedia contributors. (2020, December 22). Fowlkes\u2013Mallows index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Fowlkes%E2%80%93Mallows_index&oldid=995714222 \u21a9 E. B. Fowkles and C. L. Mallows (1983). \u201cA method for comparing two hierarchical clusterings\u201d. Journal of the American Statistical Association \u21a9","title":"References"},{"location":"api/metrics/GeometricMean/","text":"GeometricMean \u00b6 Geometric mean score. The geometric mean is a good indicator of a classifier's performance in the presence of class imbalance because it is independent of the distribution of examples between classes. This implementation computes the geometric mean of class-wise sensitivity (recall). \\[ gm = \\sqrt[n]{s_1\\cdot s_2\\cdot s_3\\cdot \\ldots\\cdot s_n} \\] where \\(s_i\\) is the sensitivity (recall) of class \\(i\\) and \\(n\\) is the number of classes. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' , 'bird' ] >>> metric = metrics . GeometricMean () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric GeometricMean : 0.693361 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Barandela, R. et al. \u201cStrategies for learning in class imbalance problems\u201d, Pattern Recognition, 36(3), (2003), pp 849-851. \u21a9","title":"GeometricMean"},{"location":"api/metrics/GeometricMean/#geometricmean","text":"Geometric mean score. The geometric mean is a good indicator of a classifier's performance in the presence of class imbalance because it is independent of the distribution of examples between classes. This implementation computes the geometric mean of class-wise sensitivity (recall). \\[ gm = \\sqrt[n]{s_1\\cdot s_2\\cdot s_3\\cdot \\ldots\\cdot s_n} \\] where \\(s_i\\) is the sensitivity (recall) of class \\(i\\) and \\(n\\) is the number of classes.","title":"GeometricMean"},{"location":"api/metrics/GeometricMean/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/GeometricMean/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/GeometricMean/#examples","text":">>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' , 'bird' ] >>> metric = metrics . GeometricMean () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric GeometricMean : 0.693361","title":"Examples"},{"location":"api/metrics/GeometricMean/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/GeometricMean/#references","text":"Barandela, R. et al. \u201cStrategies for learning in class imbalance problems\u201d, Pattern Recognition, 36(3), (2003), pp 849-851. \u21a9","title":"References"},{"location":"api/metrics/Hamming/","text":"Hamming \u00b6 Hamming score. The Hamming score is the fraction of labels that are correctly predicted. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . Hamming () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric Hamming : 0.555556 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Hamming"},{"location":"api/metrics/Hamming/#hamming","text":"Hamming score. The Hamming score is the fraction of labels that are correctly predicted.","title":"Hamming"},{"location":"api/metrics/Hamming/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Hamming/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Hamming/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . Hamming () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric Hamming : 0.555556","title":"Examples"},{"location":"api/metrics/Hamming/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/HammingLoss/","text":"HammingLoss \u00b6 Hamming loss score. The Hamming loss is the complement of the Hamming score. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . HammingLoss () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric HammingLoss : 0.444444 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"HammingLoss"},{"location":"api/metrics/HammingLoss/#hammingloss","text":"Hamming loss score. The Hamming loss is the complement of the Hamming score.","title":"HammingLoss"},{"location":"api/metrics/HammingLoss/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/HammingLoss/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/HammingLoss/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> metric = metrics . HammingLoss () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric HammingLoss : 0.444444","title":"Examples"},{"location":"api/metrics/HammingLoss/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Homogeneity/","text":"Homogeneity \u00b6 Homogeneity Score. Homogeneity metric 1 of a cluster labeling given a ground truth. In order to satisfy the homogeneity criteria, a clustering must assign only those data points that are members of a single class to a single cluster. That is, the class distribution within each cluster should be skewed to a single class, that is, zero entropy. We determine how close a given clustering is to this ideal by examining the conditional entropy of the class distribution given the proposed clustering. However, in an imperfect situation, the size of this value is dependent on the size of the dataset and the distribution of class sizes. Therefore, instead of taking the raw conditional entropy, we normalize by the maximum reduction in entropy the clustering information could provide. As such, we define homogeneity as: \\[ h = \\begin{cases} 1 if H(C) = 0, \\\\ 1 - \\frac{H(C|K)}{H(C)} otherwise. \\end{cases}. \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Homogeneity () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.311278 0.37515 0.42062 >>> metric Homogeneity : 0.42062 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"Homogeneity"},{"location":"api/metrics/Homogeneity/#homogeneity","text":"Homogeneity Score. Homogeneity metric 1 of a cluster labeling given a ground truth. In order to satisfy the homogeneity criteria, a clustering must assign only those data points that are members of a single class to a single cluster. That is, the class distribution within each cluster should be skewed to a single class, that is, zero entropy. We determine how close a given clustering is to this ideal by examining the conditional entropy of the class distribution given the proposed clustering. However, in an imperfect situation, the size of this value is dependent on the size of the dataset and the distribution of class sizes. Therefore, instead of taking the raw conditional entropy, we normalize by the maximum reduction in entropy the clustering information could provide. As such, we define homogeneity as: \\[ h = \\begin{cases} 1 if H(C) = 0, \\\\ 1 - \\frac{H(C|K)}{H(C)} otherwise. \\end{cases}. \\]","title":"Homogeneity"},{"location":"api/metrics/Homogeneity/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Homogeneity/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Homogeneity/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Homogeneity () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.311278 0.37515 0.42062 >>> metric Homogeneity : 0.42062","title":"Examples"},{"location":"api/metrics/Homogeneity/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Homogeneity/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"References"},{"location":"api/metrics/Jaccard/","text":"Jaccard \u00b6 Jaccard index for binary multi-outputs. The Jaccard index, or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare the set of predicted labels for a sample with the corresponding set of labels in y_true . The Jaccard index may be a poor metric if there are no positives for some samples or labels. The Jaccard index is undefined if there are no true or predicted labels, this implementation will return a score of 0.0 if this is the case. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... ] >>> jac = metrics . Jaccard () >>> for yt , yp in zip ( y_true , y_pred ): ... jac = jac . update ( yt , yp ) >>> jac Jaccard : 0.583333 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia section on similarity of asymmetric binary attributes \u21a9","title":"Jaccard"},{"location":"api/metrics/Jaccard/#jaccard","text":"Jaccard index for binary multi-outputs. The Jaccard index, or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare the set of predicted labels for a sample with the corresponding set of labels in y_true . The Jaccard index may be a poor metric if there are no positives for some samples or labels. The Jaccard index is undefined if there are no true or predicted labels, this implementation will return a score of 0.0 if this is the case.","title":"Jaccard"},{"location":"api/metrics/Jaccard/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Jaccard/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Jaccard/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... ] >>> jac = metrics . Jaccard () >>> for yt , yp in zip ( y_true , y_pred ): ... jac = jac . update ( yt , yp ) >>> jac Jaccard : 0.583333","title":"Examples"},{"location":"api/metrics/Jaccard/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Jaccard/#references","text":"Wikipedia section on similarity of asymmetric binary attributes \u21a9","title":"References"},{"location":"api/metrics/KappaM/","text":"KappaM \u00b6 Kappa-M score. The Kappa-M statistic compares performance with the majority class classifier. It is defined as \\[ \\kappa_{m} = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the prequential accuracy of the majority classifier . Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' , 'cat' , 'ant' , 'cat' , 'cat' , 'ant' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' , 'ant' , 'ant' , 'cat' , 'cat' , 'ant' ] >>> metric = metrics . KappaM () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric KappaM : 0.25 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 [1^]: A. Bifet et al. \"Efficient online evaluation of big data stream classifiers.\" In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 59-68. ACM, 2015.","title":"KappaM"},{"location":"api/metrics/KappaM/#kappam","text":"Kappa-M score. The Kappa-M statistic compares performance with the majority class classifier. It is defined as \\[ \\kappa_{m} = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the prequential accuracy of the majority classifier .","title":"KappaM"},{"location":"api/metrics/KappaM/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/KappaM/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/KappaM/#examples","text":">>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' , 'cat' , 'ant' , 'cat' , 'cat' , 'ant' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' , 'ant' , 'ant' , 'cat' , 'cat' , 'ant' ] >>> metric = metrics . KappaM () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric KappaM : 0.25","title":"Examples"},{"location":"api/metrics/KappaM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/KappaM/#references","text":"[1^]: A. Bifet et al. \"Efficient online evaluation of big data stream classifiers.\" In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 59-68. ACM, 2015.","title":"References"},{"location":"api/metrics/KappaT/","text":"KappaT \u00b6 Kappa-T score. The Kappa-T measures the temporal correlation between samples. It is defined as \\[ \\kappa_{t} = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the prequential accuracy of the no-change classifier that predicts only using the last class seen by the classifier. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> metric = metrics . KappaT () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric KappaT : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 A. Bifet et al. (2013). \"Pitfalls in benchmarking data stream classification and how to avoid them.\" Proc. of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD'13), Springer LNAI 8188, p. 465-479. \u21a9","title":"KappaT"},{"location":"api/metrics/KappaT/#kappat","text":"Kappa-T score. The Kappa-T measures the temporal correlation between samples. It is defined as \\[ \\kappa_{t} = (p_o - p_e) / (1 - p_e) \\] where \\(p_o\\) is the empirical probability of agreement on the label assigned to any sample (prequential accuracy), and \\(p_e\\) is the prequential accuracy of the no-change classifier that predicts only using the last class seen by the classifier.","title":"KappaT"},{"location":"api/metrics/KappaT/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/KappaT/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/KappaT/#examples","text":">>> from river import metrics >>> y_true = [ 'cat' , 'ant' , 'cat' , 'cat' , 'ant' , 'bird' ] >>> y_pred = [ 'ant' , 'ant' , 'cat' , 'cat' , 'ant' , 'cat' ] >>> metric = metrics . KappaT () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric KappaT : 0.6","title":"Examples"},{"location":"api/metrics/KappaT/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/KappaT/#references","text":"A. Bifet et al. (2013). \"Pitfalls in benchmarking data stream classification and how to avoid them.\" Proc. of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD'13), Springer LNAI 8188, p. 465-479. \u21a9","title":"References"},{"location":"api/metrics/LogLoss/","text":"LogLoss \u00b6 Binary logarithmic loss. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , False , False , True ] >>> y_pred = [ 0.9 , 0.1 , 0.2 , 0.65 ] >>> metric = metrics . LogLoss () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) ... print ( metric . get ()) 0.105360 0.105360 0.144621 0.216161 >>> metric LogLoss : 0.216162 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"LogLoss"},{"location":"api/metrics/LogLoss/#logloss","text":"Binary logarithmic loss.","title":"LogLoss"},{"location":"api/metrics/LogLoss/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/LogLoss/#examples","text":">>> from river import metrics >>> y_true = [ True , False , False , True ] >>> y_pred = [ 0.9 , 0.1 , 0.2 , 0.65 ] >>> metric = metrics . LogLoss () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) ... print ( metric . get ()) 0.105360 0.105360 0.144621 0.216161 >>> metric LogLoss : 0.216162","title":"Examples"},{"location":"api/metrics/LogLoss/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/MAE/","text":"MAE \u00b6 Mean absolute error. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . MAE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.5 0.5 0.333 0.5 >>> metric MAE : 0.5 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"MAE"},{"location":"api/metrics/MAE/#mae","text":"Mean absolute error.","title":"MAE"},{"location":"api/metrics/MAE/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MAE/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . MAE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.5 0.5 0.333 0.5 >>> metric MAE : 0.5","title":"Examples"},{"location":"api/metrics/MAE/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/MCC/","text":"MCC \u00b6 Matthews correlation coefficient. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , True , True , False ] >>> y_pred = [ True , False , True , True ] >>> mcc = metrics . MCC () >>> for yt , yp in zip ( y_true , y_pred ): ... mcc = mcc . update ( yt , yp ) >>> mcc MCC : - 0.333333 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia article \u21a9","title":"MCC"},{"location":"api/metrics/MCC/#mcc","text":"Matthews correlation coefficient.","title":"MCC"},{"location":"api/metrics/MCC/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/MCC/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MCC/#examples","text":">>> from river import metrics >>> y_true = [ True , True , True , False ] >>> y_pred = [ True , False , True , True ] >>> mcc = metrics . MCC () >>> for yt , yp in zip ( y_true , y_pred ): ... mcc = mcc . update ( yt , yp ) >>> mcc MCC : - 0.333333","title":"Examples"},{"location":"api/metrics/MCC/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MCC/#references","text":"Wikipedia article \u21a9","title":"References"},{"location":"api/metrics/MSE/","text":"MSE \u00b6 Mean squared error. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . MSE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.25 0.25 0.1666 0.375 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"MSE"},{"location":"api/metrics/MSE/#mse","text":"Mean squared error.","title":"MSE"},{"location":"api/metrics/MSE/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MSE/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . MSE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.25 0.25 0.1666 0.375","title":"Examples"},{"location":"api/metrics/MSE/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/MacroF1/","text":"MacroF1 \u00b6 Macro-average F1 score. This works by computing the F1 score per class, and then performs a global average. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroF1 : 1. MacroF1 : 0.333333 MacroF1 : 0.555556 MacroF1 : 0.555556 MacroF1 : 0.488889 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MacroF1"},{"location":"api/metrics/MacroF1/#macrof1","text":"Macro-average F1 score. This works by computing the F1 score per class, and then performs a global average.","title":"MacroF1"},{"location":"api/metrics/MacroF1/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MacroF1/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MacroF1/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroF1 : 1. MacroF1 : 0.333333 MacroF1 : 0.555556 MacroF1 : 0.555556 MacroF1 : 0.488889","title":"Examples"},{"location":"api/metrics/MacroF1/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MacroFBeta/","text":"MacroFBeta \u00b6 Macro-average F-Beta score. This works by computing the F-Beta score per class, and then performs a global average. Parameters \u00b6 beta Weight of precision in harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroFBeta ( beta = .8 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroFBeta : 1. MacroFBeta : 0.310606 MacroFBeta : 0.540404 MacroFBeta : 0.540404 MacroFBeta : 0.485982 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MacroFBeta"},{"location":"api/metrics/MacroFBeta/#macrofbeta","text":"Macro-average F-Beta score. This works by computing the F-Beta score per class, and then performs a global average.","title":"MacroFBeta"},{"location":"api/metrics/MacroFBeta/#parameters","text":"beta Weight of precision in harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MacroFBeta/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MacroFBeta/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroFBeta ( beta = .8 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroFBeta : 1. MacroFBeta : 0.310606 MacroFBeta : 0.540404 MacroFBeta : 0.540404 MacroFBeta : 0.485982","title":"Examples"},{"location":"api/metrics/MacroFBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MacroPrecision/","text":"MacroPrecision \u00b6 Macro-average precision score. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroPrecision : 1. MacroPrecision : 0.25 MacroPrecision : 0.5 MacroPrecision : 0.5 MacroPrecision : 0.5 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MacroPrecision"},{"location":"api/metrics/MacroPrecision/#macroprecision","text":"Macro-average precision score.","title":"MacroPrecision"},{"location":"api/metrics/MacroPrecision/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MacroPrecision/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MacroPrecision/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroPrecision : 1. MacroPrecision : 0.25 MacroPrecision : 0.5 MacroPrecision : 0.5 MacroPrecision : 0.5","title":"Examples"},{"location":"api/metrics/MacroPrecision/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MacroRecall/","text":"MacroRecall \u00b6 Macro-average recall score. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroRecall : 1. MacroRecall : 0.5 MacroRecall : 0.666667 MacroRecall : 0.666667 MacroRecall : 0.555556 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MacroRecall"},{"location":"api/metrics/MacroRecall/#macrorecall","text":"Macro-average recall score.","title":"MacroRecall"},{"location":"api/metrics/MacroRecall/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MacroRecall/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MacroRecall/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MacroRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MacroRecall : 1. MacroRecall : 0.5 MacroRecall : 0.666667 MacroRecall : 0.666667 MacroRecall : 0.555556","title":"Examples"},{"location":"api/metrics/MacroRecall/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MatthewsCorrCoef/","text":"MatthewsCorrCoef \u00b6 Matthews correlation coefficient. The Matthews correlation coefficient (MCC) or phi coefficient is used in Machine Learning as a measure of the quality of classifications, introduced by Brian W. Matthews in 1975. The MCC is defined identically to Pearson's phi coefficient, introduced by Karl Pearson, also known as the Yule phi coefficient from its introduction by Udny Yule in 1912. The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. It returns a value between -1 and 1, with 1 being a perfect prediction, 0 no better than random prediction and -1 means a total disagreement between prediction and observation. The MCC can be calculated directly from the (pair) confusion matrix using the original formula by Matthews. Let \\[ \\begin{cases} N = TN + TP + FN + FP = \\frac{n(n-1)}{2}, \\\\ S = \\frac{TP + FN}{N}, \\\\ P = \\frac{TP + FP}{N}. \\end{cases} \\] The MCC would be then equal to \\[ MCC = \\frac{TP / N - S \\times P}{\\sqrt{PS(1 - S)(1 - P)}}. \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . MatthewsCorrCoef () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.5773502691896258 0.36084391824351614 0.28867513459481287 >>> metric MatthewsCorrCoef : 0.288675 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, March 26). Matthews correlation coefficient. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Matthews_correlation_coefficient&oldid=1014254295 \u21a9 Jurman, G., Riccadonna, S., & Furlanello, C. (2012). A Comparison of MCC and CEN Error Measures in Multi-Class Prediction. Plos ONE, 7(8), e41882. doi: 10.1371/journal.pone.0041882 \u21a9","title":"MatthewsCorrCoef"},{"location":"api/metrics/MatthewsCorrCoef/#matthewscorrcoef","text":"Matthews correlation coefficient. The Matthews correlation coefficient (MCC) or phi coefficient is used in Machine Learning as a measure of the quality of classifications, introduced by Brian W. Matthews in 1975. The MCC is defined identically to Pearson's phi coefficient, introduced by Karl Pearson, also known as the Yule phi coefficient from its introduction by Udny Yule in 1912. The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. It returns a value between -1 and 1, with 1 being a perfect prediction, 0 no better than random prediction and -1 means a total disagreement between prediction and observation. The MCC can be calculated directly from the (pair) confusion matrix using the original formula by Matthews. Let \\[ \\begin{cases} N = TN + TP + FN + FP = \\frac{n(n-1)}{2}, \\\\ S = \\frac{TP + FN}{N}, \\\\ P = \\frac{TP + FP}{N}. \\end{cases} \\] The MCC would be then equal to \\[ MCC = \\frac{TP / N - S \\times P}{\\sqrt{PS(1 - S)(1 - P)}}. \\]","title":"MatthewsCorrCoef"},{"location":"api/metrics/MatthewsCorrCoef/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MatthewsCorrCoef/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MatthewsCorrCoef/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . MatthewsCorrCoef () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.5773502691896258 0.36084391824351614 0.28867513459481287 >>> metric MatthewsCorrCoef : 0.288675","title":"Examples"},{"location":"api/metrics/MatthewsCorrCoef/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MatthewsCorrCoef/#references","text":"Wikipedia contributors. (2021, March 26). Matthews correlation coefficient. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Matthews_correlation_coefficient&oldid=1014254295 \u21a9 Jurman, G., Riccadonna, S., & Furlanello, C. (2012). A Comparison of MCC and CEN Error Measures in Multi-Class Prediction. Plos ONE, 7(8), e41882. doi: 10.1371/journal.pone.0041882 \u21a9","title":"References"},{"location":"api/metrics/Metric/","text":"Metric \u00b6 Mother class for all metrics. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight update Update the metric. Parameters y_true y_pred sample_weight works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Metric"},{"location":"api/metrics/Metric/#metric","text":"Mother class for all metrics.","title":"Metric"},{"location":"api/metrics/Metric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Metric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight update Update the metric. Parameters y_true y_pred sample_weight works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Metrics/","text":"Metrics \u00b6 A container class for handling multiple metrics at once. Parameters \u00b6 metrics str_sep \u2013 defaults to , Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters item clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the metric. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 sort update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Metrics"},{"location":"api/metrics/Metrics/#metrics","text":"A container class for handling multiple metrics at once.","title":"Metrics"},{"location":"api/metrics/Metrics/#parameters","text":"metrics str_sep \u2013 defaults to ,","title":"Parameters"},{"location":"api/metrics/Metrics/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Metrics/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters item clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the metric. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 sort update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MicroF1/","text":"MicroF1 \u00b6 Micro-average F1 score. This computes the F1 score by merging all the predictions and true labels, and then computes a global F1 score. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 0 ] >>> y_pred = [ 0 , 1 , 1 , 2 , 1 ] >>> metric = metrics . MicroF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric MicroF1 : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"MicroF1"},{"location":"api/metrics/MicroF1/#microf1","text":"Micro-average F1 score. This computes the F1 score by merging all the predictions and true labels, and then computes a global F1 score.","title":"MicroF1"},{"location":"api/metrics/MicroF1/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MicroF1/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MicroF1/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 0 ] >>> y_pred = [ 0 , 1 , 1 , 2 , 1 ] >>> metric = metrics . MicroF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric MicroF1 : 0.6","title":"Examples"},{"location":"api/metrics/MicroF1/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MicroF1/#references","text":"Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"References"},{"location":"api/metrics/MicroFBeta/","text":"MicroFBeta \u00b6 Micro-average F-Beta score. This computes the F-Beta score by merging all the predictions and true labels, and then computes a global F-Beta score. Parameters \u00b6 beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 0 ] >>> y_pred = [ 0 , 1 , 1 , 2 , 1 ] >>> metric = metrics . MicroFBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric MicroFBeta : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?","title":"MicroFBeta"},{"location":"api/metrics/MicroFBeta/#microfbeta","text":"Micro-average F-Beta score. This computes the F-Beta score by merging all the predictions and true labels, and then computes a global F-Beta score.","title":"MicroFBeta"},{"location":"api/metrics/MicroFBeta/#parameters","text":"beta ( float ) Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MicroFBeta/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MicroFBeta/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 0 ] >>> y_pred = [ 0 , 1 , 1 , 2 , 1 ] >>> metric = metrics . MicroFBeta ( beta = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric MicroFBeta : 0.6","title":"Examples"},{"location":"api/metrics/MicroFBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MicroFBeta/#references","text":"Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem?","title":"References"},{"location":"api/metrics/MicroPrecision/","text":"MicroPrecision \u00b6 Micro-average precision score. The micro-average precision score is exactly equivalent to the micro-average recall as well as the micro-average F1 score. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MicroPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MicroPrecision : 1. MicroPrecision : 0.5 MicroPrecision : 0.666667 MicroPrecision : 0.75 MicroPrecision : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"MicroPrecision"},{"location":"api/metrics/MicroPrecision/#microprecision","text":"Micro-average precision score. The micro-average precision score is exactly equivalent to the micro-average recall as well as the micro-average F1 score.","title":"MicroPrecision"},{"location":"api/metrics/MicroPrecision/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MicroPrecision/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MicroPrecision/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MicroPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MicroPrecision : 1. MicroPrecision : 0.5 MicroPrecision : 0.666667 MicroPrecision : 0.75 MicroPrecision : 0.6","title":"Examples"},{"location":"api/metrics/MicroPrecision/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MicroPrecision/#references","text":"Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"References"},{"location":"api/metrics/MicroRecall/","text":"MicroRecall \u00b6 Micro-average recall score. The micro-average recall is exactly equivalent to the micro-average precision as well as the micro-average F1 score. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MicroRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MicroRecall : 1. MicroRecall : 0.5 MicroRecall : 0.666667 MicroRecall : 0.75 MicroRecall : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"MicroRecall"},{"location":"api/metrics/MicroRecall/#microrecall","text":"Micro-average recall score. The micro-average recall is exactly equivalent to the micro-average precision as well as the micro-average F1 score.","title":"MicroRecall"},{"location":"api/metrics/MicroRecall/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MicroRecall/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MicroRecall/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MicroRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MicroRecall : 1. MicroRecall : 0.5 MicroRecall : 0.666667 MicroRecall : 0.75 MicroRecall : 0.6","title":"Examples"},{"location":"api/metrics/MicroRecall/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MicroRecall/#references","text":"Why are precision, recall and F1 score equal when using micro averaging in a multi-class problem? \u21a9","title":"References"},{"location":"api/metrics/MultiClassMetric/","text":"MultiClassMetric \u00b6 Mother class for all multi-class classification metrics. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MultiClassMetric"},{"location":"api/metrics/MultiClassMetric/#multiclassmetric","text":"Mother class for all multi-class classification metrics.","title":"MultiClassMetric"},{"location":"api/metrics/MultiClassMetric/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MultiClassMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MultiClassMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MultiFBeta/","text":"MultiFBeta \u00b6 Multi-class F-Beta score with different betas per class. The multiclass F-Beta score is the arithmetic average of the binary F-Beta scores of each class. The mean can be weighted by providing class weights. Parameters \u00b6 betas Weight of precision in the harmonic mean of each class. weights Class weights. If not provided then uniform weights will be used. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MultiFBeta ( ... betas = { 0 : 0.25 , 1 : 1 , 2 : 4 }, ... weights = { 0 : 1 , 1 : 1 , 2 : 2 } ... ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MultiFBeta : 1. MultiFBeta : 0.257576 MultiFBeta : 0.628788 MultiFBeta : 0.628788 MultiFBeta : 0.468788 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MultiFBeta"},{"location":"api/metrics/MultiFBeta/#multifbeta","text":"Multi-class F-Beta score with different betas per class. The multiclass F-Beta score is the arithmetic average of the binary F-Beta scores of each class. The mean can be weighted by providing class weights.","title":"MultiFBeta"},{"location":"api/metrics/MultiFBeta/#parameters","text":"betas Weight of precision in the harmonic mean of each class. weights Class weights. If not provided then uniform weights will be used. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MultiFBeta/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MultiFBeta/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . MultiFBeta ( ... betas = { 0 : 0.25 , 1 : 1 , 2 : 4 }, ... weights = { 0 : 1 , 1 : 1 , 2 : 2 } ... ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MultiFBeta : 1. MultiFBeta : 0.257576 MultiFBeta : 0.628788 MultiFBeta : 0.628788 MultiFBeta : 0.468788","title":"Examples"},{"location":"api/metrics/MultiFBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MultiLabelConfusionMatrix/","text":"MultiLabelConfusionMatrix \u00b6 MultiLabelConfusionMatrix(labels=None) Multi-label Confusion Matrix. Attributes \u00b6 data exact_match_cnt jaccard_sum labels last_y_pred last_y_true n_labels n_samples precision_sum recall_sum sample_correction shape Methods \u00b6 reset revert Parameters y_true y_pred sample_weight correction update Parameters y_true y_pred sample_weight Notes \u00b6 This confusion matrix corresponds to a 3D matrix of shape `(n_labels, 2, 2)` meaning that each `label` has a corresponding binary `(2x2)` confusion matrix. The first dimension corresponds to the `label`, the second and third dimensions are binary indicators for the `true` (actual) vs `predicted` values. For example, an entry in position `[2, 0, 1]` represents a miss-classification of label 2. This structure is used to keep updated statistics about a multi-output classifier's performance and to compute multiple evaluation metrics.","title":"MultiLabelConfusionMatrix"},{"location":"api/metrics/MultiLabelConfusionMatrix/#multilabelconfusionmatrix","text":"MultiLabelConfusionMatrix(labels=None) Multi-label Confusion Matrix.","title":"MultiLabelConfusionMatrix"},{"location":"api/metrics/MultiLabelConfusionMatrix/#attributes","text":"data exact_match_cnt jaccard_sum labels last_y_pred last_y_true n_labels n_samples precision_sum recall_sum sample_correction shape","title":"Attributes"},{"location":"api/metrics/MultiLabelConfusionMatrix/#methods","text":"reset revert Parameters y_true y_pred sample_weight correction update Parameters y_true y_pred sample_weight","title":"Methods"},{"location":"api/metrics/MultiLabelConfusionMatrix/#notes","text":"This confusion matrix corresponds to a 3D matrix of shape `(n_labels, 2, 2)` meaning that each `label` has a corresponding binary `(2x2)` confusion matrix. The first dimension corresponds to the `label`, the second and third dimensions are binary indicators for the `true` (actual) vs `predicted` values. For example, an entry in position `[2, 0, 1]` represents a miss-classification of label 2. This structure is used to keep updated statistics about a multi-output classifier's performance and to compute multiple evaluation metrics.","title":"Notes"},{"location":"api/metrics/MultiOutputClassificationMetric/","text":"MultiOutputClassificationMetric \u00b6 Mother class for all multi-output classification metrics. Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MultiOutputClassificationMetric"},{"location":"api/metrics/MultiOutputClassificationMetric/#multioutputclassificationmetric","text":"Mother class for all multi-output classification metrics.","title":"MultiOutputClassificationMetric"},{"location":"api/metrics/MultiOutputClassificationMetric/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MultiOutputClassificationMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MultiOutputClassificationMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MultiOutputRegressionMetric/","text":"MultiOutputRegressionMetric \u00b6 Mother class for all multi-output regression metrics. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"MultiOutputRegressionMetric"},{"location":"api/metrics/MultiOutputRegressionMetric/#multioutputregressionmetric","text":"Mother class for all multi-output regression metrics.","title":"MultiOutputRegressionMetric"},{"location":"api/metrics/MultiOutputRegressionMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MultiOutputRegressionMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MutualInfo/","text":"MutualInfo \u00b6 Mutual Information between two clusterings. The Mutual Information 1 is a measure of the similarity between two labels of the same data. Where \\(|U_i|\\) is the number of samples in cluster \\(U_i\\) and \\(|V_j|\\) is the number of the samples in cluster \\(V_j\\) , the Mutual Information between clusterings \\(U\\) and \\(V\\) can be calculated as: \\[ MI(U,V) = \\sum_{i=1}^{|U|} \\sum_{v=1}^{|V|} \\frac{|U_i \\cup V_j|}{N} \\log \\frac{N |U_i \\cup V_j|}{|U_i| |V_j|} \\] This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score. This metric is furthermore symmetric: switching y_true and y_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known. The Mutual Information can be equivalently expressed as: \\[ MI(U,V) = H(U) - H(U | V) = H(V) - H(V | U) \\] where \\(H(U)\\) and \\(H(V)\\) are the marginal entropies, \\(H(U | V)\\) and \\(H(V | U)\\) are the conditional entropies. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . MutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.215761 0.395752 0.462098 >>> metric MutualInfo : 0.462098 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"MutualInfo"},{"location":"api/metrics/MutualInfo/#mutualinfo","text":"Mutual Information between two clusterings. The Mutual Information 1 is a measure of the similarity between two labels of the same data. Where \\(|U_i|\\) is the number of samples in cluster \\(U_i\\) and \\(|V_j|\\) is the number of the samples in cluster \\(V_j\\) , the Mutual Information between clusterings \\(U\\) and \\(V\\) can be calculated as: \\[ MI(U,V) = \\sum_{i=1}^{|U|} \\sum_{v=1}^{|V|} \\frac{|U_i \\cup V_j|}{N} \\log \\frac{N |U_i \\cup V_j|}{|U_i| |V_j|} \\] This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score. This metric is furthermore symmetric: switching y_true and y_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known. The Mutual Information can be equivalently expressed as: \\[ MI(U,V) = H(U) - H(U | V) = H(V) - H(V | U) \\] where \\(H(U)\\) and \\(H(V)\\) are the marginal entropies, \\(H(U | V)\\) and \\(H(V | U)\\) are the conditional entropies.","title":"MutualInfo"},{"location":"api/metrics/MutualInfo/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/MutualInfo/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/MutualInfo/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . MutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.215761 0.395752 0.462098 >>> metric MutualInfo : 0.462098","title":"Examples"},{"location":"api/metrics/MutualInfo/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/MutualInfo/#references","text":"Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"References"},{"location":"api/metrics/NormalizedMutualInfo/","text":"NormalizedMutualInfo \u00b6 Normalized Mutual Information between two clusterings. Normalized Mutual Information (NMI) is a normalized version of the Mutual Information (MI) score to scale the results between the range of 0 (no mutual information) and 1 (perfectly mutual information). In the formula, the mutual information will be normalized by a generalized mean of the entropy of true and predicted labels, defined by the average_method . We note that this measure is not adjusted for chance (i.e corrected the effect of result agreement solely due to chance); as a result, the Adjusted Mutual Info Score will mostly be preferred. However, this metric is still symmetric, which means that switching true and predicted labels will not alter the score value. This fact can be useful when the metric is used to measure the agreement between two indepedent label solutions on the same dataset, when the ground truth remains unknown. Another advantage of the metric is that as it is based on the calculation of entropy-related measures, it is independent of the permutation of class/cluster labels. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. average_method \u2013 defaults to arithmetic This parameter defines how to compute the normalizer in the denominator. Possible options include min , max , arithmetic and geometric . Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . NormalizedMutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.343711 0.458065 0.515803 >>> metric NormalizedMutualInfo : 0.515804 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"NormalizedMutualInfo"},{"location":"api/metrics/NormalizedMutualInfo/#normalizedmutualinfo","text":"Normalized Mutual Information between two clusterings. Normalized Mutual Information (NMI) is a normalized version of the Mutual Information (MI) score to scale the results between the range of 0 (no mutual information) and 1 (perfectly mutual information). In the formula, the mutual information will be normalized by a generalized mean of the entropy of true and predicted labels, defined by the average_method . We note that this measure is not adjusted for chance (i.e corrected the effect of result agreement solely due to chance); as a result, the Adjusted Mutual Info Score will mostly be preferred. However, this metric is still symmetric, which means that switching true and predicted labels will not alter the score value. This fact can be useful when the metric is used to measure the agreement between two indepedent label solutions on the same dataset, when the ground truth remains unknown. Another advantage of the metric is that as it is based on the calculation of entropy-related measures, it is independent of the permutation of class/cluster labels.","title":"NormalizedMutualInfo"},{"location":"api/metrics/NormalizedMutualInfo/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. average_method \u2013 defaults to arithmetic This parameter defines how to compute the normalizer in the denominator. Possible options include min , max , arithmetic and geometric .","title":"Parameters"},{"location":"api/metrics/NormalizedMutualInfo/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/NormalizedMutualInfo/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . NormalizedMutualInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.343711 0.458065 0.515803 >>> metric NormalizedMutualInfo : 0.515804","title":"Examples"},{"location":"api/metrics/NormalizedMutualInfo/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/NormalizedMutualInfo/#references","text":"Wikipedia contributors. (2021, March 17). Mutual information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Mutual_information&oldid=1012714929 \u21a9","title":"References"},{"location":"api/metrics/PairConfusionMatrix/","text":"PairConfusionMatrix \u00b6 Pair Confusion Matrix. The pair confusion matrix \\(C\\) is a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings. The pair confusion matrix has the following entries: \\(C[0][0]\\) ( True Negatives ): number of pairs of points that are in different clusters in both true and predicted labels \\(C[0][1]\\) ( False Positives ): number of pairs of points that are in the same cluster in predicted labels but not in predicted labels; \\(C[1][0]\\) ( False Negatives ): number of pairs of points that are in the same cluster in true labels but not in predicted labels; \\(C[1][1]\\) ( True Positives ): number of pairs of points that are in the same cluster in both true and predicted labels. We can also show that the four counts have the following property \\[ TP + FP + FN + TV = \\frac{n(n-1)}{2} \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> pair_confusion_matrix = metrics . PairConfusionMatrix () >>> for yt , yp in zip ( y_true , y_pred ): ... pair_confusion_matrix = pair_confusion_matrix . update ( yt , yp ) >>> pair_confusion_matrix PairConfusionMatrix : { 0 : defaultdict ( < class ' int '>, {0: 12.0, 1: 2.0}), 1: defaultdict(<class ' int '>, {0: 4.0, 1: 2.0})} Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"PairConfusionMatrix"},{"location":"api/metrics/PairConfusionMatrix/#pairconfusionmatrix","text":"Pair Confusion Matrix. The pair confusion matrix \\(C\\) is a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings. The pair confusion matrix has the following entries: \\(C[0][0]\\) ( True Negatives ): number of pairs of points that are in different clusters in both true and predicted labels \\(C[0][1]\\) ( False Positives ): number of pairs of points that are in the same cluster in predicted labels but not in predicted labels; \\(C[1][0]\\) ( False Negatives ): number of pairs of points that are in the same cluster in true labels but not in predicted labels; \\(C[1][1]\\) ( True Positives ): number of pairs of points that are in the same cluster in both true and predicted labels. We can also show that the four counts have the following property \\[ TP + FP + FN + TV = \\frac{n(n-1)}{2} \\]","title":"PairConfusionMatrix"},{"location":"api/metrics/PairConfusionMatrix/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/PairConfusionMatrix/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/PairConfusionMatrix/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> pair_confusion_matrix = metrics . PairConfusionMatrix () >>> for yt , yp in zip ( y_true , y_pred ): ... pair_confusion_matrix = pair_confusion_matrix . update ( yt , yp ) >>> pair_confusion_matrix PairConfusionMatrix : { 0 : defaultdict ( < class ' int '>, {0: 12.0, 1: 2.0}), 1: defaultdict(<class ' int '>, {0: 4.0, 1: 2.0})}","title":"Examples"},{"location":"api/metrics/PairConfusionMatrix/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Precision/","text":"Precision \u00b6 Binary precision score. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Precision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) Precision : 1. Precision : 0.5 Precision : 0.5 Precision : 0.666667 Precision : 0.75 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Precision"},{"location":"api/metrics/Precision/#precision","text":"Binary precision score.","title":"Precision"},{"location":"api/metrics/Precision/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/Precision/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Precision/#examples","text":">>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Precision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) Precision : 1. Precision : 0.5 Precision : 0.5 Precision : 0.666667 Precision : 0.75","title":"Examples"},{"location":"api/metrics/Precision/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/PrevalenceThreshold/","text":"PrevalenceThreshold \u00b6 Prevalence Threshold (PT). The relationship between a positive predicted value and its target prevalence is propotional - though not linear in all but a special case. In consequence, there is a point of local extrema and maximum curvature defined only as a function of the sensitivity and specificity beyond which the rate of change of a test's positive predictive value drops at a differential pace relative to the disease prevalence. Using differential equations, this point was first defined by Balayla et al. 1 and is termed the prevalence threshold (\\phi_e). The equation for the prevalence threshold 2 is given by the following formula \\[ \\phi_e = \\frac{\\sqrt{TPR(1 - TNR)} + TNR - 1}{TPR + TNR - 1} \\] with \\[ TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}, TNR = = \\frac{TN}{N} = \\frac{TN}{TN + FP} \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , True ] >>> metric = metrics . PrevalenceThreshold () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 1.0 0.36602540378443876 0.44948974278317827 0.41421356237309503 >>> metric PrevalenceThreshold : 0.414214 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Balayla, J. (2020). Prevalence threshold ( \\(\\phi\\) _e) and the geometry of screening curves. PLOS ONE, 15(10), e0240215. DOI: 10.1371/journal.pone.0240215 \u21a9 Wikipedia contributors. (2021, March 19). Sensitivity and specificity. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=1013004476 \u21a9","title":"PrevalenceThreshold"},{"location":"api/metrics/PrevalenceThreshold/#prevalencethreshold","text":"Prevalence Threshold (PT). The relationship between a positive predicted value and its target prevalence is propotional - though not linear in all but a special case. In consequence, there is a point of local extrema and maximum curvature defined only as a function of the sensitivity and specificity beyond which the rate of change of a test's positive predictive value drops at a differential pace relative to the disease prevalence. Using differential equations, this point was first defined by Balayla et al. 1 and is termed the prevalence threshold (\\phi_e). The equation for the prevalence threshold 2 is given by the following formula \\[ \\phi_e = \\frac{\\sqrt{TPR(1 - TNR)} + TNR - 1}{TPR + TNR - 1} \\] with \\[ TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}, TNR = = \\frac{TN}{N} = \\frac{TN}{TN + FP} \\]","title":"PrevalenceThreshold"},{"location":"api/metrics/PrevalenceThreshold/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/PrevalenceThreshold/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/PrevalenceThreshold/#examples","text":">>> from river import metrics >>> y_true = [ False , False , False , True , True , True ] >>> y_pred = [ False , False , True , True , False , True ] >>> metric = metrics . PrevalenceThreshold () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 1.0 0.36602540378443876 0.44948974278317827 0.41421356237309503 >>> metric PrevalenceThreshold : 0.414214","title":"Examples"},{"location":"api/metrics/PrevalenceThreshold/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/PrevalenceThreshold/#references","text":"Balayla, J. (2020). Prevalence threshold ( \\(\\phi\\) _e) and the geometry of screening curves. PLOS ONE, 15(10), e0240215. DOI: 10.1371/journal.pone.0240215 \u21a9 Wikipedia contributors. (2021, March 19). Sensitivity and specificity. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=1013004476 \u21a9","title":"References"},{"location":"api/metrics/Purity/","text":"Purity \u00b6 Purity. In a similar fashion with Entropy, the purity of a clustering solution, compared to the original true label is defined to be the fraction of the overall cluster size that the largest class of documents assigned to that cluster represents. The overall purity of the clustering solution is obtained as a weighted sum of the individual cluster purities and is given by: \\[ Purity = \\sum_{r=1}^k \\frac{n_r}{n} \\times \\left( \\frac{1}{n_r} \\max_i (n^i_r) \\right) = \\sum_{r=1}^k \\frac{1}{n} \\max_i (n^i_r) \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Purity () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.6666666666666666 0.75 0.6 0.6666666666666666 >>> metric Purity : 0.666667 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Ying Zhao and George Karypis. 2001. Criterion functions for ducument clustering: Experiments and analysis. Technical Report TR 01\u201340, Department of Computer Science, University of Minnesota. \u21a9","title":"Purity"},{"location":"api/metrics/Purity/#purity","text":"Purity. In a similar fashion with Entropy, the purity of a clustering solution, compared to the original true label is defined to be the fraction of the overall cluster size that the largest class of documents assigned to that cluster represents. The overall purity of the clustering solution is obtained as a weighted sum of the individual cluster purities and is given by: \\[ Purity = \\sum_{r=1}^k \\frac{n_r}{n} \\times \\left( \\frac{1}{n_r} \\max_i (n^i_r) \\right) = \\sum_{r=1}^k \\frac{1}{n} \\max_i (n^i_r) \\]","title":"Purity"},{"location":"api/metrics/Purity/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Purity/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Purity/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Purity () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.6666666666666666 0.75 0.6 0.6666666666666666 >>> metric Purity : 0.666667","title":"Examples"},{"location":"api/metrics/Purity/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Purity/#references","text":"Ying Zhao and George Karypis. 2001. Criterion functions for ducument clustering: Experiments and analysis. Technical Report TR 01\u201340, Department of Computer Science, University of Minnesota. \u21a9","title":"References"},{"location":"api/metrics/Q0/","text":"Q0 \u00b6 Q0 index. Dom's Q0 measure 2 uses conditional entropy to calculate the goodness of a clustering solution. However, this term only evaluates the homogeneity of a solution. To measure the completeness of the hypothesized clustering, Dom includes a model cost term calculated using a coding theory argument. The overall clustering quality measure presented is the sum of the costs of representing the data's conditional entropy and the model. The motivation for this approach is an appeal to parsimony: Given identical conditional entropies, H(C|K), the clustering solution with the fewest clusters should be preferred. The Q0 measure can be calculated using the following formula 1 \\[ Q_0(C, K) = H(C|K) + \\frac{1}{n} \\sum_{k=1}^{|K|} \\log \\binom{h(c) + |C| - 1}{|C| - 1}. \\] Due to the complexity of the formula, this metric and its associated normalized version (Q2) is one order of magnitude slower than most other implemented metrics. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Q0 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.9182958340544896 1.208582260960826 1.4479588303902937 1.3803939544277863 >>> metric Q0 : 1.380394 Methods \u00b6 binomial_coeff clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf. \u21a9 Byron E. Dom. 2001. An information-theoretic external cluster-validity measure. Technical Report RJ10219, IBM, October. \u21a9","title":"Q0"},{"location":"api/metrics/Q0/#q0","text":"Q0 index. Dom's Q0 measure 2 uses conditional entropy to calculate the goodness of a clustering solution. However, this term only evaluates the homogeneity of a solution. To measure the completeness of the hypothesized clustering, Dom includes a model cost term calculated using a coding theory argument. The overall clustering quality measure presented is the sum of the costs of representing the data's conditional entropy and the model. The motivation for this approach is an appeal to parsimony: Given identical conditional entropies, H(C|K), the clustering solution with the fewest clusters should be preferred. The Q0 measure can be calculated using the following formula 1 \\[ Q_0(C, K) = H(C|K) + \\frac{1}{n} \\sum_{k=1}^{|K|} \\log \\binom{h(c) + |C| - 1}{|C| - 1}. \\] Due to the complexity of the formula, this metric and its associated normalized version (Q2) is one order of magnitude slower than most other implemented metrics.","title":"Q0"},{"location":"api/metrics/Q0/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Q0/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Q0/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Q0 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.9182958340544896 1.208582260960826 1.4479588303902937 1.3803939544277863 >>> metric Q0 : 1.380394","title":"Examples"},{"location":"api/metrics/Q0/#methods","text":"binomial_coeff clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Q0/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf. \u21a9 Byron E. Dom. 2001. An information-theoretic external cluster-validity measure. Technical Report RJ10219, IBM, October. \u21a9","title":"References"},{"location":"api/metrics/Q2/","text":"Q2 \u00b6 Q2 index. Q2 index is presented by Dom 2 as a normalized version of the original Q0 index. This index has a range of \\((0, 1]\\) 1 , with greater scores being representing more preferred clustering. The Q2 index can be calculated as follows 1 \\[ Q2(C, K) = \\frac{\\frac{1}{n} \\sum_{c=1}^{|C|} \\log \\binom{h(c) + |C| - 1}{|C| - 1} }{Q_0(C, K)} \\] where \\(C\\) is the target partition, \\(K\\) is the hypothesized partition and \\(h(k)\\) is the size of cluster \\(k\\) . Due to the complexity of the formula, this metric is one order of magnitude slower than its original version (Q0) and most other implemented metrics. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Q2 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.4545045563529578 0.39923396953448914 0.3979343306829813 >>> metric Q2 : 0.397934 Methods \u00b6 binomial_coeff clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf. \u21a9 \u21a9 Byron E. Dom. 2001. An information-theoretic external cluster-validity measure. Technical Report RJ10219, IBM, October. \u21a9","title":"Q2"},{"location":"api/metrics/Q2/#q2","text":"Q2 index. Q2 index is presented by Dom 2 as a normalized version of the original Q0 index. This index has a range of \\((0, 1]\\) 1 , with greater scores being representing more preferred clustering. The Q2 index can be calculated as follows 1 \\[ Q2(C, K) = \\frac{\\frac{1}{n} \\sum_{c=1}^{|C|} \\log \\binom{h(c) + |C| - 1}{|C| - 1} }{Q_0(C, K)} \\] where \\(C\\) is the target partition, \\(K\\) is the hypothesized partition and \\(h(k)\\) is the size of cluster \\(k\\) . Due to the complexity of the formula, this metric is one order of magnitude slower than its original version (Q0) and most other implemented metrics.","title":"Q2"},{"location":"api/metrics/Q2/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Q2/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Q2/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Q2 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.0 0.4545045563529578 0.39923396953448914 0.3979343306829813 >>> metric Q2 : 0.397934","title":"Examples"},{"location":"api/metrics/Q2/#methods","text":"binomial_coeff clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Q2/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. URL: https://www.aclweb.org/anthology/D07-1043.pdf. \u21a9 \u21a9 Byron E. Dom. 2001. An information-theoretic external cluster-validity measure. Technical Report RJ10219, IBM, October. \u21a9","title":"References"},{"location":"api/metrics/R2/","text":"R2 \u00b6 Coefficient of determination ( \\(R^2\\) ) score The coefficient of determination, denoted \\(R^2\\) or \\(r^2\\) , is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). 1 Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of \\(y\\) , disregarding the input features, would get a \\(R^2\\) score of 0.0. \\(R^2\\) is not defined when less than 2 samples have been observed. This implementation returns 0.0 in this case. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . R2 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.9183 0.9230 0.9486 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) correction \u2013 defaults to None update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Coefficient of determination (Wikipedia) \u21a9","title":"R2"},{"location":"api/metrics/R2/#r2","text":"Coefficient of determination ( \\(R^2\\) ) score The coefficient of determination, denoted \\(R^2\\) or \\(r^2\\) , is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). 1 Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of \\(y\\) , disregarding the input features, would get a \\(R^2\\) score of 0.0. \\(R^2\\) is not defined when less than 2 samples have been observed. This implementation returns 0.0 in this case.","title":"R2"},{"location":"api/metrics/R2/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/R2/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . R2 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.9183 0.9230 0.9486","title":"Examples"},{"location":"api/metrics/R2/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) correction \u2013 defaults to None update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/R2/#references","text":"Coefficient of determination (Wikipedia) \u21a9","title":"References"},{"location":"api/metrics/RMSE/","text":"RMSE \u00b6 Root mean squared error. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . RMSE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.5 0.5 0.408248 0.612372 >>> metric RMSE : 0.612372 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"RMSE"},{"location":"api/metrics/RMSE/#rmse","text":"Root mean squared error.","title":"RMSE"},{"location":"api/metrics/RMSE/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/RMSE/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . RMSE () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.5 0.5 0.408248 0.612372 >>> metric RMSE : 0.612372","title":"Examples"},{"location":"api/metrics/RMSE/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/RMSLE/","text":"RMSLE \u00b6 Root mean squared logarithmic error. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . RMSLE () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric RMSLE : 0.357826 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"RMSLE"},{"location":"api/metrics/RMSLE/#rmsle","text":"Root mean squared logarithmic error.","title":"RMSLE"},{"location":"api/metrics/RMSLE/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/RMSLE/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . RMSLE () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric RMSLE : 0.357826","title":"Examples"},{"location":"api/metrics/RMSLE/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/ROCAUC/","text":"ROCAUC \u00b6 Receiving Operating Characteristic Area Under the Curve. This metric is an approximation of the true ROC AUC. Computing the true ROC AUC would require storing all the predictions and ground truths, which isn't desirable. The approximation error is not significant as long as the predicted probabilities are well calibrated. In any case, this metric can still be used to reliably compare models between each other. Parameters \u00b6 n_thresholds \u2013 defaults to 10 The number of thresholds used for discretizing the ROC curve. A higher value will lead to more accurate results, but will also cost more time and memory. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 0 , 1 , 1 ] >>> y_pred = [ .1 , .4 , .35 , .8 ] >>> metric = metrics . ROCAUC () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ROCAUC : 0.875 The true ROC AUC is in fact 0.75. We can improve the accuracy by increasing the amount of thresholds. This comes at the cost more computation time and more memory usage. >>> metric = metrics . ROCAUC ( n_thresholds = 20 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ROCAUC : 0.75 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"ROCAUC"},{"location":"api/metrics/ROCAUC/#rocauc","text":"Receiving Operating Characteristic Area Under the Curve. This metric is an approximation of the true ROC AUC. Computing the true ROC AUC would require storing all the predictions and ground truths, which isn't desirable. The approximation error is not significant as long as the predicted probabilities are well calibrated. In any case, this metric can still be used to reliably compare models between each other.","title":"ROCAUC"},{"location":"api/metrics/ROCAUC/#parameters","text":"n_thresholds \u2013 defaults to 10 The number of thresholds used for discretizing the ROC curve. A higher value will lead to more accurate results, but will also cost more time and memory. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/ROCAUC/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/ROCAUC/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 0 , 1 , 1 ] >>> y_pred = [ .1 , .4 , .35 , .8 ] >>> metric = metrics . ROCAUC () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ROCAUC : 0.875 The true ROC AUC is in fact 0.75. We can improve the accuracy by increasing the amount of thresholds. This comes at the cost more computation time and more memory usage. >>> metric = metrics . ROCAUC ( n_thresholds = 20 ) >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric ROCAUC : 0.75","title":"Examples"},{"location":"api/metrics/ROCAUC/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Rand/","text":"Rand \u00b6 Rand Index. The Rand Index 1 2 is a measure of the similarity between two data clusterings. Given a set of elements S and two partitions of S to compare, X and Y , define the following: a, the number of pairs of elements in S that are in the same subset in X and in the same subset in Y b, the number of pairs of elements in S that are in the different subset in X and in different subsets in Y c, the number of pairs of elements in S that are in the same subset in X and in different subsets in Y d, the number of pairs of elements in S that are in the different subset in X and in the same subset in Y The Rand index, R, is \\[ R = rac{a+b}{a+b+c+d} = rac{a+b}{ rac{n(n-1)}{2}}. \\] Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Rand () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.3333333333333333 0.5 0.6 0.6666666666666666 >>> metric Rand : 0.666667 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2021, January 13). Rand index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Rand_index&oldid=1000098911 \u21a9 W. M. Rand (1971). \"Objective criteria for the evaluation of clustering methods\". Journal of the American Statistical Association. American Statistical Association. 66 (336): 846\u2013850. arXiv:1704.01036. doi:10.2307/2284239. JSTOR 2284239. \u21a9","title":"Rand"},{"location":"api/metrics/Rand/#rand","text":"Rand Index. The Rand Index 1 2 is a measure of the similarity between two data clusterings. Given a set of elements S and two partitions of S to compare, X and Y , define the following: a, the number of pairs of elements in S that are in the same subset in X and in the same subset in Y b, the number of pairs of elements in S that are in the different subset in X and in different subsets in Y c, the number of pairs of elements in S that are in the same subset in X and in different subsets in Y d, the number of pairs of elements in S that are in the different subset in X and in the same subset in Y The Rand index, R, is \\[ R = rac{a+b}{a+b+c+d} = rac{a+b}{ rac{n(n-1)}{2}}. \\]","title":"Rand"},{"location":"api/metrics/Rand/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/Rand/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Rand/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . Rand () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.3333333333333333 0.5 0.6 0.6666666666666666 >>> metric Rand : 0.666667","title":"Examples"},{"location":"api/metrics/Rand/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Rand/#references","text":"Wikipedia contributors. (2021, January 13). Rand index. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Rand_index&oldid=1000098911 \u21a9 W. M. Rand (1971). \"Objective criteria for the evaluation of clustering methods\". Journal of the American Statistical Association. American Statistical Association. 66 (336): 846\u2013850. arXiv:1704.01036. doi:10.2307/2284239. JSTOR 2284239. \u21a9","title":"References"},{"location":"api/metrics/Recall/","text":"Recall \u00b6 Binary recall score. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\". Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Recall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) Recall : 1. Recall : 1. Recall : 0.5 Recall : 0.666667 Recall : 0.75 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Recall"},{"location":"api/metrics/Recall/#recall","text":"Binary recall score.","title":"Recall"},{"location":"api/metrics/Recall/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. pos_val \u2013 defaults to True Value to treat as \"positive\".","title":"Parameters"},{"location":"api/metrics/Recall/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Recall/#examples","text":">>> from river import metrics >>> y_true = [ True , False , True , True , True ] >>> y_pred = [ True , True , False , True , True ] >>> metric = metrics . Recall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) Recall : 1. Recall : 1. Recall : 0.5 Recall : 0.666667 Recall : 0.75","title":"Examples"},{"location":"api/metrics/Recall/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( bool ) y_pred ( Union[bool, float, Dict[bool, float]] ) sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/RegressionMetric/","text":"RegressionMetric \u00b6 Mother class for all regression metrics. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"RegressionMetric"},{"location":"api/metrics/RegressionMetric/#regressionmetric","text":"Mother class for all regression metrics.","title":"RegressionMetric"},{"location":"api/metrics/RegressionMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/RegressionMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/RegressionMultiOutput/","text":"RegressionMultiOutput \u00b6 Wrapper for multi-output regression. This wraps a regression metric to make it compatible with multi-output regression tasks. The value of each output will be fed sequentially to the get method of the provided metric. Parameters \u00b6 metric ( 'base.RegressionMetric' ) The regression metric to evaluate with each output. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"RegressionMultiOutput"},{"location":"api/metrics/RegressionMultiOutput/#regressionmultioutput","text":"Wrapper for multi-output regression. This wraps a regression metric to make it compatible with multi-output regression tasks. The value of each output will be fed sequentially to the get method of the provided metric.","title":"RegressionMultiOutput"},{"location":"api/metrics/RegressionMultiOutput/#parameters","text":"metric ( 'base.RegressionMetric' ) The regression metric to evaluate with each output.","title":"Parameters"},{"location":"api/metrics/RegressionMultiOutput/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/RegressionMultiOutput/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[float, int]] ) y_pred ( Dict[Union[str, int], Union[float, int]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/Rolling/","text":"Rolling \u00b6 Wrapper for computing metrics over a window. This wrapper metric allows you to apply a metric over a window of observations. Under the hood, a buffer with the window_size most recent pairs of (y_true, y_pred) is memorised. When the buffer is full, the oldest pair is removed and the revert method of the metric is called with said pair. You should use metrics.Rolling to evaluate a metric over a window of fixed sized. You can use metrics.TimeRolling to instead evaluate a metric over a period of time. Parameters \u00b6 metric ( river.metrics.base.Metric ) A metric. window_size ( int ) The number of most recent (y_true, y_pred) pairs on which to evaluate the metric. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels size works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . Rolling ( metrics . MSE (), window_size = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MSE : 0.25 ( rolling 2 ) MSE : 0.25 ( rolling 2 ) MSE : 0.125 ( rolling 2 ) MSE : 0.5 ( rolling 2 ) Methods \u00b6 append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the metric. popleft revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Rolling"},{"location":"api/metrics/Rolling/#rolling","text":"Wrapper for computing metrics over a window. This wrapper metric allows you to apply a metric over a window of observations. Under the hood, a buffer with the window_size most recent pairs of (y_true, y_pred) is memorised. When the buffer is full, the oldest pair is removed and the revert method of the metric is called with said pair. You should use metrics.Rolling to evaluate a metric over a window of fixed sized. You can use metrics.TimeRolling to instead evaluate a metric over a period of time.","title":"Rolling"},{"location":"api/metrics/Rolling/#parameters","text":"metric ( river.metrics.base.Metric ) A metric. window_size ( int ) The number of most recent (y_true, y_pred) pairs on which to evaluate the metric.","title":"Parameters"},{"location":"api/metrics/Rolling/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels size works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/Rolling/#examples","text":">>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 8 ] >>> metric = metrics . Rolling ( metrics . MSE (), window_size = 2 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) MSE : 0.25 ( rolling 2 ) MSE : 0.25 ( rolling 2 ) MSE : 0.125 ( rolling 2 ) MSE : 0.5 ( rolling 2 )","title":"Examples"},{"location":"api/metrics/Rolling/#methods","text":"append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the metric. popleft revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/SMAPE/","text":"SMAPE \u00b6 Symmetric mean absolute percentage error. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.0672 , 0.0672 ] >>> y_pred = [ 0 , 0.102 , 0.107 , 0.047 , 0.1 , 0.032 , 0.047 , 0.108 , 0.089 ] >>> metric = metrics . SMAPE () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric SMAPE : 37.869392 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"SMAPE"},{"location":"api/metrics/SMAPE/#smape","text":"Symmetric mean absolute percentage error.","title":"SMAPE"},{"location":"api/metrics/SMAPE/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/SMAPE/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.07533 , 0.0672 , 0.0672 ] >>> y_pred = [ 0 , 0.102 , 0.107 , 0.047 , 0.1 , 0.032 , 0.047 , 0.108 , 0.089 ] >>> metric = metrics . SMAPE () >>> for yt , yp in zip ( y_true , y_pred ): ... metric = metric . update ( yt , yp ) >>> metric SMAPE : 37.869392","title":"Examples"},{"location":"api/metrics/SMAPE/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 update Update the metric. Parameters y_true ( numbers.Number ) y_pred ( numbers.Number ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model","title":"Methods"},{"location":"api/metrics/SorensenDice/","text":"SorensenDice \u00b6 S\u00f8rensen-Dice coefficient. S\u00f8rensen-Dice coefficient 1 (or S\u00f8rensen Index, Dice's coefficient) is a statistic used to gauge the similarity of two samples. S\u00f8rensen's original formula was intended to be applied to discrete data. Given two sets, \\(X\\) and \\(Y\\) , it is defined as: \\[ DSC = \\frac{2 |X \\cap Y|}{|X| + |Y|}. \\] It is equal to twice the number of elements common to both sets divided by the sum of the number of elements in each set. The coefficient is not very different in form from the Jaccard index. The only difference between the two metrics is that the Jaccard index only counts true positives once in both the numerator and denominator. In fact, both are equivalent in the sense that given a value for the Sorensen-Dice index, once can canculate the respective Jaccard value and vice versa, using the equations \\[ \\begin{equation} J = \\frac{S}{2-S}, \\\\ S = \\frac{2J}{1+J}. \\end{equation} \\] Parameters \u00b6 cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... ] >>> sorensen_dice = metrics . SorensenDice () >>> for yt , yp in zip ( y_true , y_pred ): ... sorensen_dice = sorensen_dice . update ( yt , yp ) >>> sorensen_dice SorensenDice : 0.736842 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia article on S\u00f8rensen-Dice coefficient \u21a9","title":"SorensenDice"},{"location":"api/metrics/SorensenDice/#sorensendice","text":"S\u00f8rensen-Dice coefficient. S\u00f8rensen-Dice coefficient 1 (or S\u00f8rensen Index, Dice's coefficient) is a statistic used to gauge the similarity of two samples. S\u00f8rensen's original formula was intended to be applied to discrete data. Given two sets, \\(X\\) and \\(Y\\) , it is defined as: \\[ DSC = \\frac{2 |X \\cap Y|}{|X| + |Y|}. \\] It is equal to twice the number of elements common to both sets divided by the sum of the number of elements in each set. The coefficient is not very different in form from the Jaccard index. The only difference between the two metrics is that the Jaccard index only counts true positives once in both the numerator and denominator. In fact, both are equivalent in the sense that given a value for the Sorensen-Dice index, once can canculate the respective Jaccard value and vice versa, using the equations \\[ \\begin{equation} J = \\frac{S}{2-S}, \\\\ S = \\frac{2J}{1+J}. \\end{equation} \\]","title":"SorensenDice"},{"location":"api/metrics/SorensenDice/#parameters","text":"cm ( river.metrics.confusion.MultiLabelConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/SorensenDice/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/SorensenDice/#examples","text":">>> from river import metrics >>> y_true = [ ... { 0 : False , 1 : True , 2 : True }, ... { 0 : True , 1 : True , 2 : False }, ... ] >>> y_pred = [ ... { 0 : True , 1 : True , 2 : True }, ... { 0 : True , 1 : False , 2 : False }, ... ] >>> sorensen_dice = metrics . SorensenDice () >>> for yt , yp in zip ( y_true , y_pred ): ... sorensen_dice = sorensen_dice . update ( yt , yp ) >>> sorensen_dice SorensenDice : 0.736842","title":"Examples"},{"location":"api/metrics/SorensenDice/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true ( Dict[Union[str, int], Union[bool, str, int]] ) y_pred ( Union[Dict[Union[str, int], Union[bool, str, int]], Dict[Union[str, int], Dict[Union[bool, str, int], float]]] ) sample_weight ( numbers.Number ) \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/SorensenDice/#references","text":"Wikipedia article on S\u00f8rensen-Dice coefficient \u21a9","title":"References"},{"location":"api/metrics/TimeRolling/","text":"TimeRolling \u00b6 Wrapper for computing metrics over a period of time. Parameters \u00b6 metric ( river.metrics.base.Metric ) A metric. period ( datetime.timedelta ) A period of time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> import datetime as dt >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 9 ] >>> days = [ 1 , 2 , 3 , 4 ] >>> metric = metrics . TimeRolling ( metrics . MAE (), period = dt . timedelta ( days = 2 )) >>> for yt , yp , day in zip ( y_true , y_pred , days ): ... t = dt . datetime ( 2019 , 1 , day ) ... print ( metric . update ( yt , yp , t )) MAE : 0.5 ( rolling 2 days , 0 : 00 : 00 ) MAE : 0.5 ( rolling 2 days , 0 : 00 : 00 ) MAE : 0.25 ( rolling 2 days , 0 : 00 : 00 ) MAE : 1. ( rolling 2 days , 0 : 00 : 00 ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred update Update the metric. Parameters y_true y_pred t works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"TimeRolling"},{"location":"api/metrics/TimeRolling/#timerolling","text":"Wrapper for computing metrics over a period of time.","title":"TimeRolling"},{"location":"api/metrics/TimeRolling/#parameters","text":"metric ( river.metrics.base.Metric ) A metric. period ( datetime.timedelta ) A period of time.","title":"Parameters"},{"location":"api/metrics/TimeRolling/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/TimeRolling/#examples","text":">>> import datetime as dt >>> from river import metrics >>> y_true = [ 3 , - 0.5 , 2 , 7 ] >>> y_pred = [ 2.5 , 0.0 , 2 , 9 ] >>> days = [ 1 , 2 , 3 , 4 ] >>> metric = metrics . TimeRolling ( metrics . MAE (), period = dt . timedelta ( days = 2 )) >>> for yt , yp , day in zip ( y_true , y_pred , days ): ... t = dt . datetime ( 2019 , 1 , day ) ... print ( metric . update ( yt , yp , t )) MAE : 0.5 ( rolling 2 days , 0 : 00 : 00 ) MAE : 0.5 ( rolling 2 days , 0 : 00 : 00 ) MAE : 0.25 ( rolling 2 days , 0 : 00 : 00 ) MAE : 1. ( rolling 2 days , 0 : 00 : 00 )","title":"Examples"},{"location":"api/metrics/TimeRolling/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred update Update the metric. Parameters y_true y_pred t works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/VBeta/","text":"VBeta \u00b6 VBeta. VBeta (or V-Measure) 1 is an external entropy-based cluster evaluation measure. It provides an elegant solution to many problems that affect previously defined cluster evaluation measures including Dependance of clustering algorithm or dataset, The \"problem of matching\", where the clustering of only a portion of data points are evaluated, and Accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. Based upon the calculations of homogeneity and completeness, a clustering solution's V-measure is calculated by computing the weighted harmonic mean of homogeneity and completeness, \\[ V_{\\beta} = \\frac{(1 + \\beta) \\times h \\times c}{\\beta \\times h + c}. \\] Parameters \u00b6 beta ( float ) \u2013 defaults to 1.0 Weight of Homogeneity in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . VBeta ( beta = 1.0 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.3437110184854507 0.4580652856440158 0.5158037429793888 >>> metric VBeta : 0.515804 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"VBeta"},{"location":"api/metrics/VBeta/#vbeta","text":"VBeta. VBeta (or V-Measure) 1 is an external entropy-based cluster evaluation measure. It provides an elegant solution to many problems that affect previously defined cluster evaluation measures including Dependance of clustering algorithm or dataset, The \"problem of matching\", where the clustering of only a portion of data points are evaluated, and Accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. Based upon the calculations of homogeneity and completeness, a clustering solution's V-measure is calculated by computing the weighted harmonic mean of homogeneity and completeness, \\[ V_{\\beta} = \\frac{(1 + \\beta) \\times h \\times c}{\\beta \\times h + c}. \\]","title":"VBeta"},{"location":"api/metrics/VBeta/#parameters","text":"beta ( float ) \u2013 defaults to 1.0 Weight of Homogeneity in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/VBeta/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/VBeta/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . VBeta ( beta = 1.0 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 1.0 1.0 0.0 0.3437110184854507 0.4580652856440158 0.5158037429793888 >>> metric VBeta : 0.515804","title":"Examples"},{"location":"api/metrics/VBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/VBeta/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9","title":"References"},{"location":"api/metrics/VariationInfo/","text":"VariationInfo \u00b6 Variation of Information. Variation of Information (VI) 1 2 is an information-based clustering measure. It is presented as a distance measure for comparing partitions (or clusterings) of the same data. It therefore does not distinguish between hypothesised and target clustering. VI has a number of useful properties, as follows VI satisifes the metric axioms VI is convexly additive. This means that, if a cluster is split, the distance from the new cluster to the original is the distance induced by the split times the size of the cluster. This guarantees that all changes to the metrics are \"local\". VI is not affected by the number of data points in the cluster. However, it is bounded by the logarithm of the maximum number of clusters in true and predicted labels. The Variation of Information is calculated using the following formula \\[ VI(C, K) = H(C) + H(K) - 2 H(C, K) = H(C|K) + H(K|C) \\] The bound of the variation of information 3 can be written in terms of the number of elements, \\(VI(C, K) \\leq \\log(n)\\) , or with respect to the maximum number of clusters \\(K^*\\) , \\(VI(C, K) \\leq 2 \\log(K^*)\\) . Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . VariationInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.9182958340544896 1.1887218755408673 1.3509775004326938 1.2516291673878228 >>> metric VariationInfo : 1.251629 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9 Marina Meila and David Heckerman. 2001. An experimental comparison of model-based clustering methods. Mach. Learn., 42(1/2):9\u201329. \u21a9 Wikipedia contributors. (2021, February 18). Variation of information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Variation_of_information&oldid=1007562715 \u21a9","title":"VariationInfo"},{"location":"api/metrics/VariationInfo/#variationinfo","text":"Variation of Information. Variation of Information (VI) 1 2 is an information-based clustering measure. It is presented as a distance measure for comparing partitions (or clusterings) of the same data. It therefore does not distinguish between hypothesised and target clustering. VI has a number of useful properties, as follows VI satisifes the metric axioms VI is convexly additive. This means that, if a cluster is split, the distance from the new cluster to the original is the distance induced by the split times the size of the cluster. This guarantees that all changes to the metrics are \"local\". VI is not affected by the number of data points in the cluster. However, it is bounded by the logarithm of the maximum number of clusters in true and predicted labels. The Variation of Information is calculated using the following formula \\[ VI(C, K) = H(C) + H(K) - 2 H(C, K) = H(C|K) + H(K|C) \\] The bound of the variation of information 3 can be written in terms of the number of elements, \\(VI(C, K) \\leq \\log(n)\\) , or with respect to the maximum number of clusters \\(K^*\\) , \\(VI(C, K) \\leq 2 \\log(K^*)\\) .","title":"VariationInfo"},{"location":"api/metrics/VariationInfo/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/VariationInfo/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/VariationInfo/#examples","text":">>> from river import metrics >>> y_true = [ 1 , 1 , 2 , 2 , 3 , 3 ] >>> y_pred = [ 1 , 1 , 1 , 2 , 2 , 2 ] >>> metric = metrics . VariationInfo () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp ) . get ()) 0.0 0.0 0.9182958340544896 1.1887218755408673 1.3509775004326938 1.2516291673878228 >>> metric VariationInfo : 1.251629","title":"Examples"},{"location":"api/metrics/VariationInfo/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/VariationInfo/#references","text":"Andrew Rosenberg and Julia Hirschberg (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Proceedings of the 2007 Joing Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 410 - 420, Prague, June 2007. \u21a9 Marina Meila and David Heckerman. 2001. An experimental comparison of model-based clustering methods. Mach. Learn., 42(1/2):9\u201329. \u21a9 Wikipedia contributors. (2021, February 18). Variation of information. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Variation_of_information&oldid=1007562715 \u21a9","title":"References"},{"location":"api/metrics/WeightedF1/","text":"WeightedF1 \u00b6 Weighted-average F1 score. This works by computing the F1 score per class, and then performs a global weighted average by using the support of each class. Parameters \u00b6 cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedF1 : 1. WeightedF1 : 0.333333 WeightedF1 : 0.555556 WeightedF1 : 0.666667 WeightedF1 : 0.613333 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"WeightedF1"},{"location":"api/metrics/WeightedF1/#weightedf1","text":"Weighted-average F1 score. This works by computing the F1 score per class, and then performs a global weighted average by using the support of each class.","title":"WeightedF1"},{"location":"api/metrics/WeightedF1/#parameters","text":"cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/WeightedF1/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/WeightedF1/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedF1 () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedF1 : 1. WeightedF1 : 0.333333 WeightedF1 : 0.555556 WeightedF1 : 0.666667 WeightedF1 : 0.613333","title":"Examples"},{"location":"api/metrics/WeightedF1/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/WeightedFBeta/","text":"WeightedFBeta \u00b6 Weighted-average F-Beta score. This works by computing the F-Beta score per class, and then performs a global weighted average according to the support of each class. Parameters \u00b6 beta Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedFBeta ( beta = 0.8 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedFBeta : 1. WeightedFBeta : 0.310606 WeightedFBeta : 0.540404 WeightedFBeta : 0.655303 WeightedFBeta : 0.626283 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"WeightedFBeta"},{"location":"api/metrics/WeightedFBeta/#weightedfbeta","text":"Weighted-average F-Beta score. This works by computing the F-Beta score per class, and then performs a global weighted average according to the support of each class.","title":"WeightedFBeta"},{"location":"api/metrics/WeightedFBeta/#parameters","text":"beta Weight of precision in the harmonic mean. cm \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/WeightedFBeta/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/WeightedFBeta/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedFBeta ( beta = 0.8 ) >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedFBeta : 1. WeightedFBeta : 0.310606 WeightedFBeta : 0.540404 WeightedFBeta : 0.655303 WeightedFBeta : 0.626283","title":"Examples"},{"location":"api/metrics/WeightedFBeta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/WeightedPrecision/","text":"WeightedPrecision \u00b6 Weighted-average precision score. This uses the support of each label to compute an average score, whereas metrics.MacroPrecision ignores the support. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedPrecision : 1. WeightedPrecision : 0.25 WeightedPrecision : 0.5 WeightedPrecision : 0.625 WeightedPrecision : 0.7 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"WeightedPrecision"},{"location":"api/metrics/WeightedPrecision/#weightedprecision","text":"Weighted-average precision score. This uses the support of each label to compute an average score, whereas metrics.MacroPrecision ignores the support.","title":"WeightedPrecision"},{"location":"api/metrics/WeightedPrecision/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/WeightedPrecision/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/WeightedPrecision/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedPrecision () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedPrecision : 1. WeightedPrecision : 0.25 WeightedPrecision : 0.5 WeightedPrecision : 0.625 WeightedPrecision : 0.7","title":"Examples"},{"location":"api/metrics/WeightedPrecision/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/WeightedRecall/","text":"WeightedRecall \u00b6 Weighted-average recall score. This uses the support of each label to compute an average score, whereas MacroRecall ignores the support. Parameters \u00b6 cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights Examples \u00b6 >>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedRecall : 1. WeightedRecall : 0.5 WeightedRecall : 0.666667 WeightedRecall : 0.75 WeightedRecall : 0.6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"WeightedRecall"},{"location":"api/metrics/WeightedRecall/#weightedrecall","text":"Weighted-average recall score. This uses the support of each label to compute an average score, whereas MacroRecall ignores the support.","title":"WeightedRecall"},{"location":"api/metrics/WeightedRecall/#parameters","text":"cm ( river.metrics.confusion.ConfusionMatrix ) \u2013 defaults to None This parameter allows sharing the same confusion matrix between multiple metrics. Sharing a confusion matrix reduces the amount of storage and computation time.","title":"Parameters"},{"location":"api/metrics/WeightedRecall/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. requires_labels Indicates if labels are required, rather than probabilities. sample_correction works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/WeightedRecall/#examples","text":">>> from river import metrics >>> y_true = [ 0 , 1 , 2 , 2 , 2 ] >>> y_pred = [ 0 , 0 , 2 , 2 , 1 ] >>> metric = metrics . WeightedRecall () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( metric . update ( yt , yp )) WeightedRecall : 1. WeightedRecall : 0.5 WeightedRecall : 0.666667 WeightedRecall : 0.75 WeightedRecall : 0.6","title":"Examples"},{"location":"api/metrics/WeightedRecall/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters y_true y_pred sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/WrapperMetric/","text":"WrapperMetric \u00b6 Mother class for all metrics. Attributes \u00b6 bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight update Update the metric. Parameters y_true y_pred sample_weight works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"WrapperMetric"},{"location":"api/metrics/WrapperMetric/#wrappermetric","text":"Mother class for all metrics.","title":"WrapperMetric"},{"location":"api/metrics/WrapperMetric/#attributes","text":"bigger_is_better Indicate if a high value is better than a low one or not. metric Gives access to the wrapped metric. requires_labels works_with_weights Indicate whether the model takes into consideration the effect of sample weights","title":"Attributes"},{"location":"api/metrics/WrapperMetric/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the metric. revert Revert the metric. Parameters y_true y_pred sample_weight update Update the metric. Parameters y_true y_pred sample_weight works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/BIC/","text":"BIC \u00b6 Bayesian Information Criterion (BIC). In statistics, the Bayesian Information Criterion (BIC) 1 , or Schwarz Information Criterion (SIC), is a criterion for model selection among a finite set of models; the model with the highest BIC is preferred. It is based, in part, on the likelihood function and is closely related to the Akaike Information Criterion (AIC). Let k being the number of clusters, \\(n_i\\) being the number of points within each cluster, \\(n_1 + n_2 + ... + n_k = n\\) , \\(d\\) being the dimension of the clustering problem. Then, the variance of the clustering solution will be calculated as \\[ \\hat{\\sigma}^2 = \\frac{1}{(n - m) \\times d} \\sum_{i = 1}^n \\lVert x_i - c_j \\rVert^2. \\] The maximum likelihood function, used in the BIC version of River , would be \\[ \\hat{l}(D) = \\sum_{i = 1}^k n_i \\log(n_i) - n \\log n - \\frac{n_i \\times d}{2} \\times \\log(2 \\pi \\hat{\\sigma}^2) - \\frac{(n_i - 1) \\times d}{2}, \\] and the BIC will then be calculated as \\[ BIC = \\hat{l}(D) - 0.5 \\times k \\times log(n) \\times (d+1). \\] Using the previously mentioned maximum likelihood function, the higher the BIC value, the better the clustering solution is. Moreover, the BIC calculated will always be less than 0 2 . Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . BIC () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric BIC : - 30.060416 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Wikipedia contributors. (2020, December 14). Bayesian information criterion. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Bayesian_information_criterion&oldid=994127616 \u21a9 BIC Notes, https://github.com/bobhancock/goxmeans/blob/master/doc/BIC_notes.pdf \u21a9","title":"BIC"},{"location":"api/metrics/cluster/BIC/#bic","text":"Bayesian Information Criterion (BIC). In statistics, the Bayesian Information Criterion (BIC) 1 , or Schwarz Information Criterion (SIC), is a criterion for model selection among a finite set of models; the model with the highest BIC is preferred. It is based, in part, on the likelihood function and is closely related to the Akaike Information Criterion (AIC). Let k being the number of clusters, \\(n_i\\) being the number of points within each cluster, \\(n_1 + n_2 + ... + n_k = n\\) , \\(d\\) being the dimension of the clustering problem. Then, the variance of the clustering solution will be calculated as \\[ \\hat{\\sigma}^2 = \\frac{1}{(n - m) \\times d} \\sum_{i = 1}^n \\lVert x_i - c_j \\rVert^2. \\] The maximum likelihood function, used in the BIC version of River , would be \\[ \\hat{l}(D) = \\sum_{i = 1}^k n_i \\log(n_i) - n \\log n - \\frac{n_i \\times d}{2} \\times \\log(2 \\pi \\hat{\\sigma}^2) - \\frac{(n_i - 1) \\times d}{2}, \\] and the BIC will then be calculated as \\[ BIC = \\hat{l}(D) - 0.5 \\times k \\times log(n) \\times (d+1). \\] Using the previously mentioned maximum likelihood function, the higher the BIC value, the better the clustering solution is. Moreover, the BIC calculated will always be less than 0 2 .","title":"BIC"},{"location":"api/metrics/cluster/BIC/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/BIC/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . BIC () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric BIC : - 30.060416","title":"Examples"},{"location":"api/metrics/cluster/BIC/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/BIC/#references","text":"Wikipedia contributors. (2020, December 14). Bayesian information criterion. In Wikipedia, The Free Encyclopedia, from https://en.wikipedia.org/w/index.php?title=Bayesian_information_criterion&oldid=994127616 \u21a9 BIC Notes, https://github.com/bobhancock/goxmeans/blob/master/doc/BIC_notes.pdf \u21a9","title":"References"},{"location":"api/metrics/cluster/BallHall/","text":"BallHall \u00b6 Ball-Hall index Ball-Hall index is a sum-of-squared based index. It is calculated by dividing the sum-of-squares between clusters by the number of generated clusters. The index is usually used to evaluate the number of clusters by the following criteria: the maximum value of the successive difference is determined as the optimal number of clusters. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . BallHall () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric BallHall : 1.171426 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Ball, G.H., Hubert, L.J.: ISODATA, A novel method of data analysis and pattern classification (Tech. Rep. NTIS No. AD 699616). Standford Research Institute, Menlo Park (1965) \u21a9","title":"BallHall"},{"location":"api/metrics/cluster/BallHall/#ballhall","text":"Ball-Hall index Ball-Hall index is a sum-of-squared based index. It is calculated by dividing the sum-of-squares between clusters by the number of generated clusters. The index is usually used to evaluate the number of clusters by the following criteria: the maximum value of the successive difference is determined as the optimal number of clusters.","title":"BallHall"},{"location":"api/metrics/cluster/BallHall/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/BallHall/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . BallHall () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric BallHall : 1.171426","title":"Examples"},{"location":"api/metrics/cluster/BallHall/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/BallHall/#references","text":"Ball, G.H., Hubert, L.J.: ISODATA, A novel method of data analysis and pattern classification (Tech. Rep. NTIS No. AD 699616). Standford Research Institute, Menlo Park (1965) \u21a9","title":"References"},{"location":"api/metrics/cluster/CalinskiHarabasz/","text":"CalinskiHarabasz \u00b6 Calinski-Harabasz index (CH). The Calinski-Harabasz index (CH) index measures the criteria simultaneously with the help of average between and within cluster sum of squares. The numerator reflects the degree of separation in the way of how much centers are spread. The denominator corresponds to compactness, to reflect how close the in-cluster objects are gathered around the cluster center. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . CalinskiHarabasz () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric CalinskiHarabasz : 6.922666 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Calinski, T., Harabasz, J.-A. (1974). A Dendrite Method for Cluster Analysis. Communications in Statistics 3(1), 1 - 27. DOI: 10.1080/03610927408827101 \u21a9","title":"CalinskiHarabasz"},{"location":"api/metrics/cluster/CalinskiHarabasz/#calinskiharabasz","text":"Calinski-Harabasz index (CH). The Calinski-Harabasz index (CH) index measures the criteria simultaneously with the help of average between and within cluster sum of squares. The numerator reflects the degree of separation in the way of how much centers are spread. The denominator corresponds to compactness, to reflect how close the in-cluster objects are gathered around the cluster center.","title":"CalinskiHarabasz"},{"location":"api/metrics/cluster/CalinskiHarabasz/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/CalinskiHarabasz/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . CalinskiHarabasz () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric CalinskiHarabasz : 6.922666","title":"Examples"},{"location":"api/metrics/cluster/CalinskiHarabasz/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/CalinskiHarabasz/#references","text":"Calinski, T., Harabasz, J.-A. (1974). A Dendrite Method for Cluster Analysis. Communications in Statistics 3(1), 1 - 27. DOI: 10.1080/03610927408827101 \u21a9","title":"References"},{"location":"api/metrics/cluster/Cohesion/","text":"Cohesion \u00b6 Mean distance from the points to their assigned cluster centroids. The smaller the better. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Cohesion () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Cohesion : 1.682748 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"Cohesion"},{"location":"api/metrics/cluster/Cohesion/#cohesion","text":"Mean distance from the points to their assigned cluster centroids. The smaller the better.","title":"Cohesion"},{"location":"api/metrics/cluster/Cohesion/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/Cohesion/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Cohesion () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Cohesion : 1.682748","title":"Examples"},{"location":"api/metrics/cluster/Cohesion/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/Cohesion/#references","text":"Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"References"},{"location":"api/metrics/cluster/DaviesBouldin/","text":"DaviesBouldin \u00b6 Davies-Bouldin index (DB). The Davies-Bouldin index (DB) 1 is an old but still widely used inernal validaion measure. DB uses intra-cluster variance and inter-cluster center distance to find the worst partner cluster, i.e., the closest most scattered one for each cluster. Thus, minimizing DB gives us the optimal number of clusters. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . DaviesBouldin () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric DaviesBouldin : 0.22583 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 David L., D., Don, B. (1979). A Cluster Separation Measure. In: IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 1(2), 224 - 227. DOI: 10.1109/TPAMI.1979.4766909 \u21a9","title":"DaviesBouldin"},{"location":"api/metrics/cluster/DaviesBouldin/#daviesbouldin","text":"Davies-Bouldin index (DB). The Davies-Bouldin index (DB) 1 is an old but still widely used inernal validaion measure. DB uses intra-cluster variance and inter-cluster center distance to find the worst partner cluster, i.e., the closest most scattered one for each cluster. Thus, minimizing DB gives us the optimal number of clusters.","title":"DaviesBouldin"},{"location":"api/metrics/cluster/DaviesBouldin/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/DaviesBouldin/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . DaviesBouldin () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric DaviesBouldin : 0.22583","title":"Examples"},{"location":"api/metrics/cluster/DaviesBouldin/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/DaviesBouldin/#references","text":"David L., D., Don, B. (1979). A Cluster Separation Measure. In: IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 1(2), 224 - 227. DOI: 10.1109/TPAMI.1979.4766909 \u21a9","title":"References"},{"location":"api/metrics/cluster/GD43/","text":"GD43 \u00b6 Generalized Dunn's index 43 (GD43). The Generalized Dunn's indices comprise a set of 17 variants of the original Dunn's index devised to address sensitivity to noise in the latter. The formula of this index is given by: \\[ GD_{rs} = \\frac{\\min_{i \\new q} [\\delta_r (\\omega_i, \\omega_j)]}{\\max_k [\\Delta_s (\\omega_k)]}, \\] where \\(\\delta_r(.)\\) is a measure of separation, and \\(\\Delta_s(.)\\) is a measure of compactness, the parameters \\(r\\) and \\(s\\) index the measures' formulations. In particular, when employing Euclidean distance, GD43 is formulated using: \\[ \\delta_4 (\\omega_i, \\omega_j) = \\lVert v_i - v_j \\rVert_2, \\] and \\[ \\Delta_3 (\\omega_k) = \\frac{2 \\times CP_1^2 (v_k, \\omega_k)}{n_k}. \\] Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . GD43 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric GD43 : 0.731369 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 J. Bezdek and N. Pal, \"Some new indexes of cluster validity,\" IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301\u2013315, Jun. 1998. \u21a9","title":"GD43"},{"location":"api/metrics/cluster/GD43/#gd43","text":"Generalized Dunn's index 43 (GD43). The Generalized Dunn's indices comprise a set of 17 variants of the original Dunn's index devised to address sensitivity to noise in the latter. The formula of this index is given by: \\[ GD_{rs} = \\frac{\\min_{i \\new q} [\\delta_r (\\omega_i, \\omega_j)]}{\\max_k [\\Delta_s (\\omega_k)]}, \\] where \\(\\delta_r(.)\\) is a measure of separation, and \\(\\Delta_s(.)\\) is a measure of compactness, the parameters \\(r\\) and \\(s\\) index the measures' formulations. In particular, when employing Euclidean distance, GD43 is formulated using: \\[ \\delta_4 (\\omega_i, \\omega_j) = \\lVert v_i - v_j \\rVert_2, \\] and \\[ \\Delta_3 (\\omega_k) = \\frac{2 \\times CP_1^2 (v_k, \\omega_k)}{n_k}. \\]","title":"GD43"},{"location":"api/metrics/cluster/GD43/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/GD43/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . GD43 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric GD43 : 0.731369","title":"Examples"},{"location":"api/metrics/cluster/GD43/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/GD43/#references","text":"J. Bezdek and N. Pal, \"Some new indexes of cluster validity,\" IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301\u2013315, Jun. 1998. \u21a9","title":"References"},{"location":"api/metrics/cluster/GD53/","text":"GD53 \u00b6 Generalized Dunn's index 53 (GD53). The Generalized Dunn's indices comprise a set of 17 variants of the original Dunn's index devised to address sensitivity to noise in the latter. The formula of this index is given by: \\[ GD_{rs} = \\frac{\\min_{i \\new q} [\\delta_r (\\omega_i, \\omega_j)]}{\\max_k [\\Delta_s (\\omega_k)]}, \\] where \\(\\delta_r(.)\\) is a measure of separation, and \\(\\Delta_s(.)\\) is a measure of compactness, the parameters \\(r\\) and \\(s\\) index the measures' formulations. In particular, when employing Euclidean distance, GD43 is formulated using: \\[ \\delta_5 (\\omega_i, \\omega_j) = \\frac{CP_1^2 (v_i, \\omega_i) + CP_1^2 (v_j, \\omega_j)}{n_i + n_j}, \\] and \\[ \\Delta_3 (\\omega_k) = \\frac{2 \\times CP_1^2 (v_k, \\omega_k)}{n_k}. \\] Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . GD53 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric GD53 : 0.158377 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 J. Bezdek and N. Pal, \"Some new indexes of cluster validity,\" IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301\u2013315, Jun. 1998. \u21a9","title":"GD53"},{"location":"api/metrics/cluster/GD53/#gd53","text":"Generalized Dunn's index 53 (GD53). The Generalized Dunn's indices comprise a set of 17 variants of the original Dunn's index devised to address sensitivity to noise in the latter. The formula of this index is given by: \\[ GD_{rs} = \\frac{\\min_{i \\new q} [\\delta_r (\\omega_i, \\omega_j)]}{\\max_k [\\Delta_s (\\omega_k)]}, \\] where \\(\\delta_r(.)\\) is a measure of separation, and \\(\\Delta_s(.)\\) is a measure of compactness, the parameters \\(r\\) and \\(s\\) index the measures' formulations. In particular, when employing Euclidean distance, GD43 is formulated using: \\[ \\delta_5 (\\omega_i, \\omega_j) = \\frac{CP_1^2 (v_i, \\omega_i) + CP_1^2 (v_j, \\omega_j)}{n_i + n_j}, \\] and \\[ \\Delta_3 (\\omega_k) = \\frac{2 \\times CP_1^2 (v_k, \\omega_k)}{n_k}. \\]","title":"GD53"},{"location":"api/metrics/cluster/GD53/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/GD53/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . GD53 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric GD53 : 0.158377","title":"Examples"},{"location":"api/metrics/cluster/GD53/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/GD53/#references","text":"J. Bezdek and N. Pal, \"Some new indexes of cluster validity,\" IEEE Trans. Syst., Man, Cybern. B, vol. 28, no. 3, pp. 301\u2013315, Jun. 1998. \u21a9","title":"References"},{"location":"api/metrics/cluster/Hartigan/","text":"Hartigan \u00b6 Hartigan Index (H - Index) Hartigan Index (H - Index) 1 is a sum-of-square based index 2 , which is equal to the negative log of the division of SSW (Sum-of-Squares Within Clusters) by SSB (Sum-of-Squares Between Clusters). The higher the Hartigan index, the higher the clustering quality is. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Hartigan () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Hartigan : 0.836189 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Hartigan JA (1975). Clustering Algorithms. John Wiley & Sons, Inc., New York, NY, USA. ISBN 047135645X. \u21a9 Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"Hartigan"},{"location":"api/metrics/cluster/Hartigan/#hartigan","text":"Hartigan Index (H - Index) Hartigan Index (H - Index) 1 is a sum-of-square based index 2 , which is equal to the negative log of the division of SSW (Sum-of-Squares Within Clusters) by SSB (Sum-of-Squares Between Clusters). The higher the Hartigan index, the higher the clustering quality is.","title":"Hartigan"},{"location":"api/metrics/cluster/Hartigan/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/Hartigan/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Hartigan () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Hartigan : 0.836189","title":"Examples"},{"location":"api/metrics/cluster/Hartigan/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/Hartigan/#references","text":"Hartigan JA (1975). Clustering Algorithms. John Wiley & Sons, Inc., New York, NY, USA. ISBN 047135645X. \u21a9 Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"References"},{"location":"api/metrics/cluster/IIndex/","text":"IIndex \u00b6 I-Index (I). I-Index (I) 1 adopts the maximum distance between cluster centers. It also shares the type of formulation numerator-separation/denominator-compactness. For compactness, the distance from a data point to its cluster center is also used like CH. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . IIndex () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric IIndex : 6.836566 References \u00b6 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) Maulik, U., Bandyopadhyay, S. (2002). Performance evaluation of some clustering algorithms and validity indices. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 24(12) 1650 - 1654. DOI: 10.1109/TPAMI.2002.1114856 \u21a9","title":"IIndex"},{"location":"api/metrics/cluster/IIndex/#iindex","text":"I-Index (I). I-Index (I) 1 adopts the maximum distance between cluster centers. It also shares the type of formulation numerator-separation/denominator-compactness. For compactness, the distance from a data point to its cluster center is also used like CH.","title":"IIndex"},{"location":"api/metrics/cluster/IIndex/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/IIndex/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . IIndex () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric IIndex : 6.836566","title":"Examples"},{"location":"api/metrics/cluster/IIndex/#references","text":"","title":"References"},{"location":"api/metrics/cluster/IIndex/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 correction \u2013 defaults to None update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) Maulik, U., Bandyopadhyay, S. (2002). Performance evaluation of some clustering algorithms and validity indices. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 24(12) 1650 - 1654. DOI: 10.1109/TPAMI.2002.1114856 \u21a9","title":"Methods"},{"location":"api/metrics/cluster/InternalMetric/","text":"InternalMetric \u00b6 Mother class of all internal clustering metrics. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"InternalMetric"},{"location":"api/metrics/cluster/InternalMetric/#internalmetric","text":"Mother class of all internal clustering metrics.","title":"InternalMetric"},{"location":"api/metrics/cluster/InternalMetric/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/InternalMetric/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/MSSTD/","text":"MSSTD \u00b6 Mean Squared Standard Deviation. This is the pooled sample variance of all the attributes, which measures only the compactness of found clusters. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . MSSTD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric MSSTD : 2.635708 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17, 107 - 145. DOI: 10.1023/a:1012801612483. \u21a9","title":"MSSTD"},{"location":"api/metrics/cluster/MSSTD/#msstd","text":"Mean Squared Standard Deviation. This is the pooled sample variance of all the attributes, which measures only the compactness of found clusters.","title":"MSSTD"},{"location":"api/metrics/cluster/MSSTD/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/MSSTD/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . MSSTD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric MSSTD : 2.635708","title":"Examples"},{"location":"api/metrics/cluster/MSSTD/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/MSSTD/#references","text":"Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17, 107 - 145. DOI: 10.1023/a:1012801612483. \u21a9","title":"References"},{"location":"api/metrics/cluster/PS/","text":"PS \u00b6 Partition Separation (PS). The PS index 1 was originally developed for fuzzy clustering. This index only comprises a measure of separation between prototypes. Although classified as a batch clustering validity index (CVI), it can be readily used to evaluate the partitions idenified by unsupervised incremental learners tha model clusters using cenroids. Larger values of PS indicate better clustering solutions. The PS value is given by \\[ PS = \\sum_{i=1}^k PS_i, \\] where \\[ PS_i = \\frac{n_i}{\\max_j n_j} - exp \\left[ - \\frac{\\min{i \\neq j} (\\lVert v_i - v_j \\rVert_2^2)}{\\beta_T} \\right], \\] \\[ \\beta_T = \\frac{1}{k} \\sum_{l=1}^k \\lVert v_l - \\bar{v} \\rVert_2 ^2, \\] and \\[ \\bar{v} = \\frac{1}{k} \\sum_{l=1}^k v_l. \\] Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . PS () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric PS : 1.336026 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 E. Lughofer, \"Extensions of vector quantization for incremental clustering,\" Pattern Recognit., vol. 41, no. 3, pp. 995\u20131011, Mar. 2008. \u21a9","title":"PS"},{"location":"api/metrics/cluster/PS/#ps","text":"Partition Separation (PS). The PS index 1 was originally developed for fuzzy clustering. This index only comprises a measure of separation between prototypes. Although classified as a batch clustering validity index (CVI), it can be readily used to evaluate the partitions idenified by unsupervised incremental learners tha model clusters using cenroids. Larger values of PS indicate better clustering solutions. The PS value is given by \\[ PS = \\sum_{i=1}^k PS_i, \\] where \\[ PS_i = \\frac{n_i}{\\max_j n_j} - exp \\left[ - \\frac{\\min{i \\neq j} (\\lVert v_i - v_j \\rVert_2^2)}{\\beta_T} \\right], \\] \\[ \\beta_T = \\frac{1}{k} \\sum_{l=1}^k \\lVert v_l - \\bar{v} \\rVert_2 ^2, \\] and \\[ \\bar{v} = \\frac{1}{k} \\sum_{l=1}^k v_l. \\]","title":"PS"},{"location":"api/metrics/cluster/PS/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/PS/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . PS () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric PS : 1.336026","title":"Examples"},{"location":"api/metrics/cluster/PS/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/PS/#references","text":"E. Lughofer, \"Extensions of vector quantization for incremental clustering,\" Pattern Recognit., vol. 41, no. 3, pp. 995\u20131011, Mar. 2008. \u21a9","title":"References"},{"location":"api/metrics/cluster/R2/","text":"R2 \u00b6 R-Squared R-Squared (RS) 1 is the complement of the ratio of sum of squared distances between objects in different clusters to the total sum of squares. It is an intuitive and simple formulation of measuring the differences between clusters. The maximum value of R-Squared is 1, which means that the higher the index, the better the clustering results. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . R2 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric R2 : 0.509203 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276. DOI: 10.1007/3-540-45372-5_26 \u21a9","title":"R2"},{"location":"api/metrics/cluster/R2/#r2","text":"R-Squared R-Squared (RS) 1 is the complement of the ratio of sum of squared distances between objects in different clusters to the total sum of squares. It is an intuitive and simple formulation of measuring the differences between clusters. The maximum value of R-Squared is 1, which means that the higher the index, the better the clustering results.","title":"R2"},{"location":"api/metrics/cluster/R2/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/R2/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . R2 () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric R2 : 0.509203","title":"Examples"},{"location":"api/metrics/cluster/R2/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/R2/#references","text":"Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276. DOI: 10.1007/3-540-45372-5_26 \u21a9","title":"References"},{"location":"api/metrics/cluster/RMSSTD/","text":"RMSSTD \u00b6 Root Mean Squared Standard Deviation. This is the square root of the pooled sample variance of all the attributes, which measures only the compactness of found clusters. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . RMSSTD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric RMSSTD : 1.623486 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17, 107 - 145. DOI: 10.1023/a:1012801612483. \u21a9","title":"RMSSTD"},{"location":"api/metrics/cluster/RMSSTD/#rmsstd","text":"Root Mean Squared Standard Deviation. This is the square root of the pooled sample variance of all the attributes, which measures only the compactness of found clusters.","title":"RMSSTD"},{"location":"api/metrics/cluster/RMSSTD/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/RMSSTD/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . RMSSTD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric RMSSTD : 1.623486","title":"Examples"},{"location":"api/metrics/cluster/RMSSTD/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/RMSSTD/#references","text":"Halkidi, M., Batistakis, Y. and Vazirgiannis, M. (2001). On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17, 107 - 145. DOI: 10.1023/a:1012801612483. \u21a9","title":"References"},{"location":"api/metrics/cluster/SD/","text":"SD \u00b6 The SD validity index (SD). The SD validity index (SD) 1 is a more recent clustering validation measure. It is composed of two terms: Scat(NC) stands for the scattering within clusters, Dis(NC) stands for the dispersion between clusters. Like DB and SB, SD measures the compactness with variance of clustered objects and separation with distance between cluster centers, but uses them in a different way. The smaller the value of SD, the better. In the original formula for SD validation index, the ratio between the maximum and the actual number of clusters is taken into account. However, due to the fact that metrics are updated in an incremental fashion, this ratio will be automatically set to default as 1. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SD : 2.339016 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276. DOI: 10.1007/3-540-45372-5_26 \u21a9","title":"SD"},{"location":"api/metrics/cluster/SD/#sd","text":"The SD validity index (SD). The SD validity index (SD) 1 is a more recent clustering validation measure. It is composed of two terms: Scat(NC) stands for the scattering within clusters, Dis(NC) stands for the dispersion between clusters. Like DB and SB, SD measures the compactness with variance of clustered objects and separation with distance between cluster centers, but uses them in a different way. The smaller the value of SD, the better. In the original formula for SD validation index, the ratio between the maximum and the actual number of clusters is taken into account. However, due to the fact that metrics are updated in an incremental fashion, this ratio will be automatically set to default as 1.","title":"SD"},{"location":"api/metrics/cluster/SD/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/SD/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SD () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SD : 2.339016","title":"Examples"},{"location":"api/metrics/cluster/SD/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/SD/#references","text":"Halkidi, M., Vazirgiannis, M., & Batistakis, Y. (2000). Quality Scheme Assessment in the Clustering Process. Principles Of Data Mining And Knowledge Discovery, 265-276. DOI: 10.1007/3-540-45372-5_26 \u21a9","title":"References"},{"location":"api/metrics/cluster/SSB/","text":"SSB \u00b6 Sum-of-Squares Between Clusters (SSB). The Sum-of-Squares Between Clusters is the weighted mean of the squares of distances between cluster centers to the mean value of the whole dataset. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SSB () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SSB : 8.109389 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"SSB"},{"location":"api/metrics/cluster/SSB/#ssb","text":"Sum-of-Squares Between Clusters (SSB). The Sum-of-Squares Between Clusters is the weighted mean of the squares of distances between cluster centers to the mean value of the whole dataset.","title":"SSB"},{"location":"api/metrics/cluster/SSB/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/SSB/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SSB () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SSB : 8.109389","title":"Examples"},{"location":"api/metrics/cluster/SSB/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/SSB/#references","text":"Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"References"},{"location":"api/metrics/cluster/SSW/","text":"SSW \u00b6 Sum-of-Squares Within Clusters (SSW). Mean of sum of squared distances from data points to their assigned cluster centroids. The bigger the better. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SSW () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SSW : 3.514277 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"SSW"},{"location":"api/metrics/cluster/SSW/#ssw","text":"Sum-of-Squares Within Clusters (SSW). Mean of sum of squared distances from data points to their assigned cluster centroids. The bigger the better.","title":"SSW"},{"location":"api/metrics/cluster/SSW/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/SSW/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . SSW () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric SSW : 3.514277","title":"Examples"},{"location":"api/metrics/cluster/SSW/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/SSW/#references","text":"Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"References"},{"location":"api/metrics/cluster/Separation/","text":"Separation \u00b6 Average distance from a point to the points assigned to other clusters. The bigger the better. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Separation () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Separation : 4.54563 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"Separation"},{"location":"api/metrics/cluster/Separation/#separation","text":"Average distance from a point to the points assigned to other clusters. The bigger the better.","title":"Separation"},{"location":"api/metrics/cluster/Separation/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/Separation/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Separation () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Separation : 4.54563","title":"Examples"},{"location":"api/metrics/cluster/Separation/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x ( Dict[Hashable, numbers.Number] ) y_pred ( numbers.Number ) centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/Separation/#references","text":"Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"References"},{"location":"api/metrics/cluster/Silhouette/","text":"Silhouette \u00b6 Silhouette coefficient 1 , roughly speaking, is the ratio between cohesion and the average distances from the points to their second-closest centroid. It rewards the clustering algorithm where points are very close to their assigned centroids and far from any other centroids, that is, clustering results with good cohesion and good separation. It rewards clusterings where points are very close to their assigned centroids and far from any other centroids, that is clusterings with good cohesion and good separation. 2 The definition of Silhouette coefficient for online clustering evaluation is different from that of batch learning. It does not store information and calculate pairwise distances between all points at the same time, since the practice is too expensive for an incremental metric. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Silhouette () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Silhouette : 0.453723 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Rousseeuw, P. (1987). Silhouettes: a graphical aid to the intepretation and validation of cluster analysis 20, 53 - 65. DOI: 10.1016/0377-0427(87)90125-7 \u21a9 Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"Silhouette"},{"location":"api/metrics/cluster/Silhouette/#silhouette","text":"Silhouette coefficient 1 , roughly speaking, is the ratio between cohesion and the average distances from the points to their second-closest centroid. It rewards the clustering algorithm where points are very close to their assigned centroids and far from any other centroids, that is, clustering results with good cohesion and good separation. It rewards clusterings where points are very close to their assigned centroids and far from any other centroids, that is clusterings with good cohesion and good separation. 2 The definition of Silhouette coefficient for online clustering evaluation is different from that of batch learning. It does not store information and calculate pairwise distances between all points at the same time, since the practice is too expensive for an incremental metric.","title":"Silhouette"},{"location":"api/metrics/cluster/Silhouette/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/Silhouette/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Silhouette () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Silhouette : 0.453723","title":"Examples"},{"location":"api/metrics/cluster/Silhouette/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/Silhouette/#references","text":"Rousseeuw, P. (1987). Silhouettes: a graphical aid to the intepretation and validation of cluster analysis 20, 53 - 65. DOI: 10.1016/0377-0427(87)90125-7 \u21a9 Bifet, A. et al. (2018). \"Machine Learning for Data Streams\". DOI: 10.7551/mitpress/10654.001.0001. \u21a9","title":"References"},{"location":"api/metrics/cluster/WB/","text":"WB \u00b6 WB Index WB Index is a simple sum-of-square method, calculated by dividing the within cluster sum-of-squares by the between cluster sum-of-squares. Its effect is emphasized by multiplying the number of clusters. The advantages of the proposed method are that one can determine the number of clusters by minimizing the WB value, without relying on any knee point detection, and this metric is straightforward to implement. The lower the WB index, the higher the clustering quality is. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . WB () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric WB : 1.300077 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"WB"},{"location":"api/metrics/cluster/WB/#wb","text":"WB Index WB Index is a simple sum-of-square method, calculated by dividing the within cluster sum-of-squares by the between cluster sum-of-squares. Its effect is emphasized by multiplying the number of clusters. The advantages of the proposed method are that one can determine the number of clusters by minimizing the WB value, without relying on any knee point detection, and this metric is straightforward to implement. The lower the WB index, the higher the clustering quality is.","title":"WB"},{"location":"api/metrics/cluster/WB/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/WB/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . WB () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric WB : 1.300077","title":"Examples"},{"location":"api/metrics/cluster/WB/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/WB/#references","text":"Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"References"},{"location":"api/metrics/cluster/XieBeni/","text":"XieBeni \u00b6 Xie-Beni index (XB). The Xie-Beni index 1 has the form of (Compactness)/(Separation), which defines the inter-cluster separation as the minimum squared distance between cluster centers, and the intra-cluster compactness as the mean squared distance between each data object and its cluster centers. The smaller the value of XB, the better the clustering quality. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . XieBeni () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric XieBeni : 0.397043 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 X. L. Xie, G. Beni (1991). A validity measure for fuzzy clustering. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 13(8), 841 - 847. DOI: 10.1109/34.85677 \u21a9","title":"XieBeni"},{"location":"api/metrics/cluster/XieBeni/#xiebeni","text":"Xie-Beni index (XB). The Xie-Beni index 1 has the form of (Compactness)/(Separation), which defines the inter-cluster separation as the minimum squared distance between cluster centers, and the intra-cluster compactness as the mean squared distance between each data object and its cluster centers. The smaller the value of XB, the better the clustering quality.","title":"XieBeni"},{"location":"api/metrics/cluster/XieBeni/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/XieBeni/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . XieBeni () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric XieBeni : 0.397043","title":"Examples"},{"location":"api/metrics/cluster/XieBeni/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/XieBeni/#references","text":"X. L. Xie, G. Beni (1991). A validity measure for fuzzy clustering. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 13(8), 841 - 847. DOI: 10.1109/34.85677 \u21a9","title":"References"},{"location":"api/metrics/cluster/Xu/","text":"Xu \u00b6 Xu Index Xu Index is among the most complicated sum-of-squares based metrics 1 . It is calculated based on the Sum-of-Squares Within Clusters (SSW), total number of points, number of clusters, and the dimension of the cluserting problem. The lower the Xu index, the higher the clustering quality is. Attributes \u00b6 bigger_is_better Indicates if a high value is better than a low one or not. Examples \u00b6 >>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Xu () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Xu : - 2.73215 Methods \u00b6 get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator ) References \u00b6 Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"Xu"},{"location":"api/metrics/cluster/Xu/#xu","text":"Xu Index Xu Index is among the most complicated sum-of-squares based metrics 1 . It is calculated based on the Sum-of-Squares Within Clusters (SSW), total number of points, number of clusters, and the dimension of the cluserting problem. The lower the Xu index, the higher the clustering quality is.","title":"Xu"},{"location":"api/metrics/cluster/Xu/#attributes","text":"bigger_is_better Indicates if a high value is better than a low one or not.","title":"Attributes"},{"location":"api/metrics/cluster/Xu/#examples","text":">>> from river import cluster >>> from river import stream >>> from river import metrics >>> X = [ ... [ 1 , 2 ], ... [ 1 , 4 ], ... [ 1 , 0 ], ... [ 4 , 2 ], ... [ 4 , 4 ], ... [ 4 , 0 ], ... [ - 2 , 2 ], ... [ - 2 , 4 ], ... [ - 2 , 0 ] ... ] >>> k_means = cluster . KMeans ( n_clusters = 3 , halflife = 0.4 , sigma = 3 , seed = 0 ) >>> metric = metrics . cluster . Xu () >>> for x , _ in stream . iter_array ( X ): ... k_means = k_means . learn_one ( x ) ... y_pred = k_means . predict_one ( x ) ... metric = metric . update ( x , y_pred , k_means . centers ) >>> metric Xu : - 2.73215","title":"Examples"},{"location":"api/metrics/cluster/Xu/#methods","text":"get Return the current value of the metric. revert Revert the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 update Update the metric. Parameters x y_pred centers sample_weight \u2013 defaults to 1.0 works_with Indicates whether or not a metric can work with a given model. Parameters model ( river.base.estimator.Estimator )","title":"Methods"},{"location":"api/metrics/cluster/Xu/#references","text":"Q. Zhao, M. Xu, and P. Franti, \"Sum-of-squares based cluster validity index and significance analysis,\" in Adaptive and Natural Computing Algorithms, M. Kolehmainen, P. Toivanen, and B. Beliczynski, Eds. Berlin, Germany: Springer, 2009, pp. 313\u2013322. \u21a9","title":"References"},{"location":"api/multiclass/OneVsOneClassifier/","text":"OneVsOneClassifier \u00b6 One-vs-One (OvO) multiclass strategy. This strategy consists in fitting one binary classifier for each pair of classes. Because we are in a streaming context, the number of classes isn't known from the start, hence new classifiers are instantiated on the fly. The number of classifiers is k * (k - 1) / 2 , where k is the number of classes. However, each call to learn_one only requires training k - 1 models. Indeed, only the models that pertain to the given label have to be trained. Meanwhile, making a prediction requires going through each and every model. Parameters \u00b6 classifier A binary classifier, although a multi-class classifier will work too. Attributes \u00b6 classifiers ( dict ) A mapping between pairs of classes and classifiers. The keys are tuples which contain a pair of classes. Each pair is sorted in lexicographical order. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ovo = multiclass . OneVsOneClassifier ( linear_model . LogisticRegression ()) >>> model = scaler | ovo >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.807573 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"OneVsOneClassifier"},{"location":"api/multiclass/OneVsOneClassifier/#onevsoneclassifier","text":"One-vs-One (OvO) multiclass strategy. This strategy consists in fitting one binary classifier for each pair of classes. Because we are in a streaming context, the number of classes isn't known from the start, hence new classifiers are instantiated on the fly. The number of classifiers is k * (k - 1) / 2 , where k is the number of classes. However, each call to learn_one only requires training k - 1 models. Indeed, only the models that pertain to the given label have to be trained. Meanwhile, making a prediction requires going through each and every model.","title":"OneVsOneClassifier"},{"location":"api/multiclass/OneVsOneClassifier/#parameters","text":"classifier A binary classifier, although a multi-class classifier will work too.","title":"Parameters"},{"location":"api/multiclass/OneVsOneClassifier/#attributes","text":"classifiers ( dict ) A mapping between pairs of classes and classifiers. The keys are tuples which contain a pair of classes. Each pair is sorted in lexicographical order.","title":"Attributes"},{"location":"api/multiclass/OneVsOneClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ovo = multiclass . OneVsOneClassifier ( linear_model . LogisticRegression ()) >>> model = scaler | ovo >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.807573","title":"Examples"},{"location":"api/multiclass/OneVsOneClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/multiclass/OneVsRestClassifier/","text":"OneVsRestClassifier \u00b6 One-vs-the-rest (OvR) multiclass strategy. This strategy consists in fitting one binary classifier per class. Because we are in a streaming context, the number of classes isn't known from the start. Hence, new classifiers are instantiated on the fly. Likewise, the predicted probabilities will only include the classes seen up to a given point in time. Note that this classifier supports mini-batches as well as single instances. The computational complexity for both learning and predicting grows linearly with the number of classes. If you have a very large number of classes, then you might want to consider using an multiclass.OutputCodeClassifier instead. Parameters \u00b6 classifier ( base.Classifier ) A binary classifier, although a multi-class classifier will work too. Attributes \u00b6 classifiers ( dict ) A mapping between classes and classifiers. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ovr = multiclass . OneVsRestClassifier ( linear_model . LogisticRegression ()) >>> model = scaler | ovr >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.774573 This estimator also also supports mini-batching. >>> for X in pd . read_csv ( dataset . path , chunksize = 64 ): ... y = X . pop ( 'category' ) ... y_pred = model . predict_many ( X ) ... model = model . learn_many ( X , y ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X Returns Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X Returns DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"OneVsRestClassifier"},{"location":"api/multiclass/OneVsRestClassifier/#onevsrestclassifier","text":"One-vs-the-rest (OvR) multiclass strategy. This strategy consists in fitting one binary classifier per class. Because we are in a streaming context, the number of classes isn't known from the start. Hence, new classifiers are instantiated on the fly. Likewise, the predicted probabilities will only include the classes seen up to a given point in time. Note that this classifier supports mini-batches as well as single instances. The computational complexity for both learning and predicting grows linearly with the number of classes. If you have a very large number of classes, then you might want to consider using an multiclass.OutputCodeClassifier instead.","title":"OneVsRestClassifier"},{"location":"api/multiclass/OneVsRestClassifier/#parameters","text":"classifier ( base.Classifier ) A binary classifier, although a multi-class classifier will work too.","title":"Parameters"},{"location":"api/multiclass/OneVsRestClassifier/#attributes","text":"classifiers ( dict ) A mapping between classes and classifiers.","title":"Attributes"},{"location":"api/multiclass/OneVsRestClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ovr = multiclass . OneVsRestClassifier ( linear_model . LogisticRegression ()) >>> model = scaler | ovr >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.774573 This estimator also also supports mini-batching. >>> for X in pd . read_csv ( dataset . path , chunksize = 64 ): ... y = X . pop ( 'category' ) ... y_pred = model . predict_many ( X ) ... model = model . learn_many ( X , y )","title":"Examples"},{"location":"api/multiclass/OneVsRestClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X Returns Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X Returns DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/multiclass/OutputCodeClassifier/","text":"OutputCodeClassifier \u00b6 Output-code multiclass strategy. This also referred to as \"error-correcting output codes\". This class allows to learn a multi-class classification problem with a binary classifier. Each class is converted to a code of 0s and 1s. The length of the code is called the code size. A copy of the classifier made for code. The codes associated with the classes are stored in a code book. When a new sample arrives, the label's code is retrieved from the code book. Then, each classifier is trained on the relevant part of code, which is either a 0 or a 1. For predicting, each classifier outputs a probability. These are then compared to each code in the code book, and the label which is the \"closest\" is chosen as the most likely class. Closeness is determined in terms of Manhattan distance. One specificity of online learning is that we don't how many classes there are initially. Therefore, a random procedure generates random codes on the fly whenever a previously unseed label appears. Parameters \u00b6 classifier ( base.Classifier ) A binary classifier, although a multi-class classifier will work too. code_size ( int ) The code size, which dictates how many copies of the provided classifiers to train. Must be strictly positive. seed ( int ) \u2013 defaults to None A random seed number that can be set for reproducibility. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ooc = OutputCodeClassifier ( ... classifier = linear_model . LogisticRegression (), ... code_size = 10 , ... seed = 24 ... ) >>> model = scaler | ooc >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.797119 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. References \u00b6 Dietterich, T.G. and Bakiri, G., 1994. Solving multiclass learning problems via error-correcting output codes. Journal of artificial intelligence research, 2, pp.263-286. \u21a9 Allwein, E.L., Schapire, R.E. and Singer, Y., 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec), pp.113-141. \u21a9","title":"OutputCodeClassifier"},{"location":"api/multiclass/OutputCodeClassifier/#outputcodeclassifier","text":"Output-code multiclass strategy. This also referred to as \"error-correcting output codes\". This class allows to learn a multi-class classification problem with a binary classifier. Each class is converted to a code of 0s and 1s. The length of the code is called the code size. A copy of the classifier made for code. The codes associated with the classes are stored in a code book. When a new sample arrives, the label's code is retrieved from the code book. Then, each classifier is trained on the relevant part of code, which is either a 0 or a 1. For predicting, each classifier outputs a probability. These are then compared to each code in the code book, and the label which is the \"closest\" is chosen as the most likely class. Closeness is determined in terms of Manhattan distance. One specificity of online learning is that we don't how many classes there are initially. Therefore, a random procedure generates random codes on the fly whenever a previously unseed label appears.","title":"OutputCodeClassifier"},{"location":"api/multiclass/OutputCodeClassifier/#parameters","text":"classifier ( base.Classifier ) A binary classifier, although a multi-class classifier will work too. code_size ( int ) The code size, which dictates how many copies of the provided classifiers to train. Must be strictly positive. seed ( int ) \u2013 defaults to None A random seed number that can be set for reproducibility.","title":"Parameters"},{"location":"api/multiclass/OutputCodeClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multiclass >>> from river import preprocessing >>> dataset = datasets . ImageSegments () >>> scaler = preprocessing . StandardScaler () >>> ooc = OutputCodeClassifier ( ... classifier = linear_model . LogisticRegression (), ... code_size = 10 , ... seed = 24 ... ) >>> model = scaler | ooc >>> metric = metrics . MacroF1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) MacroF1 : 0.797119","title":"Examples"},{"location":"api/multiclass/OutputCodeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/multiclass/OutputCodeClassifier/#references","text":"Dietterich, T.G. and Bakiri, G., 1994. Solving multiclass learning problems via error-correcting output codes. Journal of artificial intelligence research, 2, pp.263-286. \u21a9 Allwein, E.L., Schapire, R.E. and Singer, Y., 2000. Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of machine learning research, 1(Dec), pp.113-141. \u21a9","title":"References"},{"location":"api/multioutput/ClassifierChain/","text":"ClassifierChain \u00b6 A multi-output model that arranges classifiers into a chain. This will create one model per output. The prediction of the first output will be used as a feature in the second model. The prediction for the second output will be used as a feature for the third model, etc. This \"chain model\" is therefore capable of capturing dependencies between outputs. Parameters \u00b6 model ( base.Classifier ) order ( list ) \u2013 defaults to None A list with the targets order in which to construct the chain. If None then the order will be inferred from the order of the keys in the target. Examples \u00b6 >>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import stream >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . fetch_openml ( 'yeast' , version = 4 , as_frame = False ), ... shuffle = True , ... seed = 42 ... ) >>> model = feature_selection . VarianceThreshold ( threshold = 0.01 ) >>> model |= preprocessing . StandardScaler () >>> model |= multioutput . ClassifierChain ( ... model = linear_model . LogisticRegression (), ... order = list ( range ( 14 )) ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... # Convert y values to booleans ... y = { i : yi == 'TRUE' for i , yi in y . items ()} ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.451524 Methods \u00b6 clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values References \u00b6 Multi-Output Chain Models and their Application in Data Streams \u21a9","title":"ClassifierChain"},{"location":"api/multioutput/ClassifierChain/#classifierchain","text":"A multi-output model that arranges classifiers into a chain. This will create one model per output. The prediction of the first output will be used as a feature in the second model. The prediction for the second output will be used as a feature for the third model, etc. This \"chain model\" is therefore capable of capturing dependencies between outputs.","title":"ClassifierChain"},{"location":"api/multioutput/ClassifierChain/#parameters","text":"model ( base.Classifier ) order ( list ) \u2013 defaults to None A list with the targets order in which to construct the chain. If None then the order will be inferred from the order of the keys in the target.","title":"Parameters"},{"location":"api/multioutput/ClassifierChain/#examples","text":">>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import stream >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . fetch_openml ( 'yeast' , version = 4 , as_frame = False ), ... shuffle = True , ... seed = 42 ... ) >>> model = feature_selection . VarianceThreshold ( threshold = 0.01 ) >>> model |= preprocessing . StandardScaler () >>> model |= multioutput . ClassifierChain ( ... model = linear_model . LogisticRegression (), ... order = list ( range ( 14 )) ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... # Convert y values to booleans ... y = { i : yi == 'TRUE' for i , yi in y . items ()} ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.451524","title":"Examples"},{"location":"api/multioutput/ClassifierChain/#methods","text":"clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values","title":"Methods"},{"location":"api/multioutput/ClassifierChain/#references","text":"Multi-Output Chain Models and their Application in Data Streams \u21a9","title":"References"},{"location":"api/multioutput/MonteCarloClassifierChain/","text":"MonteCarloClassifierChain \u00b6 Monte Carlo Sampling Classifier Chains. Probabilistic Classifier Chains using Monte Carlo sampling, as described in 1 . m samples are taken from the posterior distribution. Therefore we need a probabilistic interpretation of the output, and thus, this is a particular variety of ProbabilisticClassifierChain. Parameters \u00b6 model ( base.Classifier ) m ( int ) \u2013 defaults to 10 Number of samples to take from the posterior distribution. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility. Examples \u00b6 >>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import synth >>> dataset = synth . Logical ( seed = 42 , n_tiles = 100 ) >>> model = multioutput . MonteCarloClassifierChain ( ... model = linear_model . LogisticRegression (), ... m = 10 , ... seed = 42 ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.568087 Methods \u00b6 clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values References \u00b6 Read, J., Martino, L., & Luengo, D. (2014). Efficient monte carlo methods for multi-dimensional learning with classifier chains. Pattern Recognition, 47(3), 1535-1546. \u21a9","title":"MonteCarloClassifierChain"},{"location":"api/multioutput/MonteCarloClassifierChain/#montecarloclassifierchain","text":"Monte Carlo Sampling Classifier Chains. Probabilistic Classifier Chains using Monte Carlo sampling, as described in 1 . m samples are taken from the posterior distribution. Therefore we need a probabilistic interpretation of the output, and thus, this is a particular variety of ProbabilisticClassifierChain.","title":"MonteCarloClassifierChain"},{"location":"api/multioutput/MonteCarloClassifierChain/#parameters","text":"model ( base.Classifier ) m ( int ) \u2013 defaults to 10 Number of samples to take from the posterior distribution. seed ( int ) \u2013 defaults to None Random number generator seed for reproducibility.","title":"Parameters"},{"location":"api/multioutput/MonteCarloClassifierChain/#examples","text":">>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import synth >>> dataset = synth . Logical ( seed = 42 , n_tiles = 100 ) >>> model = multioutput . MonteCarloClassifierChain ( ... model = linear_model . LogisticRegression (), ... m = 10 , ... seed = 42 ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.568087","title":"Examples"},{"location":"api/multioutput/MonteCarloClassifierChain/#methods","text":"clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values","title":"Methods"},{"location":"api/multioutput/MonteCarloClassifierChain/#references","text":"Read, J., Martino, L., & Luengo, D. (2014). Efficient monte carlo methods for multi-dimensional learning with classifier chains. Pattern Recognition, 47(3), 1535-1546. \u21a9","title":"References"},{"location":"api/multioutput/ProbabilisticClassifierChain/","text":"ProbabilisticClassifierChain \u00b6 Probabilistic Classifier Chains. The Probabilistic Classifier Chains (PCC) 1 is a Bayes-optimal method based on the Classifier Chains (CC). Consider the concept of chaining classifiers as searching a path in a binary tree whose leaf nodes are associated with a label \\(y \\in Y\\) . While CC searches only a single path in the aforementioned binary tree, PCC looks at each of the \\(2^l\\) paths, where \\(l\\) is the number of labels. This limits the applicability of the method to data sets with a small to moderate number of labels. The authors recommend no more than about 15 labels for real-world applications. Parameters \u00b6 model ( base.Classifier ) Examples \u00b6 >>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import synth >>> dataset = synth . Logical ( seed = 42 , n_tiles = 100 ) >>> model = multioutput . ProbabilisticClassifierChain ( ... model = linear_model . LogisticRegression () ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.571429 Methods \u00b6 clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values References \u00b6 Cheng, W., H\u00fcllermeier, E., & Dembczynski, K. J. (2010). Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 279-286). \u21a9","title":"ProbabilisticClassifierChain"},{"location":"api/multioutput/ProbabilisticClassifierChain/#probabilisticclassifierchain","text":"Probabilistic Classifier Chains. The Probabilistic Classifier Chains (PCC) 1 is a Bayes-optimal method based on the Classifier Chains (CC). Consider the concept of chaining classifiers as searching a path in a binary tree whose leaf nodes are associated with a label \\(y \\in Y\\) . While CC searches only a single path in the aforementioned binary tree, PCC looks at each of the \\(2^l\\) paths, where \\(l\\) is the number of labels. This limits the applicability of the method to data sets with a small to moderate number of labels. The authors recommend no more than about 15 labels for real-world applications.","title":"ProbabilisticClassifierChain"},{"location":"api/multioutput/ProbabilisticClassifierChain/#parameters","text":"model ( base.Classifier )","title":"Parameters"},{"location":"api/multioutput/ProbabilisticClassifierChain/#examples","text":">>> from river import feature_selection >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import synth >>> dataset = synth . Logical ( seed = 42 , n_tiles = 100 ) >>> model = multioutput . ProbabilisticClassifierChain ( ... model = linear_model . LogisticRegression () ... ) >>> metric = metrics . Jaccard () >>> for x , y in dataset : ... y_pred = model . predict_one ( x ) ... metric = metric . update ( y , y_pred ) ... model = model . learn_one ( x , y ) >>> metric Jaccard : 0.571429","title":"Examples"},{"location":"api/multioutput/ProbabilisticClassifierChain/#methods","text":"clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Update the model with a set of features x and a label y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x Returns The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values","title":"Methods"},{"location":"api/multioutput/ProbabilisticClassifierChain/#references","text":"Cheng, W., H\u00fcllermeier, E., & Dembczynski, K. J. (2010). Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 279-286). \u21a9","title":"References"},{"location":"api/multioutput/RegressorChain/","text":"RegressorChain \u00b6 A multi-output model that arranges regressor into a chain. This will create one model per output. The prediction of the first output will be used as a feature in the second output. The prediction for the second output will be used as a feature for the third, etc. This \"chain model\" is therefore capable of capturing dependencies between outputs. Parameters \u00b6 model ( base.Regressor ) order ( list ) \u2013 defaults to None A list with the targets order in which to construct the chain. If None then the order will be inferred from the order of the keys in the target. Examples \u00b6 >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import stream >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_linnerud (), ... shuffle = True , ... seed = 42 ... ) >>> model = multioutput . RegressorChain ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = 0.3 ) ... ), ... order = [ 0 , 1 , 2 ] ... ) >>> metric = metrics . RegressionMultiOutput ( metrics . MAE ()) >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 12.649592 Methods \u00b6 clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values","title":"RegressorChain"},{"location":"api/multioutput/RegressorChain/#regressorchain","text":"A multi-output model that arranges regressor into a chain. This will create one model per output. The prediction of the first output will be used as a feature in the second output. The prediction for the second output will be used as a feature for the third, etc. This \"chain model\" is therefore capable of capturing dependencies between outputs.","title":"RegressorChain"},{"location":"api/multioutput/RegressorChain/#parameters","text":"model ( base.Regressor ) order ( list ) \u2013 defaults to None A list with the targets order in which to construct the chain. If None then the order will be inferred from the order of the keys in the target.","title":"Parameters"},{"location":"api/multioutput/RegressorChain/#examples","text":">>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import multioutput >>> from river import preprocessing >>> from river import stream >>> from sklearn import datasets >>> dataset = stream . iter_sklearn_dataset ( ... dataset = datasets . load_linnerud (), ... shuffle = True , ... seed = 42 ... ) >>> model = multioutput . RegressorChain ( ... model = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( intercept_lr = 0.3 ) ... ), ... order = [ 0 , 1 , 2 ] ... ) >>> metric = metrics . RegressionMultiOutput ( metrics . MAE ()) >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 12.649592","title":"Examples"},{"location":"api/multioutput/RegressorChain/#methods","text":"clear D.clear() -> None. Remove all items from D. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy fromkeys get D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Parameters key default \u2013 defaults to None items D.items() -> a set-like object providing a view on D's items keys D.keys() -> a set-like object providing a view on D's keys learn_one Fits to a set of features x and a real-valued target y . Parameters x y Returns self pop D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised. Parameters key default \u2013 defaults to <object object at 0x7f6d49bb1160> popitem D.popitem() -> (k, v), remove and return some (key, value) pair as a 2-tuple; but raise KeyError if D is empty. predict_one Predicts the target value of a set of features x . Parameters x Returns The prediction. setdefault D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D Parameters key default \u2013 defaults to None update D.update([E, ]**F) -> None. Update D from mapping/iterable E and F. If E present and has a .keys() method, does: for k in E: D[k] = E[k] If E present and lacks .keys() method, does: for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v Parameters other \u2013 defaults to () kwds values D.values() -> an object providing a view on D's values","title":"Methods"},{"location":"api/naive-bayes/BernoulliNB/","text":"BernoulliNB \u00b6 Bernoulli Naive Bayes. Bernoulli Naive Bayes model learns from occurrences between features such as word counts and discrete classes. The input vector must contain positive values, such as counts or TF-IDF values. Parameters \u00b6 alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing). true_threshold \u2013 defaults to 0.0 Threshold for binarizing (mapping to booleans) features. Attributes \u00b6 class_counts ( collections.Counter ) Number of times each class has been seen. feature_counts ( collections.defaultdict ) Total frequencies per feature and class. Examples \u00b6 >>> import math >>> from river import compose >>> from river import feature_extraction >>> from river import naive_bayes >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . BernoulliNB ( alpha = 1 )) ... ) >>> for sentence , label in docs : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 3 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Beijing' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Macao' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Shanghai' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Beijing' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Macao' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Shanghai' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> new_text = 'Chinese Chinese Chinese Tokyo Japan' >>> tokens = model [ 'tokenize' ] . transform_one ( new_text ) >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> math . exp ( jlh [ 'yes' ]) 0.005184 >>> math . exp ( jlh [ 'no' ]) 0.021947 >>> model . predict_one ( new_text ) 'no' >>> model . predict_proba_one ( 'test' )[ 'yes' ] 0.8831539823829913 You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'docs' , 'y' ]) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . BernoulliNB ( alpha = 1 )) ... ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 3 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Beijing' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Macao' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Shanghai' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Beijing' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Macao' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Shanghai' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> unseen_data = pd . Series ( ... [ 'Taiwanese Taipei' , 'Chinese Shanghai' ], ... name = 'docs' , ... index = [ 'river' , 'rocks' ] ... ) >>> model . predict_proba_many ( unseen_data ) no yes river 0.116846 0.883154 rocks 0.047269 0.952731 >>> model . predict_many ( unseen_data ) river yes rocks yes dtype : object >>> unseen_data = pd . Series ( ... [ 'test' ], name = 'docs' ) >>> model . predict_proba_many ( unseen_data ) no yes 0 0.116846 0.883154 >>> model . predict_proba_one ( 'test' ) { 'no' : 0.116846017617009 , 'yes' : 0.8831539823829913 } >>> bag = feature_extraction . BagOfWords ( lowercase = False ) >>> model = naive_bayes . BernoulliNB ( alpha = 1 ) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> X = bag . transform_many ( X ) >>> X = pd . DataFrame ( X . values , columns = X . columns , dtype = int ) >>> model = model . learn_many ( X , y ) >>> unseen_data = bag . transform_many ( unseen_data ) >>> unseen_data = pd . DataFrame ( unseen_data . values , columns = unseen_data . columns , index = [ 'river' ]) >>> model . predict_proba_many ( unseen_data ) no yes river 0.116846 0.883154 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many p_feature_given_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict ) References \u00b6 The Bernoulli model \u21a9","title":"BernoulliNB"},{"location":"api/naive-bayes/BernoulliNB/#bernoullinb","text":"Bernoulli Naive Bayes. Bernoulli Naive Bayes model learns from occurrences between features such as word counts and discrete classes. The input vector must contain positive values, such as counts or TF-IDF values.","title":"BernoulliNB"},{"location":"api/naive-bayes/BernoulliNB/#parameters","text":"alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing). true_threshold \u2013 defaults to 0.0 Threshold for binarizing (mapping to booleans) features.","title":"Parameters"},{"location":"api/naive-bayes/BernoulliNB/#attributes","text":"class_counts ( collections.Counter ) Number of times each class has been seen. feature_counts ( collections.defaultdict ) Total frequencies per feature and class.","title":"Attributes"},{"location":"api/naive-bayes/BernoulliNB/#examples","text":">>> import math >>> from river import compose >>> from river import feature_extraction >>> from river import naive_bayes >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . BernoulliNB ( alpha = 1 )) ... ) >>> for sentence , label in docs : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 3 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Beijing' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Macao' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Shanghai' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Beijing' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Macao' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Shanghai' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> new_text = 'Chinese Chinese Chinese Tokyo Japan' >>> tokens = model [ 'tokenize' ] . transform_one ( new_text ) >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> math . exp ( jlh [ 'yes' ]) 0.005184 >>> math . exp ( jlh [ 'no' ]) 0.021947 >>> model . predict_one ( new_text ) 'no' >>> model . predict_proba_one ( 'test' )[ 'yes' ] 0.8831539823829913 You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'docs' , 'y' ]) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . BernoulliNB ( alpha = 1 )) ... ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 3 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Beijing' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Macao' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Shanghai' , 'yes' ) == ( 1 + 1 ) / ( 3 + 2 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Beijing' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Macao' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> cp ( 'Shanghai' , 'no' ) == ( 0 + 1 ) / ( 1 + 2 ) True >>> unseen_data = pd . Series ( ... [ 'Taiwanese Taipei' , 'Chinese Shanghai' ], ... name = 'docs' , ... index = [ 'river' , 'rocks' ] ... ) >>> model . predict_proba_many ( unseen_data ) no yes river 0.116846 0.883154 rocks 0.047269 0.952731 >>> model . predict_many ( unseen_data ) river yes rocks yes dtype : object >>> unseen_data = pd . Series ( ... [ 'test' ], name = 'docs' ) >>> model . predict_proba_many ( unseen_data ) no yes 0 0.116846 0.883154 >>> model . predict_proba_one ( 'test' ) { 'no' : 0.116846017617009 , 'yes' : 0.8831539823829913 } >>> bag = feature_extraction . BagOfWords ( lowercase = False ) >>> model = naive_bayes . BernoulliNB ( alpha = 1 ) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> X = bag . transform_many ( X ) >>> X = pd . DataFrame ( X . values , columns = X . columns , dtype = int ) >>> model = model . learn_many ( X , y ) >>> unseen_data = bag . transform_many ( unseen_data ) >>> unseen_data = pd . DataFrame ( unseen_data . values , columns = unseen_data . columns , index = [ 'river' ]) >>> model . predict_proba_many ( unseen_data ) no yes river 0.116846 0.883154","title":"Examples"},{"location":"api/naive-bayes/BernoulliNB/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many p_feature_given_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict )","title":"Methods"},{"location":"api/naive-bayes/BernoulliNB/#references","text":"The Bernoulli model \u21a9","title":"References"},{"location":"api/naive-bayes/ComplementNB/","text":"ComplementNB \u00b6 Naive Bayes classifier for multinomial models. Complement Naive Bayes model learns from occurrences between features such as word counts and discrete classes. ComplementNB is suitable for imbalance dataset. The input vector must contain positive values, such as counts or TF-IDF values. Parameters \u00b6 alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing). Attributes \u00b6 class_dist ( proba.Multinomial ) Class prior probability distribution. feature_counts ( collections.defaultdict ) Total frequencies per feature and class. class_totals ( collections.Counter ) Total frequencies per class. Examples \u00b6 >>> from river import feature_extraction >>> from river import naive_bayes >>> sentences = [ ... ( 'food food meat brain' , 'health' ), ... ( 'food meat ' + 'kitchen ' * 9 + 'job' * 5 , 'butcher' ), ... ( 'food food meat job' , 'health' ) ... ] >>> model = feature_extraction . BagOfWords () | ( 'nb' , naive_bayes . ComplementNB ) >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'health' ) == 2 / 3 True >>> model [ 'nb' ] . p_class ( 'butcher' ) == 1 / 3 True >>> model . predict_proba_one ( 'food job meat' ) { 'health' : 0.9409689355477155 , 'butcher' : 0.05903106445228467 } You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'food food meat brain' , 'health' ), ... ( 'food meat ' + 'kitchen ' * 9 + 'job' * 5 , 'butcher' ), ... ( 'food food meat job' , 'health' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'X' , 'y' ]) >>> X , y = docs [ 'X' ], docs [ 'y' ] >>> model = feature_extraction . BagOfWords () | ( 'nb' , naive_bayes . ComplementNB ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'health' ) == 2 / 3 True >>> model [ 'nb' ] . p_class ( 'butcher' ) == 1 / 3 True >>> model [ 'nb' ] . p_class_many () butcher health 0 0.333333 0.666667 >>> model . predict_proba_one ( 'food job meat' ) { 'butcher' : 0.05903106445228467 , 'health' : 0.9409689355477155 } >>> model . predict_proba_one ( 'Taiwanese Taipei' ) { 'butcher' : 0.3769230769230768 , 'health' : 0.6230769230769229 } >>> unseen_data = pd . Series ( ... [ 'food job meat' , 'Taiwanese Taipei' ], name = 'X' , index = [ 'river' , 'rocks' ]) >>> model . predict_proba_many ( unseen_data ) butcher health river 0.059031 0.940969 rocks 0.376923 0.623077 >>> model . predict_many ( unseen_data ) river health rocks health dtype : object Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict ) References \u00b6 Rennie, J.D., Shih, L., Teevan, J. and Karger, D.R., 2003. Tackling the poor assumptions of naive bayes text classifiers. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 616-623) \u21a9 StackExchange discussion \u21a9","title":"ComplementNB"},{"location":"api/naive-bayes/ComplementNB/#complementnb","text":"Naive Bayes classifier for multinomial models. Complement Naive Bayes model learns from occurrences between features such as word counts and discrete classes. ComplementNB is suitable for imbalance dataset. The input vector must contain positive values, such as counts or TF-IDF values.","title":"ComplementNB"},{"location":"api/naive-bayes/ComplementNB/#parameters","text":"alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing).","title":"Parameters"},{"location":"api/naive-bayes/ComplementNB/#attributes","text":"class_dist ( proba.Multinomial ) Class prior probability distribution. feature_counts ( collections.defaultdict ) Total frequencies per feature and class. class_totals ( collections.Counter ) Total frequencies per class.","title":"Attributes"},{"location":"api/naive-bayes/ComplementNB/#examples","text":">>> from river import feature_extraction >>> from river import naive_bayes >>> sentences = [ ... ( 'food food meat brain' , 'health' ), ... ( 'food meat ' + 'kitchen ' * 9 + 'job' * 5 , 'butcher' ), ... ( 'food food meat job' , 'health' ) ... ] >>> model = feature_extraction . BagOfWords () | ( 'nb' , naive_bayes . ComplementNB ) >>> for sentence , label in sentences : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'health' ) == 2 / 3 True >>> model [ 'nb' ] . p_class ( 'butcher' ) == 1 / 3 True >>> model . predict_proba_one ( 'food job meat' ) { 'health' : 0.9409689355477155 , 'butcher' : 0.05903106445228467 } You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'food food meat brain' , 'health' ), ... ( 'food meat ' + 'kitchen ' * 9 + 'job' * 5 , 'butcher' ), ... ( 'food food meat job' , 'health' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'X' , 'y' ]) >>> X , y = docs [ 'X' ], docs [ 'y' ] >>> model = feature_extraction . BagOfWords () | ( 'nb' , naive_bayes . ComplementNB ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'health' ) == 2 / 3 True >>> model [ 'nb' ] . p_class ( 'butcher' ) == 1 / 3 True >>> model [ 'nb' ] . p_class_many () butcher health 0 0.333333 0.666667 >>> model . predict_proba_one ( 'food job meat' ) { 'butcher' : 0.05903106445228467 , 'health' : 0.9409689355477155 } >>> model . predict_proba_one ( 'Taiwanese Taipei' ) { 'butcher' : 0.3769230769230768 , 'health' : 0.6230769230769229 } >>> unseen_data = pd . Series ( ... [ 'food job meat' , 'Taiwanese Taipei' ], name = 'X' , index = [ 'river' , 'rocks' ]) >>> model . predict_proba_many ( unseen_data ) butcher health river 0.059031 0.940969 rocks 0.376923 0.623077 >>> model . predict_many ( unseen_data ) river health rocks health dtype : object","title":"Examples"},{"location":"api/naive-bayes/ComplementNB/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict )","title":"Methods"},{"location":"api/naive-bayes/ComplementNB/#references","text":"Rennie, J.D., Shih, L., Teevan, J. and Karger, D.R., 2003. Tackling the poor assumptions of naive bayes text classifiers. In Proceedings of the 20th international conference on machine learning (ICML-03) (pp. 616-623) \u21a9 StackExchange discussion \u21a9","title":"References"},{"location":"api/naive-bayes/GaussianNB/","text":"GaussianNB \u00b6 Gaussian Naive Bayes. A Gaussian distribution \\(G_{cf}\\) is maintained for each class \\(c\\) and each feature \\(f\\) . Each Gaussian is updated using the amount associated with each feature; the details can be be found in proba.Gaussian . The joint log-likelihood is then obtained by summing the log probabilities of each feature associated with each class. Examples \u00b6 >>> from river import naive_bayes >>> from river import stream >>> import numpy as np >>> X = np . array ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ - 3 , - 2 ], [ 1 , 1 ], [ 2 , 1 ], [ 3 , 2 ]]) >>> Y = np . array ([ 1 , 1 , 1 , 2 , 2 , 2 ]) >>> model = naive_bayes . GaussianNB () >>> for x , y in stream . iter_array ( X , Y ): ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 0 : - 0.8 , 1 : - 1 }) 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Compute the unnormalized posterior log-likelihood of x. The log-likelihood is log P(c) + log P(x|c) . Parameters x ( dict ) joint_log_likelihood_many Compute the unnormalized posterior log-likelihood of x in mini-batches. The log-likelihood is log P(c) + log P(x|c) . Parameters X ( pandas.core.frame.DataFrame ) learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict )","title":"GaussianNB"},{"location":"api/naive-bayes/GaussianNB/#gaussiannb","text":"Gaussian Naive Bayes. A Gaussian distribution \\(G_{cf}\\) is maintained for each class \\(c\\) and each feature \\(f\\) . Each Gaussian is updated using the amount associated with each feature; the details can be be found in proba.Gaussian . The joint log-likelihood is then obtained by summing the log probabilities of each feature associated with each class.","title":"GaussianNB"},{"location":"api/naive-bayes/GaussianNB/#examples","text":">>> from river import naive_bayes >>> from river import stream >>> import numpy as np >>> X = np . array ([[ - 1 , - 1 ], [ - 2 , - 1 ], [ - 3 , - 2 ], [ 1 , 1 ], [ 2 , 1 ], [ 3 , 2 ]]) >>> Y = np . array ([ 1 , 1 , 1 , 2 , 2 , 2 ]) >>> model = naive_bayes . GaussianNB () >>> for x , y in stream . iter_array ( X , Y ): ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 0 : - 0.8 , 1 : - 1 }) 1","title":"Examples"},{"location":"api/naive-bayes/GaussianNB/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Compute the unnormalized posterior log-likelihood of x. The log-likelihood is log P(c) + log P(x|c) . Parameters x ( dict ) joint_log_likelihood_many Compute the unnormalized posterior log-likelihood of x in mini-batches. The log-likelihood is log P(c) + log P(x|c) . Parameters X ( pandas.core.frame.DataFrame ) learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict )","title":"Methods"},{"location":"api/naive-bayes/MultinomialNB/","text":"MultinomialNB \u00b6 Naive Bayes classifier for multinomial models. Multinomial Naive Bayes model learns from occurrences between features such as word counts and discrete classes. The input vector must contain positive values, such as counts or TF-IDF values. Parameters \u00b6 alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing). Attributes \u00b6 class_dist ( proba.Multinomial ) Class prior probability distribution. feature_counts ( collections.defaultdict ) Total frequencies per feature and class. class_totals ( collections.Counter ) Total frequencies per class. Examples \u00b6 >>> import math >>> from river import compose >>> from river import feature_extraction >>> from river import naive_bayes >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . MultinomialNB ( alpha = 1 )) ... ) >>> for sentence , label in docs : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 5 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> new_text = 'Chinese Chinese Chinese Tokyo Japan' >>> tokens = model [ 'tokenize' ] . transform_one ( new_text ) >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> math . exp ( jlh [ 'yes' ]) 0.000301 >>> math . exp ( jlh [ 'no' ]) 0.000135 >>> model . predict_one ( new_text ) 'yes' >>> new_unseen_text = 'Taiwanese Taipei' >>> tokens = model [ 'tokenize' ] . transform_one ( new_unseen_text ) >>> # P(Taiwanese|yes) >>> # = (N_Taiwanese_yes + 1) / (N_yes + N_terms) >>> cp ( 'Taiwanese' , 'yes' ) == cp ( 'Taipei' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Taiwanese' , 'no' ) == cp ( 'Taipei' , 'no' ) == ( 0 + 1 ) / ( 3 + 6 ) True >>> # P(yes|Taiwanese Taipei) >>> # \u221d P(Taiwanese|yes) * P(Taipei|yes) * P(yes) >>> posterior_yes_given_new_text = ( 0 + 1 ) / ( 8 + 6 ) * ( 0 + 1 ) / ( 8 + 6 ) * 0.75 >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> jlh [ 'yes' ] == math . log ( posterior_yes_given_new_text ) True >>> model . predict_one ( new_unseen_text ) 'yes' You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'docs' , 'y' ]) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . MultinomialNB ( alpha = 1 )) ... ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 5 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> unseen_data = pd . Series ( ... [ 'Taiwanese Taipei' , 'Chinese Shanghai' ], name = 'docs' , index = [ 'river' , 'rocks' ]) >>> model . predict_proba_many ( unseen_data ) no yes river 0.446469 0.553531 rocks 0.118501 0.881499 >>> model . predict_many ( unseen_data ) river yes rocks yes dtype : object Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many p_feature_given_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict ) References \u00b6 Naive Bayes text classification \u21a9","title":"MultinomialNB"},{"location":"api/naive-bayes/MultinomialNB/#multinomialnb","text":"Naive Bayes classifier for multinomial models. Multinomial Naive Bayes model learns from occurrences between features such as word counts and discrete classes. The input vector must contain positive values, such as counts or TF-IDF values.","title":"MultinomialNB"},{"location":"api/naive-bayes/MultinomialNB/#parameters","text":"alpha \u2013 defaults to 1.0 Additive (Laplace/Lidstone) smoothing parameter (use 0 for no smoothing).","title":"Parameters"},{"location":"api/naive-bayes/MultinomialNB/#attributes","text":"class_dist ( proba.Multinomial ) Class prior probability distribution. feature_counts ( collections.defaultdict ) Total frequencies per feature and class. class_totals ( collections.Counter ) Total frequencies per class.","title":"Attributes"},{"location":"api/naive-bayes/MultinomialNB/#examples","text":">>> import math >>> from river import compose >>> from river import feature_extraction >>> from river import naive_bayes >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . MultinomialNB ( alpha = 1 )) ... ) >>> for sentence , label in docs : ... model = model . learn_one ( sentence , label ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 5 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> new_text = 'Chinese Chinese Chinese Tokyo Japan' >>> tokens = model [ 'tokenize' ] . transform_one ( new_text ) >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> math . exp ( jlh [ 'yes' ]) 0.000301 >>> math . exp ( jlh [ 'no' ]) 0.000135 >>> model . predict_one ( new_text ) 'yes' >>> new_unseen_text = 'Taiwanese Taipei' >>> tokens = model [ 'tokenize' ] . transform_one ( new_unseen_text ) >>> # P(Taiwanese|yes) >>> # = (N_Taiwanese_yes + 1) / (N_yes + N_terms) >>> cp ( 'Taiwanese' , 'yes' ) == cp ( 'Taipei' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Taiwanese' , 'no' ) == cp ( 'Taipei' , 'no' ) == ( 0 + 1 ) / ( 3 + 6 ) True >>> # P(yes|Taiwanese Taipei) >>> # \u221d P(Taiwanese|yes) * P(Taipei|yes) * P(yes) >>> posterior_yes_given_new_text = ( 0 + 1 ) / ( 8 + 6 ) * ( 0 + 1 ) / ( 8 + 6 ) * 0.75 >>> jlh = model [ 'nb' ] . joint_log_likelihood ( tokens ) >>> jlh [ 'yes' ] == math . log ( posterior_yes_given_new_text ) True >>> model . predict_one ( new_unseen_text ) 'yes' You can train the model and make predictions in mini-batch mode using the class methods learn_many and predict_many . >>> import pandas as pd >>> docs = [ ... ( 'Chinese Beijing Chinese' , 'yes' ), ... ( 'Chinese Chinese Shanghai' , 'yes' ), ... ( 'Chinese Macao' , 'yes' ), ... ( 'Tokyo Japan Chinese' , 'no' ) ... ] >>> docs = pd . DataFrame ( docs , columns = [ 'docs' , 'y' ]) >>> X , y = docs [ 'docs' ], docs [ 'y' ] >>> model = compose . Pipeline ( ... ( 'tokenize' , feature_extraction . BagOfWords ( lowercase = False )), ... ( 'nb' , naive_bayes . MultinomialNB ( alpha = 1 )) ... ) >>> model = model . learn_many ( X , y ) >>> model [ 'nb' ] . p_class ( 'yes' ) 0.75 >>> model [ 'nb' ] . p_class ( 'no' ) 0.25 >>> cp = model [ 'nb' ] . p_feature_given_class >>> cp ( 'Chinese' , 'yes' ) == ( 5 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Tokyo' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Japan' , 'yes' ) == ( 0 + 1 ) / ( 8 + 6 ) True >>> cp ( 'Chinese' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Tokyo' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> cp ( 'Japan' , 'no' ) == ( 1 + 1 ) / ( 3 + 6 ) True >>> unseen_data = pd . Series ( ... [ 'Taiwanese Taipei' , 'Chinese Shanghai' ], name = 'docs' , index = [ 'river' , 'rocks' ]) >>> model . predict_proba_many ( unseen_data ) no yes river 0.446469 0.553531 rocks 0.118501 0.881499 >>> model . predict_many ( unseen_data ) river yes rocks yes dtype : object","title":"Examples"},{"location":"api/naive-bayes/MultinomialNB/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. joint_log_likelihood Computes the joint log likelihood of input features. Parameters x ( dict ) Returns float : Mapping between classes and joint log likelihood. joint_log_likelihood_many Computes the joint log likelihood of input features. Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : Input samples joint log likelihood. learn_many Updates the model with a term-frequency or TF-IDF pandas dataframe. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.series.Series ) Returns self learn_one Updates the model with a single observation. Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self p_class p_class_many p_feature_given_class predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Return probabilities using the log-likelihoods in mini-batchs setting. Parameters X ( pandas.core.frame.DataFrame ) predict_proba_one Return probabilities using the log-likelihoods. Parameters x ( dict )","title":"Methods"},{"location":"api/naive-bayes/MultinomialNB/#references","text":"Naive Bayes text classification \u21a9","title":"References"},{"location":"api/neighbors/KNNADWINClassifier/","text":"KNNADWINClassifier \u00b6 K-Nearest Neighbors classifier with ADWIN change detector. This classifier is an improvement from the regular kNN method, as it is resistant to concept drift. It uses the ADWIN change detector to decide which samples to keep and which ones to forget, and by doing so it regulates the sample window size. Parameters \u00b6 n_neighbors \u2013 defaults to 5 The number of nearest neighbors to search for. window_size \u2013 defaults to 1000 The maximum size of the window storing the last viewed samples. leaf_size \u2013 defaults to 30 The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\) Examples \u00b6 >>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> dataset = synth . ConceptDriftStream ( position = 500 , width = 20 , seed = 1 ) . take ( 1000 ) >>> model = neighbors . KNNADWINClassifier ( window_size = 100 ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 56.66 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns proba reset Reset estimator. Notes \u00b6 This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical. This implementation is extended from the KNNClassifier, with the main difference that it keeps a dynamic window whose size changes in agreement with the amount of change detected by the ADWIN drift detector.","title":"KNNADWINClassifier"},{"location":"api/neighbors/KNNADWINClassifier/#knnadwinclassifier","text":"K-Nearest Neighbors classifier with ADWIN change detector. This classifier is an improvement from the regular kNN method, as it is resistant to concept drift. It uses the ADWIN change detector to decide which samples to keep and which ones to forget, and by doing so it regulates the sample window size.","title":"KNNADWINClassifier"},{"location":"api/neighbors/KNNADWINClassifier/#parameters","text":"n_neighbors \u2013 defaults to 5 The number of nearest neighbors to search for. window_size \u2013 defaults to 1000 The maximum size of the window storing the last viewed samples. leaf_size \u2013 defaults to 30 The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\)","title":"Parameters"},{"location":"api/neighbors/KNNADWINClassifier/#examples","text":">>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> dataset = synth . ConceptDriftStream ( position = 500 , width = 20 , seed = 1 ) . take ( 1000 ) >>> model = neighbors . KNNADWINClassifier ( window_size = 100 ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 56.66 %","title":"Examples"},{"location":"api/neighbors/KNNADWINClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns proba reset Reset estimator.","title":"Methods"},{"location":"api/neighbors/KNNADWINClassifier/#notes","text":"This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical. This implementation is extended from the KNNClassifier, with the main difference that it keeps a dynamic window whose size changes in agreement with the amount of change detected by the ADWIN drift detector.","title":"Notes"},{"location":"api/neighbors/KNNClassifier/","text":"KNNClassifier \u00b6 k-Nearest Neighbors classifier. This non-parametric classification method keeps track of the last window_size training samples. The predicted class-label for a given query sample is obtained in two steps: Find the closest n_neighbors to the query sample in the data window. 2. Aggregate the class-labels of the n_neighbors to define the predicted class for the query sample. Parameters \u00b6 n_neighbors ( int ) \u2013 defaults to 5 The number of nearest neighbors to search for. window_size ( int ) \u2013 defaults to 1000 The maximum size of the window storing the last observed samples. leaf_size ( int ) \u2013 defaults to 30 scipy.spatial.cKDTree parameter. The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p ( float ) \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\) weighted ( bool ) \u2013 defaults to True Whether to weight the contribution of each neighbor by it's inverse distance or not. kwargs Other parameters passed to scipy.spatial.cKDTree . Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... neighbors . KNNClassifier () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.07 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns proba reset Reset estimator. Notes \u00b6 This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical.","title":"KNNClassifier"},{"location":"api/neighbors/KNNClassifier/#knnclassifier","text":"k-Nearest Neighbors classifier. This non-parametric classification method keeps track of the last window_size training samples. The predicted class-label for a given query sample is obtained in two steps: Find the closest n_neighbors to the query sample in the data window. 2. Aggregate the class-labels of the n_neighbors to define the predicted class for the query sample.","title":"KNNClassifier"},{"location":"api/neighbors/KNNClassifier/#parameters","text":"n_neighbors ( int ) \u2013 defaults to 5 The number of nearest neighbors to search for. window_size ( int ) \u2013 defaults to 1000 The maximum size of the window storing the last observed samples. leaf_size ( int ) \u2013 defaults to 30 scipy.spatial.cKDTree parameter. The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p ( float ) \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\) weighted ( bool ) \u2013 defaults to True Whether to weight the contribution of each neighbor by it's inverse distance or not. kwargs Other parameters passed to scipy.spatial.cKDTree .","title":"Parameters"},{"location":"api/neighbors/KNNClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> model = ( ... preprocessing . StandardScaler () | ... neighbors . KNNClassifier () ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 88.07 %","title":"Examples"},{"location":"api/neighbors/KNNClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x y Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns proba reset Reset estimator.","title":"Methods"},{"location":"api/neighbors/KNNClassifier/#notes","text":"This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical.","title":"Notes"},{"location":"api/neighbors/KNNRegressor/","text":"KNNRegressor \u00b6 k-Nearest Neighbors regressor. This non-parametric regression method keeps track of the last window_size training samples. Predictions are obtained by aggregating the values of the closest n_neighbors stored-samples with respect to a query sample. Parameters \u00b6 n_neighbors ( int ) \u2013 defaults to 5 The number of nearest neighbors to search for. window_size ( int ) \u2013 defaults to 1000 The maximum size of the window storing the last observed samples. leaf_size ( int ) \u2013 defaults to 30 scipy.spatial.cKDTree parameter. The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p ( float ) \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\) aggregation_method ( str ) \u2013 defaults to mean The method to aggregate the target values of neighbors. | 'mean' | 'median' | 'weighted_mean' kwargs Other parameters passed to scipy.spatial.cKDTree. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... neighbors . KNNRegressor ( window_size = 50 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.441308 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a real target value y . Parameters x y Returns self predict_one Predict the target value of a set of features x . Search the KDTree for the n_neighbors nearest neighbors. Parameters x Returns The prediction. reset Reset estimator. Notes \u00b6 This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical.","title":"KNNRegressor"},{"location":"api/neighbors/KNNRegressor/#knnregressor","text":"k-Nearest Neighbors regressor. This non-parametric regression method keeps track of the last window_size training samples. Predictions are obtained by aggregating the values of the closest n_neighbors stored-samples with respect to a query sample.","title":"KNNRegressor"},{"location":"api/neighbors/KNNRegressor/#parameters","text":"n_neighbors ( int ) \u2013 defaults to 5 The number of nearest neighbors to search for. window_size ( int ) \u2013 defaults to 1000 The maximum size of the window storing the last observed samples. leaf_size ( int ) \u2013 defaults to 30 scipy.spatial.cKDTree parameter. The maximum number of samples that can be stored in one leaf node, which determines from which point the algorithm will switch for a brute-force approach. The bigger this number the faster the tree construction time, but the slower the query time will be. p ( float ) \u2013 defaults to 2 p-norm value for the Minkowski metric. When p=1 , this corresponds to the Manhattan distance, while p=2 corresponds to the Euclidean distance. Valid values are in the interval \\([1, +\\infty)\\) aggregation_method ( str ) \u2013 defaults to mean The method to aggregate the target values of neighbors. | 'mean' | 'median' | 'weighted_mean' kwargs Other parameters passed to scipy.spatial.cKDTree.","title":"Parameters"},{"location":"api/neighbors/KNNRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... neighbors . KNNRegressor ( window_size = 50 ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.441308","title":"Examples"},{"location":"api/neighbors/KNNRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a real target value y . Parameters x y Returns self predict_one Predict the target value of a set of features x . Search the KDTree for the n_neighbors nearest neighbors. Parameters x Returns The prediction. reset Reset estimator.","title":"Methods"},{"location":"api/neighbors/KNNRegressor/#notes","text":"This estimator is not optimal for a mixture of categorical and numerical features. This implementation treats all features from a given stream as numerical.","title":"Notes"},{"location":"api/neighbors/SAMKNNClassifier/","text":"SAMKNNClassifier \u00b6 Self Adjusting Memory coupled with the kNN classifier. The Self Adjusting Memory (SAM) 1 model builds an ensemble with models targeting current or former concepts. SAM is built using two memories: STM for the current concept, and the LTM to retain information about past concepts. A cleaning process is in charge of controlling the size of the STM while keeping the information in the LTM consistent with the STM. Parameters \u00b6 n_neighbors ( int ) \u2013 defaults to 5 number of evaluated nearest neighbors. distance_weighting \u2013 defaults to True Type of weighting of the nearest neighbors. It True will use 'distance'. Otherwise, will use 'uniform' (majority voting). window_size ( int ) \u2013 defaults to 5000 Maximum number of overall stored data points. ltm_size ( float ) \u2013 defaults to 0.4 Proportion of the overall instances that may be used for the LTM. This is only relevant when the maximum number(maxSize) of stored instances is reached. min_stm_size ( int ) \u2013 defaults to 50 Minimum STM size which is evaluated during the STM size adaption. stm_aprox_adaption \u2013 defaults to True Type of STM size adaption. - If True approximates the interleaved test-train error and is significantly faster than the exact version. - If False calculates the interleaved test-train error exactly for each of the evaluated window sizes, which often has to be recalculated from the scratch. - If None , the STM is not adapted at all. If additionally use_ltm=False , then this algorithm is simply a kNN with fixed sliding window size. use_ltm \u2013 defaults to True Specifies whether the LTM should be used at all. Attributes \u00b6 LTMLabels Class labels in the LTM. LTMSamples Samples in the LTM. STMLabels Class labels in the STM. STMSamples Samples in the STM. Examples \u00b6 >>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> dataset = synth . ConceptDriftStream ( position = 500 , width = 20 , seed = 1 ) . take ( 1000 ) >>> model = neighbors . SAMKNNClassifier ( window_size = 100 ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) # doctest: +SKIP Accuracy : 56.70 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label. Notes \u00b6 This modules uses libNearestNeighbor, a C++ library used to speed up some of the algorithm's computations. When invoking the library's functions it's important to pass the right argument type. Although most of this framework's functionality will work with python standard types, the C++ library will work with 8-bit labels, which is already done by the SAMKNN class, but may be absent in custom classes that use SAMKNN static methods, or other custom functions that use the C++ library. References \u00b6 Losing, Viktor, Barbara Hammer, and Heiko Wersing. \"Knn classifier with self adjusting memory for heterogeneous concept drift.\" In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pp. 291-300. IEEE, 2016. \u21a9","title":"SAMKNNClassifier"},{"location":"api/neighbors/SAMKNNClassifier/#samknnclassifier","text":"Self Adjusting Memory coupled with the kNN classifier. The Self Adjusting Memory (SAM) 1 model builds an ensemble with models targeting current or former concepts. SAM is built using two memories: STM for the current concept, and the LTM to retain information about past concepts. A cleaning process is in charge of controlling the size of the STM while keeping the information in the LTM consistent with the STM.","title":"SAMKNNClassifier"},{"location":"api/neighbors/SAMKNNClassifier/#parameters","text":"n_neighbors ( int ) \u2013 defaults to 5 number of evaluated nearest neighbors. distance_weighting \u2013 defaults to True Type of weighting of the nearest neighbors. It True will use 'distance'. Otherwise, will use 'uniform' (majority voting). window_size ( int ) \u2013 defaults to 5000 Maximum number of overall stored data points. ltm_size ( float ) \u2013 defaults to 0.4 Proportion of the overall instances that may be used for the LTM. This is only relevant when the maximum number(maxSize) of stored instances is reached. min_stm_size ( int ) \u2013 defaults to 50 Minimum STM size which is evaluated during the STM size adaption. stm_aprox_adaption \u2013 defaults to True Type of STM size adaption. - If True approximates the interleaved test-train error and is significantly faster than the exact version. - If False calculates the interleaved test-train error exactly for each of the evaluated window sizes, which often has to be recalculated from the scratch. - If None , the STM is not adapted at all. If additionally use_ltm=False , then this algorithm is simply a kNN with fixed sliding window size. use_ltm \u2013 defaults to True Specifies whether the LTM should be used at all.","title":"Parameters"},{"location":"api/neighbors/SAMKNNClassifier/#attributes","text":"LTMLabels Class labels in the LTM. LTMSamples Samples in the LTM. STMLabels Class labels in the STM. STMSamples Samples in the STM.","title":"Attributes"},{"location":"api/neighbors/SAMKNNClassifier/#examples","text":">>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import neighbors >>> dataset = synth . ConceptDriftStream ( position = 500 , width = 20 , seed = 1 ) . take ( 1000 ) >>> model = neighbors . SAMKNNClassifier ( window_size = 100 ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) # doctest: +SKIP Accuracy : 56.70 %","title":"Examples"},{"location":"api/neighbors/SAMKNNClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update the model with a set of features x and a label y . Parameters x ( dict ) y ( Union[bool, str, int] ) Returns Classifier : self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x ( dict ) Returns typing.Dict[typing.Union[bool, str, int], float] : A dictionary that associates a probability which each label.","title":"Methods"},{"location":"api/neighbors/SAMKNNClassifier/#notes","text":"This modules uses libNearestNeighbor, a C++ library used to speed up some of the algorithm's computations. When invoking the library's functions it's important to pass the right argument type. Although most of this framework's functionality will work with python standard types, the C++ library will work with 8-bit labels, which is already done by the SAMKNN class, but may be absent in custom classes that use SAMKNN static methods, or other custom functions that use the C++ library.","title":"Notes"},{"location":"api/neighbors/SAMKNNClassifier/#references","text":"Losing, Viktor, Barbara Hammer, and Heiko Wersing. \"Knn classifier with self adjusting memory for heterogeneous concept drift.\" In Data Mining (ICDM), 2016 IEEE 16th International Conference on, pp. 291-300. IEEE, 2016. \u21a9","title":"References"},{"location":"api/neural-net/MLPRegressor/","text":"MLPRegressor \u00b6 Multi-layer Perceptron for regression. This model is still work in progress. Here are some features that still need implementing: learn_one and predict_one just cast the input dict to a single row dataframe and then call learn_many and predict_many respectively. This is very inefficient. - Not all of the optimizers in the optim module can be used as they are not all vectorised. - Emerging and disappearing features are not supported. Each instance/batch has to have the same features. - The gradient haven't been numerically checked. Parameters \u00b6 hidden_dims The dimensions of the hidden layers. For example, specifying (10, 20) means that there are two hidden layers with 10 and 20 neurons, respectively. Note that the number of layers the network contains is equal to the number of hidden layers plus two (to account for the input and output layers). activations The activation functions to use at each layer, including the input and output layers. Therefore you need to specify three activation if you specify one hidden layer. loss ( optim.losses.Loss ) \u2013 defaults to None Loss function. Defaults to optim.losses.Squared . optimizer ( optim.Optimizer ) \u2013 defaults to None Optimizer. Defaults to optim.SGD(.01) . seed ( int ) \u2013 defaults to None Random number generation seed. Set this for reproducibility. Attributes \u00b6 n_layers Return the number of layers in the network. The number of layers is equal to the number of hidden layers plus 2. The 2 accounts for the input layer and the output layer. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import neural_net as nn >>> from river import optim >>> from river import preprocessing as pp >>> from river import metrics >>> model = ( ... pp . StandardScaler () | ... nn . MLPRegressor ( ... hidden_dims = ( 5 ,), ... activations = ( ... nn . activations . ReLU , ... nn . activations . ReLU , ... nn . activations . Identity ... ), ... optimizer = optim . SGD ( 1e-3 ), ... seed = 42 ... ) ... ) >>> dataset = datasets . TrumpApproval () >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.589827 You can also use this to process mini-batches of data. >>> model = ( ... pp . StandardScaler () | ... nn . MLPRegressor ( ... hidden_dims = ( 10 ,), ... activations = ( ... nn . activations . ReLU , ... nn . activations . ReLU , ... nn . activations . ReLU ... ), ... optimizer = optim . SGD ( 1e-4 ), ... seed = 42 ... ) ... ) >>> dataset = datasets . TrumpApproval () >>> batch_size = 32 >>> for epoch in range ( 10 ): ... for xb in pd . read_csv ( dataset . path , chunksize = batch_size ): ... yb = xb . pop ( 'five_thirty_eight' ) ... y_pred = model . predict_many ( xb ) ... model = model . learn_many ( xb , yb ) >>> model . predict_many ( xb ) five_thirty_eight 992 39.361609 993 46.398536 994 42.094086 995 40.195802 996 40.782954 997 40.839678 998 40.896403 999 48.362659 1000 42.021849 Methods \u00b6 call Make predictions. Parameters X ( pandas.core.frame.DataFrame ) clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Train the network. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.frame.DataFrame ) learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_many predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"MLPRegressor"},{"location":"api/neural-net/MLPRegressor/#mlpregressor","text":"Multi-layer Perceptron for regression. This model is still work in progress. Here are some features that still need implementing: learn_one and predict_one just cast the input dict to a single row dataframe and then call learn_many and predict_many respectively. This is very inefficient. - Not all of the optimizers in the optim module can be used as they are not all vectorised. - Emerging and disappearing features are not supported. Each instance/batch has to have the same features. - The gradient haven't been numerically checked.","title":"MLPRegressor"},{"location":"api/neural-net/MLPRegressor/#parameters","text":"hidden_dims The dimensions of the hidden layers. For example, specifying (10, 20) means that there are two hidden layers with 10 and 20 neurons, respectively. Note that the number of layers the network contains is equal to the number of hidden layers plus two (to account for the input and output layers). activations The activation functions to use at each layer, including the input and output layers. Therefore you need to specify three activation if you specify one hidden layer. loss ( optim.losses.Loss ) \u2013 defaults to None Loss function. Defaults to optim.losses.Squared . optimizer ( optim.Optimizer ) \u2013 defaults to None Optimizer. Defaults to optim.SGD(.01) . seed ( int ) \u2013 defaults to None Random number generation seed. Set this for reproducibility.","title":"Parameters"},{"location":"api/neural-net/MLPRegressor/#attributes","text":"n_layers Return the number of layers in the network. The number of layers is equal to the number of hidden layers plus 2. The 2 accounts for the input layer and the output layer.","title":"Attributes"},{"location":"api/neural-net/MLPRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import neural_net as nn >>> from river import optim >>> from river import preprocessing as pp >>> from river import metrics >>> model = ( ... pp . StandardScaler () | ... nn . MLPRegressor ( ... hidden_dims = ( 5 ,), ... activations = ( ... nn . activations . ReLU , ... nn . activations . ReLU , ... nn . activations . Identity ... ), ... optimizer = optim . SGD ( 1e-3 ), ... seed = 42 ... ) ... ) >>> dataset = datasets . TrumpApproval () >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.589827 You can also use this to process mini-batches of data. >>> model = ( ... pp . StandardScaler () | ... nn . MLPRegressor ( ... hidden_dims = ( 10 ,), ... activations = ( ... nn . activations . ReLU , ... nn . activations . ReLU , ... nn . activations . ReLU ... ), ... optimizer = optim . SGD ( 1e-4 ), ... seed = 42 ... ) ... ) >>> dataset = datasets . TrumpApproval () >>> batch_size = 32 >>> for epoch in range ( 10 ): ... for xb in pd . read_csv ( dataset . path , chunksize = batch_size ): ... yb = xb . pop ( 'five_thirty_eight' ) ... y_pred = model . predict_many ( xb ) ... model = model . learn_many ( xb , yb ) >>> model . predict_many ( xb ) five_thirty_eight 992 39.361609 993 46.398536 994 42.094086 995 40.195802 996 40.782954 997 40.839678 998 40.896403 999 48.362659 1000 42.021849","title":"Examples"},{"location":"api/neural-net/MLPRegressor/#methods","text":"call Make predictions. Parameters X ( pandas.core.frame.DataFrame ) clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Train the network. Parameters X ( pandas.core.frame.DataFrame ) y ( pandas.core.frame.DataFrame ) learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_many predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/neural-net/activations/Identity/","text":"Identity \u00b6 Identity activation function. Methods \u00b6 apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"Identity"},{"location":"api/neural-net/activations/Identity/#identity","text":"Identity activation function.","title":"Identity"},{"location":"api/neural-net/activations/Identity/#methods","text":"apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"Methods"},{"location":"api/neural-net/activations/ReLU/","text":"ReLU \u00b6 Rectified Linear Unit (ReLU) activation function. Methods \u00b6 apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"ReLU"},{"location":"api/neural-net/activations/ReLU/#relu","text":"Rectified Linear Unit (ReLU) activation function.","title":"ReLU"},{"location":"api/neural-net/activations/ReLU/#methods","text":"apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"Methods"},{"location":"api/neural-net/activations/Sigmoid/","text":"Sigmoid \u00b6 Sigmoid activation function. Methods \u00b6 apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"Sigmoid"},{"location":"api/neural-net/activations/Sigmoid/#sigmoid","text":"Sigmoid activation function.","title":"Sigmoid"},{"location":"api/neural-net/activations/Sigmoid/#methods","text":"apply Apply the activation function to a layer output z. z gradient Return the gradient with respect to a layer output z. z","title":"Methods"},{"location":"api/optim/AMSGrad/","text":"AMSGrad \u00b6 AMSGrad optimizer. Parameters \u00b6 lr ( Union[float, optim.schedulers.Scheduler ] ) \u2013 defaults to 0.1 The learning rate. beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 correct_bias \u2013 defaults to True Attributes \u00b6 m ( collections.defaultdict ) v ( collections.defaultdict ) v_hat ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AMSGrad () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.865724 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Reddi, S.J., Kale, S. and Kumar, S., 2019. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237 \u21a9","title":"AMSGrad"},{"location":"api/optim/AMSGrad/#amsgrad","text":"AMSGrad optimizer.","title":"AMSGrad"},{"location":"api/optim/AMSGrad/#parameters","text":"lr ( Union[float, optim.schedulers.Scheduler ] ) \u2013 defaults to 0.1 The learning rate. beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 correct_bias \u2013 defaults to True","title":"Parameters"},{"location":"api/optim/AMSGrad/#attributes","text":"m ( collections.defaultdict ) v ( collections.defaultdict ) v_hat ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/AMSGrad/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AMSGrad () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.865724","title":"Examples"},{"location":"api/optim/AMSGrad/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/AMSGrad/#references","text":"Reddi, S.J., Kale, S. and Kumar, S., 2019. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237 \u21a9","title":"References"},{"location":"api/optim/AdaBound/","text":"AdaBound \u00b6 AdaBound optimizer. Parameters \u00b6 lr \u2013 defaults to 0.001 The learning rate. beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 gamma \u2013 defaults to 0.001 final_lr \u2013 defaults to 0.1 Attributes \u00b6 m ( collections.defaultdict ) s ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaBound () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.879004 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Luo, L., Xiong, Y., Liu, Y. and Sun, X., 2019. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843 \u21a9","title":"AdaBound"},{"location":"api/optim/AdaBound/#adabound","text":"AdaBound optimizer.","title":"AdaBound"},{"location":"api/optim/AdaBound/#parameters","text":"lr \u2013 defaults to 0.001 The learning rate. beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 gamma \u2013 defaults to 0.001 final_lr \u2013 defaults to 0.1","title":"Parameters"},{"location":"api/optim/AdaBound/#attributes","text":"m ( collections.defaultdict ) s ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/AdaBound/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaBound () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.879004","title":"Examples"},{"location":"api/optim/AdaBound/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/AdaBound/#references","text":"Luo, L., Xiong, Y., Liu, Y. and Sun, X., 2019. Adaptive gradient methods with dynamic bound of learning rate. arXiv preprint arXiv:1902.09843 \u21a9","title":"References"},{"location":"api/optim/AdaDelta/","text":"AdaDelta \u00b6 AdaDelta optimizer. Parameters \u00b6 rho \u2013 defaults to 0.95 eps \u2013 defaults to 1e-08 Attributes \u00b6 g2 ( collections.defaultdict ) s2 ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaDelta () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.805611 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Zeiler, M.D., 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. \u21a9","title":"AdaDelta"},{"location":"api/optim/AdaDelta/#adadelta","text":"AdaDelta optimizer.","title":"AdaDelta"},{"location":"api/optim/AdaDelta/#parameters","text":"rho \u2013 defaults to 0.95 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/AdaDelta/#attributes","text":"g2 ( collections.defaultdict ) s2 ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/AdaDelta/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaDelta () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.805611","title":"Examples"},{"location":"api/optim/AdaDelta/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/AdaDelta/#references","text":"Zeiler, M.D., 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. \u21a9","title":"References"},{"location":"api/optim/AdaGrad/","text":"AdaGrad \u00b6 AdaGrad optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 eps \u2013 defaults to 1e-08 Attributes \u00b6 g2 ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaGrad () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.880143 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul), pp.2121-2159. \u21a9","title":"AdaGrad"},{"location":"api/optim/AdaGrad/#adagrad","text":"AdaGrad optimizer.","title":"AdaGrad"},{"location":"api/optim/AdaGrad/#parameters","text":"lr \u2013 defaults to 0.1 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/AdaGrad/#attributes","text":"g2 ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/AdaGrad/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaGrad () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.880143","title":"Examples"},{"location":"api/optim/AdaGrad/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/AdaGrad/#references","text":"Duchi, J., Hazan, E. and Singer, Y., 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(Jul), pp.2121-2159. \u21a9","title":"References"},{"location":"api/optim/AdaMax/","text":"AdaMax \u00b6 AdaMax optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 Attributes \u00b6 m ( collections.defaultdict ) v ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaMax () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.875332 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. \u21a9 Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747. \u21a9","title":"AdaMax"},{"location":"api/optim/AdaMax/#adamax","text":"AdaMax optimizer.","title":"AdaMax"},{"location":"api/optim/AdaMax/#parameters","text":"lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/AdaMax/#attributes","text":"m ( collections.defaultdict ) v ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/AdaMax/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . AdaMax () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.875332","title":"Examples"},{"location":"api/optim/AdaMax/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/AdaMax/#references","text":"Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. \u21a9 Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747. \u21a9","title":"References"},{"location":"api/optim/Adam/","text":"Adam \u00b6 Adam optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 Attributes \u00b6 m ( collections.defaultdict ) v ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Adam () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.86496 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. \u21a9","title":"Adam"},{"location":"api/optim/Adam/#adam","text":"Adam optimizer.","title":"Adam"},{"location":"api/optim/Adam/#parameters","text":"lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/Adam/#attributes","text":"m ( collections.defaultdict ) v ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/Adam/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Adam () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.86496","title":"Examples"},{"location":"api/optim/Adam/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/Adam/#references","text":"Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. \u21a9","title":"References"},{"location":"api/optim/Averager/","text":"Averager \u00b6 Averaged stochastic gradient descent. This is a wrapper that can be applied to any stochastic gradient descent optimiser. Note that this implementation differs than what may be found elsewhere. Essentially, the average of the weights is usually only used at the end of the optimisation, once all the data has been seen. However, in this implementation the optimiser returns the current averaged weights. Parameters \u00b6 optimizer ( optim.Optimizer ) An optimizer for which the produced weights will be averaged. start ( int ) \u2013 defaults to 0 Indicates the number of iterations to wait before starting the average. Essentially, nothing happens differently before the number of iterations reaches this value. Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Averager ( optim . SGD ( 0.01 ), 100 ) >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878924 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Bottou, L., 2010. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD. \u21a9 Stochastic Algorithms for One-Pass Learning slides by L\u00e9on Bottou \u21a9 Xu, W., 2011. Towards optimal one pass large scale learning with averaged stochastic gradient descent. arXiv preprint arXiv:1107.2490. \u21a9","title":"Averager"},{"location":"api/optim/Averager/#averager","text":"Averaged stochastic gradient descent. This is a wrapper that can be applied to any stochastic gradient descent optimiser. Note that this implementation differs than what may be found elsewhere. Essentially, the average of the weights is usually only used at the end of the optimisation, once all the data has been seen. However, in this implementation the optimiser returns the current averaged weights.","title":"Averager"},{"location":"api/optim/Averager/#parameters","text":"optimizer ( optim.Optimizer ) An optimizer for which the produced weights will be averaged. start ( int ) \u2013 defaults to 0 Indicates the number of iterations to wait before starting the average. Essentially, nothing happens differently before the number of iterations reaches this value.","title":"Parameters"},{"location":"api/optim/Averager/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/Averager/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Averager ( optim . SGD ( 0.01 ), 100 ) >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878924","title":"Examples"},{"location":"api/optim/Averager/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/Averager/#references","text":"Bottou, L., 2010. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD. \u21a9 Stochastic Algorithms for One-Pass Learning slides by L\u00e9on Bottou \u21a9 Xu, W., 2011. Towards optimal one pass large scale learning with averaged stochastic gradient descent. arXiv preprint arXiv:1107.2490. \u21a9","title":"References"},{"location":"api/optim/FTRLProximal/","text":"FTRLProximal \u00b6 FTRL-Proximal optimizer. Parameters \u00b6 alpha \u2013 defaults to 0.05 beta \u2013 defaults to 1.0 l1 \u2013 defaults to 0.0 l2 \u2013 defaults to 1.0 Attributes \u00b6 z ( collections.defaultdict ) n ( collections.defaultdict ) Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . FTRLProximal () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.876588 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230) \u21a9 Tensorflow's FtrlOptimizer \u21a9","title":"FTRLProximal"},{"location":"api/optim/FTRLProximal/#ftrlproximal","text":"FTRL-Proximal optimizer.","title":"FTRLProximal"},{"location":"api/optim/FTRLProximal/#parameters","text":"alpha \u2013 defaults to 0.05 beta \u2013 defaults to 1.0 l1 \u2013 defaults to 0.0 l2 \u2013 defaults to 1.0","title":"Parameters"},{"location":"api/optim/FTRLProximal/#attributes","text":"z ( collections.defaultdict ) n ( collections.defaultdict )","title":"Attributes"},{"location":"api/optim/FTRLProximal/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . FTRLProximal () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.876588","title":"Examples"},{"location":"api/optim/FTRLProximal/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/FTRLProximal/#references","text":"McMahan, H.B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie, L., Phillips, T., Davydov, E., Golovin, D. and Chikkerur, S., 2013, August. Ad click prediction: a view from the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1222-1230) \u21a9 Tensorflow's FtrlOptimizer \u21a9","title":"References"},{"location":"api/optim/Momentum/","text":"Momentum \u00b6 Momentum optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9 Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Momentum () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.841645 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Momentum"},{"location":"api/optim/Momentum/#momentum","text":"Momentum optimizer.","title":"Momentum"},{"location":"api/optim/Momentum/#parameters","text":"lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9","title":"Parameters"},{"location":"api/optim/Momentum/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/Momentum/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Momentum () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.841645","title":"Examples"},{"location":"api/optim/Momentum/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/Nadam/","text":"Nadam \u00b6 Nadam optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08 Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Nadam () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.865961 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Nadam: A combination of adam and nesterov \u21a9","title":"Nadam"},{"location":"api/optim/Nadam/#nadam","text":"Nadam optimizer.","title":"Nadam"},{"location":"api/optim/Nadam/#parameters","text":"lr \u2013 defaults to 0.1 beta_1 \u2013 defaults to 0.9 beta_2 \u2013 defaults to 0.999 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/Nadam/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/Nadam/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . Nadam () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.865961","title":"Examples"},{"location":"api/optim/Nadam/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/Nadam/#references","text":"Nadam: A combination of adam and nesterov \u21a9","title":"References"},{"location":"api/optim/NesterovMomentum/","text":"NesterovMomentum \u00b6 Nesterov Momentum optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9 Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . NesterovMomentum () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.842932 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"NesterovMomentum"},{"location":"api/optim/NesterovMomentum/#nesterovmomentum","text":"Nesterov Momentum optimizer.","title":"NesterovMomentum"},{"location":"api/optim/NesterovMomentum/#parameters","text":"lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9","title":"Parameters"},{"location":"api/optim/NesterovMomentum/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/NesterovMomentum/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . NesterovMomentum () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.842932","title":"Examples"},{"location":"api/optim/NesterovMomentum/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/Optimizer/","text":"Optimizer \u00b6 Optimizer interface. Every optimizer inherits from this base interface. Parameters \u00b6 lr ( Union[ optim.schedulers.Scheduler , float] ) Attributes \u00b6 learning_rate ( float ) Returns the current learning rate value. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Optimizer"},{"location":"api/optim/Optimizer/#optimizer","text":"Optimizer interface. Every optimizer inherits from this base interface.","title":"Optimizer"},{"location":"api/optim/Optimizer/#parameters","text":"lr ( Union[ optim.schedulers.Scheduler , float] )","title":"Parameters"},{"location":"api/optim/Optimizer/#attributes","text":"learning_rate ( float ) Returns the current learning rate value.","title":"Attributes"},{"location":"api/optim/Optimizer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/RMSProp/","text":"RMSProp \u00b6 RMSProp optimizer. Parameters \u00b6 lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9 eps \u2013 defaults to 1e-08 Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . RMSProp () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.872378 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Divide the gradient by a running average of itsrecent magnitude \u21a9","title":"RMSProp"},{"location":"api/optim/RMSProp/#rmsprop","text":"RMSProp optimizer.","title":"RMSProp"},{"location":"api/optim/RMSProp/#parameters","text":"lr \u2013 defaults to 0.1 rho \u2013 defaults to 0.9 eps \u2013 defaults to 1e-08","title":"Parameters"},{"location":"api/optim/RMSProp/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/RMSProp/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . RMSProp () >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.872378","title":"Examples"},{"location":"api/optim/RMSProp/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/RMSProp/#references","text":"Divide the gradient by a running average of itsrecent magnitude \u21a9","title":"References"},{"location":"api/optim/SGD/","text":"SGD \u00b6 Plain stochastic gradient descent. Parameters \u00b6 lr \u2013 defaults to 0.01 Attributes \u00b6 learning_rate Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . SGD ( 0.1 ) >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878521 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict ) References \u00b6 Robbins, H. and Monro, S., 1951. A stochastic approximation method. The annals of mathematical statistics, pp.400-407 \u21a9","title":"SGD"},{"location":"api/optim/SGD/#sgd","text":"Plain stochastic gradient descent.","title":"SGD"},{"location":"api/optim/SGD/#parameters","text":"lr \u2013 defaults to 0.01","title":"Parameters"},{"location":"api/optim/SGD/#attributes","text":"learning_rate","title":"Attributes"},{"location":"api/optim/SGD/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> dataset = datasets . Phishing () >>> optimizer = optim . SGD ( 0.1 ) >>> model = ( ... preprocessing . StandardScaler () | ... linear_model . LogisticRegression ( optimizer ) ... ) >>> metric = metrics . F1 () >>> evaluate . progressive_val_score ( dataset , model , metric ) F1 : 0.878521","title":"Examples"},{"location":"api/optim/SGD/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. look_ahead Updates a weight vector before a prediction is made. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. Returns: The updated weights. Parameters w ( dict ) step Updates a weight vector given a gradient. Parameters: w (dict): A dictionary of weight parameters. The weights are modified in-place. g (dict): A dictionary of gradients. Returns: The updated weights. Parameters w ( dict ) g ( dict )","title":"Methods"},{"location":"api/optim/SGD/#references","text":"Robbins, H. and Monro, S., 1951. A stochastic approximation method. The annals of mathematical statistics, pp.400-407 \u21a9","title":"References"},{"location":"api/optim/initializers/Constant/","text":"Constant \u00b6 Constant initializer which always returns the same value. Parameters \u00b6 value ( float ) Examples \u00b6 >>> from river import optim >>> init = optim . initializers . Constant ( value = 3.14 ) >>> init ( shape = 1 ) 3.14 >>> init ( shape = 2 ) array ([ 3.14 , 3.14 ]) Methods \u00b6 call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Constant"},{"location":"api/optim/initializers/Constant/#constant","text":"Constant initializer which always returns the same value.","title":"Constant"},{"location":"api/optim/initializers/Constant/#parameters","text":"value ( float )","title":"Parameters"},{"location":"api/optim/initializers/Constant/#examples","text":">>> from river import optim >>> init = optim . initializers . Constant ( value = 3.14 ) >>> init ( shape = 1 ) 3.14 >>> init ( shape = 2 ) array ([ 3.14 , 3.14 ])","title":"Examples"},{"location":"api/optim/initializers/Constant/#methods","text":"call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Methods"},{"location":"api/optim/initializers/Normal/","text":"Normal \u00b6 Random normal initializer which simulate a normal distribution with specified parameters. Parameters \u00b6 mu \u2013 defaults to 0.0 The mean of the normal distribution sigma \u2013 defaults to 1.0 The standard deviation of the normal distribution seed \u2013 defaults to None Random number generation seed that can be set for reproducibility. Examples \u00b6 >>> from river import optim >>> init = optim . initializers . Normal ( mu = 0 , sigma = 1 , seed = 42 ) >>> init ( shape = 1 ) 0.496714 >>> init ( shape = 2 ) array ([ - 0.1382643 , 0.64768854 ]) Methods \u00b6 call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Normal"},{"location":"api/optim/initializers/Normal/#normal","text":"Random normal initializer which simulate a normal distribution with specified parameters.","title":"Normal"},{"location":"api/optim/initializers/Normal/#parameters","text":"mu \u2013 defaults to 0.0 The mean of the normal distribution sigma \u2013 defaults to 1.0 The standard deviation of the normal distribution seed \u2013 defaults to None Random number generation seed that can be set for reproducibility.","title":"Parameters"},{"location":"api/optim/initializers/Normal/#examples","text":">>> from river import optim >>> init = optim . initializers . Normal ( mu = 0 , sigma = 1 , seed = 42 ) >>> init ( shape = 1 ) 0.496714 >>> init ( shape = 2 ) array ([ - 0.1382643 , 0.64768854 ])","title":"Examples"},{"location":"api/optim/initializers/Normal/#methods","text":"call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Methods"},{"location":"api/optim/initializers/Zeros/","text":"Zeros \u00b6 Constant initializer which always returns zeros. Examples \u00b6 >>> from river import optim >>> init = optim . initializers . Zeros () >>> init ( shape = 1 ) 0.0 >>> init ( shape = 2 ) array ([ 0. , 0. ]) Methods \u00b6 call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Zeros"},{"location":"api/optim/initializers/Zeros/#zeros","text":"Constant initializer which always returns zeros.","title":"Zeros"},{"location":"api/optim/initializers/Zeros/#examples","text":">>> from river import optim >>> init = optim . initializers . Zeros () >>> init ( shape = 1 ) 0.0 >>> init ( shape = 2 ) array ([ 0. , 0. ])","title":"Examples"},{"location":"api/optim/initializers/Zeros/#methods","text":"call Returns a fresh set of weights. Parameters shape \u2013 defaults to 1 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters.","title":"Methods"},{"location":"api/optim/losses/Absolute/","text":"Absolute \u00b6 Absolute loss, also known as the mean absolute error or L1 loss. Mathematically, it is defined as \\[L = |p_i - y_i|\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = sgn(p_i - y_i)\\] Examples \u00b6 >>> from river import optim >>> loss = optim . losses . Absolute () >>> loss ( - 42 , 42 ) 84 >>> loss . gradient ( 1 , 2 ) 1 >>> loss . gradient ( 2 , 1 ) - 1 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Absolute"},{"location":"api/optim/losses/Absolute/#absolute","text":"Absolute loss, also known as the mean absolute error or L1 loss. Mathematically, it is defined as \\[L = |p_i - y_i|\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = sgn(p_i - y_i)\\]","title":"Absolute"},{"location":"api/optim/losses/Absolute/#examples","text":">>> from river import optim >>> loss = optim . losses . Absolute () >>> loss ( - 42 , 42 ) 84 >>> loss . gradient ( 1 , 2 ) 1 >>> loss . gradient ( 2 , 1 ) - 1","title":"Examples"},{"location":"api/optim/losses/Absolute/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/BinaryFocalLoss/","text":"BinaryFocalLoss \u00b6 Binary focal loss. This implements the \"star\" algorithm from the appendix of the focal loss paper. Parameters \u00b6 gamma \u2013 defaults to 2 beta \u2013 defaults to 1 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s). References \u00b6 Lin, T.Y., Goyal, P., Girshick, R., He, K. and Doll\u00e1r, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988)","title":"BinaryFocalLoss"},{"location":"api/optim/losses/BinaryFocalLoss/#binaryfocalloss","text":"Binary focal loss. This implements the \"star\" algorithm from the appendix of the focal loss paper.","title":"BinaryFocalLoss"},{"location":"api/optim/losses/BinaryFocalLoss/#parameters","text":"gamma \u2013 defaults to 2 beta \u2013 defaults to 1","title":"Parameters"},{"location":"api/optim/losses/BinaryFocalLoss/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/BinaryFocalLoss/#references","text":"Lin, T.Y., Goyal, P., Girshick, R., He, K. and Doll\u00e1r, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988)","title":"References"},{"location":"api/optim/losses/BinaryLoss/","text":"BinaryLoss \u00b6 A loss appropriate for binary classification tasks. Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"BinaryLoss"},{"location":"api/optim/losses/BinaryLoss/#binaryloss","text":"A loss appropriate for binary classification tasks.","title":"BinaryLoss"},{"location":"api/optim/losses/BinaryLoss/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Cauchy/","text":"Cauchy \u00b6 Cauchy loss function. Parameters \u00b6 C \u2013 defaults to 80 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s). References \u00b6 \"Effect of MAE\" Kaggle discussion \u21a9 Paris Madness Kaggle kernel \u21a9","title":"Cauchy"},{"location":"api/optim/losses/Cauchy/#cauchy","text":"Cauchy loss function.","title":"Cauchy"},{"location":"api/optim/losses/Cauchy/#parameters","text":"C \u2013 defaults to 80","title":"Parameters"},{"location":"api/optim/losses/Cauchy/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Cauchy/#references","text":"\"Effect of MAE\" Kaggle discussion \u21a9 Paris Madness Kaggle kernel \u21a9","title":"References"},{"location":"api/optim/losses/CrossEntropy/","text":"CrossEntropy \u00b6 Cross entropy loss. This is a generalization of logistic loss to multiple classes. Parameters \u00b6 class_weight ( Dict[Union[bool, str, int], float] ) \u2013 defaults to None A dictionary that indicates what weight to associate with each class. Examples \u00b6 >>> from river import optim >>> y_true = [ 0 , 1 , 2 , 2 ] >>> y_pred = [ ... { 0 : 0.29450637 , 1 : 0.34216758 , 2 : 0.36332605 }, ... { 0 : 0.21290077 , 1 : 0.32728332 , 2 : 0.45981591 }, ... { 0 : 0.42860913 , 1 : 0.33380113 , 2 : 0.23758974 }, ... { 0 : 0.44941979 , 1 : 0.32962558 , 2 : 0.22095463 } ... ] >>> loss = optim . losses . CrossEntropy () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( loss ( yt , yp )) 1.222454 1.116929 1.437209 1.509797 >>> for yt , yp in zip ( y_true , y_pred ): ... print ( loss . gradient ( yt , yp )) { 0 : - 0.70549363 , 1 : 0.34216758 , 2 : 0.36332605 } { 0 : 0.21290077 , 1 : - 0.67271668 , 2 : 0.45981591 } { 0 : 0.42860913 , 1 : 0.33380113 , 2 : - 0.76241026 } { 0 : 0.44941979 , 1 : 0.32962558 , 2 : - 0.77904537 } Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s). References \u00b6 What is Softmax regression and how is it related to Logistic regression? \u21a9","title":"CrossEntropy"},{"location":"api/optim/losses/CrossEntropy/#crossentropy","text":"Cross entropy loss. This is a generalization of logistic loss to multiple classes.","title":"CrossEntropy"},{"location":"api/optim/losses/CrossEntropy/#parameters","text":"class_weight ( Dict[Union[bool, str, int], float] ) \u2013 defaults to None A dictionary that indicates what weight to associate with each class.","title":"Parameters"},{"location":"api/optim/losses/CrossEntropy/#examples","text":">>> from river import optim >>> y_true = [ 0 , 1 , 2 , 2 ] >>> y_pred = [ ... { 0 : 0.29450637 , 1 : 0.34216758 , 2 : 0.36332605 }, ... { 0 : 0.21290077 , 1 : 0.32728332 , 2 : 0.45981591 }, ... { 0 : 0.42860913 , 1 : 0.33380113 , 2 : 0.23758974 }, ... { 0 : 0.44941979 , 1 : 0.32962558 , 2 : 0.22095463 } ... ] >>> loss = optim . losses . CrossEntropy () >>> for yt , yp in zip ( y_true , y_pred ): ... print ( loss ( yt , yp )) 1.222454 1.116929 1.437209 1.509797 >>> for yt , yp in zip ( y_true , y_pred ): ... print ( loss . gradient ( yt , yp )) { 0 : - 0.70549363 , 1 : 0.34216758 , 2 : 0.36332605 } { 0 : 0.21290077 , 1 : - 0.67271668 , 2 : 0.45981591 } { 0 : 0.42860913 , 1 : 0.33380113 , 2 : - 0.76241026 } { 0 : 0.44941979 , 1 : 0.32962558 , 2 : - 0.77904537 }","title":"Examples"},{"location":"api/optim/losses/CrossEntropy/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/CrossEntropy/#references","text":"What is Softmax regression and how is it related to Logistic regression? \u21a9","title":"References"},{"location":"api/optim/losses/EpsilonInsensitiveHinge/","text":"EpsilonInsensitiveHinge \u00b6 Epsilon-insensitive hinge loss. Parameters \u00b6 eps \u2013 defaults to 0.1 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"EpsilonInsensitiveHinge"},{"location":"api/optim/losses/EpsilonInsensitiveHinge/#epsiloninsensitivehinge","text":"Epsilon-insensitive hinge loss.","title":"EpsilonInsensitiveHinge"},{"location":"api/optim/losses/EpsilonInsensitiveHinge/#parameters","text":"eps \u2013 defaults to 0.1","title":"Parameters"},{"location":"api/optim/losses/EpsilonInsensitiveHinge/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Hinge/","text":"Hinge \u00b6 Computes the hinge loss. Mathematically, it is defined as \\[L = max(0, 1 - p_i * y_i)\\] It's gradient w.r.t. to \\(p_i\\) is \\[ \\\\frac{\\\\partial L}{\\\\partial y_i} = \\\\left\\{ \\\\begin{array}{ll} \\\\ 0 & p_iy_i \\geqslant 1 \\\\\\\\ \\\\ - y_i & p_iy_i < 1 \\\\end{array} \\\\right. \\] Parameters \u00b6 threshold \u2013 defaults to 1.0 Margin threshold. 1 yield the loss used in SVMs, whilst 0 is equivalent to the loss used in the Perceptron algorithm. Examples \u00b6 >>> from river import optim >>> loss = optim . losses . Hinge ( threshold = 1 ) >>> loss ( 1 , .2 ) 0.8 >>> loss . gradient ( 1 , .2 ) - 1 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Hinge"},{"location":"api/optim/losses/Hinge/#hinge","text":"Computes the hinge loss. Mathematically, it is defined as \\[L = max(0, 1 - p_i * y_i)\\] It's gradient w.r.t. to \\(p_i\\) is \\[ \\\\frac{\\\\partial L}{\\\\partial y_i} = \\\\left\\{ \\\\begin{array}{ll} \\\\ 0 & p_iy_i \\geqslant 1 \\\\\\\\ \\\\ - y_i & p_iy_i < 1 \\\\end{array} \\\\right. \\]","title":"Hinge"},{"location":"api/optim/losses/Hinge/#parameters","text":"threshold \u2013 defaults to 1.0 Margin threshold. 1 yield the loss used in SVMs, whilst 0 is equivalent to the loss used in the Perceptron algorithm.","title":"Parameters"},{"location":"api/optim/losses/Hinge/#examples","text":">>> from river import optim >>> loss = optim . losses . Hinge ( threshold = 1 ) >>> loss ( 1 , .2 ) 0.8 >>> loss . gradient ( 1 , .2 ) - 1","title":"Examples"},{"location":"api/optim/losses/Hinge/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Log/","text":"Log \u00b6 Logarithmic loss. This loss function expects each provided y_pred to be a logit. In other words if must be the raw output of a linear model or a neural network. Parameters \u00b6 weight_pos \u2013 defaults to 1.0 weight_neg \u2013 defaults to 1.0 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s). References \u00b6 Logit Wikipedia page \u21a9","title":"Log"},{"location":"api/optim/losses/Log/#log","text":"Logarithmic loss. This loss function expects each provided y_pred to be a logit. In other words if must be the raw output of a linear model or a neural network.","title":"Log"},{"location":"api/optim/losses/Log/#parameters","text":"weight_pos \u2013 defaults to 1.0 weight_neg \u2013 defaults to 1.0","title":"Parameters"},{"location":"api/optim/losses/Log/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Log/#references","text":"Logit Wikipedia page \u21a9","title":"References"},{"location":"api/optim/losses/MultiClassLoss/","text":"MultiClassLoss \u00b6 A loss appropriate for multi-class classification tasks. Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"MultiClassLoss"},{"location":"api/optim/losses/MultiClassLoss/#multiclassloss","text":"A loss appropriate for multi-class classification tasks.","title":"MultiClassLoss"},{"location":"api/optim/losses/MultiClassLoss/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Poisson/","text":"Poisson \u00b6 Poisson loss. The Poisson loss is usually more suited for regression with count data than the squared loss. Mathematically, it is defined as \\[L = exp(p_i) - y_i \\times p_i\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = exp(p_i) - y_i\\] Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Poisson"},{"location":"api/optim/losses/Poisson/#poisson","text":"Poisson loss. The Poisson loss is usually more suited for regression with count data than the squared loss. Mathematically, it is defined as \\[L = exp(p_i) - y_i \\times p_i\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = exp(p_i) - y_i\\]","title":"Poisson"},{"location":"api/optim/losses/Poisson/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Quantile/","text":"Quantile \u00b6 Quantile loss. Parameters \u00b6 alpha \u2013 defaults to 0.5 Desired quantile to attain. Examples \u00b6 >>> from river import optim >>> loss = optim . losses . Quantile ( 0.5 ) >>> loss ( 1 , 3 ) 1.0 >>> loss . gradient ( 1 , 3 ) 0.5 >>> loss . gradient ( 3 , 1 ) - 0.5 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s). References \u00b6 Wikipedia article on quantile regression \u21a9 Derivative from WolframAlpha \u21a9","title":"Quantile"},{"location":"api/optim/losses/Quantile/#quantile","text":"Quantile loss.","title":"Quantile"},{"location":"api/optim/losses/Quantile/#parameters","text":"alpha \u2013 defaults to 0.5 Desired quantile to attain.","title":"Parameters"},{"location":"api/optim/losses/Quantile/#examples","text":">>> from river import optim >>> loss = optim . losses . Quantile ( 0.5 ) >>> loss ( 1 , 3 ) 1.0 >>> loss . gradient ( 1 , 3 ) 0.5 >>> loss . gradient ( 3 , 1 ) - 0.5","title":"Examples"},{"location":"api/optim/losses/Quantile/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Quantile/#references","text":"Wikipedia article on quantile regression \u21a9 Derivative from WolframAlpha \u21a9","title":"References"},{"location":"api/optim/losses/RegressionLoss/","text":"RegressionLoss \u00b6 A loss appropriate for regression tasks. Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"RegressionLoss"},{"location":"api/optim/losses/RegressionLoss/#regressionloss","text":"A loss appropriate for regression tasks.","title":"RegressionLoss"},{"location":"api/optim/losses/RegressionLoss/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/losses/Squared/","text":"Squared \u00b6 Squared loss, also known as the L2 loss. Mathematically, it is defined as \\[L = (p_i - y_i) ^ 2\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = 2 imes (p_i - y_i)\\] One thing to note is that this convention is consistent with Vowpal Wabbit and PyTorch, but not with scikit-learn. Indeed, scikit-learn divides the loss by 2, making the 2 disappear in the gradient. Examples \u00b6 >>> from river import optim >>> loss = optim . losses . Squared () >>> loss ( - 4 , 5 ) 81 >>> loss . gradient ( - 4 , 5 ) 18 >>> loss . gradient ( 5 , - 4 ) - 18 Methods \u00b6 call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Squared"},{"location":"api/optim/losses/Squared/#squared","text":"Squared loss, also known as the L2 loss. Mathematically, it is defined as \\[L = (p_i - y_i) ^ 2\\] It's gradient w.r.t. to \\(p_i\\) is \\[\\frac{\\partial L}{\\partial p_i} = 2 imes (p_i - y_i)\\] One thing to note is that this convention is consistent with Vowpal Wabbit and PyTorch, but not with scikit-learn. Indeed, scikit-learn divides the loss by 2, making the 2 disappear in the gradient.","title":"Squared"},{"location":"api/optim/losses/Squared/#examples","text":">>> from river import optim >>> loss = optim . losses . Squared () >>> loss ( - 4 , 5 ) 81 >>> loss . gradient ( - 4 , 5 ) 18 >>> loss . gradient ( 5 , - 4 ) - 18","title":"Examples"},{"location":"api/optim/losses/Squared/#methods","text":"call Returns the loss. Parameters y_true y_pred Returns The loss(es). clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. gradient Return the gradient with respect to y_pred. Parameters y_true y_pred Returns The gradient(s). mean_func Mean function. This is the inverse of the link function. Typically, a loss function takes as input the raw output of a model. In the case of classification, the raw output would be logits. The mean function can be used to convert the raw output into a value that makes sense to the user, such as a probability. Parameters y_pred Returns The adjusted prediction(s).","title":"Methods"},{"location":"api/optim/schedulers/Constant/","text":"Constant \u00b6 Always uses the same learning rate. Parameters \u00b6 learning_rate ( float ) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Constant"},{"location":"api/optim/schedulers/Constant/#constant","text":"Always uses the same learning rate.","title":"Constant"},{"location":"api/optim/schedulers/Constant/#parameters","text":"learning_rate ( float )","title":"Parameters"},{"location":"api/optim/schedulers/Constant/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Methods"},{"location":"api/optim/schedulers/InverseScaling/","text":"InverseScaling \u00b6 Reduces the learning rate using a power schedule. Assuming an initial learning rate \\(\\eta\\) , the learning rate at step \\(t\\) is: \\[\\\\frac{eta}{(t + 1) ^ p}\\] where \\(p\\) is a user-defined parameter. Parameters \u00b6 learning_rate ( float ) power \u2013 defaults to 0.5 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"InverseScaling"},{"location":"api/optim/schedulers/InverseScaling/#inversescaling","text":"Reduces the learning rate using a power schedule. Assuming an initial learning rate \\(\\eta\\) , the learning rate at step \\(t\\) is: \\[\\\\frac{eta}{(t + 1) ^ p}\\] where \\(p\\) is a user-defined parameter.","title":"InverseScaling"},{"location":"api/optim/schedulers/InverseScaling/#parameters","text":"learning_rate ( float ) power \u2013 defaults to 0.5","title":"Parameters"},{"location":"api/optim/schedulers/InverseScaling/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Methods"},{"location":"api/optim/schedulers/Optimal/","text":"Optimal \u00b6 Optimal learning schedule as proposed by L\u00e9on Bottou. Parameters \u00b6 loss ( optim.losses.Loss ) alpha \u2013 defaults to 0.0001 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int ) References \u00b6 Bottou, L., 2012. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade (pp. 421-436). Springer, Berlin, Heidelberg. \u21a9","title":"Optimal"},{"location":"api/optim/schedulers/Optimal/#optimal","text":"Optimal learning schedule as proposed by L\u00e9on Bottou.","title":"Optimal"},{"location":"api/optim/schedulers/Optimal/#parameters","text":"loss ( optim.losses.Loss ) alpha \u2013 defaults to 0.0001","title":"Parameters"},{"location":"api/optim/schedulers/Optimal/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Methods"},{"location":"api/optim/schedulers/Optimal/#references","text":"Bottou, L., 2012. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade (pp. 421-436). Springer, Berlin, Heidelberg. \u21a9","title":"References"},{"location":"api/optim/schedulers/Scheduler/","text":"Scheduler \u00b6 Can be used to program the learning rate schedule of an optim.Optimizer . Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Scheduler"},{"location":"api/optim/schedulers/Scheduler/#scheduler","text":"Can be used to program the learning rate schedule of an optim.Optimizer .","title":"Scheduler"},{"location":"api/optim/schedulers/Scheduler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Returns the learning rate at a given iteration. Parameters t ( int )","title":"Methods"},{"location":"api/preprocessing/AdaptiveStandardScaler/","text":"AdaptiveStandardScaler \u00b6 Scales data using exponentially weighted moving average and variance. Under the hood, a exponentially weighted running mean and variance are maintained for each feature. This can potentially provide better results for drifting data in comparison to preprocessing.StandardScaler . Indeed, the latter computes a global mean and variance for each feature, whereas this scaler weights data in proportion to their recency. Parameters \u00b6 alpha \u2013 defaults to 0.3 This parameter is passed to stats.EWVar . It is expected to be in [0, 1]. More weight is assigned to recent samples the closer alpha is to 1. Examples \u00b6 Consider the following series which contains a positive trend. >>> import random >>> random . seed ( 42 ) >>> X = [ ... { 'x' : random . uniform ( 4 + i , 6 + i )} ... for i in range ( 8 ) ... ] >>> for x in X : ... print ( x ) { 'x' : 5.278 } { 'x' : 5.050 } { 'x' : 6.550 } { 'x' : 7.446 } { 'x' : 9.472 } { 'x' : 10.353 } { 'x' : 11.784 } { 'x' : 11.173 } This scaler works well with this kind of data because it uses statistics that assign higher weight to more recent data. >>> from river import preprocessing >>> scaler = preprocessing . AdaptiveStandardScaler ( alpha = .6 ) >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : - 0.816 } { 'x' : 0.812 } { 'x' : 0.695 } { 'x' : 0.754 } { 'x' : 0.598 } { 'x' : 0.651 } { 'x' : 0.124 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"AdaptiveStandardScaler"},{"location":"api/preprocessing/AdaptiveStandardScaler/#adaptivestandardscaler","text":"Scales data using exponentially weighted moving average and variance. Under the hood, a exponentially weighted running mean and variance are maintained for each feature. This can potentially provide better results for drifting data in comparison to preprocessing.StandardScaler . Indeed, the latter computes a global mean and variance for each feature, whereas this scaler weights data in proportion to their recency.","title":"AdaptiveStandardScaler"},{"location":"api/preprocessing/AdaptiveStandardScaler/#parameters","text":"alpha \u2013 defaults to 0.3 This parameter is passed to stats.EWVar . It is expected to be in [0, 1]. More weight is assigned to recent samples the closer alpha is to 1.","title":"Parameters"},{"location":"api/preprocessing/AdaptiveStandardScaler/#examples","text":"Consider the following series which contains a positive trend. >>> import random >>> random . seed ( 42 ) >>> X = [ ... { 'x' : random . uniform ( 4 + i , 6 + i )} ... for i in range ( 8 ) ... ] >>> for x in X : ... print ( x ) { 'x' : 5.278 } { 'x' : 5.050 } { 'x' : 6.550 } { 'x' : 7.446 } { 'x' : 9.472 } { 'x' : 10.353 } { 'x' : 11.784 } { 'x' : 11.173 } This scaler works well with this kind of data because it uses statistics that assign higher weight to more recent data. >>> from river import preprocessing >>> scaler = preprocessing . AdaptiveStandardScaler ( alpha = .6 ) >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : - 0.816 } { 'x' : 0.812 } { 'x' : 0.695 } { 'x' : 0.754 } { 'x' : 0.598 } { 'x' : 0.651 } { 'x' : 0.124 }","title":"Examples"},{"location":"api/preprocessing/AdaptiveStandardScaler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/Binarizer/","text":"Binarizer \u00b6 Binarizes the data to 0 or 1 according to a threshold. Parameters \u00b6 threshold \u2013 defaults to 0.0 Values above this are replaced by 1 and the others by 0. dtype \u2013 defaults to <class 'bool'> The desired data type to apply. Examples \u00b6 >>> import river >>> import numpy as np >>> rng = np . random . RandomState ( 42 ) >>> X = [{ 'x1' : v , 'x2' : int ( v )} for v in rng . uniform ( low =- 4 , high = 4 , size = 6 )] >>> binarizer = river . preprocessing . Binarizer () >>> for x in X : ... print ( binarizer . learn_one ( x ) . transform_one ( x )) { 'x1' : False , 'x2' : False } { 'x1' : True , 'x2' : True } { 'x1' : True , 'x2' : True } { 'x1' : True , 'x2' : False } { 'x1' : False , 'x2' : False } { 'x1' : False , 'x2' : False } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Binarizer"},{"location":"api/preprocessing/Binarizer/#binarizer","text":"Binarizes the data to 0 or 1 according to a threshold.","title":"Binarizer"},{"location":"api/preprocessing/Binarizer/#parameters","text":"threshold \u2013 defaults to 0.0 Values above this are replaced by 1 and the others by 0. dtype \u2013 defaults to <class 'bool'> The desired data type to apply.","title":"Parameters"},{"location":"api/preprocessing/Binarizer/#examples","text":">>> import river >>> import numpy as np >>> rng = np . random . RandomState ( 42 ) >>> X = [{ 'x1' : v , 'x2' : int ( v )} for v in rng . uniform ( low =- 4 , high = 4 , size = 6 )] >>> binarizer = river . preprocessing . Binarizer () >>> for x in X : ... print ( binarizer . learn_one ( x ) . transform_one ( x )) { 'x1' : False , 'x2' : False } { 'x1' : True , 'x2' : True } { 'x1' : True , 'x2' : True } { 'x1' : True , 'x2' : False } { 'x1' : False , 'x2' : False } { 'x1' : False , 'x2' : False }","title":"Examples"},{"location":"api/preprocessing/Binarizer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/FeatureHasher/","text":"FeatureHasher \u00b6 Implements the hashing trick. Each pair of (name, value) features is hashed into a random integer. A module operator is then used to make sure the hash is in a certain range. We use the Murmurhash implementation from scikit-learn. Parameters \u00b6 n_features \u2013 defaults to 1048576 The number by which each hash will be moduloed by. seed ( int ) \u2013 defaults to None Set the seed to produce identical results. Examples \u00b6 >>> import river >>> hasher = river . preprocessing . FeatureHasher ( n_features = 10 , seed = 42 ) >>> X = [ ... { 'dog' : 1 , 'cat' : 2 , 'elephant' : 4 }, ... { 'dog' : 2 , 'run' : 5 } ... ] >>> for x in X : ... print ( hasher . transform_one ( x )) Counter ({ 1 : 4 , 9 : 2 , 8 : 1 }) Counter ({ 4 : 5 , 8 : 2 }) Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 Wikipedia article on feature vectorization using the hashing trick \u21a9","title":"FeatureHasher"},{"location":"api/preprocessing/FeatureHasher/#featurehasher","text":"Implements the hashing trick. Each pair of (name, value) features is hashed into a random integer. A module operator is then used to make sure the hash is in a certain range. We use the Murmurhash implementation from scikit-learn.","title":"FeatureHasher"},{"location":"api/preprocessing/FeatureHasher/#parameters","text":"n_features \u2013 defaults to 1048576 The number by which each hash will be moduloed by. seed ( int ) \u2013 defaults to None Set the seed to produce identical results.","title":"Parameters"},{"location":"api/preprocessing/FeatureHasher/#examples","text":">>> import river >>> hasher = river . preprocessing . FeatureHasher ( n_features = 10 , seed = 42 ) >>> X = [ ... { 'dog' : 1 , 'cat' : 2 , 'elephant' : 4 }, ... { 'dog' : 2 , 'run' : 5 } ... ] >>> for x in X : ... print ( hasher . transform_one ( x )) Counter ({ 1 : 4 , 9 : 2 , 8 : 1 }) Counter ({ 4 : 5 , 8 : 2 })","title":"Examples"},{"location":"api/preprocessing/FeatureHasher/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/FeatureHasher/#references","text":"Wikipedia article on feature vectorization using the hashing trick \u21a9","title":"References"},{"location":"api/preprocessing/LDA/","text":"LDA \u00b6 Online Latent Dirichlet Allocation with Infinite Vocabulary. Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document collections. The key advantage of this variant is that it assumes an infinite vocabulary, meaning that the set of tokens does not have to known in advance, as opposed to the implementation from sklearn The results produced by this implementation are identical to those from the original implementation proposed by the method's authors. This class takes as input token counts. Therefore, it requires you to tokenize beforehand. You can do so by using a feature_extraction.BagOfWords instance, as shown in the example below. Parameters \u00b6 n_components \u2013 defaults to 10 Number of topics of the latent Drichlet allocation. number_of_documents \u2013 defaults to 1000000.0 Estimated number of documents. alpha_theta \u2013 defaults to 0.5 Hyper-parameter of the Dirichlet distribution of topics. alpha_beta \u2013 defaults to 100.0 Hyper-parameter of the Dirichlet process of distribution over words. tau \u2013 defaults to 64.0 Learning inertia to prevent premature convergence. kappa \u2013 defaults to 0.75 The learning rate kappa controls how quickly new parameters estimates replace the old ones. kappa \u2208 (0.5, 1] is required for convergence. vocab_prune_interval \u2013 defaults to 10 Interval at which to refresh the words topics distribution. number_of_samples \u2013 defaults to 10 Number of iteration to computes documents topics distribution. ranking_smooth_factor \u2013 defaults to 1e-12 burn_in_sweeps \u2013 defaults to 5 Number of iteration necessaries while analyzing a document before updating document topics distribution. maximum_size_vocabulary \u2013 defaults to 4000 Maximum size of the stored vocabulary. seed ( int ) \u2013 defaults to None Random number seed used for reproducibility. Attributes \u00b6 counter ( int ) The current number of observed documents. truncation_size_prime ( int ) Number of distincts words stored in the vocabulary. Updated before processing a document. truncation_size ( int ) Number of distincts words stored in the vocabulary. Updated after processing a document. word_to_index ( dict ) Words as keys and indexes as values. index_to_word ( dict ) Indexes as keys and words as values. nu_1 ( dict ) Weights of the words. Component of the variational inference. nu_2 ( dict ) Weights of the words. Component of the variational inference. Examples \u00b6 >>> from river import compose >>> from river import feature_extraction >>> from river import preprocessing >>> X = [ ... 'weather cold' , ... 'weather hot dry' , ... 'weather cold rainy' , ... 'weather hot' , ... 'weather cold humid' , ... ] >>> lda = compose . Pipeline ( ... feature_extraction . BagOfWords (), ... preprocessing . LDA ( ... n_components = 2 , ... number_of_documents = 60 , ... seed = 42 ... ) ... ) >>> for x in X : ... lda = lda . learn_one ( x ) ... topics = lda . transform_one ( x ) ... print ( topics ) { 0 : 2.5 , 1 : 0.5 } { 0 : 2.5 , 1 : 1.5 } { 0 : 1.5 , 1 : 2.5 } { 0 : 1.5 , 1 : 1.5 } { 0 : 0.5 , 1 : 3.5 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self learn_transform_one Equivalent to lda.learn_one(x).transform_one(x) s, but faster. Parameters x ( dict ) Returns dict : Component attributions for the input document. transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 Zhai, K. and Boyd-Graber, J., 2013, February. Online latent Dirichlet allocation with infinite vocabulary. In International Conference on Machine Learning (pp. 561-569). \u21a9 PyInfVoc on GitHub \u21a9","title":"LDA"},{"location":"api/preprocessing/LDA/#lda","text":"Online Latent Dirichlet Allocation with Infinite Vocabulary. Latent Dirichlet allocation (LDA) is a probabilistic approach for exploring topics in document collections. The key advantage of this variant is that it assumes an infinite vocabulary, meaning that the set of tokens does not have to known in advance, as opposed to the implementation from sklearn The results produced by this implementation are identical to those from the original implementation proposed by the method's authors. This class takes as input token counts. Therefore, it requires you to tokenize beforehand. You can do so by using a feature_extraction.BagOfWords instance, as shown in the example below.","title":"LDA"},{"location":"api/preprocessing/LDA/#parameters","text":"n_components \u2013 defaults to 10 Number of topics of the latent Drichlet allocation. number_of_documents \u2013 defaults to 1000000.0 Estimated number of documents. alpha_theta \u2013 defaults to 0.5 Hyper-parameter of the Dirichlet distribution of topics. alpha_beta \u2013 defaults to 100.0 Hyper-parameter of the Dirichlet process of distribution over words. tau \u2013 defaults to 64.0 Learning inertia to prevent premature convergence. kappa \u2013 defaults to 0.75 The learning rate kappa controls how quickly new parameters estimates replace the old ones. kappa \u2208 (0.5, 1] is required for convergence. vocab_prune_interval \u2013 defaults to 10 Interval at which to refresh the words topics distribution. number_of_samples \u2013 defaults to 10 Number of iteration to computes documents topics distribution. ranking_smooth_factor \u2013 defaults to 1e-12 burn_in_sweeps \u2013 defaults to 5 Number of iteration necessaries while analyzing a document before updating document topics distribution. maximum_size_vocabulary \u2013 defaults to 4000 Maximum size of the stored vocabulary. seed ( int ) \u2013 defaults to None Random number seed used for reproducibility.","title":"Parameters"},{"location":"api/preprocessing/LDA/#attributes","text":"counter ( int ) The current number of observed documents. truncation_size_prime ( int ) Number of distincts words stored in the vocabulary. Updated before processing a document. truncation_size ( int ) Number of distincts words stored in the vocabulary. Updated after processing a document. word_to_index ( dict ) Words as keys and indexes as values. index_to_word ( dict ) Indexes as keys and words as values. nu_1 ( dict ) Weights of the words. Component of the variational inference. nu_2 ( dict ) Weights of the words. Component of the variational inference.","title":"Attributes"},{"location":"api/preprocessing/LDA/#examples","text":">>> from river import compose >>> from river import feature_extraction >>> from river import preprocessing >>> X = [ ... 'weather cold' , ... 'weather hot dry' , ... 'weather cold rainy' , ... 'weather hot' , ... 'weather cold humid' , ... ] >>> lda = compose . Pipeline ( ... feature_extraction . BagOfWords (), ... preprocessing . LDA ( ... n_components = 2 , ... number_of_documents = 60 , ... seed = 42 ... ) ... ) >>> for x in X : ... lda = lda . learn_one ( x ) ... topics = lda . transform_one ( x ) ... print ( topics ) { 0 : 2.5 , 1 : 0.5 } { 0 : 2.5 , 1 : 1.5 } { 0 : 1.5 , 1 : 2.5 } { 0 : 1.5 , 1 : 1.5 } { 0 : 0.5 , 1 : 3.5 }","title":"Examples"},{"location":"api/preprocessing/LDA/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self learn_transform_one Equivalent to lda.learn_one(x).transform_one(x) s, but faster. Parameters x ( dict ) Returns dict : Component attributions for the input document. transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/LDA/#references","text":"Zhai, K. and Boyd-Graber, J., 2013, February. Online latent Dirichlet allocation with infinite vocabulary. In International Conference on Machine Learning (pp. 561-569). \u21a9 PyInfVoc on GitHub \u21a9","title":"References"},{"location":"api/preprocessing/MaxAbsScaler/","text":"MaxAbsScaler \u00b6 Scales the data to a [-1, 1] range based on absolute maximum. Under the hood a running absolute max is maintained. This scaler is meant for data that is already centered at zero or sparse data. It does not shift/center the data, and thus does not destroy any sparsity. Attributes \u00b6 abs_max ( dict ) Mapping between features and instances of stats.AbsMax . Examples \u00b6 >>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> for x in X : ... print ( x ) { 'x' : 10.557707 } { 'x' : 8.100043 } { 'x' : 9.100117 } { 'x' : 8.892842 } { 'x' : 10.945884 } >>> scaler = preprocessing . MaxAbsScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 1.0 } { 'x' : 0.767216 } { 'x' : 0.861940 } { 'x' : 0.842308 } { 'x' : 1.0 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"MaxAbsScaler"},{"location":"api/preprocessing/MaxAbsScaler/#maxabsscaler","text":"Scales the data to a [-1, 1] range based on absolute maximum. Under the hood a running absolute max is maintained. This scaler is meant for data that is already centered at zero or sparse data. It does not shift/center the data, and thus does not destroy any sparsity.","title":"MaxAbsScaler"},{"location":"api/preprocessing/MaxAbsScaler/#attributes","text":"abs_max ( dict ) Mapping between features and instances of stats.AbsMax .","title":"Attributes"},{"location":"api/preprocessing/MaxAbsScaler/#examples","text":">>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> for x in X : ... print ( x ) { 'x' : 10.557707 } { 'x' : 8.100043 } { 'x' : 9.100117 } { 'x' : 8.892842 } { 'x' : 10.945884 } >>> scaler = preprocessing . MaxAbsScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 1.0 } { 'x' : 0.767216 } { 'x' : 0.861940 } { 'x' : 0.842308 } { 'x' : 1.0 }","title":"Examples"},{"location":"api/preprocessing/MaxAbsScaler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/MinMaxScaler/","text":"MinMaxScaler \u00b6 Scales the data to a fixed range from 0 to 1. Under the hood a running min and a running peak to peak (max - min) are maintained. Attributes \u00b6 min ( dict ) Mapping between features and instances of stats.Min . max ( dict ) Mapping between features and instances of stats.Max . Examples \u00b6 >>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> for x in X : ... print ( x ) { 'x' : 10.557707 } { 'x' : 8.100043 } { 'x' : 9.100117 } { 'x' : 8.892842 } { 'x' : 10.945884 } >>> scaler = preprocessing . MinMaxScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : 0.0 } { 'x' : 0.406920 } { 'x' : 0.322582 } { 'x' : 1.0 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"MinMaxScaler"},{"location":"api/preprocessing/MinMaxScaler/#minmaxscaler","text":"Scales the data to a fixed range from 0 to 1. Under the hood a running min and a running peak to peak (max - min) are maintained.","title":"MinMaxScaler"},{"location":"api/preprocessing/MinMaxScaler/#attributes","text":"min ( dict ) Mapping between features and instances of stats.Min . max ( dict ) Mapping between features and instances of stats.Max .","title":"Attributes"},{"location":"api/preprocessing/MinMaxScaler/#examples","text":">>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> for x in X : ... print ( x ) { 'x' : 10.557707 } { 'x' : 8.100043 } { 'x' : 9.100117 } { 'x' : 8.892842 } { 'x' : 10.945884 } >>> scaler = preprocessing . MinMaxScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : 0.0 } { 'x' : 0.406920 } { 'x' : 0.322582 } { 'x' : 1.0 }","title":"Examples"},{"location":"api/preprocessing/MinMaxScaler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/Normalizer/","text":"Normalizer \u00b6 Scales a set of features so that it has unit norm. This is particularly useful when used after a feature_extraction.TFIDF . Parameters \u00b6 order \u2013 defaults to 2 Order of the norm (e.g. 2 corresponds to the \\(L^2\\) norm). Examples \u00b6 >>> from river import preprocessing >>> from river import stream >>> scaler = preprocessing . Normalizer ( order = 2 ) >>> X = [[ 4 , 1 , 2 , 2 ], ... [ 1 , 3 , 9 , 3 ], ... [ 5 , 7 , 5 , 1 ]] >>> for x , _ in stream . iter_array ( X ): ... print ( scaler . transform_one ( x )) { 0 : 0.8 , 1 : 0.2 , 2 : 0.4 , 3 : 0.4 } { 0 : 0.1 , 1 : 0.3 , 2 : 0.9 , 3 : 0.3 } { 0 : 0.5 , 1 : 0.7 , 2 : 0.5 , 3 : 0.1 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Normalizer"},{"location":"api/preprocessing/Normalizer/#normalizer","text":"Scales a set of features so that it has unit norm. This is particularly useful when used after a feature_extraction.TFIDF .","title":"Normalizer"},{"location":"api/preprocessing/Normalizer/#parameters","text":"order \u2013 defaults to 2 Order of the norm (e.g. 2 corresponds to the \\(L^2\\) norm).","title":"Parameters"},{"location":"api/preprocessing/Normalizer/#examples","text":">>> from river import preprocessing >>> from river import stream >>> scaler = preprocessing . Normalizer ( order = 2 ) >>> X = [[ 4 , 1 , 2 , 2 ], ... [ 1 , 3 , 9 , 3 ], ... [ 5 , 7 , 5 , 1 ]] >>> for x , _ in stream . iter_array ( X ): ... print ( scaler . transform_one ( x )) { 0 : 0.8 , 1 : 0.2 , 2 : 0.4 , 3 : 0.4 } { 0 : 0.1 , 1 : 0.3 , 2 : 0.9 , 3 : 0.3 } { 0 : 0.5 , 1 : 0.7 , 2 : 0.5 , 3 : 0.1 }","title":"Examples"},{"location":"api/preprocessing/Normalizer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) kwargs Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/OneHotEncoder/","text":"OneHotEncoder \u00b6 One-hot encoding. This transformer will encode every feature it is provided it with. You can apply it to a subset of features by composing it with compose.Select or compose.SelectType . Parameters \u00b6 sparse \u2013 defaults to False Whether or not 0s should be made explicit or not. Examples \u00b6 Let us first create an example dataset. >>> from pprint import pprint >>> import random >>> import string >>> random . seed ( 42 ) >>> alphabet = list ( string . ascii_lowercase ) >>> X = [ ... { ... 'c1' : random . choice ( alphabet ), ... 'c2' : random . choice ( alphabet ), ... } ... for _ in range ( 4 ) ... ] >>> pprint ( X ) [{ 'c1' : 'u' , 'c2' : 'd' }, { 'c1' : 'a' , 'c2' : 'x' }, { 'c1' : 'i' , 'c2' : 'h' }, { 'c1' : 'h' , 'c2' : 'e' }] We can now apply one-hot encoding. All the provided are one-hot encoded, there is therefore no need to specify which features to encode. >>> import river.preprocessing >>> oh = river . preprocessing . OneHotEncoder ( sparse = True ) >>> for x in X : ... oh = oh . learn_one ( x ) ... pprint ( oh . transform_one ( x )) { 'c1_u' : 1 , 'c2_d' : 1 } { 'c1_a' : 1 , 'c2_x' : 1 } { 'c1_i' : 1 , 'c2_h' : 1 } { 'c1_h' : 1 , 'c2_e' : 1 } The sparse parameter can be set to False in order to include the values that are not present in the output. >>> oh = river . preprocessing . OneHotEncoder ( sparse = False ) >>> for x in X [: 2 ]: ... oh = oh . learn_one ( x ) ... pprint ( oh . transform_one ( x )) { 'c1_u' : 1 , 'c2_d' : 1 } { 'c1_a' : 1 , 'c1_u' : 0 , 'c2_d' : 0 , 'c2_x' : 1 } A subset of the features can be one-hot encoded by using an instance of compose.Select . >>> from river import compose >>> pp = compose . Select ( 'c1' ) | river . preprocessing . OneHotEncoder () >>> for x in X : ... pp = pp . learn_one ( x ) ... pprint ( pp . transform_one ( x )) { 'c1_u' : 1 } { 'c1_a' : 1 , 'c1_u' : 0 } { 'c1_a' : 0 , 'c1_i' : 1 , 'c1_u' : 0 } { 'c1_a' : 0 , 'c1_h' : 1 , 'c1_i' : 0 , 'c1_u' : 0 } You can preserve the c2 feature by using a union: >>> pp = compose . Select ( 'c1' ) | river . preprocessing . OneHotEncoder () >>> pp += compose . Select ( 'c2' ) >>> for x in X : ... pp = pp . learn_one ( x ) ... pprint ( pp . transform_one ( x )) { 'c1_u' : 1 , 'c2' : 'd' } { 'c1_a' : 1 , 'c1_u' : 0 , 'c2' : 'x' } { 'c1_a' : 0 , 'c1_i' : 1 , 'c1_u' : 0 , 'c2' : 'h' } { 'c1_a' : 0 , 'c1_h' : 1 , 'c1_i' : 0 , 'c1_u' : 0 , 'c2' : 'e' } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) y \u2013 defaults to None Returns dict : The transformed values.","title":"OneHotEncoder"},{"location":"api/preprocessing/OneHotEncoder/#onehotencoder","text":"One-hot encoding. This transformer will encode every feature it is provided it with. You can apply it to a subset of features by composing it with compose.Select or compose.SelectType .","title":"OneHotEncoder"},{"location":"api/preprocessing/OneHotEncoder/#parameters","text":"sparse \u2013 defaults to False Whether or not 0s should be made explicit or not.","title":"Parameters"},{"location":"api/preprocessing/OneHotEncoder/#examples","text":"Let us first create an example dataset. >>> from pprint import pprint >>> import random >>> import string >>> random . seed ( 42 ) >>> alphabet = list ( string . ascii_lowercase ) >>> X = [ ... { ... 'c1' : random . choice ( alphabet ), ... 'c2' : random . choice ( alphabet ), ... } ... for _ in range ( 4 ) ... ] >>> pprint ( X ) [{ 'c1' : 'u' , 'c2' : 'd' }, { 'c1' : 'a' , 'c2' : 'x' }, { 'c1' : 'i' , 'c2' : 'h' }, { 'c1' : 'h' , 'c2' : 'e' }] We can now apply one-hot encoding. All the provided are one-hot encoded, there is therefore no need to specify which features to encode. >>> import river.preprocessing >>> oh = river . preprocessing . OneHotEncoder ( sparse = True ) >>> for x in X : ... oh = oh . learn_one ( x ) ... pprint ( oh . transform_one ( x )) { 'c1_u' : 1 , 'c2_d' : 1 } { 'c1_a' : 1 , 'c2_x' : 1 } { 'c1_i' : 1 , 'c2_h' : 1 } { 'c1_h' : 1 , 'c2_e' : 1 } The sparse parameter can be set to False in order to include the values that are not present in the output. >>> oh = river . preprocessing . OneHotEncoder ( sparse = False ) >>> for x in X [: 2 ]: ... oh = oh . learn_one ( x ) ... pprint ( oh . transform_one ( x )) { 'c1_u' : 1 , 'c2_d' : 1 } { 'c1_a' : 1 , 'c1_u' : 0 , 'c2_d' : 0 , 'c2_x' : 1 } A subset of the features can be one-hot encoded by using an instance of compose.Select . >>> from river import compose >>> pp = compose . Select ( 'c1' ) | river . preprocessing . OneHotEncoder () >>> for x in X : ... pp = pp . learn_one ( x ) ... pprint ( pp . transform_one ( x )) { 'c1_u' : 1 } { 'c1_a' : 1 , 'c1_u' : 0 } { 'c1_a' : 0 , 'c1_i' : 1 , 'c1_u' : 0 } { 'c1_a' : 0 , 'c1_h' : 1 , 'c1_i' : 0 , 'c1_u' : 0 } You can preserve the c2 feature by using a union: >>> pp = compose . Select ( 'c1' ) | river . preprocessing . OneHotEncoder () >>> pp += compose . Select ( 'c2' ) >>> for x in X : ... pp = pp . learn_one ( x ) ... pprint ( pp . transform_one ( x )) { 'c1_u' : 1 , 'c2' : 'd' } { 'c1_a' : 1 , 'c1_u' : 0 , 'c2' : 'x' } { 'c1_a' : 0 , 'c1_i' : 1 , 'c1_u' : 0 , 'c2' : 'h' } { 'c1_a' : 0 , 'c1_h' : 1 , 'c1_i' : 0 , 'c1_u' : 0 , 'c2' : 'e' }","title":"Examples"},{"location":"api/preprocessing/OneHotEncoder/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) y \u2013 defaults to None Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/PreviousImputer/","text":"PreviousImputer \u00b6 Imputes missing values by using the most recent value. Examples \u00b6 >>> from river import preprocessing >>> imputer = preprocessing . PreviousImputer () >>> imputer = imputer . learn_one ({ 'x' : 1 , 'y' : 2 }) >>> imputer . transform_one ({ 'y' : None }) { 'y' : 2 } >>> imputer . transform_one ({ 'x' : None }) { 'x' : 1 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"PreviousImputer"},{"location":"api/preprocessing/PreviousImputer/#previousimputer","text":"Imputes missing values by using the most recent value.","title":"PreviousImputer"},{"location":"api/preprocessing/PreviousImputer/#examples","text":">>> from river import preprocessing >>> imputer = preprocessing . PreviousImputer () >>> imputer = imputer . learn_one ({ 'x' : 1 , 'y' : 2 }) >>> imputer . transform_one ({ 'y' : None }) { 'y' : 2 } >>> imputer . transform_one ({ 'x' : None }) { 'x' : 1 }","title":"Examples"},{"location":"api/preprocessing/PreviousImputer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/RobustScaler/","text":"RobustScaler \u00b6 Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the interquantile range. Parameters \u00b6 with_centering \u2013 defaults to True Whether to centre the data before scaling. with_scaling \u2013 defaults to True Whether to scale data to IQR. q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1. Attributes \u00b6 median ( dict ) Mapping between features and instances of stats.Quantile(0.5) . iqr ( dict ) Mapping between features and instances of stats.IQR . Examples \u00b6 >>> from pprint import pprint >>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> pprint ( X ) [{ 'x' : 10.557707 }, { 'x' : 8.100043 }, { 'x' : 9.100117 }, { 'x' : 8.892842 }, { 'x' : 10.945884 }] >>> scaler = preprocessing . RobustScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : - 1.0 } { 'x' : 0.0 } { 'x' : - 0.124499 } { 'x' : 1.108659 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"RobustScaler"},{"location":"api/preprocessing/RobustScaler/#robustscaler","text":"Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the interquantile range.","title":"RobustScaler"},{"location":"api/preprocessing/RobustScaler/#parameters","text":"with_centering \u2013 defaults to True Whether to centre the data before scaling. with_scaling \u2013 defaults to True Whether to scale data to IQR. q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1.","title":"Parameters"},{"location":"api/preprocessing/RobustScaler/#attributes","text":"median ( dict ) Mapping between features and instances of stats.Quantile(0.5) . iqr ( dict ) Mapping between features and instances of stats.IQR .","title":"Attributes"},{"location":"api/preprocessing/RobustScaler/#examples","text":">>> from pprint import pprint >>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 )} for _ in range ( 5 )] >>> pprint ( X ) [{ 'x' : 10.557707 }, { 'x' : 8.100043 }, { 'x' : 9.100117 }, { 'x' : 8.892842 }, { 'x' : 10.945884 }] >>> scaler = preprocessing . RobustScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 } { 'x' : - 1.0 } { 'x' : 0.0 } { 'x' : - 0.124499 } { 'x' : 1.108659 }","title":"Examples"},{"location":"api/preprocessing/RobustScaler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/StandardScaler/","text":"StandardScaler \u00b6 Scales the data so that it has zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. This transformer supports mini-batches as well as single instances. In the mini-batch case, the number of columns and the ordering of the columns are allowed to change between subsequent calls. In other words, this transformer will keep working even if you add and/or remove features every time you call learn_many and transform_many . Examples \u00b6 >>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 ), 'y' : random . uniform ( 8 , 12 )} for _ in range ( 6 )] >>> for x in X : ... print ( x ) { 'x' : 10.557 , 'y' : 8.100 } { 'x' : 9.100 , 'y' : 8.892 } { 'x' : 10.945 , 'y' : 10.706 } { 'x' : 11.568 , 'y' : 8.347 } { 'x' : 9.687 , 'y' : 8.119 } { 'x' : 8.874 , 'y' : 10.021 } >>> scaler = preprocessing . StandardScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 , 'y' : 0.0 } { 'x' : - 0.999 , 'y' : 0.999 } { 'x' : 0.937 , 'y' : 1.350 } { 'x' : 1.129 , 'y' : - 0.651 } { 'x' : - 0.776 , 'y' : - 0.729 } { 'x' : - 1.274 , 'y' : 0.992 } This transformer also supports mini-batch updates. You can call learn_many and provide a pandas.DataFrame : >>> import pandas as pd >>> X = pd . DataFrame . from_dict ( X ) >>> scaler = preprocessing . StandardScaler () >>> scaler = scaler . learn_many ( X [: 3 ]) >>> scaler = scaler . learn_many ( X [ 3 :]) You can then call transform_many to scale a mini-batch of features: >>> scaler . transform_many ( X ) x y 0 0.444600 - 0.933384 1 - 1.044259 - 0.138809 2 0.841106 1.679208 3 1.477301 - 0.685117 4 - 0.444084 - 0.914195 5 - 1.274664 0.992296 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update with a mini-batch of features. Note that the update formulas for mean and variance are slightly different than in the single instance case, but they produce exactly the same result. Parameters X ( pandas.core.frame.DataFrame ) learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_many Scale a mini-batch of features. Parameters X ( pandas.core.frame.DataFrame ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values. References \u00b6 Welford's Method (and Friends) \u21a9 Batch updates for simple statistics \u21a9","title":"StandardScaler"},{"location":"api/preprocessing/StandardScaler/#standardscaler","text":"Scales the data so that it has zero mean and unit variance. Under the hood, a running mean and a running variance are maintained. The scaling is slightly different than when scaling the data in batch because the exact means and variances are not known in advance. However, this doesn't have a detrimental impact on performance in the long run. This transformer supports mini-batches as well as single instances. In the mini-batch case, the number of columns and the ordering of the columns are allowed to change between subsequent calls. In other words, this transformer will keep working even if you add and/or remove features every time you call learn_many and transform_many .","title":"StandardScaler"},{"location":"api/preprocessing/StandardScaler/#examples","text":">>> import random >>> from river import preprocessing >>> random . seed ( 42 ) >>> X = [{ 'x' : random . uniform ( 8 , 12 ), 'y' : random . uniform ( 8 , 12 )} for _ in range ( 6 )] >>> for x in X : ... print ( x ) { 'x' : 10.557 , 'y' : 8.100 } { 'x' : 9.100 , 'y' : 8.892 } { 'x' : 10.945 , 'y' : 10.706 } { 'x' : 11.568 , 'y' : 8.347 } { 'x' : 9.687 , 'y' : 8.119 } { 'x' : 8.874 , 'y' : 10.021 } >>> scaler = preprocessing . StandardScaler () >>> for x in X : ... print ( scaler . learn_one ( x ) . transform_one ( x )) { 'x' : 0.0 , 'y' : 0.0 } { 'x' : - 0.999 , 'y' : 0.999 } { 'x' : 0.937 , 'y' : 1.350 } { 'x' : 1.129 , 'y' : - 0.651 } { 'x' : - 0.776 , 'y' : - 0.729 } { 'x' : - 1.274 , 'y' : 0.992 } This transformer also supports mini-batch updates. You can call learn_many and provide a pandas.DataFrame : >>> import pandas as pd >>> X = pd . DataFrame . from_dict ( X ) >>> scaler = preprocessing . StandardScaler () >>> scaler = scaler . learn_many ( X [: 3 ]) >>> scaler = scaler . learn_many ( X [ 3 :]) You can then call transform_many to scale a mini-batch of features: >>> scaler . transform_many ( X ) x y 0 0.444600 - 0.933384 1 - 1.044259 - 0.138809 2 0.841106 1.679208 3 1.477301 - 0.685117 4 - 0.444084 - 0.914195 5 - 1.274664 0.992296","title":"Examples"},{"location":"api/preprocessing/StandardScaler/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_many Update with a mini-batch of features. Note that the update formulas for mean and variance are slightly different than in the single instance case, but they produce exactly the same result. Parameters X ( pandas.core.frame.DataFrame ) learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_many Scale a mini-batch of features. Parameters X ( pandas.core.frame.DataFrame ) transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/preprocessing/StandardScaler/#references","text":"Welford's Method (and Friends) \u21a9 Batch updates for simple statistics \u21a9","title":"References"},{"location":"api/preprocessing/StatImputer/","text":"StatImputer \u00b6 Replaces missing values with a statistic. This transformer allows you to replace missing values with the value of a running statistic. During a call to learn_one , for each feature, a statistic is updated whenever a numeric feature is observed. When transform_one is called, each feature with a None value is replaced with the current value of the corresponding statistic. Parameters \u00b6 imputers A list of tuples where each tuple has two elements. The first elements is a feature name and the second value is an instance of stats.Univariate . The second value can also be an arbitrary value, such as -1, in which case the missing values will be replaced with it. Examples \u00b6 >>> from river import preprocessing >>> from river import stats For numeric data, we can use a stats.Mean() to replace missing values by the running average of the previously seen values: >>> X = [ ... { 'temperature' : 1 }, ... { 'temperature' : 8 }, ... { 'temperature' : 3 }, ... { 'temperature' : None }, ... { 'temperature' : 4 } ... ] >>> imp = preprocessing . StatImputer (( 'temperature' , stats . Mean ())) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'temperature' : 1 } { 'temperature' : 8 } { 'temperature' : 3 } { 'temperature' : 4.0 } { 'temperature' : 4 } For discrete/categorical data, a common practice is to stats.Mode to replace missing values by the most commonly seen value: >>> X = [ ... { 'weather' : 'sunny' }, ... { 'weather' : 'rainy' }, ... { 'weather' : 'sunny' }, ... { 'weather' : None }, ... { 'weather' : 'rainy' }, ... { 'weather' : 'rainy' }, ... { 'weather' : None } ... ] >>> imp = preprocessing . StatImputer (( 'weather' , stats . Mode ())) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'sunny' } { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } You can also choose to replace missing values with a constant value, as so: >>> imp = preprocessing . StatImputer (( 'weather' , 'missing' )) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'sunny' } { 'weather' : 'missing' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } { 'weather' : 'missing' } Multiple imputers can be defined by providing a tuple for each feature which you want to impute: >>> X = [ ... { 'weather' : 'sunny' , 'temperature' : 8 }, ... { 'weather' : 'rainy' , 'temperature' : 3 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : None , 'temperature' : 4 }, ... { 'weather' : 'snowy' , 'temperature' : - 4 }, ... { 'weather' : 'snowy' , 'temperature' : - 3 }, ... { 'weather' : 'snowy' , 'temperature' : - 3 }, ... { 'weather' : None , 'temperature' : None } ... ] >>> imp = preprocessing . StatImputer ( ... ( 'temperature' , stats . Mean ()), ... ( 'weather' , stats . Mode ()) ... ) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' , 'temperature' : 8 } { 'weather' : 'rainy' , 'temperature' : 3 } { 'weather' : 'sunny' , 'temperature' : 5.5 } { 'weather' : 'sunny' , 'temperature' : 4 } { 'weather' : 'snowy' , 'temperature' : - 4 } { 'weather' : 'snowy' , 'temperature' : - 3 } { 'weather' : 'snowy' , 'temperature' : - 3 } { 'weather' : 'snowy' , 'temperature' : 0.8333 } A sophisticated way to go about imputation is condition the statistics on a given feature. For instance, we might want to replace a missing temperature with the average temperature of a particular weather condition. As an example, consider the following dataset where the temperature is missing, but not the weather condition: >>> X = [ ... { 'weather' : 'sunny' , 'temperature' : 8 }, ... { 'weather' : 'rainy' , 'temperature' : 3 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : 'rainy' , 'temperature' : 4 }, ... { 'weather' : 'sunny' , 'temperature' : 10 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : 'sunny' , 'temperature' : 12 }, ... { 'weather' : 'rainy' , 'temperature' : None } ... ] Each missing temperature can be replaced with the average temperature of the corresponding weather condition as so: >>> from river import compose >>> imp = compose . Grouper ( ... preprocessing . StatImputer (( 'temperature' , stats . Mean ())), ... by = 'weather' ... ) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' , 'temperature' : 8 } { 'weather' : 'rainy' , 'temperature' : 3 } { 'weather' : 'sunny' , 'temperature' : 8.0 } { 'weather' : 'rainy' , 'temperature' : 4 } { 'weather' : 'sunny' , 'temperature' : 10 } { 'weather' : 'sunny' , 'temperature' : 9.0 } { 'weather' : 'sunny' , 'temperature' : 12 } { 'weather' : 'rainy' , 'temperature' : 3.5 } Note that you can also create a Grouper with the * operator: >>> imp = preprocessing . StatImputer (( 'temperature' , stats . Mean ())) * 'weather' Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"StatImputer"},{"location":"api/preprocessing/StatImputer/#statimputer","text":"Replaces missing values with a statistic. This transformer allows you to replace missing values with the value of a running statistic. During a call to learn_one , for each feature, a statistic is updated whenever a numeric feature is observed. When transform_one is called, each feature with a None value is replaced with the current value of the corresponding statistic.","title":"StatImputer"},{"location":"api/preprocessing/StatImputer/#parameters","text":"imputers A list of tuples where each tuple has two elements. The first elements is a feature name and the second value is an instance of stats.Univariate . The second value can also be an arbitrary value, such as -1, in which case the missing values will be replaced with it.","title":"Parameters"},{"location":"api/preprocessing/StatImputer/#examples","text":">>> from river import preprocessing >>> from river import stats For numeric data, we can use a stats.Mean() to replace missing values by the running average of the previously seen values: >>> X = [ ... { 'temperature' : 1 }, ... { 'temperature' : 8 }, ... { 'temperature' : 3 }, ... { 'temperature' : None }, ... { 'temperature' : 4 } ... ] >>> imp = preprocessing . StatImputer (( 'temperature' , stats . Mean ())) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'temperature' : 1 } { 'temperature' : 8 } { 'temperature' : 3 } { 'temperature' : 4.0 } { 'temperature' : 4 } For discrete/categorical data, a common practice is to stats.Mode to replace missing values by the most commonly seen value: >>> X = [ ... { 'weather' : 'sunny' }, ... { 'weather' : 'rainy' }, ... { 'weather' : 'sunny' }, ... { 'weather' : None }, ... { 'weather' : 'rainy' }, ... { 'weather' : 'rainy' }, ... { 'weather' : None } ... ] >>> imp = preprocessing . StatImputer (( 'weather' , stats . Mode ())) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'sunny' } { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } You can also choose to replace missing values with a constant value, as so: >>> imp = preprocessing . StatImputer (( 'weather' , 'missing' )) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' } { 'weather' : 'rainy' } { 'weather' : 'sunny' } { 'weather' : 'missing' } { 'weather' : 'rainy' } { 'weather' : 'rainy' } { 'weather' : 'missing' } Multiple imputers can be defined by providing a tuple for each feature which you want to impute: >>> X = [ ... { 'weather' : 'sunny' , 'temperature' : 8 }, ... { 'weather' : 'rainy' , 'temperature' : 3 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : None , 'temperature' : 4 }, ... { 'weather' : 'snowy' , 'temperature' : - 4 }, ... { 'weather' : 'snowy' , 'temperature' : - 3 }, ... { 'weather' : 'snowy' , 'temperature' : - 3 }, ... { 'weather' : None , 'temperature' : None } ... ] >>> imp = preprocessing . StatImputer ( ... ( 'temperature' , stats . Mean ()), ... ( 'weather' , stats . Mode ()) ... ) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' , 'temperature' : 8 } { 'weather' : 'rainy' , 'temperature' : 3 } { 'weather' : 'sunny' , 'temperature' : 5.5 } { 'weather' : 'sunny' , 'temperature' : 4 } { 'weather' : 'snowy' , 'temperature' : - 4 } { 'weather' : 'snowy' , 'temperature' : - 3 } { 'weather' : 'snowy' , 'temperature' : - 3 } { 'weather' : 'snowy' , 'temperature' : 0.8333 } A sophisticated way to go about imputation is condition the statistics on a given feature. For instance, we might want to replace a missing temperature with the average temperature of a particular weather condition. As an example, consider the following dataset where the temperature is missing, but not the weather condition: >>> X = [ ... { 'weather' : 'sunny' , 'temperature' : 8 }, ... { 'weather' : 'rainy' , 'temperature' : 3 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : 'rainy' , 'temperature' : 4 }, ... { 'weather' : 'sunny' , 'temperature' : 10 }, ... { 'weather' : 'sunny' , 'temperature' : None }, ... { 'weather' : 'sunny' , 'temperature' : 12 }, ... { 'weather' : 'rainy' , 'temperature' : None } ... ] Each missing temperature can be replaced with the average temperature of the corresponding weather condition as so: >>> from river import compose >>> imp = compose . Grouper ( ... preprocessing . StatImputer (( 'temperature' , stats . Mean ())), ... by = 'weather' ... ) >>> for x in X : ... imp = imp . learn_one ( x ) ... print ( imp . transform_one ( x )) { 'weather' : 'sunny' , 'temperature' : 8 } { 'weather' : 'rainy' , 'temperature' : 3 } { 'weather' : 'sunny' , 'temperature' : 8.0 } { 'weather' : 'rainy' , 'temperature' : 4 } { 'weather' : 'sunny' , 'temperature' : 10 } { 'weather' : 'sunny' , 'temperature' : 9.0 } { 'weather' : 'sunny' , 'temperature' : 12 } { 'weather' : 'rainy' , 'temperature' : 3.5 } Note that you can also create a Grouper with the * operator: >>> imp = preprocessing . StatImputer (( 'temperature' , stats . Mean ())) * 'weather'","title":"Examples"},{"location":"api/preprocessing/StatImputer/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Update with a set of features x . A lot of transformers don't actually have to do anything during the learn_one step because they are stateless. For this reason the default behavior of this function is to do nothing. Transformers that however do something during the learn_one can override this method. Parameters x ( dict ) Returns Transformer : self transform_one Transform a set of features x . Parameters x ( dict ) Returns dict : The transformed values.","title":"Methods"},{"location":"api/proba/Gaussian/","text":"Gaussian \u00b6 Normal distribution with parameters mu and sigma. Attributes \u00b6 mode Most likely value. mu n_samples The number of observed samples. sigma Examples \u00b6 >>> from river import proba >>> p = proba . Gaussian () . update ( 6 ) . update ( 7 ) >>> p \ud835\udca9 ( \u03bc = 6.500 , \u03c3 = 0.707 ) >>> p . pdf ( 6.5 ) 0.564189 Methods \u00b6 cdf Cumulative density function, i.e. P(X <= x). Parameters x pdf Probability density function, i.e. P(x <= X < x+dx) / dx. Parameters x update Updates the parameters of the distribution given a new observation. Parameters x w \u2013 defaults to 1.0","title":"Gaussian"},{"location":"api/proba/Gaussian/#gaussian","text":"Normal distribution with parameters mu and sigma.","title":"Gaussian"},{"location":"api/proba/Gaussian/#attributes","text":"mode Most likely value. mu n_samples The number of observed samples. sigma","title":"Attributes"},{"location":"api/proba/Gaussian/#examples","text":">>> from river import proba >>> p = proba . Gaussian () . update ( 6 ) . update ( 7 ) >>> p \ud835\udca9 ( \u03bc = 6.500 , \u03c3 = 0.707 ) >>> p . pdf ( 6.5 ) 0.564189","title":"Examples"},{"location":"api/proba/Gaussian/#methods","text":"cdf Cumulative density function, i.e. P(X <= x). Parameters x pdf Probability density function, i.e. P(x <= X < x+dx) / dx. Parameters x update Updates the parameters of the distribution given a new observation. Parameters x w \u2013 defaults to 1.0","title":"Methods"},{"location":"api/proba/Multinomial/","text":"Multinomial \u00b6 Multinomial distribution for categorical data. Parameters \u00b6 events ( Union[dict, list] ) \u2013 defaults to None An optional list of events that already occurred. Attributes \u00b6 mode n_samples The number of observed samples. Examples \u00b6 >>> from river import proba >>> p = proba . Multinomial ([ 'green' ] * 3 ) >>> p = p . update ( 'red' ) >>> p . pmf ( 'red' ) 0.25 >>> p . update ( 'red' ) . update ( 'red' ) . pmf ( 'green' ) 0.5 Methods \u00b6 pmf Probability mass function. Parameters x update Updates the parameters of the distribution given a new observation. Parameters x","title":"Multinomial"},{"location":"api/proba/Multinomial/#multinomial","text":"Multinomial distribution for categorical data.","title":"Multinomial"},{"location":"api/proba/Multinomial/#parameters","text":"events ( Union[dict, list] ) \u2013 defaults to None An optional list of events that already occurred.","title":"Parameters"},{"location":"api/proba/Multinomial/#attributes","text":"mode n_samples The number of observed samples.","title":"Attributes"},{"location":"api/proba/Multinomial/#examples","text":">>> from river import proba >>> p = proba . Multinomial ([ 'green' ] * 3 ) >>> p = p . update ( 'red' ) >>> p . pmf ( 'red' ) 0.25 >>> p . update ( 'red' ) . update ( 'red' ) . pmf ( 'green' ) 0.5","title":"Examples"},{"location":"api/proba/Multinomial/#methods","text":"pmf Probability mass function. Parameters x update Updates the parameters of the distribution given a new observation. Parameters x","title":"Methods"},{"location":"api/reco/Baseline/","text":"Baseline \u00b6 Baseline for recommender systems. A first-order approximation of the bias involved in target. The model equation is defined as: \\[\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i}\\] Where \\(bu_{u}\\) and \\(bi_{i}\\) are respectively the user and item biases. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored. Parameters \u00b6 optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 regularization amount used to push weights towards 0. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. Attributes \u00b6 global_mean ( stats.Mean ) The target arithmetic mean. u_biases ( collections.defaultdict ) The user bias weights. i_biases ( collections.defaultdict ) The item bias weights. u_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user bias weights. i_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item bias weights. Examples \u00b6 >>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . Baseline ( optimizer = optim . SGD ( 0.005 )) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.538120 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. References \u00b6 Matrix factorization techniques for recommender systems \u21a9","title":"Baseline"},{"location":"api/reco/Baseline/#baseline","text":"Baseline for recommender systems. A first-order approximation of the bias involved in target. The model equation is defined as: \\[\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i}\\] Where \\(bu_{u}\\) and \\(bi_{i}\\) are respectively the user and item biases. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored.","title":"Baseline"},{"location":"api/reco/Baseline/#parameters","text":"optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the weights. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 regularization amount used to push weights towards 0. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value.","title":"Parameters"},{"location":"api/reco/Baseline/#attributes","text":"global_mean ( stats.Mean ) The target arithmetic mean. u_biases ( collections.defaultdict ) The user bias weights. i_biases ( collections.defaultdict ) The item bias weights. u_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user bias weights. i_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item bias weights.","title":"Attributes"},{"location":"api/reco/Baseline/#examples","text":">>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . Baseline ( optimizer = optim . SGD ( 0.005 )) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.538120","title":"Examples"},{"location":"api/reco/Baseline/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/reco/Baseline/#references","text":"Matrix factorization techniques for recommender systems \u21a9","title":"References"},{"location":"api/reco/BiasedMF/","text":"BiasedMF \u00b6 Biased Matrix Factorization for recommender systems. The model equation is defined as: \\[\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle\\] Where \\(bu_{u}\\) and \\(bi_{i}\\) are respectively the user and item biases. The last term being simply the dot product between the latent vectors of the given user-item pair: \\[\\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}\\] where \\(k\\) is the number of latent factors. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored. Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. bias_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the bias weights. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent weights. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2_bias \u2013 defaults to 0.0 Amount of L2 regularization used to push bias weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 global_mean ( stats.Mean ) The target arithmetic mean. u_biases ( collections.defaultdict ) The user bias weights. i_biases ( collections.defaultdict ) The item bias weights. u_latents ( collections.defaultdict ) The user latent vectors randomly initialized. i_latents ( collections.defaultdict ) The item latent vectors randomly initialized. u_bias_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user bias weights. i_bias_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item bias weights. u_latent_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user latent weights. i_latent_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item latent weights. Examples \u00b6 >>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . BiasedMF ( ... n_factors = 10 , ... bias_optimizer = optim . SGD ( 0.025 ), ... latent_optimizer = optim . SGD ( 0.025 ), ... latent_initializer = optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 71 ) ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.489025 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. References \u00b6 Paterek, A., 2007, August. Improving regularized singular value decomposition for collaborative filtering. In Proceedings of KDD cup and workshop (Vol. 2007, pp. 5-8) \u21a9 Matrix factorization techniques for recommender systems \u21a9","title":"BiasedMF"},{"location":"api/reco/BiasedMF/#biasedmf","text":"Biased Matrix Factorization for recommender systems. The model equation is defined as: \\[\\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle\\] Where \\(bu_{u}\\) and \\(bi_{i}\\) are respectively the user and item biases. The last term being simply the dot product between the latent vectors of the given user-item pair: \\[\\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}\\] where \\(k\\) is the number of latent factors. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored.","title":"BiasedMF"},{"location":"api/reco/BiasedMF/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. bias_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the bias weights. latent_optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent weights. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2_bias \u2013 defaults to 0.0 Amount of L2 regularization used to push bias weights towards 0. l2_latent \u2013 defaults to 0.0 Amount of L2 regularization used to push latent weights towards 0. weight_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Weights initialization scheme. latent_initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/reco/BiasedMF/#attributes","text":"global_mean ( stats.Mean ) The target arithmetic mean. u_biases ( collections.defaultdict ) The user bias weights. i_biases ( collections.defaultdict ) The item bias weights. u_latents ( collections.defaultdict ) The user latent vectors randomly initialized. i_latents ( collections.defaultdict ) The item latent vectors randomly initialized. u_bias_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user bias weights. i_bias_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item bias weights. u_latent_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user latent weights. i_latent_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item latent weights.","title":"Attributes"},{"location":"api/reco/BiasedMF/#examples","text":">>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . BiasedMF ( ... n_factors = 10 , ... bias_optimizer = optim . SGD ( 0.025 ), ... latent_optimizer = optim . SGD ( 0.025 ), ... latent_initializer = optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 71 ) ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.489025","title":"Examples"},{"location":"api/reco/BiasedMF/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/reco/BiasedMF/#references","text":"Paterek, A., 2007, August. Improving regularized singular value decomposition for collaborative filtering. In Proceedings of KDD cup and workshop (Vol. 2007, pp. 5-8) \u21a9 Matrix factorization techniques for recommender systems \u21a9","title":"References"},{"location":"api/reco/FunkMF/","text":"FunkMF \u00b6 Funk Matrix Factorization for recommender systems. The model equation is defined as: \\[\\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}\\] where \\(k\\) is the number of latent factors. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored. Parameters \u00b6 n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 u_latents ( collections.defaultdict ) The user latent vectors randomly initialized. i_latents ( collections.defaultdict ) The item latent vectors randomly initialized. u_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user latent weights. i_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item latent weights. Examples \u00b6 >>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . FunkMF ( ... n_factors = 10 , ... optimizer = optim . SGD ( 0.1 ), ... initializer = optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 11 ), ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 1.866272 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. References \u00b6 Netflix update: Try this at home \u21a9 Matrix factorization techniques for recommender systems \u21a9","title":"FunkMF"},{"location":"api/reco/FunkMF/#funkmf","text":"Funk Matrix Factorization for recommender systems. The model equation is defined as: \\[\\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f}\\] where \\(k\\) is the number of latent factors. This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored.","title":"FunkMF"},{"location":"api/reco/FunkMF/#parameters","text":"n_factors \u2013 defaults to 10 Dimensionality of the factorization or number of latent factors. optimizer ( optim.Optimizer ) \u2013 defaults to None The sequential optimizer used for updating the latent factors. loss ( optim.losses.Loss ) \u2013 defaults to None The loss function to optimize for. l2 \u2013 defaults to 0.0 Amount of L2 regularization used to push weights towards 0. initializer ( optim.initializers.Initializer ) \u2013 defaults to None Latent factors initialization scheme. clip_gradient \u2013 defaults to 1000000000000.0 Clips the absolute value of each gradient value. seed ( int ) \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/reco/FunkMF/#attributes","text":"u_latents ( collections.defaultdict ) The user latent vectors randomly initialized. i_latents ( collections.defaultdict ) The item latent vectors randomly initialized. u_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the user latent weights. i_optimizer ( optim.Optimizer ) The sequential optimizer used for updating the item latent weights.","title":"Attributes"},{"location":"api/reco/FunkMF/#examples","text":">>> from river import optim >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . FunkMF ( ... n_factors = 10 , ... optimizer = optim . SGD ( 0.1 ), ... initializer = optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 11 ), ... ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 1.866272","title":"Examples"},{"location":"api/reco/FunkMF/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/reco/FunkMF/#references","text":"Netflix update: Try this at home \u21a9 Matrix factorization techniques for recommender systems \u21a9","title":"References"},{"location":"api/reco/RandomNormal/","text":"RandomNormal \u00b6 Predicts random values sampled from a normal distribution. The parameters of the normal distribution are fitted with running statistics. This is equivalent to using surprise.prediction_algorithms.random_pred.NormalPredictor . This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored. Parameters \u00b6 seed \u2013 defaults to None Randomization seed used for reproducibility. Attributes \u00b6 mean stats.Mean variance stats.Var Examples \u00b6 >>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . RandomNormal ( seed = 42 ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.883895 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"RandomNormal"},{"location":"api/reco/RandomNormal/#randomnormal","text":"Predicts random values sampled from a normal distribution. The parameters of the normal distribution are fitted with running statistics. This is equivalent to using surprise.prediction_algorithms.random_pred.NormalPredictor . This model expects a dict input with a user and an item entries without any type constraint on their values (i.e. can be strings or numbers). Other entries are ignored.","title":"RandomNormal"},{"location":"api/reco/RandomNormal/#parameters","text":"seed \u2013 defaults to None Randomization seed used for reproducibility.","title":"Parameters"},{"location":"api/reco/RandomNormal/#attributes","text":"mean stats.Mean variance stats.Var","title":"Attributes"},{"location":"api/reco/RandomNormal/#examples","text":">>> from river import reco >>> dataset = ( ... ({ 'user' : 'Alice' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Alice' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Alice' , 'item' : 'Notting Hill' }, 2 ), ... ({ 'user' : 'Alice' , 'item' : 'Harry Potter' }, 5 ), ... ({ 'user' : 'Bob' , 'item' : 'Superman' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Terminator' }, 9 ), ... ({ 'user' : 'Bob' , 'item' : 'Star Wars' }, 8 ), ... ({ 'user' : 'Bob' , 'item' : 'Notting Hill' }, 2 ) ... ) >>> model = reco . RandomNormal ( seed = 42 ) >>> for x , y in dataset : ... _ = model . learn_one ( x , y ) >>> model . predict_one ({ 'user' : 'Bob' , 'item' : 'Harry Potter' }) 6.883895","title":"Examples"},{"location":"api/reco/RandomNormal/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/rules/AMRules/","text":"AMRules \u00b6 Adaptive Model Rules. AMRules 1 is a rule-based algorithm for incremental regression tasks. AMRules relies on the Hoeffding bound to build its rule set, similarly to Hoeffding Trees. The Variance-Ratio heuristic is used to evaluate rules' splits. Moreover, this rule-based regressor has additional capacities not usually found in decision trees. Firstly, each created decision rule has a built-in drift detection mechanism. Every time a drift is detected, the affected decision rule is removed. In addition, AMRules' rules also have anomaly detection capabilities. After a warm-up period, each rule tests whether or not the incoming instances are anomalies. Anomalous instances are not used for training. Every time no rule is covering an incoming example, a default rule is used to learn from it. A rule covers an instance when all of the rule's literals (tests joined by the logical operation and ) match the input case. The default rule is also applied for predicting examples not covered by any rules from the rule set. Parameters \u00b6 n_min ( int ) \u2013 defaults to 200 The total weight that must be observed by a rule between expansion attempts. delta ( float ) \u2013 defaults to 1e-07 The split test significance. The split confidence is given by 1 - delta . tau ( float ) \u2013 defaults to 0.05 The tie-breaking threshold. pred_type ( str ) \u2013 defaults to adaptive The prediction strategy used by the decision rules. Can be either: - \"mean\" : outputs the target mean within the partitions defined by the decision rules. - \"model\" : always use instances of the model passed pred_model to make predictions. - \"adaptive\" : dynamically selects between \"mean\" and \"model\" for each incoming example. The most accurate option at the moment will be used. pred_model ( base.Regressor ) \u2013 defaults to None The regression model that will be replicated for every rule when pred_type is either \"model\" or \"adaptive\" . splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . drift_detector ( base.DriftDetector ) \u2013 defaults to None The drift detection model that is used by each rule. Care must be taken to avoid the triggering of too many false alarms or delaying too much the concept drift detection. By default, drift.PageHinckley is used if drift_detector is None . alpha ( float ) \u2013 defaults to 0.99 The exponential decaying factor applied to the learning models' absolute errors, that are monitored if pred_type='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. anomaly_threshold ( float ) \u2013 defaults to -0.75 The threshold below which instances will be considered anomalies by the rules. m_min ( int ) \u2013 defaults to 30 The minimum total weight a rule must observe before it starts to skip anomalous instances during training. ordered_rule_set ( bool ) \u2013 defaults to True If True , only the first rule that covers an instance will be used for training or prediction. If False , all the rules covering an instance will be updated during training, and the predictions for an instance will be the average prediction of all rules covering that example. min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples each partition of a binary split candidate must have to be considered valid. Attributes \u00b6 n_drifts_detected The number of detected concept drifts. Examples \u00b6 >>> from river import datasets >>> from river import drift >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import rules >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... rules . AMRules ( ... delta = 0.00001 , ... n_min = 50 , ... drift_detector = drift . ADWIN () ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.079129 Methods \u00b6 anomaly_score Aggregated anomaly score computed using all the rules that cover the input instance. Returns the mean anomaly score, the standard deviation of the score, and the proportion of rules that cover the instance (support). If the support is zero, it means that the default rule was used (not other rule covered x ). Parameters x Returns typing.Tuple[float, float, float] : mean_anomaly_score, std_anomaly_score, support clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Return an explanation of how x is predicted Parameters x Returns str : A representation of the rules that cover the input and their prediction. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) w ( int ) \u2013 defaults to 1 Returns AMRules : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction. Notes \u00b6 AMRules treats all the non-numerical inputs as nominal features. All instances of numbers.Number will be treated as continuous, even if they represent integer categories. When using nominal features, pred_type should be set to \"mean\", otherwise errors will be thrown while trying to update the underlying rules' prediction models. Prediction strategies other than \"mean\" can be used, as long as the prediction model passed to pred_model supports nominal features. References \u00b6 Duarte, J., Gama, J. and Bifet, A., 2016. Adaptive model rules from high-speed data streams. ACM Transactions on Knowledge Discovery from Data (TKDD), 10(3), pp.1-22. \u21a9","title":"AMRules"},{"location":"api/rules/AMRules/#amrules","text":"Adaptive Model Rules. AMRules 1 is a rule-based algorithm for incremental regression tasks. AMRules relies on the Hoeffding bound to build its rule set, similarly to Hoeffding Trees. The Variance-Ratio heuristic is used to evaluate rules' splits. Moreover, this rule-based regressor has additional capacities not usually found in decision trees. Firstly, each created decision rule has a built-in drift detection mechanism. Every time a drift is detected, the affected decision rule is removed. In addition, AMRules' rules also have anomaly detection capabilities. After a warm-up period, each rule tests whether or not the incoming instances are anomalies. Anomalous instances are not used for training. Every time no rule is covering an incoming example, a default rule is used to learn from it. A rule covers an instance when all of the rule's literals (tests joined by the logical operation and ) match the input case. The default rule is also applied for predicting examples not covered by any rules from the rule set.","title":"AMRules"},{"location":"api/rules/AMRules/#parameters","text":"n_min ( int ) \u2013 defaults to 200 The total weight that must be observed by a rule between expansion attempts. delta ( float ) \u2013 defaults to 1e-07 The split test significance. The split confidence is given by 1 - delta . tau ( float ) \u2013 defaults to 0.05 The tie-breaking threshold. pred_type ( str ) \u2013 defaults to adaptive The prediction strategy used by the decision rules. Can be either: - \"mean\" : outputs the target mean within the partitions defined by the decision rules. - \"model\" : always use instances of the model passed pred_model to make predictions. - \"adaptive\" : dynamically selects between \"mean\" and \"model\" for each incoming example. The most accurate option at the moment will be used. pred_model ( base.Regressor ) \u2013 defaults to None The regression model that will be replicated for every rule when pred_type is either \"model\" or \"adaptive\" . splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . drift_detector ( base.DriftDetector ) \u2013 defaults to None The drift detection model that is used by each rule. Care must be taken to avoid the triggering of too many false alarms or delaying too much the concept drift detection. By default, drift.PageHinckley is used if drift_detector is None . alpha ( float ) \u2013 defaults to 0.99 The exponential decaying factor applied to the learning models' absolute errors, that are monitored if pred_type='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. anomaly_threshold ( float ) \u2013 defaults to -0.75 The threshold below which instances will be considered anomalies by the rules. m_min ( int ) \u2013 defaults to 30 The minimum total weight a rule must observe before it starts to skip anomalous instances during training. ordered_rule_set ( bool ) \u2013 defaults to True If True , only the first rule that covers an instance will be used for training or prediction. If False , all the rules covering an instance will be updated during training, and the predictions for an instance will be the average prediction of all rules covering that example. min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples each partition of a binary split candidate must have to be considered valid.","title":"Parameters"},{"location":"api/rules/AMRules/#attributes","text":"n_drifts_detected The number of detected concept drifts.","title":"Attributes"},{"location":"api/rules/AMRules/#examples","text":">>> from river import datasets >>> from river import drift >>> from river import evaluate >>> from river import metrics >>> from river import preprocessing >>> from river import rules >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... rules . AMRules ( ... delta = 0.00001 , ... n_min = 50 , ... drift_detector = drift . ADWIN () ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 1.079129","title":"Examples"},{"location":"api/rules/AMRules/#methods","text":"anomaly_score Aggregated anomaly score computed using all the rules that cover the input instance. Returns the mean anomaly score, the standard deviation of the score, and the proportion of rules that cover the instance (support). If the support is zero, it means that the default rule was used (not other rule covered x ). Parameters x Returns typing.Tuple[float, float, float] : mean_anomaly_score, std_anomaly_score, support clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Return an explanation of how x is predicted Parameters x Returns str : A representation of the rules that cover the input and their prediction. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) w ( int ) \u2013 defaults to 1 Returns AMRules : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/rules/AMRules/#notes","text":"AMRules treats all the non-numerical inputs as nominal features. All instances of numbers.Number will be treated as continuous, even if they represent integer categories. When using nominal features, pred_type should be set to \"mean\", otherwise errors will be thrown while trying to update the underlying rules' prediction models. Prediction strategies other than \"mean\" can be used, as long as the prediction model passed to pred_model supports nominal features.","title":"Notes"},{"location":"api/rules/AMRules/#references","text":"Duarte, J., Gama, J. and Bifet, A., 2016. Adaptive model rules from high-speed data streams. ACM Transactions on Knowledge Discovery from Data (TKDD), 10(3), pp.1-22. \u21a9","title":"References"},{"location":"api/stats/AbsMax/","text":"AbsMax \u00b6 Running absolute max. Attributes \u00b6 abs_max ( float ) The current absolute max. Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 5 , - 6 ] >>> abs_max = stats . AbsMax () >>> for x in X : ... print ( abs_max . update ( x ) . get ()) 1 4 4 4 5 6 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"AbsMax"},{"location":"api/stats/AbsMax/#absmax","text":"Running absolute max.","title":"AbsMax"},{"location":"api/stats/AbsMax/#attributes","text":"abs_max ( float ) The current absolute max.","title":"Attributes"},{"location":"api/stats/AbsMax/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 5 , - 6 ] >>> abs_max = stats . AbsMax () >>> for x in X : ... print ( abs_max . update ( x ) . get ()) 1 4 4 4 5 6","title":"Examples"},{"location":"api/stats/AbsMax/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/AutoCorr/","text":"AutoCorr \u00b6 Measures the serial correlation. This method computes the Pearson correlation between the current value and the value seen n steps before. Parameters \u00b6 lag ( int ) Attributes \u00b6 name Examples \u00b6 The following examples are taken from the pandas documentation . >>> from river import stats >>> auto_corr = stats . AutoCorr ( lag = 1 ) >>> for x in [ 0.25 , 0.5 , 0.2 , - 0.05 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 - 1.0 0.103552 >>> auto_corr = stats . AutoCorr ( lag = 2 ) >>> for x in [ 0.25 , 0.5 , 0.2 , - 0.05 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 0 - 1.0 >>> auto_corr = stats . AutoCorr ( lag = 1 ) >>> for x in [ 1 , 0 , 0 , 0 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 0 0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"AutoCorr"},{"location":"api/stats/AutoCorr/#autocorr","text":"Measures the serial correlation. This method computes the Pearson correlation between the current value and the value seen n steps before.","title":"AutoCorr"},{"location":"api/stats/AutoCorr/#parameters","text":"lag ( int )","title":"Parameters"},{"location":"api/stats/AutoCorr/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/AutoCorr/#examples","text":"The following examples are taken from the pandas documentation . >>> from river import stats >>> auto_corr = stats . AutoCorr ( lag = 1 ) >>> for x in [ 0.25 , 0.5 , 0.2 , - 0.05 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 - 1.0 0.103552 >>> auto_corr = stats . AutoCorr ( lag = 2 ) >>> for x in [ 0.25 , 0.5 , 0.2 , - 0.05 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 0 - 1.0 >>> auto_corr = stats . AutoCorr ( lag = 1 ) >>> for x in [ 1 , 0 , 0 , 0 ]: ... print ( auto_corr . update ( x ) . get ()) 0 0 0 0","title":"Examples"},{"location":"api/stats/AutoCorr/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/BayesianMean/","text":"BayesianMean \u00b6 Estimates a mean using outside information. Parameters \u00b6 prior ( float ) prior_weight ( float ) Attributes \u00b6 name Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Additive smoothing \u21a9 Bayesian average \u21a9 Practical example of Bayes estimators \u21a9","title":"BayesianMean"},{"location":"api/stats/BayesianMean/#bayesianmean","text":"Estimates a mean using outside information.","title":"BayesianMean"},{"location":"api/stats/BayesianMean/#parameters","text":"prior ( float ) prior_weight ( float )","title":"Parameters"},{"location":"api/stats/BayesianMean/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/BayesianMean/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/BayesianMean/#references","text":"Additive smoothing \u21a9 Bayesian average \u21a9 Practical example of Bayes estimators \u21a9","title":"References"},{"location":"api/stats/Bivariate/","text":"Bivariate \u00b6 A bivariate statistic measures a relationship between two variables. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"Bivariate"},{"location":"api/stats/Bivariate/#bivariate","text":"A bivariate statistic measures a relationship between two variables.","title":"Bivariate"},{"location":"api/stats/Bivariate/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"Methods"},{"location":"api/stats/Count/","text":"Count \u00b6 A simple counter. Attributes \u00b6 n ( int ) The current number of observations. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x \u2013 defaults to None","title":"Count"},{"location":"api/stats/Count/#count","text":"A simple counter.","title":"Count"},{"location":"api/stats/Count/#attributes","text":"n ( int ) The current number of observations.","title":"Attributes"},{"location":"api/stats/Count/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x \u2013 defaults to None","title":"Methods"},{"location":"api/stats/Cov/","text":"Cov \u00b6 Covariance. Parameters \u00b6 ddof \u2013 defaults to 1 Delta Degrees of Freedom. Examples \u00b6 >>> from river import stats >>> x = [ - 2.1 , - 1 , 4.3 ] >>> y = [ 3 , 1.1 , 0.12 ] >>> cov = stats . Cov () >>> for xi , yi in zip ( x , y ): ... print ( cov . update ( xi , yi ) . get ()) 0.0 - 1.044999 - 4.286 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y w \u2013 defaults to 1.0 Notes \u00b6 The outcomes of the incremental and parallel updates are consistent with numpy's batch processing when \\(\\text{ddof} \\le 1\\) . References \u00b6 Wikipedia article on algorithms for calculating variance \u21a9 Schubert, E. and Gertz, M., 2018, July. Numerically stable parallel computation of (co-) variance. In Proceedings of the 30th International Conference on Scientific and Statistical Database Management (pp. 1-12). \u21a9","title":"Cov"},{"location":"api/stats/Cov/#cov","text":"Covariance.","title":"Cov"},{"location":"api/stats/Cov/#parameters","text":"ddof \u2013 defaults to 1 Delta Degrees of Freedom.","title":"Parameters"},{"location":"api/stats/Cov/#examples","text":">>> from river import stats >>> x = [ - 2.1 , - 1 , 4.3 ] >>> y = [ 3 , 1.1 , 0.12 ] >>> cov = stats . Cov () >>> for xi , yi in zip ( x , y ): ... print ( cov . update ( xi , yi ) . get ()) 0.0 - 1.044999 - 4.286","title":"Examples"},{"location":"api/stats/Cov/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y w \u2013 defaults to 1.0","title":"Methods"},{"location":"api/stats/Cov/#notes","text":"The outcomes of the incremental and parallel updates are consistent with numpy's batch processing when \\(\\text{ddof} \\le 1\\) .","title":"Notes"},{"location":"api/stats/Cov/#references","text":"Wikipedia article on algorithms for calculating variance \u21a9 Schubert, E. and Gertz, M., 2018, July. Numerically stable parallel computation of (co-) variance. In Proceedings of the 30th International Conference on Scientific and Statistical Database Management (pp. 1-12). \u21a9","title":"References"},{"location":"api/stats/EWMean/","text":"EWMean \u00b6 Exponentially weighted mean. Parameters \u00b6 alpha \u2013 defaults to 0.5 The closer alpha is to 1 the more the statistic will adapt to recent values. Attributes \u00b6 mean ( float ) The running exponentially weighted mean. Examples \u00b6 >>> from river import stats >>> X = [ 1 , 3 , 5 , 4 , 6 , 8 , 7 , 9 , 11 ] >>> ewm = stats . EWMean ( alpha = 0.5 ) >>> for x in X : ... print ( ewm . update ( x ) . get ()) 1 2.0 3.5 3.75 4.875 6.4375 6.71875 7.859375 9.4296875 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Exponential Moving Average on Streaming Data \u21a9","title":"EWMean"},{"location":"api/stats/EWMean/#ewmean","text":"Exponentially weighted mean.","title":"EWMean"},{"location":"api/stats/EWMean/#parameters","text":"alpha \u2013 defaults to 0.5 The closer alpha is to 1 the more the statistic will adapt to recent values.","title":"Parameters"},{"location":"api/stats/EWMean/#attributes","text":"mean ( float ) The running exponentially weighted mean.","title":"Attributes"},{"location":"api/stats/EWMean/#examples","text":">>> from river import stats >>> X = [ 1 , 3 , 5 , 4 , 6 , 8 , 7 , 9 , 11 ] >>> ewm = stats . EWMean ( alpha = 0.5 ) >>> for x in X : ... print ( ewm . update ( x ) . get ()) 1 2.0 3.5 3.75 4.875 6.4375 6.71875 7.859375 9.4296875","title":"Examples"},{"location":"api/stats/EWMean/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/EWMean/#references","text":"Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Exponential Moving Average on Streaming Data \u21a9","title":"References"},{"location":"api/stats/EWVar/","text":"EWVar \u00b6 Exponentially weighted variance. To calculate the variance we use the fact that Var(X) = Mean(x^2) - Mean(x)^2 and internally we use the exponentially weighted mean of x/x^2 to calculate this. Parameters \u00b6 alpha \u2013 defaults to 0.5 The closer alpha is to 1 the more the statistic will adapt to recent values. Attributes \u00b6 variance ( float ) The running exponentially weighted variance. Examples \u00b6 >>> from river import stats >>> X = [ 1 , 3 , 5 , 4 , 6 , 8 , 7 , 9 , 11 ] >>> ewv = stats . EWVar ( alpha = 0.5 ) >>> for x in X : ... print ( ewv . update ( x ) . get ()) 0 1.0 2.75 1.4375 1.984375 3.43359375 1.7958984375 2.198974609375 3.56536865234375 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Exponential Moving Average on Streaming Data \u21a9","title":"EWVar"},{"location":"api/stats/EWVar/#ewvar","text":"Exponentially weighted variance. To calculate the variance we use the fact that Var(X) = Mean(x^2) - Mean(x)^2 and internally we use the exponentially weighted mean of x/x^2 to calculate this.","title":"EWVar"},{"location":"api/stats/EWVar/#parameters","text":"alpha \u2013 defaults to 0.5 The closer alpha is to 1 the more the statistic will adapt to recent values.","title":"Parameters"},{"location":"api/stats/EWVar/#attributes","text":"variance ( float ) The running exponentially weighted variance.","title":"Attributes"},{"location":"api/stats/EWVar/#examples","text":">>> from river import stats >>> X = [ 1 , 3 , 5 , 4 , 6 , 8 , 7 , 9 , 11 ] >>> ewv = stats . EWVar ( alpha = 0.5 ) >>> for x in X : ... print ( ewv . update ( x ) . get ()) 0 1.0 2.75 1.4375 1.984375 3.43359375 1.7958984375 2.198974609375 3.56536865234375","title":"Examples"},{"location":"api/stats/EWVar/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/EWVar/#references","text":"Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Exponential Moving Average on Streaming Data \u21a9","title":"References"},{"location":"api/stats/Entropy/","text":"Entropy \u00b6 Running entropy. Parameters \u00b6 alpha \u2013 defaults to 1 Fading factor. eps \u2013 defaults to 1e-08 Small value that will be added to the denominator to avoid division by zero. Attributes \u00b6 entropy ( float ) The running entropy. n ( int ) The current number of observations. counter ( collections.Counter ) Count the number of times the values have occurred Examples \u00b6 >>> import math >>> import random >>> import numpy as np >>> from scipy.stats import entropy >>> from river import stats >>> def entropy_list ( labels , base = None ): ... value , counts = np . unique ( labels , return_counts = True ) ... return entropy ( counts , base = base ) >>> SEED = 42 * 1337 >>> random . seed ( SEED ) >>> entro = stats . Entropy ( alpha = 1 ) >>> list_animal = [] >>> for animal , num_val in zip ([ 'cat' , 'dog' , 'bird' ],[ 301 , 401 , 601 ]): ... list_animal += [ animal for i in range ( num_val )] >>> random . shuffle ( list_animal ) >>> for animal in list_animal : ... _ = entro . update ( animal ) >>> print ( f ' { entro . get () : .6f } ' ) 1.058093 >>> print ( f ' { entropy_list ( list_animal ) : .6f } ' ) 1.058093 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Sovdat, B., 2014. Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams. arXiv preprint arXiv:1403.6348. \u21a9","title":"Entropy"},{"location":"api/stats/Entropy/#entropy","text":"Running entropy.","title":"Entropy"},{"location":"api/stats/Entropy/#parameters","text":"alpha \u2013 defaults to 1 Fading factor. eps \u2013 defaults to 1e-08 Small value that will be added to the denominator to avoid division by zero.","title":"Parameters"},{"location":"api/stats/Entropy/#attributes","text":"entropy ( float ) The running entropy. n ( int ) The current number of observations. counter ( collections.Counter ) Count the number of times the values have occurred","title":"Attributes"},{"location":"api/stats/Entropy/#examples","text":">>> import math >>> import random >>> import numpy as np >>> from scipy.stats import entropy >>> from river import stats >>> def entropy_list ( labels , base = None ): ... value , counts = np . unique ( labels , return_counts = True ) ... return entropy ( counts , base = base ) >>> SEED = 42 * 1337 >>> random . seed ( SEED ) >>> entro = stats . Entropy ( alpha = 1 ) >>> list_animal = [] >>> for animal , num_val in zip ([ 'cat' , 'dog' , 'bird' ],[ 301 , 401 , 601 ]): ... list_animal += [ animal for i in range ( num_val )] >>> random . shuffle ( list_animal ) >>> for animal in list_animal : ... _ = entro . update ( animal ) >>> print ( f ' { entro . get () : .6f } ' ) 1.058093 >>> print ( f ' { entropy_list ( list_animal ) : .6f } ' ) 1.058093","title":"Examples"},{"location":"api/stats/Entropy/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Entropy/#references","text":"Sovdat, B., 2014. Updating Formulas and Algorithms for Computing Entropy and Gini Index from Time-Changing Data Streams. arXiv preprint arXiv:1403.6348. \u21a9","title":"References"},{"location":"api/stats/IQR/","text":"IQR \u00b6 Computes the interquartile range. Parameters \u00b6 q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. Defaults to 0.25 . q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1. Defaults to 0.75 . Attributes \u00b6 name Examples \u00b6 >>> from river import stats >>> iqr = stats . IQR ( q_inf = 0.25 , q_sup = 0.75 ) >>> for i in range ( 0 , 1001 ): ... iqr = iqr . update ( i ) ... if i % 100 == 0 : ... print ( iqr . get ()) 0 50.0 100.0 150.0 200.0 250.0 300.0 350.0 400.0 450.0 500.0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"IQR"},{"location":"api/stats/IQR/#iqr","text":"Computes the interquartile range.","title":"IQR"},{"location":"api/stats/IQR/#parameters","text":"q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. Defaults to 0.25 . q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1. Defaults to 0.75 .","title":"Parameters"},{"location":"api/stats/IQR/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/IQR/#examples","text":">>> from river import stats >>> iqr = stats . IQR ( q_inf = 0.25 , q_sup = 0.75 ) >>> for i in range ( 0 , 1001 ): ... iqr = iqr . update ( i ) ... if i % 100 == 0 : ... print ( iqr . get ()) 0 50.0 100.0 150.0 200.0 250.0 300.0 350.0 400.0 450.0 500.0","title":"Examples"},{"location":"api/stats/IQR/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Kurtosis/","text":"Kurtosis \u00b6 Running kurtosis using Welford's algorithm. Parameters \u00b6 bias \u2013 defaults to False If False , then the calculations are corrected for statistical bias. Attributes \u00b6 name Examples \u00b6 >>> import river.stats >>> import scipy.stats >>> import numpy as np >>> np . random . seed ( 42 ) >>> X = np . random . normal ( loc = 0 , scale = 1 , size = 10 ) >>> kurtosis = river . stats . Kurtosis ( bias = False ) >>> for x in X : ... print ( kurtosis . update ( x ) . get ()) - 3 - 2.0 - 1.5 1.4130027920707047 0.15367976585756438 0.46142633246812653 - 1.620647789230658 - 1.3540178492487054 - 1.2310268787102745 - 0.9490372374384453 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . kurtosis ( X [: i ], bias = False )) - 3.0 - 2.0 - 1.4999999999999998 1.4130027920707082 0.15367976585756082 0.46142633246812403 - 1.620647789230658 - 1.3540178492487063 - 1.2310268787102738 - 0.9490372374384459 >>> kurtosis = river . stats . Kurtosis ( bias = True ) >>> for x in X : ... print ( kurtosis . update ( x ) . get ()) - 3 - 2.0 - 1.5 - 1.011599627723906 - 0.9615800585356089 - 0.6989395431537853 - 1.4252699121794408 - 1.311437071070812 - 1.246289111322894 - 1.082283689864171 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . kurtosis ( X [: i ], bias = True )) - 3.0 - 2.0 - 1.4999999999999998 - 1.0115996277239057 - 0.9615800585356098 - 0.6989395431537861 - 1.425269912179441 - 1.3114370710708125 - 1.2462891113228936 - 1.0822836898641714 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Wikipedia article on algorithms for calculating variance \u21a9","title":"Kurtosis"},{"location":"api/stats/Kurtosis/#kurtosis","text":"Running kurtosis using Welford's algorithm.","title":"Kurtosis"},{"location":"api/stats/Kurtosis/#parameters","text":"bias \u2013 defaults to False If False , then the calculations are corrected for statistical bias.","title":"Parameters"},{"location":"api/stats/Kurtosis/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Kurtosis/#examples","text":">>> import river.stats >>> import scipy.stats >>> import numpy as np >>> np . random . seed ( 42 ) >>> X = np . random . normal ( loc = 0 , scale = 1 , size = 10 ) >>> kurtosis = river . stats . Kurtosis ( bias = False ) >>> for x in X : ... print ( kurtosis . update ( x ) . get ()) - 3 - 2.0 - 1.5 1.4130027920707047 0.15367976585756438 0.46142633246812653 - 1.620647789230658 - 1.3540178492487054 - 1.2310268787102745 - 0.9490372374384453 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . kurtosis ( X [: i ], bias = False )) - 3.0 - 2.0 - 1.4999999999999998 1.4130027920707082 0.15367976585756082 0.46142633246812403 - 1.620647789230658 - 1.3540178492487063 - 1.2310268787102738 - 0.9490372374384459 >>> kurtosis = river . stats . Kurtosis ( bias = True ) >>> for x in X : ... print ( kurtosis . update ( x ) . get ()) - 3 - 2.0 - 1.5 - 1.011599627723906 - 0.9615800585356089 - 0.6989395431537853 - 1.4252699121794408 - 1.311437071070812 - 1.246289111322894 - 1.082283689864171 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . kurtosis ( X [: i ], bias = True )) - 3.0 - 2.0 - 1.4999999999999998 - 1.0115996277239057 - 0.9615800585356098 - 0.6989395431537861 - 1.425269912179441 - 1.3114370710708125 - 1.2462891113228936 - 1.0822836898641714","title":"Examples"},{"location":"api/stats/Kurtosis/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Kurtosis/#references","text":"Wikipedia article on algorithms for calculating variance \u21a9","title":"References"},{"location":"api/stats/Link/","text":"Link \u00b6 A link joins two univariate statistics as a sequence. This can be used to pipe the output of one statistic to the input of another. This can be used, for instance, to calculate the mean of the variance of a variable. It can also be used to compute shifted statistics by piping statistics with an instance of stats.Shift . Note that a link is not meant to be instantiated via this class definition. Instead, users can link statistics together via the | operator. Parameters \u00b6 left ( river.stats.base.Univariate ) right ( river.stats.base.Univariate ) The output from left 's get method is passed to right 's update method if left 's get method doesn't produce None. Attributes \u00b6 name Examples \u00b6 >>> from river import stats >>> stat = stats . Shift ( 1 ) | stats . Mean () No values have been seen, therefore get defaults to the initial value of stats.Mean , which is 0. >>> stat . get () 0. Let us now call update . >>> stat = stat . update ( 1 ) The output from get will still be 0. The reason is that stats.Shift has not enough values, and therefore outputs it's default value, which is None . The stats.Mean instance is therefore not updated. >>> stat . get () 0.0 On the next call to update , the stats.Shift instance has seen enough values, and therefore the mean can be updated. The mean is therefore equal to 1, because that's the only value from the past. >>> stat = stat . update ( 3 ) >>> stat . get () 1.0 On the subsequent call to update, the mean will be updated with the value 3. >>> stat = stat . update ( 4 ) >>> stat . get () 2.0 Note that composing statistics returns a new statistic with it's own name. >>> stat . name 'mean_of_shift_1' Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Link"},{"location":"api/stats/Link/#link","text":"A link joins two univariate statistics as a sequence. This can be used to pipe the output of one statistic to the input of another. This can be used, for instance, to calculate the mean of the variance of a variable. It can also be used to compute shifted statistics by piping statistics with an instance of stats.Shift . Note that a link is not meant to be instantiated via this class definition. Instead, users can link statistics together via the | operator.","title":"Link"},{"location":"api/stats/Link/#parameters","text":"left ( river.stats.base.Univariate ) right ( river.stats.base.Univariate ) The output from left 's get method is passed to right 's update method if left 's get method doesn't produce None.","title":"Parameters"},{"location":"api/stats/Link/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Link/#examples","text":">>> from river import stats >>> stat = stats . Shift ( 1 ) | stats . Mean () No values have been seen, therefore get defaults to the initial value of stats.Mean , which is 0. >>> stat . get () 0. Let us now call update . >>> stat = stat . update ( 1 ) The output from get will still be 0. The reason is that stats.Shift has not enough values, and therefore outputs it's default value, which is None . The stats.Mean instance is therefore not updated. >>> stat . get () 0.0 On the next call to update , the stats.Shift instance has seen enough values, and therefore the mean can be updated. The mean is therefore equal to 1, because that's the only value from the past. >>> stat = stat . update ( 3 ) >>> stat . get () 1.0 On the subsequent call to update, the mean will be updated with the value 3. >>> stat = stat . update ( 4 ) >>> stat . get () 2.0 Note that composing statistics returns a new statistic with it's own name. >>> stat . name 'mean_of_shift_1'","title":"Examples"},{"location":"api/stats/Link/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Max/","text":"Max \u00b6 Running max. Attributes \u00b6 max ( float ) The current max. Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 5 , - 6 ] >>> _max = stats . Max () >>> for x in X : ... print ( _max . update ( x ) . get ()) 1 1 3 3 5 5 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Max"},{"location":"api/stats/Max/#max","text":"Running max.","title":"Max"},{"location":"api/stats/Max/#attributes","text":"max ( float ) The current max.","title":"Attributes"},{"location":"api/stats/Max/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 5 , - 6 ] >>> _max = stats . Max () >>> for x in X : ... print ( _max . update ( x ) . get ()) 1 1 3 3 5 5","title":"Examples"},{"location":"api/stats/Max/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Mean/","text":"Mean \u00b6 Running mean. Attributes \u00b6 mean ( float ) The current value of the mean. n ( float ) The current sum of weights. If each passed weight was 1, then this is equal to the number of seen observations. Examples \u00b6 >>> from river import stats >>> X = [ - 5 , - 3 , - 1 , 1 , 3 , 5 ] >>> mean = stats . Mean () >>> for x in X : ... print ( mean . update ( x ) . get ()) - 5.0 - 4.0 - 3.0 - 2.0 - 1.0 0.0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x w \u2013 defaults to 1.0 update Update and return the called instance. Parameters x w \u2013 defaults to 1.0 References \u00b6 West, D. H. D. (1979). Updating mean and variance estimates: An improved method. Communications of the ACM, 22(9), 532-535. \u21a9 Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Chan, T.F., Golub, G.H. and LeVeque, R.J., 1983. Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician, 37(3), pp.242-247. \u21a9","title":"Mean"},{"location":"api/stats/Mean/#mean","text":"Running mean.","title":"Mean"},{"location":"api/stats/Mean/#attributes","text":"mean ( float ) The current value of the mean. n ( float ) The current sum of weights. If each passed weight was 1, then this is equal to the number of seen observations.","title":"Attributes"},{"location":"api/stats/Mean/#examples","text":">>> from river import stats >>> X = [ - 5 , - 3 , - 1 , 1 , 3 , 5 ] >>> mean = stats . Mean () >>> for x in X : ... print ( mean . update ( x ) . get ()) - 5.0 - 4.0 - 3.0 - 2.0 - 1.0 0.0","title":"Examples"},{"location":"api/stats/Mean/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x w \u2013 defaults to 1.0 update Update and return the called instance. Parameters x w \u2013 defaults to 1.0","title":"Methods"},{"location":"api/stats/Mean/#references","text":"West, D. H. D. (1979). Updating mean and variance estimates: An improved method. Communications of the ACM, 22(9), 532-535. \u21a9 Finch, T., 2009. Incremental calculation of weighted mean and variance. University of Cambridge, 4(11-5), pp.41-42. \u21a9 Chan, T.F., Golub, G.H. and LeVeque, R.J., 1983. Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician, 37(3), pp.242-247. \u21a9","title":"References"},{"location":"api/stats/Min/","text":"Min \u00b6 Running min. Attributes \u00b6 min ( float ) The current min. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Min"},{"location":"api/stats/Min/#min","text":"Running min.","title":"Min"},{"location":"api/stats/Min/#attributes","text":"min ( float ) The current min.","title":"Attributes"},{"location":"api/stats/Min/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Mode/","text":"Mode \u00b6 Running mode. The mode is simply the most common value. An approximate mode can be computed by setting the number of first unique values to count. Parameters \u00b6 k \u2013 defaults to 25 Only the first k unique values will be included. If k equals -1, the exact mode is computed. Attributes \u00b6 name Examples \u00b6 >>> from river import stats >>> X = [ 'sunny' , 'cloudy' , 'cloudy' , 'rainy' , 'rainy' , 'rainy' ] >>> mode = stats . Mode ( k = 2 ) >>> for x in X : ... print ( mode . update ( x ) . get ()) sunny sunny cloudy cloudy cloudy cloudy >>> mode = stats . Mode ( k =- 1 ) >>> for x in X : ... print ( mode . update ( x ) . get ()) sunny sunny cloudy cloudy cloudy rainy Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Mode"},{"location":"api/stats/Mode/#mode","text":"Running mode. The mode is simply the most common value. An approximate mode can be computed by setting the number of first unique values to count.","title":"Mode"},{"location":"api/stats/Mode/#parameters","text":"k \u2013 defaults to 25 Only the first k unique values will be included. If k equals -1, the exact mode is computed.","title":"Parameters"},{"location":"api/stats/Mode/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Mode/#examples","text":">>> from river import stats >>> X = [ 'sunny' , 'cloudy' , 'cloudy' , 'rainy' , 'rainy' , 'rainy' ] >>> mode = stats . Mode ( k = 2 ) >>> for x in X : ... print ( mode . update ( x ) . get ()) sunny sunny cloudy cloudy cloudy cloudy >>> mode = stats . Mode ( k =- 1 ) >>> for x in X : ... print ( mode . update ( x ) . get ()) sunny sunny cloudy cloudy cloudy rainy","title":"Examples"},{"location":"api/stats/Mode/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/NUnique/","text":"NUnique \u00b6 Approximate number of unique values counter. This is basically an implementation of the HyperLogLog algorithm. Adapted from hypy . The code is a bit too terse but it will do for now. Parameters \u00b6 error_rate \u2013 defaults to 0.01 Desired error rate. Memory usage is inversely proportional to this value. seed ( int ) \u2013 defaults to None Set the seed to produce identical results. Attributes \u00b6 n_bits ( int ) n_buckets ( int ) buckets ( list ) Examples \u00b6 >>> import string >>> from river import stats >>> alphabet = string . ascii_lowercase >>> n_unique = stats . NUnique ( error_rate = 0.2 , seed = 42 ) >>> n_unique . update ( 'a' ) . get () 1 >>> n_unique . update ( 'b' ) . get () 2 >>> for letter in alphabet : ... n_unique = n_unique . update ( letter ) >>> n_unique . get () 31 Lowering the error_rate parameter will increase the precision. >>> n_unique = stats . NUnique ( error_rate = 0.01 , seed = 42 ) >>> for letter in alphabet : ... n_unique = n_unique . update ( letter ) >>> n_unique . get () 26 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 My favorite algorithm (and data structure): HyperLogLog \u21a9 Flajolet, P., Fusy, \u00c9., Gandouet, O. and Meunier, F., 2007, June. Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm. \u21a9","title":"NUnique"},{"location":"api/stats/NUnique/#nunique","text":"Approximate number of unique values counter. This is basically an implementation of the HyperLogLog algorithm. Adapted from hypy . The code is a bit too terse but it will do for now.","title":"NUnique"},{"location":"api/stats/NUnique/#parameters","text":"error_rate \u2013 defaults to 0.01 Desired error rate. Memory usage is inversely proportional to this value. seed ( int ) \u2013 defaults to None Set the seed to produce identical results.","title":"Parameters"},{"location":"api/stats/NUnique/#attributes","text":"n_bits ( int ) n_buckets ( int ) buckets ( list )","title":"Attributes"},{"location":"api/stats/NUnique/#examples","text":">>> import string >>> from river import stats >>> alphabet = string . ascii_lowercase >>> n_unique = stats . NUnique ( error_rate = 0.2 , seed = 42 ) >>> n_unique . update ( 'a' ) . get () 1 >>> n_unique . update ( 'b' ) . get () 2 >>> for letter in alphabet : ... n_unique = n_unique . update ( letter ) >>> n_unique . get () 31 Lowering the error_rate parameter will increase the precision. >>> n_unique = stats . NUnique ( error_rate = 0.01 , seed = 42 ) >>> for letter in alphabet : ... n_unique = n_unique . update ( letter ) >>> n_unique . get () 26","title":"Examples"},{"location":"api/stats/NUnique/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/NUnique/#references","text":"My favorite algorithm (and data structure): HyperLogLog \u21a9 Flajolet, P., Fusy, \u00c9., Gandouet, O. and Meunier, F., 2007, June. Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm. \u21a9","title":"References"},{"location":"api/stats/PeakToPeak/","text":"PeakToPeak \u00b6 Running peak to peak (max - min). Attributes \u00b6 max ( stats.Max ) The running max. min ( stats.Min ) The running min. p2p ( float ) The running peak to peak. Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 4 ] >>> ptp = stats . PeakToPeak () >>> for x in X : ... print ( ptp . update ( x ) . get ()) 0 5 7 7 7 8 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"PeakToPeak"},{"location":"api/stats/PeakToPeak/#peaktopeak","text":"Running peak to peak (max - min).","title":"PeakToPeak"},{"location":"api/stats/PeakToPeak/#attributes","text":"max ( stats.Max ) The running max. min ( stats.Min ) The running min. p2p ( float ) The running peak to peak.","title":"Attributes"},{"location":"api/stats/PeakToPeak/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 4 ] >>> ptp = stats . PeakToPeak () >>> for x in X : ... print ( ptp . update ( x ) . get ()) 0 5 7 7 7 8","title":"Examples"},{"location":"api/stats/PeakToPeak/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/PearsonCorr/","text":"PearsonCorr \u00b6 Online Pearson correlation. Parameters \u00b6 ddof \u2013 defaults to 1 Delta Degrees of Freedom. Attributes \u00b6 var_x ( stats.Var ) Running variance of x . var_y ( stats.Var ) Running variance of y . cov_xy ( stats.Cov ) Running covariance of x and y . Examples \u00b6 >>> from river import stats >>> x = [ 0 , 0 , 0 , 1 , 1 , 1 , 1 ] >>> y = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] >>> pearson = stats . PearsonCorr () >>> for xi , yi in zip ( x , y ): ... print ( pearson . update ( xi , yi ) . get ()) 0 0 0 0.774596 0.866025 0.878310 0.866025 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"PearsonCorr"},{"location":"api/stats/PearsonCorr/#pearsoncorr","text":"Online Pearson correlation.","title":"PearsonCorr"},{"location":"api/stats/PearsonCorr/#parameters","text":"ddof \u2013 defaults to 1 Delta Degrees of Freedom.","title":"Parameters"},{"location":"api/stats/PearsonCorr/#attributes","text":"var_x ( stats.Var ) Running variance of x . var_y ( stats.Var ) Running variance of y . cov_xy ( stats.Cov ) Running covariance of x and y .","title":"Attributes"},{"location":"api/stats/PearsonCorr/#examples","text":">>> from river import stats >>> x = [ 0 , 0 , 0 , 1 , 1 , 1 , 1 ] >>> y = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] >>> pearson = stats . PearsonCorr () >>> for xi , yi in zip ( x , y ): ... print ( pearson . update ( xi , yi ) . get ()) 0 0 0 0.774596 0.866025 0.878310 0.866025","title":"Examples"},{"location":"api/stats/PearsonCorr/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"Methods"},{"location":"api/stats/Quantile/","text":"Quantile \u00b6 Running quantile. Uses the P\u00b2 algorithm, which is also known as the \"Piecewise-Parabolic quantile estimator\". The code is inspired by LiveStat's implementation 2 . Parameters \u00b6 q \u2013 defaults to 0.5 Determines which quantile to compute, must be comprised between 0 and 1. Attributes \u00b6 name Examples \u00b6 >>> from river import stats >>> import numpy as np >>> np . random . seed ( 42 * 1337 ) >>> mu , sigma = 0 , 1 >>> s = np . random . normal ( mu , sigma , 500 ) >>> median = stats . Quantile ( 0.5 ) >>> for x in s : ... _ = median . update ( x ) >>> print ( f 'The estimated value of the 50th (median) quantile is { median . get () : .4f } ' ) The estimated value of the 50 th ( median ) quantile is - 0.0275 >>> print ( f 'The real value of the 50th (median) quantile is { np . median ( s ) : .4f } ' ) The real value of the 50 th ( median ) quantile is - 0.0135 >>> percentile_17 = stats . Quantile ( 0.17 ) >>> for x in s : ... _ = percentile_17 . update ( x ) >>> print ( f 'The estimated value of the 17th quantile is { percentile_17 . get () : .4f } ' ) The estimated value of the 17 th quantile is - 0.8652 >>> print ( f 'The real value of the 17th quantile is { np . percentile ( s , 17 ) : .4f } ' ) The real value of the 17 th quantile is - 0.9072 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 The P\u00b2 Algorithm for Dynamic Univariateal Computing Calculation of Quantiles and Editor Histograms Without Storing Observations \u21a9 LiveStats \u21a9 P\u00b2 quantile estimator: estimating the median without storing values \u21a9","title":"Quantile"},{"location":"api/stats/Quantile/#quantile","text":"Running quantile. Uses the P\u00b2 algorithm, which is also known as the \"Piecewise-Parabolic quantile estimator\". The code is inspired by LiveStat's implementation 2 .","title":"Quantile"},{"location":"api/stats/Quantile/#parameters","text":"q \u2013 defaults to 0.5 Determines which quantile to compute, must be comprised between 0 and 1.","title":"Parameters"},{"location":"api/stats/Quantile/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Quantile/#examples","text":">>> from river import stats >>> import numpy as np >>> np . random . seed ( 42 * 1337 ) >>> mu , sigma = 0 , 1 >>> s = np . random . normal ( mu , sigma , 500 ) >>> median = stats . Quantile ( 0.5 ) >>> for x in s : ... _ = median . update ( x ) >>> print ( f 'The estimated value of the 50th (median) quantile is { median . get () : .4f } ' ) The estimated value of the 50 th ( median ) quantile is - 0.0275 >>> print ( f 'The real value of the 50th (median) quantile is { np . median ( s ) : .4f } ' ) The real value of the 50 th ( median ) quantile is - 0.0135 >>> percentile_17 = stats . Quantile ( 0.17 ) >>> for x in s : ... _ = percentile_17 . update ( x ) >>> print ( f 'The estimated value of the 17th quantile is { percentile_17 . get () : .4f } ' ) The estimated value of the 17 th quantile is - 0.8652 >>> print ( f 'The real value of the 17th quantile is { np . percentile ( s , 17 ) : .4f } ' ) The real value of the 17 th quantile is - 0.9072","title":"Examples"},{"location":"api/stats/Quantile/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Quantile/#references","text":"The P\u00b2 Algorithm for Dynamic Univariateal Computing Calculation of Quantiles and Editor Histograms Without Storing Observations \u21a9 LiveStats \u21a9 P\u00b2 quantile estimator: estimating the median without storing values \u21a9","title":"References"},{"location":"api/stats/RollingAbsMax/","text":"RollingAbsMax \u00b6 Running absolute max over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_absmax = stats . RollingAbsMax ( window_size = 2 ) >>> for x in X : ... print ( rolling_absmax . update ( x ) . get ()) 1 4 4 3 2 2 Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"RollingAbsMax"},{"location":"api/stats/RollingAbsMax/#rollingabsmax","text":"Running absolute max over a window.","title":"RollingAbsMax"},{"location":"api/stats/RollingAbsMax/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingAbsMax/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingAbsMax/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_absmax = stats . RollingAbsMax ( window_size = 2 ) >>> for x in X : ... print ( rolling_absmax . update ( x ) . get ()) 1 4 4 3 2 2","title":"Examples"},{"location":"api/stats/RollingAbsMax/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingCov/","text":"RollingCov \u00b6 Rolling covariance. Parameters \u00b6 window_size Size of the window over which to compute the covariance. ddof \u2013 defaults to 1 Delta Degrees of Freedom. Attributes \u00b6 window_size Examples \u00b6 >>> from river import stats >>> x = [ - 2.1 , - 1 , 4.3 , 1 , - 2.1 , - 1 , 4.3 ] >>> y = [ 3 , 1.1 , .12 , 1 , 3 , 1.1 , .12 ] >>> rcov = stats . RollingCov ( 3 ) >>> for xi , yi in zip ( x , y ): ... print ( rcov . update ( xi , yi ) . get ()) 0.0 - 1.045 - 4.286 - 1.382 - 4.589 - 1.415 - 4.286 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"RollingCov"},{"location":"api/stats/RollingCov/#rollingcov","text":"Rolling covariance.","title":"RollingCov"},{"location":"api/stats/RollingCov/#parameters","text":"window_size Size of the window over which to compute the covariance. ddof \u2013 defaults to 1 Delta Degrees of Freedom.","title":"Parameters"},{"location":"api/stats/RollingCov/#attributes","text":"window_size","title":"Attributes"},{"location":"api/stats/RollingCov/#examples","text":">>> from river import stats >>> x = [ - 2.1 , - 1 , 4.3 , 1 , - 2.1 , - 1 , 4.3 ] >>> y = [ 3 , 1.1 , .12 , 1 , 3 , 1.1 , .12 ] >>> rcov = stats . RollingCov ( 3 ) >>> for xi , yi in zip ( x , y ): ... print ( rcov . update ( xi , yi ) . get ()) 0.0 - 1.045 - 4.286 - 1.382 - 4.589 - 1.415 - 4.286","title":"Examples"},{"location":"api/stats/RollingCov/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"Methods"},{"location":"api/stats/RollingIQR/","text":"RollingIQR \u00b6 Computes the rolling interquartile range. Parameters \u00b6 window_size ( int ) Size of the window. q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. Defaults to 0.25 . q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1. Defaults to 0.75 . Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> rolling_iqr = stats . RollingIQR ( ... q_inf = 0.25 , ... q_sup = 0.75 , ... window_size = 100 ... ) >>> for i in range ( 0 , 1001 ): ... rolling_iqr = rolling_iqr . update ( i ) ... if i % 100 == 0 : ... print ( rolling_iqr . get ()) 0 50 50 50 50 50 50 50 50 50 50 Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"RollingIQR"},{"location":"api/stats/RollingIQR/#rollingiqr","text":"Computes the rolling interquartile range.","title":"RollingIQR"},{"location":"api/stats/RollingIQR/#parameters","text":"window_size ( int ) Size of the window. q_inf \u2013 defaults to 0.25 Desired inferior quantile, must be between 0 and 1. Defaults to 0.25 . q_sup \u2013 defaults to 0.75 Desired superior quantile, must be between 0 and 1. Defaults to 0.75 .","title":"Parameters"},{"location":"api/stats/RollingIQR/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingIQR/#examples","text":">>> from river import stats >>> rolling_iqr = stats . RollingIQR ( ... q_inf = 0.25 , ... q_sup = 0.75 , ... window_size = 100 ... ) >>> for i in range ( 0 , 1001 ): ... rolling_iqr = rolling_iqr . update ( i ) ... if i % 100 == 0 : ... print ( rolling_iqr . get ()) 0 50 50 50 50 50 50 50 50 50 50","title":"Examples"},{"location":"api/stats/RollingIQR/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingMax/","text":"RollingMax \u00b6 Running max over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_max = stats . RollingMax ( window_size = 2 ) >>> for x in X : ... print ( rolling_max . update ( x ) . get ()) 1 1 3 3 2 2 Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"RollingMax"},{"location":"api/stats/RollingMax/#rollingmax","text":"Running max over a window.","title":"RollingMax"},{"location":"api/stats/RollingMax/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingMax/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingMax/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_max = stats . RollingMax ( window_size = 2 ) >>> for x in X : ... print ( rolling_max . update ( x ) . get ()) 1 1 3 3 2 2","title":"Examples"},{"location":"api/stats/RollingMax/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingMean/","text":"RollingMean \u00b6 Running average over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> X = [ 1 , 2 , 3 , 4 , 5 , 6 ] >>> rmean = stats . RollingMean ( window_size = 2 ) >>> for x in X : ... print ( rmean . update ( x ) . get ()) 1.0 1.5 2.5 3.5 4.5 5.5 >>> rmean = stats . RollingMean ( window_size = 3 ) >>> for x in X : ... print ( rmean . update ( x ) . get ()) 1.0 1.5 2.0 3.0 4.0 5.0 Methods \u00b6 append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingMean"},{"location":"api/stats/RollingMean/#rollingmean","text":"Running average over a window.","title":"RollingMean"},{"location":"api/stats/RollingMean/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingMean/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingMean/#examples","text":">>> from river import stats >>> X = [ 1 , 2 , 3 , 4 , 5 , 6 ] >>> rmean = stats . RollingMean ( window_size = 2 ) >>> for x in X : ... print ( rmean . update ( x ) . get ()) 1.0 1.5 2.5 3.5 4.5 5.5 >>> rmean = stats . RollingMean ( window_size = 3 ) >>> for x in X : ... print ( rmean . update ( x ) . get ()) 1.0 1.5 2.0 3.0 4.0 5.0","title":"Examples"},{"location":"api/stats/RollingMean/#methods","text":"append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingMin/","text":"RollingMin \u00b6 Running min over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_min = stats . RollingMin ( 2 ) >>> for x in X : ... print ( rolling_min . update ( x ) . get ()) 1 - 4 - 4 - 2 - 2 1 Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"RollingMin"},{"location":"api/stats/RollingMin/#rollingmin","text":"Running min over a window.","title":"RollingMin"},{"location":"api/stats/RollingMin/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingMin/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingMin/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_min = stats . RollingMin ( 2 ) >>> for x in X : ... print ( rolling_min . update ( x ) . get ()) 1 - 4 - 4 - 2 - 2 1","title":"Examples"},{"location":"api/stats/RollingMin/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingMode/","text":"RollingMode \u00b6 Running mode over a window. The mode is the most common value. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 counts ( collections.defaultdict ) Value counts. Examples \u00b6 >>> from river import stats >>> X = [ 'sunny' , 'sunny' , 'sunny' , 'rainy' , 'rainy' , 'rainy' , 'rainy' ] >>> rolling_mode = stats . RollingMode ( window_size = 2 ) >>> for x in X : ... print ( rolling_mode . update ( x ) . get ()) sunny sunny sunny sunny rainy rainy rainy >>> rolling_mode = stats . RollingMode ( window_size = 5 ) >>> for x in X : ... print ( rolling_mode . update ( x ) . get ()) sunny sunny sunny sunny sunny rainy rainy Methods \u00b6 append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingMode"},{"location":"api/stats/RollingMode/#rollingmode","text":"Running mode over a window. The mode is the most common value.","title":"RollingMode"},{"location":"api/stats/RollingMode/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingMode/#attributes","text":"counts ( collections.defaultdict ) Value counts.","title":"Attributes"},{"location":"api/stats/RollingMode/#examples","text":">>> from river import stats >>> X = [ 'sunny' , 'sunny' , 'sunny' , 'rainy' , 'rainy' , 'rainy' , 'rainy' ] >>> rolling_mode = stats . RollingMode ( window_size = 2 ) >>> for x in X : ... print ( rolling_mode . update ( x ) . get ()) sunny sunny sunny sunny rainy rainy rainy >>> rolling_mode = stats . RollingMode ( window_size = 5 ) >>> for x in X : ... print ( rolling_mode . update ( x ) . get ()) sunny sunny sunny sunny sunny rainy rainy","title":"Examples"},{"location":"api/stats/RollingMode/#methods","text":"append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingPeakToPeak/","text":"RollingPeakToPeak \u00b6 Running peak to peak (max - min) over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 max ( stats.RollingMax ) The running rolling max. min ( stats.RollingMin ) The running rolling min. Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> ptp = stats . RollingPeakToPeak ( window_size = 2 ) >>> for x in X : ... print ( ptp . update ( x ) . get ()) 0 5 7 5 4 1 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingPeakToPeak"},{"location":"api/stats/RollingPeakToPeak/#rollingpeaktopeak","text":"Running peak to peak (max - min) over a window.","title":"RollingPeakToPeak"},{"location":"api/stats/RollingPeakToPeak/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingPeakToPeak/#attributes","text":"max ( stats.RollingMax ) The running rolling max. min ( stats.RollingMin ) The running rolling min.","title":"Attributes"},{"location":"api/stats/RollingPeakToPeak/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> ptp = stats . RollingPeakToPeak ( window_size = 2 ) >>> for x in X : ... print ( ptp . update ( x ) . get ()) 0 5 7 5 4 1","title":"Examples"},{"location":"api/stats/RollingPeakToPeak/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingPearsonCorr/","text":"RollingPearsonCorr \u00b6 Rolling Pearson correlation. Parameters \u00b6 window_size Amount of samples over which to compute the correlation. ddof \u2013 defaults to 1 Delta Degrees of Freedom. Attributes \u00b6 var_x ( stats.Var ) Running variance of x . var_y ( stats.Var ) Running variance of y . cov_xy ( stats.Cov ) Running covariance of x and y . Examples \u00b6 >>> from river import stats >>> x = [ 0 , 0 , 0 , 1 , 1 , 1 , 1 ] >>> y = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] >>> pearson = stats . RollingPearsonCorr ( window_size = 4 ) >>> for xi , yi in zip ( x , y ): ... print ( pearson . update ( xi , yi ) . get ()) 0 0 0 0.7745966692414834 0.894427190999916 0.7745966692414834 0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"RollingPearsonCorr"},{"location":"api/stats/RollingPearsonCorr/#rollingpearsoncorr","text":"Rolling Pearson correlation.","title":"RollingPearsonCorr"},{"location":"api/stats/RollingPearsonCorr/#parameters","text":"window_size Amount of samples over which to compute the correlation. ddof \u2013 defaults to 1 Delta Degrees of Freedom.","title":"Parameters"},{"location":"api/stats/RollingPearsonCorr/#attributes","text":"var_x ( stats.Var ) Running variance of x . var_y ( stats.Var ) Running variance of y . cov_xy ( stats.Cov ) Running covariance of x and y .","title":"Attributes"},{"location":"api/stats/RollingPearsonCorr/#examples","text":">>> from river import stats >>> x = [ 0 , 0 , 0 , 1 , 1 , 1 , 1 ] >>> y = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 ] >>> pearson = stats . RollingPearsonCorr ( window_size = 4 ) >>> for xi , yi in zip ( x , y ): ... print ( pearson . update ( xi , yi ) . get ()) 0 0 0 0.7745966692414834 0.894427190999916 0.7745966692414834 0","title":"Examples"},{"location":"api/stats/RollingPearsonCorr/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. update Update and return the called instance. Parameters x y","title":"Methods"},{"location":"api/stats/RollingQuantile/","text":"RollingQuantile \u00b6 Running quantile over a window. Parameters \u00b6 q Determines which quantile to compute, must be comprised between 0 and 1. window_size Size of the window. Attributes \u00b6 name size window_size Examples \u00b6 >>> from river import stats >>> rolling_quantile = stats . RollingQuantile ( ... q = .5 , ... window_size = 100 , ... ) >>> for i in range ( 0 , 1001 ): ... rolling_quantile = rolling_quantile . update ( i ) ... if i % 100 == 0 : ... print ( rolling_quantile . get ()) 0 50 150 250 350 450 550 650 750 850 950 Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x References \u00b6 Left sorted \u21a9","title":"RollingQuantile"},{"location":"api/stats/RollingQuantile/#rollingquantile","text":"Running quantile over a window.","title":"RollingQuantile"},{"location":"api/stats/RollingQuantile/#parameters","text":"q Determines which quantile to compute, must be comprised between 0 and 1. window_size Size of the window.","title":"Parameters"},{"location":"api/stats/RollingQuantile/#attributes","text":"name size window_size","title":"Attributes"},{"location":"api/stats/RollingQuantile/#examples","text":">>> from river import stats >>> rolling_quantile = stats . RollingQuantile ( ... q = .5 , ... window_size = 100 , ... ) >>> for i in range ( 0 , 1001 ): ... rolling_quantile = rolling_quantile . update ( i ) ... if i % 100 == 0 : ... print ( rolling_quantile . get ()) 0 50 150 250 350 450 550 650 750 850 950","title":"Examples"},{"location":"api/stats/RollingQuantile/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other get Return the current value of the statistic. index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE revert Revert and return the called instance. Parameters x sort update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingQuantile/#references","text":"Left sorted \u21a9","title":"References"},{"location":"api/stats/RollingSEM/","text":"RollingSEM \u00b6 Running standard error of the mean over a window. Parameters \u00b6 window_size Size of the rolling window. ddof \u2013 defaults to 1 Delta Degrees of Freedom for the variance. Attributes \u00b6 correction_factor name window_size Examples \u00b6 >>> import river >>> X = [ 1 , 4 , 2 , - 4 , - 8 , 0 ] >>> rolling_sem = river . stats . RollingSEM ( ddof = 1 , window_size = 2 ) >>> for x in X : ... print ( rolling_sem . update ( x ) . get ()) 0.0 1.5 1.0 3.0 2.0 4.0 >>> rolling_sem = river . stats . RollingSEM ( ddof = 1 , window_size = 3 ) >>> for x in X : ... print ( rolling_sem . update ( x ) . get ()) 0.0 1.5 0.881917 2.403700 2.905932 2.309401 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingSEM"},{"location":"api/stats/RollingSEM/#rollingsem","text":"Running standard error of the mean over a window.","title":"RollingSEM"},{"location":"api/stats/RollingSEM/#parameters","text":"window_size Size of the rolling window. ddof \u2013 defaults to 1 Delta Degrees of Freedom for the variance.","title":"Parameters"},{"location":"api/stats/RollingSEM/#attributes","text":"correction_factor name window_size","title":"Attributes"},{"location":"api/stats/RollingSEM/#examples","text":">>> import river >>> X = [ 1 , 4 , 2 , - 4 , - 8 , 0 ] >>> rolling_sem = river . stats . RollingSEM ( ddof = 1 , window_size = 2 ) >>> for x in X : ... print ( rolling_sem . update ( x ) . get ()) 0.0 1.5 1.0 3.0 2.0 4.0 >>> rolling_sem = river . stats . RollingSEM ( ddof = 1 , window_size = 3 ) >>> for x in X : ... print ( rolling_sem . update ( x ) . get ()) 0.0 1.5 0.881917 2.403700 2.905932 2.309401","title":"Examples"},{"location":"api/stats/RollingSEM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingSum/","text":"RollingSum \u00b6 Running sum over a window. Parameters \u00b6 window_size ( int ) Size of the rolling window. Attributes \u00b6 sum ( int ) The running rolling sum. Examples \u00b6 >>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_sum = stats . RollingSum ( 2 ) >>> for x in X : ... print ( rolling_sum . update ( x ) . get ()) 1 - 3 - 1 1 0 3 Methods \u00b6 append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingSum"},{"location":"api/stats/RollingSum/#rollingsum","text":"Running sum over a window.","title":"RollingSum"},{"location":"api/stats/RollingSum/#parameters","text":"window_size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/stats/RollingSum/#attributes","text":"sum ( int ) The running rolling sum.","title":"Attributes"},{"location":"api/stats/RollingSum/#examples","text":">>> from river import stats >>> X = [ 1 , - 4 , 3 , - 2 , 2 , 1 ] >>> rolling_sum = stats . RollingSum ( 2 ) >>> for x in X : ... print ( rolling_sum . update ( x ) . get ()) 1 - 3 - 1 1 0 3","title":"Examples"},{"location":"api/stats/RollingSum/#methods","text":"append clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. extend get Return the current value of the statistic. popleft revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/RollingVar/","text":"RollingVar \u00b6 Running variance over a window. Parameters \u00b6 window_size Size of the rolling window. ddof \u2013 defaults to 1 Delta Degrees of Freedom for the variance. Attributes \u00b6 sos ( float ) Sum of squares over the current window. rmean ( stats.RollingMean ) Examples \u00b6 >>> import river >>> X = [ 1 , 4 , 2 , - 4 , - 8 , 0 ] >>> rvar = river . stats . RollingVar ( ddof = 1 , window_size = 2 ) >>> for x in X : ... print ( rvar . update ( x ) . get ()) 0.0 4.5 2.0 18.0 8.0 32.0 >>> rvar = river . stats . RollingVar ( ddof = 1 , window_size = 3 ) >>> for x in X : ... print ( rvar . update ( x ) . get ()) 0.0 4.5 2.333333 17.333333 25.333333 16.0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"RollingVar"},{"location":"api/stats/RollingVar/#rollingvar","text":"Running variance over a window.","title":"RollingVar"},{"location":"api/stats/RollingVar/#parameters","text":"window_size Size of the rolling window. ddof \u2013 defaults to 1 Delta Degrees of Freedom for the variance.","title":"Parameters"},{"location":"api/stats/RollingVar/#attributes","text":"sos ( float ) Sum of squares over the current window. rmean ( stats.RollingMean )","title":"Attributes"},{"location":"api/stats/RollingVar/#examples","text":">>> import river >>> X = [ 1 , 4 , 2 , - 4 , - 8 , 0 ] >>> rvar = river . stats . RollingVar ( ddof = 1 , window_size = 2 ) >>> for x in X : ... print ( rvar . update ( x ) . get ()) 0.0 4.5 2.0 18.0 8.0 32.0 >>> rvar = river . stats . RollingVar ( ddof = 1 , window_size = 3 ) >>> for x in X : ... print ( rvar . update ( x ) . get ()) 0.0 4.5 2.333333 17.333333 25.333333 16.0","title":"Examples"},{"location":"api/stats/RollingVar/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/SEM/","text":"SEM \u00b6 Running standard error of the mean using Welford's algorithm. Parameters \u00b6 ddof \u2013 defaults to 1 Delta Degrees of Freedom. The divisor used in calculations is n - ddof , where n is the number of seen elements. Attributes \u00b6 n ( int ) Number of observations. Examples \u00b6 >>> import river.stats >>> X = [ 3 , 5 , 4 , 7 , 10 , 12 ] >>> sem = river . stats . SEM () >>> for x in X : ... print ( sem . update ( x ) . get ()) 0.0 1.0 0.577350 0.853912 1.240967 1.447219 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x w \u2013 defaults to 1.0 References \u00b6 Wikipedia article on algorithms for calculating variance \u21a9","title":"SEM"},{"location":"api/stats/SEM/#sem","text":"Running standard error of the mean using Welford's algorithm.","title":"SEM"},{"location":"api/stats/SEM/#parameters","text":"ddof \u2013 defaults to 1 Delta Degrees of Freedom. The divisor used in calculations is n - ddof , where n is the number of seen elements.","title":"Parameters"},{"location":"api/stats/SEM/#attributes","text":"n ( int ) Number of observations.","title":"Attributes"},{"location":"api/stats/SEM/#examples","text":">>> import river.stats >>> X = [ 3 , 5 , 4 , 7 , 10 , 12 ] >>> sem = river . stats . SEM () >>> for x in X : ... print ( sem . update ( x ) . get ()) 0.0 1.0 0.577350 0.853912 1.240967 1.447219","title":"Examples"},{"location":"api/stats/SEM/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x w \u2013 defaults to 1.0","title":"Methods"},{"location":"api/stats/SEM/#references","text":"Wikipedia article on algorithms for calculating variance \u21a9","title":"References"},{"location":"api/stats/Shift/","text":"Shift \u00b6 Shifts a data stream by returning past values. This can be used to compute statistics over past data. For instance, if you're computing daily averages, then shifting by 7 will be equivalent to computing averages from a week ago. Shifting values is useful when you're calculating an average over a target value. Indeed, in this case it's important to shift the values in order not to introduce leakage. The recommended way to do this is to feature_extraction.TargetAgg , which already takes care of shifting the target values once. Parameters \u00b6 amount \u2013 defaults to 1 Shift amount. The get method will return the t - amount value, where t is the current moment. fill_value \u2013 defaults to None This value will be returned by the get method if not enough values have been observed. Attributes \u00b6 name Examples \u00b6 It is rare to have to use Shift by itself. A more common usage is to compose it with other statistics. This can be done via the | operator. >>> from river import stats >>> stat = stats . Shift ( 1 ) | stats . Mean () >>> for i in range ( 5 ): ... stat = stat . update ( i ) ... print ( stat . get ()) 0.0 0.0 0.5 1.0 1.5 A common usecase for using Shift is when computing statistics on shifted data. For instance, say you have a dataset which records the amount of sales for a set of shops. You might then have a shop field and a sales field. Let's say you want to look at the average amount of sales per shop. You can do this by using a feature_extraction.Agg . When you call transform_one , you're expecting it to return the average amount of sales, without including today's sales. You can do this by prepending an instance of stats.Mean with an instance of stats.Shift . >>> from river import feature_extraction >>> agg = feature_extraction . Agg ( ... on = 'sales' , ... how = stats . Shift ( 1 ) | stats . Mean (), ... by = 'shop' ... ) Let's define a little example dataset. >>> X = iter ([ ... { 'shop' : 'Ikea' , 'sales' : 10 }, ... { 'shop' : 'Ikea' , 'sales' : 15 }, ... { 'shop' : 'Ikea' , 'sales' : 20 } ... ]) Now let's call the learn_one method to update our feature extractor. >>> x = next ( X ) >>> agg = agg . learn_one ( x ) At this point, the average defaults to the initial value of stats.Mean , which is 0. >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 0.0 } We can now update our feature extractor with the next data point and check the output. >>> agg = agg . learn_one ( next ( X )) >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 10.0 } >>> agg = agg . learn_one ( next ( X )) >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 12.5 } Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Shift"},{"location":"api/stats/Shift/#shift","text":"Shifts a data stream by returning past values. This can be used to compute statistics over past data. For instance, if you're computing daily averages, then shifting by 7 will be equivalent to computing averages from a week ago. Shifting values is useful when you're calculating an average over a target value. Indeed, in this case it's important to shift the values in order not to introduce leakage. The recommended way to do this is to feature_extraction.TargetAgg , which already takes care of shifting the target values once.","title":"Shift"},{"location":"api/stats/Shift/#parameters","text":"amount \u2013 defaults to 1 Shift amount. The get method will return the t - amount value, where t is the current moment. fill_value \u2013 defaults to None This value will be returned by the get method if not enough values have been observed.","title":"Parameters"},{"location":"api/stats/Shift/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Shift/#examples","text":"It is rare to have to use Shift by itself. A more common usage is to compose it with other statistics. This can be done via the | operator. >>> from river import stats >>> stat = stats . Shift ( 1 ) | stats . Mean () >>> for i in range ( 5 ): ... stat = stat . update ( i ) ... print ( stat . get ()) 0.0 0.0 0.5 1.0 1.5 A common usecase for using Shift is when computing statistics on shifted data. For instance, say you have a dataset which records the amount of sales for a set of shops. You might then have a shop field and a sales field. Let's say you want to look at the average amount of sales per shop. You can do this by using a feature_extraction.Agg . When you call transform_one , you're expecting it to return the average amount of sales, without including today's sales. You can do this by prepending an instance of stats.Mean with an instance of stats.Shift . >>> from river import feature_extraction >>> agg = feature_extraction . Agg ( ... on = 'sales' , ... how = stats . Shift ( 1 ) | stats . Mean (), ... by = 'shop' ... ) Let's define a little example dataset. >>> X = iter ([ ... { 'shop' : 'Ikea' , 'sales' : 10 }, ... { 'shop' : 'Ikea' , 'sales' : 15 }, ... { 'shop' : 'Ikea' , 'sales' : 20 } ... ]) Now let's call the learn_one method to update our feature extractor. >>> x = next ( X ) >>> agg = agg . learn_one ( x ) At this point, the average defaults to the initial value of stats.Mean , which is 0. >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 0.0 } We can now update our feature extractor with the next data point and check the output. >>> agg = agg . learn_one ( next ( X )) >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 10.0 } >>> agg = agg . learn_one ( next ( X )) >>> agg . transform_one ( x ) { 'sales_mean_of_shift_1_by_shop' : 12.5 }","title":"Examples"},{"location":"api/stats/Shift/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Skew/","text":"Skew \u00b6 Running skew using Welford's algorithm. Parameters \u00b6 bias \u2013 defaults to False If False , then the calculations are corrected for statistical bias. Attributes \u00b6 name Examples \u00b6 >>> import river.stats >>> import scipy.stats >>> import numpy as np >>> np . random . seed ( 42 ) >>> X = np . random . normal ( loc = 0 , scale = 1 , size = 10 ) >>> skew = river . stats . Skew ( bias = False ) >>> for x in X : ... print ( skew . update ( x ) . get ()) 0 0.0 - 1.4802398132849872 0.5127437186677888 0.7803466510704751 1.056115628922055 0.5057840774320389 0.3478402420400934 0.4536710660918704 0.4123070197493227 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . skew ( X [: i ], bias = False )) 0.0 0.0 - 1.4802398132849874 0.5127437186677893 0.7803466510704746 1.056115628922055 0.5057840774320389 0.3478402420400927 0.4536710660918703 0.4123070197493223 >>> skew = river . stats . Skew ( bias = True ) >>> for x in X : ... print ( skew . update ( x ) . get ()) 0 0.0 - 0.6043053732501439 0.2960327239981376 0.5234724473423674 0.7712778043924866 0.39022088752624845 0.278892645224261 0.37425953513864063 0.3476878073823696 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . skew ( X [: i ], bias = True )) 0.0 0.0 - 0.604305373250144 0.29603272399813796 0.5234724473423671 0.7712778043924865 0.39022088752624845 0.2788926452242604 0.3742595351386406 0.34768780738236926 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x References \u00b6 Wikipedia article on algorithms for calculating variance \u21a9","title":"Skew"},{"location":"api/stats/Skew/#skew","text":"Running skew using Welford's algorithm.","title":"Skew"},{"location":"api/stats/Skew/#parameters","text":"bias \u2013 defaults to False If False , then the calculations are corrected for statistical bias.","title":"Parameters"},{"location":"api/stats/Skew/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Skew/#examples","text":">>> import river.stats >>> import scipy.stats >>> import numpy as np >>> np . random . seed ( 42 ) >>> X = np . random . normal ( loc = 0 , scale = 1 , size = 10 ) >>> skew = river . stats . Skew ( bias = False ) >>> for x in X : ... print ( skew . update ( x ) . get ()) 0 0.0 - 1.4802398132849872 0.5127437186677888 0.7803466510704751 1.056115628922055 0.5057840774320389 0.3478402420400934 0.4536710660918704 0.4123070197493227 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . skew ( X [: i ], bias = False )) 0.0 0.0 - 1.4802398132849874 0.5127437186677893 0.7803466510704746 1.056115628922055 0.5057840774320389 0.3478402420400927 0.4536710660918703 0.4123070197493223 >>> skew = river . stats . Skew ( bias = True ) >>> for x in X : ... print ( skew . update ( x ) . get ()) 0 0.0 - 0.6043053732501439 0.2960327239981376 0.5234724473423674 0.7712778043924866 0.39022088752624845 0.278892645224261 0.37425953513864063 0.3476878073823696 >>> for i in range ( 1 , len ( X ) + 1 ): ... print ( scipy . stats . skew ( X [: i ], bias = True )) 0.0 0.0 - 0.604305373250144 0.29603272399813796 0.5234724473423671 0.7712778043924865 0.39022088752624845 0.2788926452242604 0.3742595351386406 0.34768780738236926","title":"Examples"},{"location":"api/stats/Skew/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Skew/#references","text":"Wikipedia article on algorithms for calculating variance \u21a9","title":"References"},{"location":"api/stats/Sum/","text":"Sum \u00b6 Running sum. Attributes \u00b6 sum ( float ) The running sum. Examples \u00b6 >>> from river import stats >>> X = [ - 5 , - 3 , - 1 , 1 , 3 , 5 ] >>> mean = stats . Sum () >>> for x in X : ... print ( mean . update ( x ) . get ()) - 5.0 - 8.0 - 9.0 - 8.0 - 5.0 0.0 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Sum"},{"location":"api/stats/Sum/#sum","text":"Running sum.","title":"Sum"},{"location":"api/stats/Sum/#attributes","text":"sum ( float ) The running sum.","title":"Attributes"},{"location":"api/stats/Sum/#examples","text":">>> from river import stats >>> X = [ - 5 , - 3 , - 1 , 1 , 3 , 5 ] >>> mean = stats . Sum () >>> for x in X : ... print ( mean . update ( x ) . get ()) - 5.0 - 8.0 - 9.0 - 8.0 - 5.0 0.0","title":"Examples"},{"location":"api/stats/Sum/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Univariate/","text":"Univariate \u00b6 A univariate statistic measures a property of a variable. Attributes \u00b6 name Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Univariate"},{"location":"api/stats/Univariate/#univariate","text":"A univariate statistic measures a property of a variable.","title":"Univariate"},{"location":"api/stats/Univariate/#attributes","text":"name","title":"Attributes"},{"location":"api/stats/Univariate/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x","title":"Methods"},{"location":"api/stats/Var/","text":"Var \u00b6 Running variance using Welford's algorithm. Parameters \u00b6 ddof \u2013 defaults to 1 Delta Degrees of Freedom. The divisor used in calculations is n - ddof , where n represents the number of seen elements. Attributes \u00b6 mean ( stats.Mean ) The running mean. sigma ( float ) The running variance. Examples \u00b6 >>> import river.stats >>> X = [ 3 , 5 , 4 , 7 , 10 , 12 ] >>> var = river . stats . Var () >>> for x in X : ... print ( var . update ( x ) . get ()) 0.0 2.0 1.0 2.916666 7.7 12.56666 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x w \u2013 defaults to 1.0 Notes \u00b6 The outcomes of the incremental and parallel updates are consistent with numpy's batch processing when \\(\\\\text{ddof} \\le 1\\) . References \u00b6 Wikipedia article on algorithms for calculating variance \u21a9 Chan, T.F., Golub, G.H. and LeVeque, R.J., 1983. Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician, 37(3), pp.242-247. \u21a9","title":"Var"},{"location":"api/stats/Var/#var","text":"Running variance using Welford's algorithm.","title":"Var"},{"location":"api/stats/Var/#parameters","text":"ddof \u2013 defaults to 1 Delta Degrees of Freedom. The divisor used in calculations is n - ddof , where n represents the number of seen elements.","title":"Parameters"},{"location":"api/stats/Var/#attributes","text":"mean ( stats.Mean ) The running mean. sigma ( float ) The running variance.","title":"Attributes"},{"location":"api/stats/Var/#examples","text":">>> import river.stats >>> X = [ 3 , 5 , 4 , 7 , 10 , 12 ] >>> var = river . stats . Var () >>> for x in X : ... print ( var . update ( x ) . get ()) 0.0 2.0 1.0 2.916666 7.7 12.56666","title":"Examples"},{"location":"api/stats/Var/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. get Return the current value of the statistic. revert Revert and return the called instance. Parameters x update Update and return the called instance. Parameters x w \u2013 defaults to 1.0","title":"Methods"},{"location":"api/stats/Var/#notes","text":"The outcomes of the incremental and parallel updates are consistent with numpy's batch processing when \\(\\\\text{ddof} \\le 1\\) .","title":"Notes"},{"location":"api/stats/Var/#references","text":"Wikipedia article on algorithms for calculating variance \u21a9 Chan, T.F., Golub, G.H. and LeVeque, R.J., 1983. Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician, 37(3), pp.242-247. \u21a9","title":"References"},{"location":"api/stream/Cache/","text":"Cache \u00b6 Utility for caching iterables. This can be used to save a stream of data to the disk in order to iterate over it faster the following time. This can save time depending on the nature of stream. The more processing happens in a stream, the more time will be saved. Even in the case where no processing is done apart from reading the data, the cache will save some time because it is using the pickle binary protocol. It can thus improve the speed in common cases such as reading from a CSV file. Parameters \u00b6 directory \u2013 defaults to None The path where to store the pickled data streams. If not provided, then it will be automatically inferred whenever possible, if not an exception will be raised. Attributes \u00b6 keys ( set ) The set of keys that are being cached. Examples \u00b6 >>> import time >>> from river import datasets >>> from river import stream >>> dataset = datasets . Phishing () >>> cache = stream . Cache () The cache can be used by wrapping it around an iterable. Because this is the first time are iterating over the data, nothing is cached. >>> tic = time . time () >>> for x , y in cache ( dataset , key = 'phishing' ): ... pass >>> toc = time . time () >>> print ( toc - tic ) # doctest: +SKIP 0.012813 If we do the same thing again, we can see the loop is now faster. >>> tic = time . time () >>> for x , y in cache ( dataset , key = 'phishing' ): ... pass >>> toc = time . time () >>> print ( toc - tic ) # doctest: +SKIP 0.001927 We can see an overview of the cache. The first line indicates the location of the cache. >>> cache # doctest: +SKIP / tmp phishing - 125.2 KiB Finally, we can clear the stream from the cache. >>> cache . clear ( 'phishing' ) >>> cache # doctest: +SKIP / tmp There is also a clear_all method to remove all the items in the cache. >>> cache . clear_all () Methods \u00b6 call Call self as a function. Parameters stream key \u2013 defaults to None clear Delete the cached stream associated with the given key. Parameters key ( str ) clear_all Delete all the cached streams.","title":"Cache"},{"location":"api/stream/Cache/#cache","text":"Utility for caching iterables. This can be used to save a stream of data to the disk in order to iterate over it faster the following time. This can save time depending on the nature of stream. The more processing happens in a stream, the more time will be saved. Even in the case where no processing is done apart from reading the data, the cache will save some time because it is using the pickle binary protocol. It can thus improve the speed in common cases such as reading from a CSV file.","title":"Cache"},{"location":"api/stream/Cache/#parameters","text":"directory \u2013 defaults to None The path where to store the pickled data streams. If not provided, then it will be automatically inferred whenever possible, if not an exception will be raised.","title":"Parameters"},{"location":"api/stream/Cache/#attributes","text":"keys ( set ) The set of keys that are being cached.","title":"Attributes"},{"location":"api/stream/Cache/#examples","text":">>> import time >>> from river import datasets >>> from river import stream >>> dataset = datasets . Phishing () >>> cache = stream . Cache () The cache can be used by wrapping it around an iterable. Because this is the first time are iterating over the data, nothing is cached. >>> tic = time . time () >>> for x , y in cache ( dataset , key = 'phishing' ): ... pass >>> toc = time . time () >>> print ( toc - tic ) # doctest: +SKIP 0.012813 If we do the same thing again, we can see the loop is now faster. >>> tic = time . time () >>> for x , y in cache ( dataset , key = 'phishing' ): ... pass >>> toc = time . time () >>> print ( toc - tic ) # doctest: +SKIP 0.001927 We can see an overview of the cache. The first line indicates the location of the cache. >>> cache # doctest: +SKIP / tmp phishing - 125.2 KiB Finally, we can clear the stream from the cache. >>> cache . clear ( 'phishing' ) >>> cache # doctest: +SKIP / tmp There is also a clear_all method to remove all the items in the cache. >>> cache . clear_all ()","title":"Examples"},{"location":"api/stream/Cache/#methods","text":"call Call self as a function. Parameters stream key \u2013 defaults to None clear Delete the cached stream associated with the given key. Parameters key ( str ) clear_all Delete all the cached streams.","title":"Methods"},{"location":"api/stream/iter-arff/","text":"iter_arff \u00b6 Iterates over rows from an ARFF file. Parameters \u00b6 filepath_or_buffer Either a string indicating the location of a CSV file, or a buffer object that has a read method. target ( str ) \u2013 defaults to None Name of the target field. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'.","title":"iter_arff"},{"location":"api/stream/iter-arff/#iter_arff","text":"Iterates over rows from an ARFF file.","title":"iter_arff"},{"location":"api/stream/iter-arff/#parameters","text":"filepath_or_buffer Either a string indicating the location of a CSV file, or a buffer object that has a read method. target ( str ) \u2013 defaults to None Name of the target field. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'.","title":"Parameters"},{"location":"api/stream/iter-array/","text":"iter_array \u00b6 Iterates over the rows from an array of features and an array of targets. This method is intended to work with numpy arrays, but should also work with Python lists. Parameters \u00b6 X ( numpy.ndarray ) A 2D array of features. y ( numpy.ndarray ) \u2013 defaults to None An optional array of targets. feature_names ( List[Hashable] ) \u2013 defaults to None An optional list of feature names. The features will be labeled with integers if no names are provided. target_names ( List[Hashable] ) \u2013 defaults to None An optional list of output names. The outputs will be labeled with integers if no names are provided. Only applies if there are multiple outputs, i.e. if y is a 2D array. shuffle ( bool ) \u2013 defaults to False Indicates whether or not to shuffle the input arrays before iterating over them. seed ( int ) \u2013 defaults to None Random seed used for shuffling the data. Examples \u00b6 >>> from river import stream >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 11 , 12 , 13 ]]) >>> Y = np . array ([ True , False ]) >>> dataset = stream . iter_array ( ... X , Y , ... feature_names = [ 'x1' , 'x2' , 'x3' ] ... ) >>> for x , y in dataset : ... print ( x , y ) { 'x1' : 1 , 'x2' : 2 , 'x3' : 3 } True { 'x1' : 11 , 'x2' : 12 , 'x3' : 13 } False","title":"iter_array"},{"location":"api/stream/iter-array/#iter_array","text":"Iterates over the rows from an array of features and an array of targets. This method is intended to work with numpy arrays, but should also work with Python lists.","title":"iter_array"},{"location":"api/stream/iter-array/#parameters","text":"X ( numpy.ndarray ) A 2D array of features. y ( numpy.ndarray ) \u2013 defaults to None An optional array of targets. feature_names ( List[Hashable] ) \u2013 defaults to None An optional list of feature names. The features will be labeled with integers if no names are provided. target_names ( List[Hashable] ) \u2013 defaults to None An optional list of output names. The outputs will be labeled with integers if no names are provided. Only applies if there are multiple outputs, i.e. if y is a 2D array. shuffle ( bool ) \u2013 defaults to False Indicates whether or not to shuffle the input arrays before iterating over them. seed ( int ) \u2013 defaults to None Random seed used for shuffling the data.","title":"Parameters"},{"location":"api/stream/iter-array/#examples","text":">>> from river import stream >>> import numpy as np >>> X = np . array ([[ 1 , 2 , 3 ], [ 11 , 12 , 13 ]]) >>> Y = np . array ([ True , False ]) >>> dataset = stream . iter_array ( ... X , Y , ... feature_names = [ 'x1' , 'x2' , 'x3' ] ... ) >>> for x , y in dataset : ... print ( x , y ) { 'x1' : 1 , 'x2' : 2 , 'x3' : 3 } True { 'x1' : 11 , 'x2' : 12 , 'x3' : 13 } False","title":"Examples"},{"location":"api/stream/iter-csv/","text":"iter_csv \u00b6 Iterates over rows from a CSV file. Reading CSV files can be quite slow. If, for whatever reason, you're going to loop through the same file multiple times, then we recommend that you to use the stream.Cache utility. Parameters \u00b6 filepath_or_buffer Either a string indicating the location of a CSV file, or a buffer object that has a read method. target ( Union[str, List[str]] ) \u2013 defaults to None A single target column is assumed if a string is passed. A multiple output scenario is assumed if a list of strings is passed. A None value will be assigned to each y if this parameter is omitted. converters ( dict ) \u2013 defaults to None A dict mapping feature names to callables used to parse their associated values. parse_dates ( dict ) \u2013 defaults to None A dict mapping feature names to a format passed to the datetime.datetime.strptime method. drop ( List[str] ) \u2013 defaults to None Fields to ignore. drop_nones \u2013 defaults to False Whether or not to drop fields where the value is a None . fraction \u2013 defaults to 1.0 Sampling fraction. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'. seed ( int ) \u2013 defaults to None If specified, the sampling will be deterministic. field_size_limit ( int ) \u2013 defaults to None If not None , this will be passed to the csv.field_size_limit function. kwargs All other keyword arguments are passed to the underlying csv.DictReader . Examples \u00b6 Although this function is designed to handle different kinds of inputs, the most common use case is to read a file on the disk. We'll first create a little CSV file to illustrate. >>> tv_shows = '''name,year,rating ... Planet Earth II,2016,9.5 ... Planet Earth,2006,9.4 ... Band of Brothers,2001,9.4 ... Breaking Bad,2008,9.4 ... Chernobyl,2019,9.4 ... ''' >>> with open ( 'tv_shows.csv' , mode = 'w' ) as f : ... _ = f . write ( tv_shows ) We can now go through the rows one by one. We can use the converters parameter to cast the rating field value as a float . We can also convert the year to a datetime via the parse_dates parameter. >>> from river import stream >>> params = { ... 'converters' : { 'rating' : float }, ... 'parse_dates' : { 'year' : '%Y' } ... } >>> for x , y in stream . iter_csv ( 'tv_shows.csv' , ** params ): ... print ( x , y ) { 'name' : 'Planet Earth II' , 'year' : datetime . datetime ( 2016 , 1 , 1 , 0 , 0 ), 'rating' : 9.5 } None { 'name' : 'Planet Earth' , 'year' : datetime . datetime ( 2006 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Band of Brothers' , 'year' : datetime . datetime ( 2001 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Breaking Bad' , 'year' : datetime . datetime ( 2008 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Chernobyl' , 'year' : datetime . datetime ( 2019 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None The value of y is always None because we haven't provided a value for the target parameter. Here is an example where a target is provided: >>> dataset = stream . iter_csv ( 'tv_shows.csv' , target = 'rating' , ** params ) >>> for x , y in dataset : ... print ( x , y ) { 'name' : 'Planet Earth II' , 'year' : datetime . datetime ( 2016 , 1 , 1 , 0 , 0 )} 9.5 { 'name' : 'Planet Earth' , 'year' : datetime . datetime ( 2006 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Band of Brothers' , 'year' : datetime . datetime ( 2001 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Breaking Bad' , 'year' : datetime . datetime ( 2008 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Chernobyl' , 'year' : datetime . datetime ( 2019 , 1 , 1 , 0 , 0 )} 9.4 Finally, let's delete the example file. >>> import os ; os . remove ( 'tv_shows.csv' )","title":"iter_csv"},{"location":"api/stream/iter-csv/#iter_csv","text":"Iterates over rows from a CSV file. Reading CSV files can be quite slow. If, for whatever reason, you're going to loop through the same file multiple times, then we recommend that you to use the stream.Cache utility.","title":"iter_csv"},{"location":"api/stream/iter-csv/#parameters","text":"filepath_or_buffer Either a string indicating the location of a CSV file, or a buffer object that has a read method. target ( Union[str, List[str]] ) \u2013 defaults to None A single target column is assumed if a string is passed. A multiple output scenario is assumed if a list of strings is passed. A None value will be assigned to each y if this parameter is omitted. converters ( dict ) \u2013 defaults to None A dict mapping feature names to callables used to parse their associated values. parse_dates ( dict ) \u2013 defaults to None A dict mapping feature names to a format passed to the datetime.datetime.strptime method. drop ( List[str] ) \u2013 defaults to None Fields to ignore. drop_nones \u2013 defaults to False Whether or not to drop fields where the value is a None . fraction \u2013 defaults to 1.0 Sampling fraction. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'. seed ( int ) \u2013 defaults to None If specified, the sampling will be deterministic. field_size_limit ( int ) \u2013 defaults to None If not None , this will be passed to the csv.field_size_limit function. kwargs All other keyword arguments are passed to the underlying csv.DictReader .","title":"Parameters"},{"location":"api/stream/iter-csv/#examples","text":"Although this function is designed to handle different kinds of inputs, the most common use case is to read a file on the disk. We'll first create a little CSV file to illustrate. >>> tv_shows = '''name,year,rating ... Planet Earth II,2016,9.5 ... Planet Earth,2006,9.4 ... Band of Brothers,2001,9.4 ... Breaking Bad,2008,9.4 ... Chernobyl,2019,9.4 ... ''' >>> with open ( 'tv_shows.csv' , mode = 'w' ) as f : ... _ = f . write ( tv_shows ) We can now go through the rows one by one. We can use the converters parameter to cast the rating field value as a float . We can also convert the year to a datetime via the parse_dates parameter. >>> from river import stream >>> params = { ... 'converters' : { 'rating' : float }, ... 'parse_dates' : { 'year' : '%Y' } ... } >>> for x , y in stream . iter_csv ( 'tv_shows.csv' , ** params ): ... print ( x , y ) { 'name' : 'Planet Earth II' , 'year' : datetime . datetime ( 2016 , 1 , 1 , 0 , 0 ), 'rating' : 9.5 } None { 'name' : 'Planet Earth' , 'year' : datetime . datetime ( 2006 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Band of Brothers' , 'year' : datetime . datetime ( 2001 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Breaking Bad' , 'year' : datetime . datetime ( 2008 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None { 'name' : 'Chernobyl' , 'year' : datetime . datetime ( 2019 , 1 , 1 , 0 , 0 ), 'rating' : 9.4 } None The value of y is always None because we haven't provided a value for the target parameter. Here is an example where a target is provided: >>> dataset = stream . iter_csv ( 'tv_shows.csv' , target = 'rating' , ** params ) >>> for x , y in dataset : ... print ( x , y ) { 'name' : 'Planet Earth II' , 'year' : datetime . datetime ( 2016 , 1 , 1 , 0 , 0 )} 9.5 { 'name' : 'Planet Earth' , 'year' : datetime . datetime ( 2006 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Band of Brothers' , 'year' : datetime . datetime ( 2001 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Breaking Bad' , 'year' : datetime . datetime ( 2008 , 1 , 1 , 0 , 0 )} 9.4 { 'name' : 'Chernobyl' , 'year' : datetime . datetime ( 2019 , 1 , 1 , 0 , 0 )} 9.4 Finally, let's delete the example file. >>> import os ; os . remove ( 'tv_shows.csv' )","title":"Examples"},{"location":"api/stream/iter-libsvm/","text":"iter_libsvm \u00b6 Iterates over a dataset in LIBSVM format. The LIBSVM format is a popular way in the machine learning community to store sparse datasets. Only numerical feature values are supported. The feature names will be considered as strings. Parameters \u00b6 filepath_or_buffer ( str ) Either a string indicating the location of a CSV file, or a buffer object that has a read method. target_type \u2013 defaults to <class 'float'> The type of the target value. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'. Examples \u00b6 >>> import io >>> from river import stream >>> data = io . StringIO ( '''+1 x:-134.26 y:0.2563 ... 1 x:-12 z:0.3 ... -1 y:.25 ... ''' ) >>> for x , y in stream . iter_libsvm ( data , target_type = int ): ... print ( y , x ) 1 { 'x' : - 134.26 , 'y' : 0.2563 } 1 { 'x' : - 12.0 , 'z' : 0.3 } - 1 { 'y' : 0.25 } References \u00b6 LIBSVM documentation \u21a9","title":"iter_libsvm"},{"location":"api/stream/iter-libsvm/#iter_libsvm","text":"Iterates over a dataset in LIBSVM format. The LIBSVM format is a popular way in the machine learning community to store sparse datasets. Only numerical feature values are supported. The feature names will be considered as strings.","title":"iter_libsvm"},{"location":"api/stream/iter-libsvm/#parameters","text":"filepath_or_buffer ( str ) Either a string indicating the location of a CSV file, or a buffer object that has a read method. target_type \u2013 defaults to <class 'float'> The type of the target value. compression \u2013 defaults to infer For on-the-fly decompression of on-disk data. If this is set to 'infer' and filepath_or_buffer is a path, then the decompression method is inferred for the following extensions: '.gz', '.zip'.","title":"Parameters"},{"location":"api/stream/iter-libsvm/#examples","text":">>> import io >>> from river import stream >>> data = io . StringIO ( '''+1 x:-134.26 y:0.2563 ... 1 x:-12 z:0.3 ... -1 y:.25 ... ''' ) >>> for x , y in stream . iter_libsvm ( data , target_type = int ): ... print ( y , x ) 1 { 'x' : - 134.26 , 'y' : 0.2563 } 1 { 'x' : - 12.0 , 'z' : 0.3 } - 1 { 'y' : 0.25 }","title":"Examples"},{"location":"api/stream/iter-libsvm/#references","text":"LIBSVM documentation \u21a9","title":"References"},{"location":"api/stream/iter-pandas/","text":"iter_pandas \u00b6 Iterates over the rows of a pandas.DataFrame . Parameters \u00b6 X ( pandas.core.frame.DataFrame ) A dataframe of features. y ( Union[pandas.core.series.Series, pandas.core.frame.DataFrame] ) \u2013 defaults to None A series or a dataframe with one column per target. kwargs Extra keyword arguments are passed to the underlying call to stream.iter_array . Examples \u00b6 >>> import pandas as pd >>> from river import stream >>> X = pd . DataFrame ({ ... 'x1' : [ 1 , 2 , 3 , 4 ], ... 'x2' : [ 'blue' , 'yellow' , 'yellow' , 'blue' ], ... 'y' : [ True , False , False , True ] ... }) >>> y = X . pop ( 'y' ) >>> for xi , yi in stream . iter_pandas ( X , y ): ... print ( xi , yi ) { 'x1' : 1 , 'x2' : 'blue' } True { 'x1' : 2 , 'x2' : 'yellow' } False { 'x1' : 3 , 'x2' : 'yellow' } False { 'x1' : 4 , 'x2' : 'blue' } True","title":"iter_pandas"},{"location":"api/stream/iter-pandas/#iter_pandas","text":"Iterates over the rows of a pandas.DataFrame .","title":"iter_pandas"},{"location":"api/stream/iter-pandas/#parameters","text":"X ( pandas.core.frame.DataFrame ) A dataframe of features. y ( Union[pandas.core.series.Series, pandas.core.frame.DataFrame] ) \u2013 defaults to None A series or a dataframe with one column per target. kwargs Extra keyword arguments are passed to the underlying call to stream.iter_array .","title":"Parameters"},{"location":"api/stream/iter-pandas/#examples","text":">>> import pandas as pd >>> from river import stream >>> X = pd . DataFrame ({ ... 'x1' : [ 1 , 2 , 3 , 4 ], ... 'x2' : [ 'blue' , 'yellow' , 'yellow' , 'blue' ], ... 'y' : [ True , False , False , True ] ... }) >>> y = X . pop ( 'y' ) >>> for xi , yi in stream . iter_pandas ( X , y ): ... print ( xi , yi ) { 'x1' : 1 , 'x2' : 'blue' } True { 'x1' : 2 , 'x2' : 'yellow' } False { 'x1' : 3 , 'x2' : 'yellow' } False { 'x1' : 4 , 'x2' : 'blue' } True","title":"Examples"},{"location":"api/stream/iter-sklearn-dataset/","text":"iter_sklearn_dataset \u00b6 Iterates rows from one of the datasets provided by scikit-learn. This allows you to use any dataset from scikit-learn's datasets module . For instance, you can use the fetch_openml function to get access to all of the datasets from the OpenML website. Parameters \u00b6 dataset ( 'sklearn.utils.Bunch' ) A scikit-learn dataset. kwargs Extra keyword arguments are passed to the underlying call to stream.iter_array . Examples \u00b6 >>> import pprint >>> from sklearn import datasets >>> from river import stream >>> dataset = datasets . load_boston () >>> for xi , yi in stream . iter_sklearn_dataset ( dataset ): ... pprint . pprint ( xi ) ... print ( yi ) ... break { 'AGE' : 65.2 , 'B' : 396.9 , 'CHAS' : 0.0 , 'CRIM' : 0.00632 , 'DIS' : 4.09 , 'INDUS' : 2.31 , 'LSTAT' : 4.98 , 'NOX' : 0.538 , 'PTRATIO' : 15.3 , 'RAD' : 1.0 , 'RM' : 6.575 , 'TAX' : 296.0 , 'ZN' : 18.0 } 24.0","title":"iter_sklearn_dataset"},{"location":"api/stream/iter-sklearn-dataset/#iter_sklearn_dataset","text":"Iterates rows from one of the datasets provided by scikit-learn. This allows you to use any dataset from scikit-learn's datasets module . For instance, you can use the fetch_openml function to get access to all of the datasets from the OpenML website.","title":"iter_sklearn_dataset"},{"location":"api/stream/iter-sklearn-dataset/#parameters","text":"dataset ( 'sklearn.utils.Bunch' ) A scikit-learn dataset. kwargs Extra keyword arguments are passed to the underlying call to stream.iter_array .","title":"Parameters"},{"location":"api/stream/iter-sklearn-dataset/#examples","text":">>> import pprint >>> from sklearn import datasets >>> from river import stream >>> dataset = datasets . load_boston () >>> for xi , yi in stream . iter_sklearn_dataset ( dataset ): ... pprint . pprint ( xi ) ... print ( yi ) ... break { 'AGE' : 65.2 , 'B' : 396.9 , 'CHAS' : 0.0 , 'CRIM' : 0.00632 , 'DIS' : 4.09 , 'INDUS' : 2.31 , 'LSTAT' : 4.98 , 'NOX' : 0.538 , 'PTRATIO' : 15.3 , 'RAD' : 1.0 , 'RM' : 6.575 , 'TAX' : 296.0 , 'ZN' : 18.0 } 24.0","title":"Examples"},{"location":"api/stream/iter-sql/","text":"iter_sql \u00b6 Iterates over the results from an SQL query. By default, SQLAlchemy prefetches results. Therefore, even though you can iterate over the resulting rows one by one, the results are in fact loaded in batch. You can modify this behavior by configuring the connection you pass to iter_sql . For instance, you can set the stream_results parameter to True , as explained in SQLAlchemy's documentation . Note, however, that this isn't available for all database engines. Parameters \u00b6 query ( Union[str, sqlalchemy.sql.selectable.Selectable] ) SQL query to be executed. conn ( sqlalchemy.engine.interfaces.Connectable ) An SQLAlchemy construct which has an execute method. In other words you can pass an engine, a connection, or a session. target_name ( str ) \u2013 defaults to None The name of the target field. If this is None , then y will also be None . Examples \u00b6 As an example we'll create an in-memory database with SQLAlchemy. >>> import datetime as dt >>> import sqlalchemy >>> engine = sqlalchemy . create_engine ( 'sqlite://' ) >>> metadata = sqlalchemy . MetaData () >>> t_sales = sqlalchemy . Table ( 'sales' , metadata , ... sqlalchemy . Column ( 'shop' , sqlalchemy . String , primary_key = True ), ... sqlalchemy . Column ( 'date' , sqlalchemy . Date , primary_key = True ), ... sqlalchemy . Column ( 'amount' , sqlalchemy . Integer ) ... ) >>> metadata . create_all ( engine ) >>> sales = [ ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 2 ), 'amount' : 20 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 2 ), 'amount' : 18 }, ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 3 ), 'amount' : 22 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 3 ), 'amount' : 14 }, ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 4 ), 'amount' : 12 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 4 ), 'amount' : 16 } ... ] >>> with engine . connect () as conn : ... _ = conn . execute ( t_sales . insert (), sales ) We can now query the database. We will set amount to be the target field. >>> from river import stream >>> with engine . connect () as conn : ... query = 'SELECT * FROM sales;' ... dataset = stream . iter_sql ( query , conn , target_name = 'amount' ) ... for x , y in dataset : ... print ( x , y ) { 'shop' : 'Hema' , 'date' : '2016-08-02' } 20 { 'shop' : 'Ikea' , 'date' : '2016-08-02' } 18 { 'shop' : 'Hema' , 'date' : '2016-08-03' } 22 { 'shop' : 'Ikea' , 'date' : '2016-08-03' } 14 { 'shop' : 'Hema' , 'date' : '2016-08-04' } 12 { 'shop' : 'Ikea' , 'date' : '2016-08-04' } 16","title":"iter_sql"},{"location":"api/stream/iter-sql/#iter_sql","text":"Iterates over the results from an SQL query. By default, SQLAlchemy prefetches results. Therefore, even though you can iterate over the resulting rows one by one, the results are in fact loaded in batch. You can modify this behavior by configuring the connection you pass to iter_sql . For instance, you can set the stream_results parameter to True , as explained in SQLAlchemy's documentation . Note, however, that this isn't available for all database engines.","title":"iter_sql"},{"location":"api/stream/iter-sql/#parameters","text":"query ( Union[str, sqlalchemy.sql.selectable.Selectable] ) SQL query to be executed. conn ( sqlalchemy.engine.interfaces.Connectable ) An SQLAlchemy construct which has an execute method. In other words you can pass an engine, a connection, or a session. target_name ( str ) \u2013 defaults to None The name of the target field. If this is None , then y will also be None .","title":"Parameters"},{"location":"api/stream/iter-sql/#examples","text":"As an example we'll create an in-memory database with SQLAlchemy. >>> import datetime as dt >>> import sqlalchemy >>> engine = sqlalchemy . create_engine ( 'sqlite://' ) >>> metadata = sqlalchemy . MetaData () >>> t_sales = sqlalchemy . Table ( 'sales' , metadata , ... sqlalchemy . Column ( 'shop' , sqlalchemy . String , primary_key = True ), ... sqlalchemy . Column ( 'date' , sqlalchemy . Date , primary_key = True ), ... sqlalchemy . Column ( 'amount' , sqlalchemy . Integer ) ... ) >>> metadata . create_all ( engine ) >>> sales = [ ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 2 ), 'amount' : 20 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 2 ), 'amount' : 18 }, ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 3 ), 'amount' : 22 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 3 ), 'amount' : 14 }, ... { 'shop' : 'Hema' , 'date' : dt . date ( 2016 , 8 , 4 ), 'amount' : 12 }, ... { 'shop' : 'Ikea' , 'date' : dt . date ( 2016 , 8 , 4 ), 'amount' : 16 } ... ] >>> with engine . connect () as conn : ... _ = conn . execute ( t_sales . insert (), sales ) We can now query the database. We will set amount to be the target field. >>> from river import stream >>> with engine . connect () as conn : ... query = 'SELECT * FROM sales;' ... dataset = stream . iter_sql ( query , conn , target_name = 'amount' ) ... for x , y in dataset : ... print ( x , y ) { 'shop' : 'Hema' , 'date' : '2016-08-02' } 20 { 'shop' : 'Ikea' , 'date' : '2016-08-02' } 18 { 'shop' : 'Hema' , 'date' : '2016-08-03' } 22 { 'shop' : 'Ikea' , 'date' : '2016-08-03' } 14 { 'shop' : 'Hema' , 'date' : '2016-08-04' } 12 { 'shop' : 'Ikea' , 'date' : '2016-08-04' } 16","title":"Examples"},{"location":"api/stream/shuffle/","text":"shuffle \u00b6 Shuffles a stream of data. This works by maintaining a buffer of elements. The first buffer_size elements are stored in memory. Once the buffer is full, a random element inside the buffer is yielded. Every time an element is yielded, the next element in the stream replaces it and the buffer is sampled again. Increasing buffer_size will improve the quality of the shuffling. If you really want to stream over your dataset in a \"good\" random order, the best way is to split your dataset into smaller datasets and loop over them in a round-robin fashion. You may do this by using the roundrobin recipe from the itertools module. Parameters \u00b6 stream ( Iterator ) The stream to shuffle. buffer_size ( int ) The size of the buffer which contains the elements help in memory. Increasing this will increase randomness but will incur more memory usage. seed ( int ) \u2013 defaults to None Random seed used for sampling. Examples \u00b6 >>> from river import stream >>> for i in stream . shuffle ( range ( 15 ), buffer_size = 5 , seed = 42 ): ... print ( i ) 0 5 2 1 8 9 6 4 11 12 10 7 14 13 3 References \u00b6 Visualizing TensorFlow's streaming shufflers \u21a9","title":"shuffle"},{"location":"api/stream/shuffle/#shuffle","text":"Shuffles a stream of data. This works by maintaining a buffer of elements. The first buffer_size elements are stored in memory. Once the buffer is full, a random element inside the buffer is yielded. Every time an element is yielded, the next element in the stream replaces it and the buffer is sampled again. Increasing buffer_size will improve the quality of the shuffling. If you really want to stream over your dataset in a \"good\" random order, the best way is to split your dataset into smaller datasets and loop over them in a round-robin fashion. You may do this by using the roundrobin recipe from the itertools module.","title":"shuffle"},{"location":"api/stream/shuffle/#parameters","text":"stream ( Iterator ) The stream to shuffle. buffer_size ( int ) The size of the buffer which contains the elements help in memory. Increasing this will increase randomness but will incur more memory usage. seed ( int ) \u2013 defaults to None Random seed used for sampling.","title":"Parameters"},{"location":"api/stream/shuffle/#examples","text":">>> from river import stream >>> for i in stream . shuffle ( range ( 15 ), buffer_size = 5 , seed = 42 ): ... print ( i ) 0 5 2 1 8 9 6 4 11 12 10 7 14 13 3","title":"Examples"},{"location":"api/stream/shuffle/#references","text":"Visualizing TensorFlow's streaming shufflers \u21a9","title":"References"},{"location":"api/stream/simulate-qa/","text":"simulate_qa \u00b6 Simulate a time-ordered question and answer session. This method allows looping through a dataset in the order in which it arrived. Indeed, it usually is the case that labels arrive after features. Being able to go through a dataset in arrival order enables assessing a model's performance in a reliable manner. For instance, the evaluate.progressive_val_score is a high-level method that can be used to score a model on a dataset. Under the hood it uses this method to determine the correct arrival order. Parameters \u00b6 dataset ( Iterator[Tuple[dict, Any]] ) A stream of (features, target) tuples. moment ( Union[str, Callable] ) The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitly timestamped in the order in which they arrive. If a str is passed, then it will be used to obtain the time from the input features. delay ( Union[str, int, datetime.timedelta, Callable] ) The amount of time to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. If a scalar is passed, such an int or a datetime.timedelta , then the delay is constant. copy ( bool ) \u2013 defaults to True If True , then a separate copy of the features are yielded the second time around. This ensures that inadvertent modifications in downstream code don't have any effect. Examples \u00b6 The arrival delay isn't usually indicated in a dataset, but it might be able to be inferred from the features. As an example, we'll simulate the departure and arrival time of taxi trips. Let's first create a time table which records the departure time and the duration of seconds of several taxi trips. >>> import datetime as dt >>> time_table = [ ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 0 , 0 ), 900 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 10 , 0 ), 1800 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 20 , 0 ), 300 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 45 , 0 ), 400 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 50 , 0 ), 240 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 55 , 0 ), 450 ) ... ] We can now create a streaming dataset where the features are the departure dates and the targets are the durations. >>> dataset = ( ... ({ 'date' : date }, duration ) ... for date , duration in time_table ... ) Now, we can use simulate_qa to iterate over the events in the order in which they are meant to occur. >>> delay = lambda _ , y : dt . timedelta ( seconds = y ) >>> for i , x , y in simulate_qa ( dataset , moment = 'date' , delay = delay ): ... if y is None : ... print ( f ' { x [ \"date\" ] } - trip # { i } departs' ) ... else : ... arrival_date = x [ 'date' ] + dt . timedelta ( seconds = y ) ... print ( f ' { arrival_date } - trip # { i } arrives after { y } seconds' ) 2020 - 01 - 01 20 : 00 : 00 - trip #0 departs 2020 - 01 - 01 20 : 10 : 00 - trip #1 departs 2020 - 01 - 01 20 : 15 : 00 - trip #0 arrives after 900 seconds 2020 - 01 - 01 20 : 20 : 00 - trip #2 departs 2020 - 01 - 01 20 : 25 : 00 - trip #2 arrives after 300 seconds 2020 - 01 - 01 20 : 40 : 00 - trip #1 arrives after 1800 seconds 2020 - 01 - 01 20 : 45 : 00 - trip #3 departs 2020 - 01 - 01 20 : 50 : 00 - trip #4 departs 2020 - 01 - 01 20 : 51 : 40 - trip #3 arrives after 400 seconds 2020 - 01 - 01 20 : 54 : 00 - trip #4 arrives after 240 seconds 2020 - 01 - 01 20 : 55 : 00 - trip #5 departs 2020 - 01 - 01 21 : 02 : 30 - trip #5 arrives after 450 seconds This function is extremely practical because it provides a reliable way to evaluate the performance of a model in a real scenario. Indeed, it allows to make predictions and perform model updates in exactly the same manner that would happen live. For instance, it is used in evaluate.progressive_val_score , which is a higher level function for evaluating models in an online manner.","title":"simulate_qa"},{"location":"api/stream/simulate-qa/#simulate_qa","text":"Simulate a time-ordered question and answer session. This method allows looping through a dataset in the order in which it arrived. Indeed, it usually is the case that labels arrive after features. Being able to go through a dataset in arrival order enables assessing a model's performance in a reliable manner. For instance, the evaluate.progressive_val_score is a high-level method that can be used to score a model on a dataset. Under the hood it uses this method to determine the correct arrival order.","title":"simulate_qa"},{"location":"api/stream/simulate-qa/#parameters","text":"dataset ( Iterator[Tuple[dict, Any]] ) A stream of (features, target) tuples. moment ( Union[str, Callable] ) The attribute used for measuring time. If a callable is passed, then it is expected to take as input a dict of features. If None , then the observations are implicitly timestamped in the order in which they arrive. If a str is passed, then it will be used to obtain the time from the input features. delay ( Union[str, int, datetime.timedelta, Callable] ) The amount of time to wait before revealing the target associated with each observation to the model. This value is expected to be able to sum with the moment value. For instance, if moment is a datetime.date , then delay is expected to be a datetime.timedelta . If a callable is passed, then it is expected to take as input a dict of features and the target. If a str is passed, then it will be used to access the relevant field from the features. If None is passed, then no delay will be used, which leads to doing standard online validation. If a scalar is passed, such an int or a datetime.timedelta , then the delay is constant. copy ( bool ) \u2013 defaults to True If True , then a separate copy of the features are yielded the second time around. This ensures that inadvertent modifications in downstream code don't have any effect.","title":"Parameters"},{"location":"api/stream/simulate-qa/#examples","text":"The arrival delay isn't usually indicated in a dataset, but it might be able to be inferred from the features. As an example, we'll simulate the departure and arrival time of taxi trips. Let's first create a time table which records the departure time and the duration of seconds of several taxi trips. >>> import datetime as dt >>> time_table = [ ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 0 , 0 ), 900 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 10 , 0 ), 1800 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 20 , 0 ), 300 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 45 , 0 ), 400 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 50 , 0 ), 240 ), ... ( dt . datetime ( 2020 , 1 , 1 , 20 , 55 , 0 ), 450 ) ... ] We can now create a streaming dataset where the features are the departure dates and the targets are the durations. >>> dataset = ( ... ({ 'date' : date }, duration ) ... for date , duration in time_table ... ) Now, we can use simulate_qa to iterate over the events in the order in which they are meant to occur. >>> delay = lambda _ , y : dt . timedelta ( seconds = y ) >>> for i , x , y in simulate_qa ( dataset , moment = 'date' , delay = delay ): ... if y is None : ... print ( f ' { x [ \"date\" ] } - trip # { i } departs' ) ... else : ... arrival_date = x [ 'date' ] + dt . timedelta ( seconds = y ) ... print ( f ' { arrival_date } - trip # { i } arrives after { y } seconds' ) 2020 - 01 - 01 20 : 00 : 00 - trip #0 departs 2020 - 01 - 01 20 : 10 : 00 - trip #1 departs 2020 - 01 - 01 20 : 15 : 00 - trip #0 arrives after 900 seconds 2020 - 01 - 01 20 : 20 : 00 - trip #2 departs 2020 - 01 - 01 20 : 25 : 00 - trip #2 arrives after 300 seconds 2020 - 01 - 01 20 : 40 : 00 - trip #1 arrives after 1800 seconds 2020 - 01 - 01 20 : 45 : 00 - trip #3 departs 2020 - 01 - 01 20 : 50 : 00 - trip #4 departs 2020 - 01 - 01 20 : 51 : 40 - trip #3 arrives after 400 seconds 2020 - 01 - 01 20 : 54 : 00 - trip #4 arrives after 240 seconds 2020 - 01 - 01 20 : 55 : 00 - trip #5 departs 2020 - 01 - 01 21 : 02 : 30 - trip #5 arrives after 450 seconds This function is extremely practical because it provides a reliable way to evaluate the performance of a model in a real scenario. Indeed, it allows to make predictions and perform model updates in exactly the same manner that would happen live. For instance, it is used in evaluate.progressive_val_score , which is a higher level function for evaluating models in an online manner.","title":"Examples"},{"location":"api/synth/Agrawal/","text":"Agrawal \u00b6 Agrawal stream generator. The generator was introduced by Agrawal et al. 1 , and was a common source of data for early work on scaling up decision tree learners. The generator produces a stream containing nine features, six numeric and three categorical. There are 10 functions defined for generating binary class labels from the features. Presumably these determine whether the loan should be approved. Classification functions are listed in the original paper 1 . Feature | Description | Values salary | salary | uniformly distributed from 20k to 150k commission | commission | 0 if salary < 75k else uniformly distributed from 10k to 75k age | age | uniformly distributed from 20 to 80 elevel | education level | uniformly chosen from 0 to 4 car | car maker | uniformly chosen from 1 to 20 zipcode | zip code of the town | uniformly chosen from 0 to 8 hvalue | house value | uniformly distributed from 50k x zipcode to 100k x zipcode hyears | years house owned | uniformly distributed from 1 to 30 loan | total loan amount | uniformly distributed from 0 to 500k Parameters \u00b6 classification_function ( int ) \u2013 defaults to 0 The classification function to use for the generation. Valid values are from 0 to 9. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False If True, the class distribution will converge to a uniform distribution. perturbation ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. Each new sample will be perturbed by the magnitude of perturbation . Valid values are in the range [0.0 to 1.0]. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Agrawal ( ... classification_function = 0 , ... seed = 42 ... ) >>> dataset Synthetic data generator < BLANKLINE > Name Agrawal Task Binary classification Samples \u221e Features 9 Outputs 1 Classes 2 Sparse False < BLANKLINE > Configuration ------------- classification_function 0 seed 42 balance_classes False perturbation 0.0 >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 68690.2154 , 81303.5729 , 62 , 4 , 6 , 2 , 419982.4410 , 11 , 433088.0728 ] 1 [ 98144.9515 , 0 , 43 , 2 , 1 , 7 , 266488.5281 , 6 , 389.3829 ] 0 [ 148987.502 , 0 , 52 , 3 , 11 , 8 , 79122.9140 , 27 , 199930.4858 ] 0 [ 26066.5362 , 83031.6639 , 34 , 2 , 11 , 6 , 444969.2657 , 25 , 23225.2063 ] 1 [ 98980.8307 , 0 , 40 , 0 , 6 , 1 , 1159108.4298 , 28 , 281644.1089 ] 0 Methods \u00b6 generate_drift Generate drift by switching the classification function randomly. take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 The sample generation works as follows: The 9 features are generated with the random generator, initialized with the seed passed by the user. Then, the classification function decides, as a function of all the attributes, whether to classify the instance as class 0 or class 1. The next step is to verify if the classes should be balanced, and if so, balance the classes. Finally, add noise if perturbation > 0.0. References \u00b6 Rakesh Agrawal, Tomasz Imielinksi, and Arun Swami. \"Database Mining: A Performance Perspective\", IEEE Transactions on Knowledge and Data Engineering, 5(6), December 1993. \u21a9 \u21a9","title":"Agrawal"},{"location":"api/synth/Agrawal/#agrawal","text":"Agrawal stream generator. The generator was introduced by Agrawal et al. 1 , and was a common source of data for early work on scaling up decision tree learners. The generator produces a stream containing nine features, six numeric and three categorical. There are 10 functions defined for generating binary class labels from the features. Presumably these determine whether the loan should be approved. Classification functions are listed in the original paper 1 . Feature | Description | Values salary | salary | uniformly distributed from 20k to 150k commission | commission | 0 if salary < 75k else uniformly distributed from 10k to 75k age | age | uniformly distributed from 20 to 80 elevel | education level | uniformly chosen from 0 to 4 car | car maker | uniformly chosen from 1 to 20 zipcode | zip code of the town | uniformly chosen from 0 to 8 hvalue | house value | uniformly distributed from 50k x zipcode to 100k x zipcode hyears | years house owned | uniformly distributed from 1 to 30 loan | total loan amount | uniformly distributed from 0 to 500k","title":"Agrawal"},{"location":"api/synth/Agrawal/#parameters","text":"classification_function ( int ) \u2013 defaults to 0 The classification function to use for the generation. Valid values are from 0 to 9. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False If True, the class distribution will converge to a uniform distribution. perturbation ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. Each new sample will be perturbed by the magnitude of perturbation . Valid values are in the range [0.0 to 1.0].","title":"Parameters"},{"location":"api/synth/Agrawal/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Agrawal/#examples","text":">>> from river import synth >>> dataset = synth . Agrawal ( ... classification_function = 0 , ... seed = 42 ... ) >>> dataset Synthetic data generator < BLANKLINE > Name Agrawal Task Binary classification Samples \u221e Features 9 Outputs 1 Classes 2 Sparse False < BLANKLINE > Configuration ------------- classification_function 0 seed 42 balance_classes False perturbation 0.0 >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 68690.2154 , 81303.5729 , 62 , 4 , 6 , 2 , 419982.4410 , 11 , 433088.0728 ] 1 [ 98144.9515 , 0 , 43 , 2 , 1 , 7 , 266488.5281 , 6 , 389.3829 ] 0 [ 148987.502 , 0 , 52 , 3 , 11 , 8 , 79122.9140 , 27 , 199930.4858 ] 0 [ 26066.5362 , 83031.6639 , 34 , 2 , 11 , 6 , 444969.2657 , 25 , 23225.2063 ] 1 [ 98980.8307 , 0 , 40 , 0 , 6 , 1 , 1159108.4298 , 28 , 281644.1089 ] 0","title":"Examples"},{"location":"api/synth/Agrawal/#methods","text":"generate_drift Generate drift by switching the classification function randomly. take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Agrawal/#notes","text":"The sample generation works as follows: The 9 features are generated with the random generator, initialized with the seed passed by the user. Then, the classification function decides, as a function of all the attributes, whether to classify the instance as class 0 or class 1. The next step is to verify if the classes should be balanced, and if so, balance the classes. Finally, add noise if perturbation > 0.0.","title":"Notes"},{"location":"api/synth/Agrawal/#references","text":"Rakesh Agrawal, Tomasz Imielinksi, and Arun Swami. \"Database Mining: A Performance Perspective\", IEEE Transactions on Knowledge and Data Engineering, 5(6), December 1993. \u21a9 \u21a9","title":"References"},{"location":"api/synth/AnomalySine/","text":"AnomalySine \u00b6 Simulate a stream with anomalies in sine waves The data generated corresponds to sine ( attribute 1 ) and cosine ( attribute 2 ) functions. Anomalies are induced by replacing values from attribute 2 with values from a sine function different to the one used in attribute 1 . The contextual flag can be used to introduce contextual anomalies which are values in the normal global range, but abnormal compared to the seasonal pattern. Contextual attributes are introduced by replacing values in attribute 2 with values from attribute 1 . Parameters \u00b6 n_samples ( int ) \u2013 defaults to 10000 Number of samples n_anomalies ( int ) \u2013 defaults to 2500 Number of anomalies. Can't be larger than n_samples . contextual ( bool ) \u2013 defaults to False If True, will add contextual anomalies n_contextual ( int ) \u2013 defaults to 2500 Number of contextual anomalies. Can't be larger than n_samples . shift ( int ) \u2013 defaults to 4 Shift in number of samples applied when retrieving contextual anomalies noise ( float ) \u2013 defaults to 0.5 Amount of noise replace ( bool ) \u2013 defaults to True If True, anomalies are randomly sampled with replacement seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . AnomalySine ( seed = 12345 , ... n_samples = 100 , ... n_anomalies = 25 , ... contextual = True , ... n_contextual = 10 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'sine' : - 0.1023 , 'cosine' : 0.2171 } 0.0 { 'sine' : 0.4868 , 'cosine' : 0.6876 } 0.0 { 'sine' : 0.2197 , 'cosine' : 0.8612 } 0.0 { 'sine' : 0.4037 , 'cosine' : 0.2671 } 0.0 { 'sine' : 1.8243 , 'cosine' : 1.8268 } 1.0 Methods \u00b6 take Iterate over the k samples. Parameters k ( int )","title":"AnomalySine"},{"location":"api/synth/AnomalySine/#anomalysine","text":"Simulate a stream with anomalies in sine waves The data generated corresponds to sine ( attribute 1 ) and cosine ( attribute 2 ) functions. Anomalies are induced by replacing values from attribute 2 with values from a sine function different to the one used in attribute 1 . The contextual flag can be used to introduce contextual anomalies which are values in the normal global range, but abnormal compared to the seasonal pattern. Contextual attributes are introduced by replacing values in attribute 2 with values from attribute 1 .","title":"AnomalySine"},{"location":"api/synth/AnomalySine/#parameters","text":"n_samples ( int ) \u2013 defaults to 10000 Number of samples n_anomalies ( int ) \u2013 defaults to 2500 Number of anomalies. Can't be larger than n_samples . contextual ( bool ) \u2013 defaults to False If True, will add contextual anomalies n_contextual ( int ) \u2013 defaults to 2500 Number of contextual anomalies. Can't be larger than n_samples . shift ( int ) \u2013 defaults to 4 Shift in number of samples applied when retrieving contextual anomalies noise ( float ) \u2013 defaults to 0.5 Amount of noise replace ( bool ) \u2013 defaults to True If True, anomalies are randomly sampled with replacement seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random .","title":"Parameters"},{"location":"api/synth/AnomalySine/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/AnomalySine/#examples","text":">>> from river import synth >>> dataset = synth . AnomalySine ( seed = 12345 , ... n_samples = 100 , ... n_anomalies = 25 , ... contextual = True , ... n_contextual = 10 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'sine' : - 0.1023 , 'cosine' : 0.2171 } 0.0 { 'sine' : 0.4868 , 'cosine' : 0.6876 } 0.0 { 'sine' : 0.2197 , 'cosine' : 0.8612 } 0.0 { 'sine' : 0.4037 , 'cosine' : 0.2671 } 0.0 { 'sine' : 1.8243 , 'cosine' : 1.8268 } 1.0","title":"Examples"},{"location":"api/synth/AnomalySine/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/ConceptDriftStream/","text":"ConceptDriftStream \u00b6 Generates a stream with concept drift. A stream generator that adds concept drift or change by joining two streams. This is done by building a weighted combination of two pure distributions that characterizes the target concepts before and after the change. The sigmoid function is an elegant and practical solution to define the probability that each new instance of the stream belongs to the new concept after the drift. The sigmoid function introduces a gradual, smooth transition whose duration is controlled with two parameters: \\(p\\) , the position of the change. \\(w\\) , the width of the transition. The sigmoid function at sample \\(t\\) is \\[f(t) = 1/(1+e^{-4(t-p)/w})\\] Parameters \u00b6 stream ( river.datasets.base.SyntheticDataset ) \u2013 defaults to None Original stream drift_stream ( river.datasets.base.SyntheticDataset ) \u2013 defaults to None Drift stream position ( int ) \u2013 defaults to 5000 Central position of the concept drift change. width ( int ) \u2013 defaults to 1000 Width of concept drift change. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . alpha ( float ) \u2013 defaults to None Angle of change used to estimate the width of concept drift change. If set, it will override the width parameter. Valid values are in the range (0.0, 90.0]. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . ConceptDriftStream ( stream = synth . SEA ( seed = 42 , variant = 0 ), ... drift_stream = synth . SEA ( seed = 42 , variant = 1 ), ... seed = 1 , position = 5 , width = 2 ) >>> for x , y in dataset . take ( 10 ): ... print ( x , y ) { 0 : 6.3942 , 1 : 0.2501 , 2 : 2.7502 } False { 0 : 2.2321 , 1 : 7.3647 , 2 : 6.7669 } True { 0 : 6.3942 , 1 : 0.2501 , 2 : 2.7502 } False { 0 : 8.9217 , 1 : 0.8693 , 2 : 4.2192 } True { 0 : 2.2321 , 1 : 7.3647 , 2 : 6.7669 } True { 0 : 8.9217 , 1 : 0.8693 , 2 : 4.2192 } True { 0 : 0.2979 , 1 : 2.1863 , 2 : 5.0535 } False { 0 : 0.2653 , 1 : 1.9883 , 2 : 6.4988 } False { 0 : 5.4494 , 1 : 2.2044 , 2 : 5.8926 } False { 0 : 8.0943 , 1 : 0.0649 , 2 : 8.0581 } False Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 An optional way to estimate the width of the transition \\(w\\) is based on the angle \\(\u0007lpha\\) , \\(w = 1/ tan(\u0007lpha)\\) . Since width corresponds to the number of samples for the transition, the width is rounded to the nearest smaller integer. Notice that larger values of \\(\u0007lpha\\) result in smaller widths. For \\(\u0007lpha > 45.0\\) , the width is smaller than 1 so values are rounded to 1 to avoid division by zero errors.","title":"ConceptDriftStream"},{"location":"api/synth/ConceptDriftStream/#conceptdriftstream","text":"Generates a stream with concept drift. A stream generator that adds concept drift or change by joining two streams. This is done by building a weighted combination of two pure distributions that characterizes the target concepts before and after the change. The sigmoid function is an elegant and practical solution to define the probability that each new instance of the stream belongs to the new concept after the drift. The sigmoid function introduces a gradual, smooth transition whose duration is controlled with two parameters: \\(p\\) , the position of the change. \\(w\\) , the width of the transition. The sigmoid function at sample \\(t\\) is \\[f(t) = 1/(1+e^{-4(t-p)/w})\\]","title":"ConceptDriftStream"},{"location":"api/synth/ConceptDriftStream/#parameters","text":"stream ( river.datasets.base.SyntheticDataset ) \u2013 defaults to None Original stream drift_stream ( river.datasets.base.SyntheticDataset ) \u2013 defaults to None Drift stream position ( int ) \u2013 defaults to 5000 Central position of the concept drift change. width ( int ) \u2013 defaults to 1000 Width of concept drift change. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . alpha ( float ) \u2013 defaults to None Angle of change used to estimate the width of concept drift change. If set, it will override the width parameter. Valid values are in the range (0.0, 90.0].","title":"Parameters"},{"location":"api/synth/ConceptDriftStream/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/ConceptDriftStream/#examples","text":">>> from river import synth >>> dataset = synth . ConceptDriftStream ( stream = synth . SEA ( seed = 42 , variant = 0 ), ... drift_stream = synth . SEA ( seed = 42 , variant = 1 ), ... seed = 1 , position = 5 , width = 2 ) >>> for x , y in dataset . take ( 10 ): ... print ( x , y ) { 0 : 6.3942 , 1 : 0.2501 , 2 : 2.7502 } False { 0 : 2.2321 , 1 : 7.3647 , 2 : 6.7669 } True { 0 : 6.3942 , 1 : 0.2501 , 2 : 2.7502 } False { 0 : 8.9217 , 1 : 0.8693 , 2 : 4.2192 } True { 0 : 2.2321 , 1 : 7.3647 , 2 : 6.7669 } True { 0 : 8.9217 , 1 : 0.8693 , 2 : 4.2192 } True { 0 : 0.2979 , 1 : 2.1863 , 2 : 5.0535 } False { 0 : 0.2653 , 1 : 1.9883 , 2 : 6.4988 } False { 0 : 5.4494 , 1 : 2.2044 , 2 : 5.8926 } False { 0 : 8.0943 , 1 : 0.0649 , 2 : 8.0581 } False","title":"Examples"},{"location":"api/synth/ConceptDriftStream/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/ConceptDriftStream/#notes","text":"An optional way to estimate the width of the transition \\(w\\) is based on the angle \\(\u0007lpha\\) , \\(w = 1/ tan(\u0007lpha)\\) . Since width corresponds to the number of samples for the transition, the width is rounded to the nearest smaller integer. Notice that larger values of \\(\u0007lpha\\) result in smaller widths. For \\(\u0007lpha > 45.0\\) , the width is smaller than 1 so values are rounded to 1 to avoid division by zero errors.","title":"Notes"},{"location":"api/synth/Friedman/","text":"Friedman \u00b6 Friedman synthetic dataset. Each observation is composed of 10 features. Each feature value is sampled uniformly in [0, 1]. The target is defined by the following function: \\[y = 10 sin(\\pi x_0 x_1) + 20 (x_2 - 0.5)^2 + 10 x_3 + 5 x_4 + \\epsilon\\] In the last expression, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , is the noise. Therefore, only the first 5 features are relevant. Parameters \u00b6 seed ( int ) \u2013 defaults to None Random seed number used for reproducibility. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Friedman ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 7.04 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 8.90 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Friedman, J.H., 1991. Multivariate adaptive regression splines. The annals of statistics, pp.1-67. \u21a9","title":"Friedman"},{"location":"api/synth/Friedman/#friedman","text":"Friedman synthetic dataset. Each observation is composed of 10 features. Each feature value is sampled uniformly in [0, 1]. The target is defined by the following function: \\[y = 10 sin(\\pi x_0 x_1) + 20 (x_2 - 0.5)^2 + 10 x_3 + 5 x_4 + \\epsilon\\] In the last expression, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , is the noise. Therefore, only the first 5 features are relevant.","title":"Friedman"},{"location":"api/synth/Friedman/#parameters","text":"seed ( int ) \u2013 defaults to None Random seed number used for reproducibility.","title":"Parameters"},{"location":"api/synth/Friedman/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Friedman/#examples","text":">>> from river import synth >>> dataset = synth . Friedman ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 7.04 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 8.90","title":"Examples"},{"location":"api/synth/Friedman/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Friedman/#references","text":"Friedman, J.H., 1991. Multivariate adaptive regression splines. The annals of statistics, pp.1-67. \u21a9","title":"References"},{"location":"api/synth/FriedmanDrift/","text":"FriedmanDrift \u00b6 Friedman synthetic dataset with concept drifts. Each observation is composed of 10 features. Each feature value is sampled uniformly in [0, 1]. Only the first 5 features are relevant. The target is defined by different functions depending on the type of the drift. The three available modes of operation of the data generator are described in 1 . Parameters \u00b6 drift_type ( str ) \u2013 defaults to lea The variant of concept drift. - 'lea' : Local Expanding Abrupt drift. The concept drift appears in two distinct regions of the instance space, while the remaining regions are left unaltered. There are three points of abrupt change in the training dataset. At every consecutive change the regions of drift are expanded. - 'gra' : Global Recurring Abrupt drift. The concept drift appears over the whole instance space. There are two points of concept drift. At the second point of drift the old concept reoccurs. - 'gsg' : Global and Slow Gradual drift. The concept drift affects all the instance space. However, the change is gradual and not abrupt. After each one of the two change points covered by this variant, and during a window of length transition_window , examples from both old and the new concepts are generated with equal probability. After the transition period, only the examples from the new concept are generated. position ( Tuple[int, ...] ) \u2013 defaults to (50000, 100000, 150000) The amount of monitored instances after which each concept drift occurs. A tuple with at least two element must be passed, where each number is greater than the preceding one. If drift_type='lea' , then the tuple must have three elements. transition_window ( int ) \u2013 defaults to 10000 The length of the transition window between two concepts. Only applicable when drift_type='gsg' . If set to zero, the drifts will be abrupt. Anytime transition_window > 0 , it defines a window in which instances of the new concept are gradually introduced among the examples from the old concept. During this transition phase, both old and new concepts appear with equal probability. seed ( int ) \u2013 defaults to None Random seed number used for reproducibility. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . FriedmanDrift ( ... drift_type = 'lea' , ... position = ( 1 , 2 , 3 ), ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 7.04 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] - 2.65 >>> dataset = synth . FriedmanDrift ( ... drift_type = 'gra' , ... position = ( 2 , 3 ), ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 8.96 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 8.90 >>> dataset = synth . FriedmanDrift ( ... drift_type = 'gsg' , ... position = ( 1 , 4 ), ... transition_window = 2 , ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 8.92 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 17.32 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 6.05 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Ikonomovska, E., Gama, J. and D\u017eeroski, S., 2011. Learning model trees from evolving data streams. Data mining and knowledge discovery, 23(1), pp.128-168. \u21a9","title":"FriedmanDrift"},{"location":"api/synth/FriedmanDrift/#friedmandrift","text":"Friedman synthetic dataset with concept drifts. Each observation is composed of 10 features. Each feature value is sampled uniformly in [0, 1]. Only the first 5 features are relevant. The target is defined by different functions depending on the type of the drift. The three available modes of operation of the data generator are described in 1 .","title":"FriedmanDrift"},{"location":"api/synth/FriedmanDrift/#parameters","text":"drift_type ( str ) \u2013 defaults to lea The variant of concept drift. - 'lea' : Local Expanding Abrupt drift. The concept drift appears in two distinct regions of the instance space, while the remaining regions are left unaltered. There are three points of abrupt change in the training dataset. At every consecutive change the regions of drift are expanded. - 'gra' : Global Recurring Abrupt drift. The concept drift appears over the whole instance space. There are two points of concept drift. At the second point of drift the old concept reoccurs. - 'gsg' : Global and Slow Gradual drift. The concept drift affects all the instance space. However, the change is gradual and not abrupt. After each one of the two change points covered by this variant, and during a window of length transition_window , examples from both old and the new concepts are generated with equal probability. After the transition period, only the examples from the new concept are generated. position ( Tuple[int, ...] ) \u2013 defaults to (50000, 100000, 150000) The amount of monitored instances after which each concept drift occurs. A tuple with at least two element must be passed, where each number is greater than the preceding one. If drift_type='lea' , then the tuple must have three elements. transition_window ( int ) \u2013 defaults to 10000 The length of the transition window between two concepts. Only applicable when drift_type='gsg' . If set to zero, the drifts will be abrupt. Anytime transition_window > 0 , it defines a window in which instances of the new concept are gradually introduced among the examples from the old concept. During this transition phase, both old and new concepts appear with equal probability. seed ( int ) \u2013 defaults to None Random seed number used for reproducibility.","title":"Parameters"},{"location":"api/synth/FriedmanDrift/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/FriedmanDrift/#examples","text":">>> from river import synth >>> dataset = synth . FriedmanDrift ( ... drift_type = 'lea' , ... position = ( 1 , 2 , 3 ), ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 7.04 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] - 2.65 >>> dataset = synth . FriedmanDrift ( ... drift_type = 'gra' , ... position = ( 2 , 3 ), ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 8.96 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 18.16 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 8.90 >>> dataset = synth . FriedmanDrift ( ... drift_type = 'gsg' , ... position = ( 1 , 4 ), ... transition_window = 2 , ... seed = 42 ... ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.63 , 0.02 , 0.27 , 0.22 , 0.73 , 0.67 , 0.89 , 0.08 , 0.42 , 0.02 ] 7.66 [ 0.02 , 0.19 , 0.64 , 0.54 , 0.22 , 0.58 , 0.80 , 0.00 , 0.80 , 0.69 ] 8.33 [ 0.34 , 0.15 , 0.95 , 0.33 , 0.09 , 0.09 , 0.84 , 0.60 , 0.80 , 0.72 ] 8.92 [ 0.37 , 0.55 , 0.82 , 0.61 , 0.86 , 0.57 , 0.70 , 0.04 , 0.22 , 0.28 ] 17.32 [ 0.07 , 0.23 , 0.10 , 0.27 , 0.63 , 0.36 , 0.37 , 0.20 , 0.26 , 0.93 ] 6.05","title":"Examples"},{"location":"api/synth/FriedmanDrift/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/FriedmanDrift/#references","text":"Ikonomovska, E., Gama, J. and D\u017eeroski, S., 2011. Learning model trees from evolving data streams. Data mining and knowledge discovery, 23(1), pp.128-168. \u21a9","title":"References"},{"location":"api/synth/Hyperplane/","text":"Hyperplane \u00b6 Hyperplane stream generator. Generates a problem of prediction class of a rotation hyperplane. It was used as testbed for CVFDT and VFDT in 1 . A hyperplane in d-dimensional space is the set of points \\(x\\) that satisfy \\[\\sum^{d}_{i=1} w_i x_i = w_0 = \\sum^{d}_{i=1} w_i\\] where \\(x_i\\) is the i-th coordinate of \\(x\\) . Examples for which \\(\\sum^{d}_{i=1} w_i x_i > w_0\\) , are labeled positive. Examples for which \\(\\sum^{d}_{i=1} w_i x_i \\leq w_0\\) , are labeled negative. Hyperplanes are useful for simulating time-changing concepts because we can change the orientation and position of the hyperplane in a smooth manner by changing the relative size of the weights. We introduce change to this dataset by adding drift to each weighted feature \\(w_i = w_i + d \\sigma\\) , where \\(\\sigma\\) is the probability that the direction of change is reversed and \\(d\\) is the change applied to each example. Parameters \u00b6 seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_features ( int ) \u2013 defaults to 10 The number of attributes to generate. Higher than 2. n_drift_features ( int ) \u2013 defaults to 2 The number of attributes with drift. Higher than 2. mag_change ( float ) \u2013 defaults to 0.0 Magnitude of the change for every example. From 0.0 to 1.0. noise_percentage ( float ) \u2013 defaults to 0.05 Percentage of noise to add to the data. From 0.0 to 1.0. sigma ( float ) \u2013 defaults to 0.1 Probability that the direction of change is reversed. From 0.0 to 1.0. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Hyperplane ( seed = 42 , n_features = 2 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.7319 , 1 : 0.5986 } 1 { 0 : 0.8661 , 1 : 0.6011 } 1 { 0 : 0.8324 , 1 : 0.2123 } 0 { 0 : 0.5247 , 1 : 0.4319 } 0 { 0 : 0.2921 , 1 : 0.3663 } 0 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 The sample generation works as follows: The features are generated with the random number generator, initialized with the seed passed by the user. Then the classification function decides, as a function of the sum of the weighted features and the sum of the weights, whether the instance belongs to class 0 or class 1. The last step is to add noise and generate drift. References \u00b6 G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In KDD\u201901, pages 97\u2013106, San Francisco, CA, 2001. ACM Press. \u21a9","title":"Hyperplane"},{"location":"api/synth/Hyperplane/#hyperplane","text":"Hyperplane stream generator. Generates a problem of prediction class of a rotation hyperplane. It was used as testbed for CVFDT and VFDT in 1 . A hyperplane in d-dimensional space is the set of points \\(x\\) that satisfy \\[\\sum^{d}_{i=1} w_i x_i = w_0 = \\sum^{d}_{i=1} w_i\\] where \\(x_i\\) is the i-th coordinate of \\(x\\) . Examples for which \\(\\sum^{d}_{i=1} w_i x_i > w_0\\) , are labeled positive. Examples for which \\(\\sum^{d}_{i=1} w_i x_i \\leq w_0\\) , are labeled negative. Hyperplanes are useful for simulating time-changing concepts because we can change the orientation and position of the hyperplane in a smooth manner by changing the relative size of the weights. We introduce change to this dataset by adding drift to each weighted feature \\(w_i = w_i + d \\sigma\\) , where \\(\\sigma\\) is the probability that the direction of change is reversed and \\(d\\) is the change applied to each example.","title":"Hyperplane"},{"location":"api/synth/Hyperplane/#parameters","text":"seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_features ( int ) \u2013 defaults to 10 The number of attributes to generate. Higher than 2. n_drift_features ( int ) \u2013 defaults to 2 The number of attributes with drift. Higher than 2. mag_change ( float ) \u2013 defaults to 0.0 Magnitude of the change for every example. From 0.0 to 1.0. noise_percentage ( float ) \u2013 defaults to 0.05 Percentage of noise to add to the data. From 0.0 to 1.0. sigma ( float ) \u2013 defaults to 0.1 Probability that the direction of change is reversed. From 0.0 to 1.0.","title":"Parameters"},{"location":"api/synth/Hyperplane/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Hyperplane/#examples","text":">>> from river import synth >>> dataset = synth . Hyperplane ( seed = 42 , n_features = 2 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.7319 , 1 : 0.5986 } 1 { 0 : 0.8661 , 1 : 0.6011 } 1 { 0 : 0.8324 , 1 : 0.2123 } 0 { 0 : 0.5247 , 1 : 0.4319 } 0 { 0 : 0.2921 , 1 : 0.3663 } 0","title":"Examples"},{"location":"api/synth/Hyperplane/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Hyperplane/#notes","text":"The sample generation works as follows: The features are generated with the random number generator, initialized with the seed passed by the user. Then the classification function decides, as a function of the sum of the weighted features and the sum of the weights, whether the instance belongs to class 0 or class 1. The last step is to add noise and generate drift.","title":"Notes"},{"location":"api/synth/Hyperplane/#references","text":"G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In KDD\u201901, pages 97\u2013106, San Francisco, CA, 2001. ACM Press. \u21a9","title":"References"},{"location":"api/synth/LED/","text":"LED \u00b6 LED stream generator. This data source originates from the CART book 1 . An implementation in C was donated to the UCI 2 machine learning repository by David Aha. The goal is to predict the digit displayed on a seven-segment LED display, where each attribute has a 10% chance of being inverted. It has an optimal Bayes classification rate of 74%. The particular configuration of the generator used for experiments (LED) produces 24 binary attributes, 17 of which are irrelevant. Parameters \u00b6 seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . noise_percentage ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. At each new sample generated, a random number is generated, and if it is equal or less than the noise_percentage, the led value will be switched irrelevant_features ( bool ) \u2013 defaults to False Adds 17 non-relevant attributes to the stream. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . LED ( seed = 112 , noise_percentage = 0.28 , irrelevant_features = False ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0 , 1 : 1 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 0 } 4 { 0 : 0 , 1 : 1 , 2 : 0 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 0 } 4 { 0 : 1 , 1 : 0 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 1 } 3 { 0 : 0 , 1 : 1 , 2 : 1 , 3 : 0 , 4 : 0 , 5 : 1 , 6 : 1 } 0 { 0 : 1 , 1 : 1 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 1 , 6 : 0 } 4 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 An instance is generated based on the parameters passed. If has_noise is set then the total number of attributes will be 24, otherwise there will be 7 attributes. References \u00b6 Leo Breiman, Jerome Friedman, R. Olshen, and Charles J. Stone. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA,1984. \u21a9 A. Asuncion and D. J. Newman. UCI Machine Learning Repository [http://www.ics.uci.edu/\u223cmlearn/mlrepository.html]. University of California, Irvine, School of Information and Computer Sciences,2007. \u21a9","title":"LED"},{"location":"api/synth/LED/#led","text":"LED stream generator. This data source originates from the CART book 1 . An implementation in C was donated to the UCI 2 machine learning repository by David Aha. The goal is to predict the digit displayed on a seven-segment LED display, where each attribute has a 10% chance of being inverted. It has an optimal Bayes classification rate of 74%. The particular configuration of the generator used for experiments (LED) produces 24 binary attributes, 17 of which are irrelevant.","title":"LED"},{"location":"api/synth/LED/#parameters","text":"seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . noise_percentage ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. At each new sample generated, a random number is generated, and if it is equal or less than the noise_percentage, the led value will be switched irrelevant_features ( bool ) \u2013 defaults to False Adds 17 non-relevant attributes to the stream.","title":"Parameters"},{"location":"api/synth/LED/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/LED/#examples","text":">>> from river import synth >>> dataset = synth . LED ( seed = 112 , noise_percentage = 0.28 , irrelevant_features = False ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0 , 1 : 1 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 0 } 4 { 0 : 0 , 1 : 1 , 2 : 0 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 0 } 4 { 0 : 1 , 1 : 0 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 0 , 6 : 1 } 3 { 0 : 0 , 1 : 1 , 2 : 1 , 3 : 0 , 4 : 0 , 5 : 1 , 6 : 1 } 0 { 0 : 1 , 1 : 1 , 2 : 1 , 3 : 1 , 4 : 0 , 5 : 1 , 6 : 0 } 4","title":"Examples"},{"location":"api/synth/LED/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/LED/#notes","text":"An instance is generated based on the parameters passed. If has_noise is set then the total number of attributes will be 24, otherwise there will be 7 attributes.","title":"Notes"},{"location":"api/synth/LED/#references","text":"Leo Breiman, Jerome Friedman, R. Olshen, and Charles J. Stone. Classification and Regression Trees. Wadsworth and Brooks, Monterey, CA,1984. \u21a9 A. Asuncion and D. J. Newman. UCI Machine Learning Repository [http://www.ics.uci.edu/\u223cmlearn/mlrepository.html]. University of California, Irvine, School of Information and Computer Sciences,2007. \u21a9","title":"References"},{"location":"api/synth/LEDDrift/","text":"LEDDrift \u00b6 LED stream generator with concept drift. This class is an extension of the LED generator whose purpose is to add concept drift to the stream. Parameters \u00b6 seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . noise_percentage ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. At each new sample generated, a random number is generated, and if it is equal or less than the noise_percentage, the led value will be switched irrelevant_features ( bool ) \u2013 defaults to False Adds 17 non-relevant attributes to the stream. n_drift_features ( int ) \u2013 defaults to 0 The number of attributes that have drift. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . LEDDrift ( seed = 112 , noise_percentage = 0.28 , ... irrelevant_features = True , n_drift_features = 4 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 1 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ] 8 [ 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] 5 [ 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 1 ] 8 [ 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 1 , 0 ] 3 [ 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 ] 5 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 An instance is generated based on the parameters passed. If has_noise is set then the total number of attributes will be 24, otherwise there will be 7 attributes.","title":"LEDDrift"},{"location":"api/synth/LEDDrift/#leddrift","text":"LED stream generator with concept drift. This class is an extension of the LED generator whose purpose is to add concept drift to the stream.","title":"LEDDrift"},{"location":"api/synth/LEDDrift/#parameters","text":"seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . noise_percentage ( float ) \u2013 defaults to 0.0 The probability that noise will happen in the generation. At each new sample generated, a random number is generated, and if it is equal or less than the noise_percentage, the led value will be switched irrelevant_features ( bool ) \u2013 defaults to False Adds 17 non-relevant attributes to the stream. n_drift_features ( int ) \u2013 defaults to 0 The number of attributes that have drift.","title":"Parameters"},{"location":"api/synth/LEDDrift/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/LEDDrift/#examples","text":">>> from river import synth >>> dataset = synth . LEDDrift ( seed = 112 , noise_percentage = 0.28 , ... irrelevant_features = True , n_drift_features = 4 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 1 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ] 8 [ 0 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 ] 5 [ 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 1 , 0 , 1 ] 8 [ 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 1 , 0 ] 3 [ 0 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 ] 5","title":"Examples"},{"location":"api/synth/LEDDrift/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/LEDDrift/#notes","text":"An instance is generated based on the parameters passed. If has_noise is set then the total number of attributes will be 24, otherwise there will be 7 attributes.","title":"Notes"},{"location":"api/synth/Logical/","text":"Logical \u00b6 Logical functions stream generator. Make a toy dataset with three labels that represent the logical functions: OR , XOR , AND (functions of the 2D input). Data is generated in 'tiles' which contain the complete set of logical operations results. The tiles are repeated n_tiles times. Optionally, the generated data can be shuffled. Parameters \u00b6 n_tiles ( int ) \u2013 defaults to 1 Number of tiles to generate. shuffle ( bool ) \u2013 defaults to True If set, generated data will be shuffled. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Logical ( n_tiles = 2 , shuffle = True , seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'A' : 0 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 } { 'A' : 0 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 } { 'A' : 0 , 'B' : 0 } { 'OR' : 0 , 'XOR' : 0 , 'AND' : 0 } { 'A' : 1 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 0 , 'AND' : 1 } { 'A' : 1 , 'B' : 0 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 } Methods \u00b6 take Iterate over the k samples. Parameters k ( int )","title":"Logical"},{"location":"api/synth/Logical/#logical","text":"Logical functions stream generator. Make a toy dataset with three labels that represent the logical functions: OR , XOR , AND (functions of the 2D input). Data is generated in 'tiles' which contain the complete set of logical operations results. The tiles are repeated n_tiles times. Optionally, the generated data can be shuffled.","title":"Logical"},{"location":"api/synth/Logical/#parameters","text":"n_tiles ( int ) \u2013 defaults to 1 Number of tiles to generate. shuffle ( bool ) \u2013 defaults to True If set, generated data will be shuffled. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random .","title":"Parameters"},{"location":"api/synth/Logical/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Logical/#examples","text":">>> from river import synth >>> dataset = synth . Logical ( n_tiles = 2 , shuffle = True , seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'A' : 0 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 } { 'A' : 0 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 } { 'A' : 0 , 'B' : 0 } { 'OR' : 0 , 'XOR' : 0 , 'AND' : 0 } { 'A' : 1 , 'B' : 1 } { 'OR' : 1 , 'XOR' : 0 , 'AND' : 1 } { 'A' : 1 , 'B' : 0 } { 'OR' : 1 , 'XOR' : 1 , 'AND' : 0 }","title":"Examples"},{"location":"api/synth/Logical/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Mixed/","text":"Mixed \u00b6 Mixed data stream generator. This generator is an implementation of a data stream with abrupt concept drift and boolean noise-free examples as described in 1 . It has four relevant attributes, two boolean attributes \\(v, w\\) and two numeric attributes \\(x, y\\) uniformly distributed from 0 to 1. The examples are labeled depending on the classification function chosen from below. function 0 : if \\(v\\) and \\(w\\) are true or \\(v\\) and \\(z\\) are true or \\(w\\) and \\(z\\) are true then 0 else 1, where \\(z\\) is \\(y < 0.5 + 0.3 sin(3 \\pi x)\\) function 1 : The opposite of function 0 . Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream . Parameters \u00b6 classification_function ( int ) \u2013 defaults to 0 Which of the two classification functions to use for the generation. Valid options are 0 or 1. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to a uniform distribution. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> >>> dataset = synth . Mixed ( seed = 42 , classification_function = 1 , balance_classes = True ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : False , 1 : True , 2 : 0.7319 , 3 : 0.5986 } 1 { 0 : False , 1 : False , 2 : 0.0580 , 3 : 0.8661 } 0 { 0 : True , 1 : True , 2 : 0.0205 , 3 : 0.9699 } 1 { 0 : False , 1 : True , 2 : 0.4319 , 3 : 0.2912 } 0 { 0 : True , 1 : False , 2 : 0.2921 , 3 : 0.3663 } 1 Methods \u00b6 generate_drift Generate drift by switching the classification function. take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 The sample generation works as follows: The two numeric attributes are generated with the random generator initialized with the seed passed by the user (optional). The boolean attributes are either 0 or 1 based on the comparison of the random number generator and 0.5 , the classification function decides whether to classify the instance as class 0 or class 1. The next step is to verify if the classes should be balanced, and if so, balance the classes. The generated sample will have 4 relevant features and 1 label (it is a binary-classification task). References \u00b6 Gama, Joao, et al. \"Learning with drift detection.\" Advances in artificial intelligence\u2013SBIA 2004. Springer Berlin Heidelberg, 2004. 286-295\" \u21a9","title":"Mixed"},{"location":"api/synth/Mixed/#mixed","text":"Mixed data stream generator. This generator is an implementation of a data stream with abrupt concept drift and boolean noise-free examples as described in 1 . It has four relevant attributes, two boolean attributes \\(v, w\\) and two numeric attributes \\(x, y\\) uniformly distributed from 0 to 1. The examples are labeled depending on the classification function chosen from below. function 0 : if \\(v\\) and \\(w\\) are true or \\(v\\) and \\(z\\) are true or \\(w\\) and \\(z\\) are true then 0 else 1, where \\(z\\) is \\(y < 0.5 + 0.3 sin(3 \\pi x)\\) function 1 : The opposite of function 0 . Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream .","title":"Mixed"},{"location":"api/synth/Mixed/#parameters","text":"classification_function ( int ) \u2013 defaults to 0 Which of the two classification functions to use for the generation. Valid options are 0 or 1. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to a uniform distribution.","title":"Parameters"},{"location":"api/synth/Mixed/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Mixed/#examples","text":">>> from river import synth >>> >>> dataset = synth . Mixed ( seed = 42 , classification_function = 1 , balance_classes = True ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : False , 1 : True , 2 : 0.7319 , 3 : 0.5986 } 1 { 0 : False , 1 : False , 2 : 0.0580 , 3 : 0.8661 } 0 { 0 : True , 1 : True , 2 : 0.0205 , 3 : 0.9699 } 1 { 0 : False , 1 : True , 2 : 0.4319 , 3 : 0.2912 } 0 { 0 : True , 1 : False , 2 : 0.2921 , 3 : 0.3663 } 1","title":"Examples"},{"location":"api/synth/Mixed/#methods","text":"generate_drift Generate drift by switching the classification function. take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Mixed/#notes","text":"The sample generation works as follows: The two numeric attributes are generated with the random generator initialized with the seed passed by the user (optional). The boolean attributes are either 0 or 1 based on the comparison of the random number generator and 0.5 , the classification function decides whether to classify the instance as class 0 or class 1. The next step is to verify if the classes should be balanced, and if so, balance the classes. The generated sample will have 4 relevant features and 1 label (it is a binary-classification task).","title":"Notes"},{"location":"api/synth/Mixed/#references","text":"Gama, Joao, et al. \"Learning with drift detection.\" Advances in artificial intelligence\u2013SBIA 2004. Springer Berlin Heidelberg, 2004. 286-295\" \u21a9","title":"References"},{"location":"api/synth/Mv/","text":"Mv \u00b6 Mv artificial dataset. Artificial dataset composed of both nominal and numeric features, whose features present co-dependencies. Originally described in 1 . The features are generated using the following expressions: \\(x_1\\) : uniformly distributed over [-5, 5] . \\(x_2\\) : uniformly distributed over [-15, -10] . \\(x_3\\) : if \\(x_1 > 0\\) , \\(x_3 \\leftarrow\\) 'green' else \\(x_3 \\leftarrow\\) 'red' with probability \\(0.4\\) and \\(x_3 \\leftarrow\\) 'brown' with probability \\(0.6\\) . \\(x_4\\) : if \\(x_3 =\\) 'green' , \\(x_4 \\leftarrow x_1 + 2 x_2\\) else \\(x_4 = \\frac{x_1}{2}\\) with probability \\(0.3\\) and \\(x_4 = \\frac{x_2}{2}\\) with probability \\(0.7\\) . \\(x_5\\) : uniformly distributed over [-1, 1] . \\(x_6 \\leftarrow x_4 \\times \\epsilon\\) , where \\(\\epsilon\\) is uniformly distributed over [0, 5] . \\(x_7\\) : 'yes' with probability \\(0.3\\) , and 'no' with probability \\(0.7\\) . \\(x_8\\) : 'normal' if \\(x_5 < 0.5\\) else 'large' . \\(x_9\\) : uniformly distributed over [100, 500] . \\(x_{10}\\) : uniformly distributed integer over the interval [1000, 1200] . The target value is generated using the following rules: if \\(x_2 > 2\\) , \\(y \\leftarrow 35 - 0.5 x_4\\) else if \\(-2 \\le x_4 \\le 2\\) , \\(y \\leftarrow 10 - 2 x_1\\) else if \\(x_7 =\\) 'yes' , \\(y \\leftarrow 3 - \\frac{x_1}{x_4}\\) else if \\(x_8 =\\) 'normal' , \\(y \\leftarrow x_6 + x_1\\) else \\(y \\leftarrow \\frac{x_1}{2}\\) . Parameters \u00b6 seed ( int ) \u2013 defaults to None Random seed number used for reproducibility. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Mv ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 1.39 , - 14.87 , 'green' , - 28.35 , - 0.44 , - 31.64 , 'no' , 'normal' , 370.67 , 1178.43 ] - 30.25 [ - 4.13 , - 12.89 , 'red' , - 2.06 , 0.01 , - 0.27 , 'yes' , 'normal' , 359.95 , 1108.98 ] 1.00 [ - 2.79 , - 12.05 , 'brown' , - 1.39 , 0.61 , - 4.87 , 'no' , 'large' , 162.19 , 1191.44 ] 15.59 [ - 1.63 , - 14.53 , 'red' , - 7.26 , 0.20 , - 29.33 , 'no' , 'normal' , 314.49 , 1194.62 ] - 30.96 [ - 1.21 , - 12.23 , 'brown' , - 6.11 , 0.72 , - 17.66 , 'no' , 'large' , 118.32 , 1045.57 ] - 0.60 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Mv in Lu\u00eds Torgo regression datasets \u21a9","title":"Mv"},{"location":"api/synth/Mv/#mv","text":"Mv artificial dataset. Artificial dataset composed of both nominal and numeric features, whose features present co-dependencies. Originally described in 1 . The features are generated using the following expressions: \\(x_1\\) : uniformly distributed over [-5, 5] . \\(x_2\\) : uniformly distributed over [-15, -10] . \\(x_3\\) : if \\(x_1 > 0\\) , \\(x_3 \\leftarrow\\) 'green' else \\(x_3 \\leftarrow\\) 'red' with probability \\(0.4\\) and \\(x_3 \\leftarrow\\) 'brown' with probability \\(0.6\\) . \\(x_4\\) : if \\(x_3 =\\) 'green' , \\(x_4 \\leftarrow x_1 + 2 x_2\\) else \\(x_4 = \\frac{x_1}{2}\\) with probability \\(0.3\\) and \\(x_4 = \\frac{x_2}{2}\\) with probability \\(0.7\\) . \\(x_5\\) : uniformly distributed over [-1, 1] . \\(x_6 \\leftarrow x_4 \\times \\epsilon\\) , where \\(\\epsilon\\) is uniformly distributed over [0, 5] . \\(x_7\\) : 'yes' with probability \\(0.3\\) , and 'no' with probability \\(0.7\\) . \\(x_8\\) : 'normal' if \\(x_5 < 0.5\\) else 'large' . \\(x_9\\) : uniformly distributed over [100, 500] . \\(x_{10}\\) : uniformly distributed integer over the interval [1000, 1200] . The target value is generated using the following rules: if \\(x_2 > 2\\) , \\(y \\leftarrow 35 - 0.5 x_4\\) else if \\(-2 \\le x_4 \\le 2\\) , \\(y \\leftarrow 10 - 2 x_1\\) else if \\(x_7 =\\) 'yes' , \\(y \\leftarrow 3 - \\frac{x_1}{x_4}\\) else if \\(x_8 =\\) 'normal' , \\(y \\leftarrow x_6 + x_1\\) else \\(y \\leftarrow \\frac{x_1}{2}\\) .","title":"Mv"},{"location":"api/synth/Mv/#parameters","text":"seed ( int ) \u2013 defaults to None Random seed number used for reproducibility.","title":"Parameters"},{"location":"api/synth/Mv/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Mv/#examples","text":">>> from river import synth >>> dataset = synth . Mv ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 1.39 , - 14.87 , 'green' , - 28.35 , - 0.44 , - 31.64 , 'no' , 'normal' , 370.67 , 1178.43 ] - 30.25 [ - 4.13 , - 12.89 , 'red' , - 2.06 , 0.01 , - 0.27 , 'yes' , 'normal' , 359.95 , 1108.98 ] 1.00 [ - 2.79 , - 12.05 , 'brown' , - 1.39 , 0.61 , - 4.87 , 'no' , 'large' , 162.19 , 1191.44 ] 15.59 [ - 1.63 , - 14.53 , 'red' , - 7.26 , 0.20 , - 29.33 , 'no' , 'normal' , 314.49 , 1194.62 ] - 30.96 [ - 1.21 , - 12.23 , 'brown' , - 6.11 , 0.72 , - 17.66 , 'no' , 'large' , 118.32 , 1045.57 ] - 0.60","title":"Examples"},{"location":"api/synth/Mv/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Mv/#references","text":"Mv in Lu\u00eds Torgo regression datasets \u21a9","title":"References"},{"location":"api/synth/Planes2D/","text":"Planes2D \u00b6 2D Planes synthetic dataset. This dataset is described in 1 and was adapted from 2 . The features are generated using the following probabilities: \\[P(x_1 = -1) = P(x_1 = 1) = \\frac{1}{2}\\] \\[P(x_m = -1) = P(x_m = 0) = P(x_m = 1) = \\frac{1}{3}, m=2,\\ldots, 10\\] The target value is defined by the following rule: \\[\\text{if}~x_1 = 1, y \\leftarrow 3 + 3x_2 + 2x_3 + x_4 + \\epsilon\\] \\[\\text{if}~x_1 = -1, y \\leftarrow -3 + 3x_5 + 2x_6 + x_7 + \\epsilon\\] In the expressions, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , is the noise. Parameters \u00b6 seed ( int ) \u2013 defaults to None Random seed number used for reproducibility. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Planes2D ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ - 1 , - 1 , 1 , 0 , - 1 , - 1 , - 1 , 1 , - 1 , 1 ] - 9.07 [ 1 , - 1 , - 1 , - 1 , - 1 , - 1 , 1 , 1 , - 1 , 1 ] - 4.25 [ - 1 , 1 , 1 , 1 , 1 , 0 , - 1 , 0 , 1 , 0 ] - 0.95 [ - 1 , 1 , 0 , 0 , 0 , - 1 , - 1 , 0 , - 1 , - 1 ] - 6.10 [ 1 , - 1 , 0 , 0 , 1 , 0 , - 1 , 1 , 0 , 1 ] 1.60 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 2DPlanes in Lu\u00eds Torgo regression datasets \u21a9 Breiman, L., Friedman, J., Stone, C.J. and Olshen, R.A., 1984. Classification and regression trees. CRC press. \u21a9","title":"Planes2D"},{"location":"api/synth/Planes2D/#planes2d","text":"2D Planes synthetic dataset. This dataset is described in 1 and was adapted from 2 . The features are generated using the following probabilities: \\[P(x_1 = -1) = P(x_1 = 1) = \\frac{1}{2}\\] \\[P(x_m = -1) = P(x_m = 0) = P(x_m = 1) = \\frac{1}{3}, m=2,\\ldots, 10\\] The target value is defined by the following rule: \\[\\text{if}~x_1 = 1, y \\leftarrow 3 + 3x_2 + 2x_3 + x_4 + \\epsilon\\] \\[\\text{if}~x_1 = -1, y \\leftarrow -3 + 3x_5 + 2x_6 + x_7 + \\epsilon\\] In the expressions, \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , is the noise.","title":"Planes2D"},{"location":"api/synth/Planes2D/#parameters","text":"seed ( int ) \u2013 defaults to None Random seed number used for reproducibility.","title":"Parameters"},{"location":"api/synth/Planes2D/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Planes2D/#examples","text":">>> from river import synth >>> dataset = synth . Planes2D ( seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ - 1 , - 1 , 1 , 0 , - 1 , - 1 , - 1 , 1 , - 1 , 1 ] - 9.07 [ 1 , - 1 , - 1 , - 1 , - 1 , - 1 , 1 , 1 , - 1 , 1 ] - 4.25 [ - 1 , 1 , 1 , 1 , 1 , 0 , - 1 , 0 , 1 , 0 ] - 0.95 [ - 1 , 1 , 0 , 0 , 0 , - 1 , - 1 , 0 , - 1 , - 1 ] - 6.10 [ 1 , - 1 , 0 , 0 , 1 , 0 , - 1 , 1 , 0 , 1 ] 1.60","title":"Examples"},{"location":"api/synth/Planes2D/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Planes2D/#references","text":"2DPlanes in Lu\u00eds Torgo regression datasets \u21a9 Breiman, L., Friedman, J., Stone, C.J. and Olshen, R.A., 1984. Classification and regression trees. CRC press. \u21a9","title":"References"},{"location":"api/synth/RandomRBF/","text":"RandomRBF \u00b6 Random Radial Basis Function generator. Produces a radial basis function stream. A number of centroids, having a random central position, a standard deviation, a class label and weight are generated. A new sample is created by choosing one of the centroids at random, taking into account their weights, and offsetting the attributes in a random direction from the centroid's center. The offset length is drawn from a Gaussian distribution. This process will create a normally distributed hypersphere of samples on the surrounds of each centroid. Parameters \u00b6 seed_model ( int ) \u2013 defaults to None Model's seed to generate centroids If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . seed_sample ( int ) \u2013 defaults to None Sample's seed If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_classes ( int ) \u2013 defaults to 2 The number of class labels to generate. n_features ( int ) \u2013 defaults to 10 The number of numerical features to generate. n_centroids ( int ) \u2013 defaults to 50 The number of centroids to generate. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> >>> dataset = synth . RandomRBF ( seed_model = 42 , seed_sample = 42 , ... n_classes = 4 , n_features = 4 , n_centroids = 20 ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.9518 , 1 : 0.5263 , 2 : 0.2509 , 3 : 0.4177 } 0 { 0 : 0.3383 , 1 : 0.8072 , 2 : 0.8051 , 3 : 0.4140 } 3 { 0 : - 0.2640 , 1 : 0.2275 , 2 : 0.6286 , 3 : - 0.0532 } 2 { 0 : 0.9050 , 1 : 0.6443 , 2 : 0.1270 , 3 : 0.4520 } 2 { 0 : 0.1874 , 1 : 0.4348 , 2 : 0.9819 , 3 : - 0.0459 } 2 Methods \u00b6 take Iterate over the k samples. Parameters k ( int )","title":"RandomRBF"},{"location":"api/synth/RandomRBF/#randomrbf","text":"Random Radial Basis Function generator. Produces a radial basis function stream. A number of centroids, having a random central position, a standard deviation, a class label and weight are generated. A new sample is created by choosing one of the centroids at random, taking into account their weights, and offsetting the attributes in a random direction from the centroid's center. The offset length is drawn from a Gaussian distribution. This process will create a normally distributed hypersphere of samples on the surrounds of each centroid.","title":"RandomRBF"},{"location":"api/synth/RandomRBF/#parameters","text":"seed_model ( int ) \u2013 defaults to None Model's seed to generate centroids If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . seed_sample ( int ) \u2013 defaults to None Sample's seed If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_classes ( int ) \u2013 defaults to 2 The number of class labels to generate. n_features ( int ) \u2013 defaults to 10 The number of numerical features to generate. n_centroids ( int ) \u2013 defaults to 50 The number of centroids to generate.","title":"Parameters"},{"location":"api/synth/RandomRBF/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/RandomRBF/#examples","text":">>> from river import synth >>> >>> dataset = synth . RandomRBF ( seed_model = 42 , seed_sample = 42 , ... n_classes = 4 , n_features = 4 , n_centroids = 20 ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.9518 , 1 : 0.5263 , 2 : 0.2509 , 3 : 0.4177 } 0 { 0 : 0.3383 , 1 : 0.8072 , 2 : 0.8051 , 3 : 0.4140 } 3 { 0 : - 0.2640 , 1 : 0.2275 , 2 : 0.6286 , 3 : - 0.0532 } 2 { 0 : 0.9050 , 1 : 0.6443 , 2 : 0.1270 , 3 : 0.4520 } 2 { 0 : 0.1874 , 1 : 0.4348 , 2 : 0.9819 , 3 : - 0.0459 } 2","title":"Examples"},{"location":"api/synth/RandomRBF/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/RandomRBFDrift/","text":"RandomRBFDrift \u00b6 Random Radial Basis Function generator with concept drift. This class is an extension from the RandomRBF generator. Concept drift can be introduced in instances of this class. The drift is created by adding a \"speed\" to certain centroids. As the samples are generated each of the moving centroids' centers is changed by an amount determined by its speed. Parameters \u00b6 seed_model ( int ) \u2013 defaults to None Model's seed to generate centroids If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . seed_sample ( int ) \u2013 defaults to None Sample's seed If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_classes ( int ) \u2013 defaults to 2 The number of class labels to generate. n_features ( int ) \u2013 defaults to 10 The number of numerical features to generate. n_centroids ( int ) \u2013 defaults to 50 The number of centroids to generate. change_speed ( float ) \u2013 defaults to 0.0 The concept drift speed. n_drift_centroids ( int ) \u2013 defaults to 50 The number of centroids that will drift. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> >>> dataset = synth . RandomRBFDrift ( seed_model = 42 , seed_sample = 42 , ... n_classes = 4 , n_features = 4 , n_centroids = 20 , ... change_speed = 0.87 , n_drift_centroids = 10 ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 1.1965 , 1 : 0.5729 , 2 : 0.8607 , 3 : 0.5888 } 0 { 0 : 0.3383 , 1 : 0.8072 , 2 : 0.8051 , 3 : 0.4140 } 3 { 0 : 0.5362 , 1 : - 0.2867 , 2 : 0.0962 , 3 : 0.8974 } 2 { 0 : 1.1875 , 1 : 1.0385 , 2 : 0.8323 , 3 : - 0.0553 } 2 { 0 : 0.3256 , 1 : 0.9206 , 2 : 0.8595 , 3 : 0.5907 } 2 Methods \u00b6 take Iterate over the k samples. Parameters k ( int )","title":"RandomRBFDrift"},{"location":"api/synth/RandomRBFDrift/#randomrbfdrift","text":"Random Radial Basis Function generator with concept drift. This class is an extension from the RandomRBF generator. Concept drift can be introduced in instances of this class. The drift is created by adding a \"speed\" to certain centroids. As the samples are generated each of the moving centroids' centers is changed by an amount determined by its speed.","title":"RandomRBFDrift"},{"location":"api/synth/RandomRBFDrift/#parameters","text":"seed_model ( int ) \u2013 defaults to None Model's seed to generate centroids If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . seed_sample ( int ) \u2013 defaults to None Sample's seed If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_classes ( int ) \u2013 defaults to 2 The number of class labels to generate. n_features ( int ) \u2013 defaults to 10 The number of numerical features to generate. n_centroids ( int ) \u2013 defaults to 50 The number of centroids to generate. change_speed ( float ) \u2013 defaults to 0.0 The concept drift speed. n_drift_centroids ( int ) \u2013 defaults to 50 The number of centroids that will drift.","title":"Parameters"},{"location":"api/synth/RandomRBFDrift/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/RandomRBFDrift/#examples","text":">>> from river import synth >>> >>> dataset = synth . RandomRBFDrift ( seed_model = 42 , seed_sample = 42 , ... n_classes = 4 , n_features = 4 , n_centroids = 20 , ... change_speed = 0.87 , n_drift_centroids = 10 ) >>> >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 1.1965 , 1 : 0.5729 , 2 : 0.8607 , 3 : 0.5888 } 0 { 0 : 0.3383 , 1 : 0.8072 , 2 : 0.8051 , 3 : 0.4140 } 3 { 0 : 0.5362 , 1 : - 0.2867 , 2 : 0.0962 , 3 : 0.8974 } 2 { 0 : 1.1875 , 1 : 1.0385 , 2 : 0.8323 , 3 : - 0.0553 } 2 { 0 : 0.3256 , 1 : 0.9206 , 2 : 0.8595 , 3 : 0.5907 } 2","title":"Examples"},{"location":"api/synth/RandomRBFDrift/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/RandomTree/","text":"RandomTree \u00b6 Random Tree generator. This generator is based on 1 . The generator creates a random tree by splitting features at random and setting labels at its leaves. The tree structure is composed of node objects, which can be either inner nodes or leaf nodes. The choice comes as a function of the parameters passed to its initializer. Since the concepts are generated and classified according to a tree structure, in theory, it should favor decision tree learners. Parameters \u00b6 seed_tree ( int ) \u2013 defaults to None Seed for random generation of tree. seed_sample ( int ) \u2013 defaults to None Seed for random generation of instances. n_classes ( int ) \u2013 defaults to 2 The number of classes to generate. n_num_features ( int ) \u2013 defaults to 5 The number of numerical features to generate. n_cat_features ( int ) \u2013 defaults to 5 The number of categorical features to generate. n_categories_per_feature ( int ) \u2013 defaults to 5 The number of values to generate per categorical feature. max_tree_depth ( int ) \u2013 defaults to 5 The maximum depth of the tree concept. first_leaf_level ( int ) \u2013 defaults to 3 The first level of the tree above max_tree_depth that can have leaves. fraction_leaves_per_level ( float ) \u2013 defaults to 0.15 The fraction of leaves per level from first_leaf_level onwards. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . RandomTree ( seed_tree = 42 , seed_sample = 42 , n_classes = 2 , ... n_num_features = 2 , n_cat_features = 2 , ... n_categories_per_feature = 2 , max_tree_depth = 6 , ... first_leaf_level = 3 , fraction_leaves_per_level = 0.15 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'x_num_0' : 0.3745 , 'x_num_1' : 0.9507 , 'x_cat_0' : 0 , 'x_cat_1' : 1 } 1 { 'x_num_0' : 0.5986 , 'x_num_1' : 0.1560 , 'x_cat_0' : 0 , 'x_cat_1' : 0 } 1 { 'x_num_0' : 0.0580 , 'x_num_1' : 0.8661 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0 { 'x_num_0' : 0.7080 , 'x_num_1' : 0.0205 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0 { 'x_num_0' : 0.8324 , 'x_num_1' : 0.2123 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 Domingos, Pedro, and Geoff Hulten. \"Mining high-speed data streams.\" In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 71-80. 2000. \u21a9","title":"RandomTree"},{"location":"api/synth/RandomTree/#randomtree","text":"Random Tree generator. This generator is based on 1 . The generator creates a random tree by splitting features at random and setting labels at its leaves. The tree structure is composed of node objects, which can be either inner nodes or leaf nodes. The choice comes as a function of the parameters passed to its initializer. Since the concepts are generated and classified according to a tree structure, in theory, it should favor decision tree learners.","title":"RandomTree"},{"location":"api/synth/RandomTree/#parameters","text":"seed_tree ( int ) \u2013 defaults to None Seed for random generation of tree. seed_sample ( int ) \u2013 defaults to None Seed for random generation of instances. n_classes ( int ) \u2013 defaults to 2 The number of classes to generate. n_num_features ( int ) \u2013 defaults to 5 The number of numerical features to generate. n_cat_features ( int ) \u2013 defaults to 5 The number of categorical features to generate. n_categories_per_feature ( int ) \u2013 defaults to 5 The number of values to generate per categorical feature. max_tree_depth ( int ) \u2013 defaults to 5 The maximum depth of the tree concept. first_leaf_level ( int ) \u2013 defaults to 3 The first level of the tree above max_tree_depth that can have leaves. fraction_leaves_per_level ( float ) \u2013 defaults to 0.15 The fraction of leaves per level from first_leaf_level onwards.","title":"Parameters"},{"location":"api/synth/RandomTree/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/RandomTree/#examples","text":">>> from river import synth >>> dataset = synth . RandomTree ( seed_tree = 42 , seed_sample = 42 , n_classes = 2 , ... n_num_features = 2 , n_cat_features = 2 , ... n_categories_per_feature = 2 , max_tree_depth = 6 , ... first_leaf_level = 3 , fraction_leaves_per_level = 0.15 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'x_num_0' : 0.3745 , 'x_num_1' : 0.9507 , 'x_cat_0' : 0 , 'x_cat_1' : 1 } 1 { 'x_num_0' : 0.5986 , 'x_num_1' : 0.1560 , 'x_cat_0' : 0 , 'x_cat_1' : 0 } 1 { 'x_num_0' : 0.0580 , 'x_num_1' : 0.8661 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0 { 'x_num_0' : 0.7080 , 'x_num_1' : 0.0205 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0 { 'x_num_0' : 0.8324 , 'x_num_1' : 0.2123 , 'x_cat_0' : 1 , 'x_cat_1' : 1 } 0","title":"Examples"},{"location":"api/synth/RandomTree/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/RandomTree/#references","text":"Domingos, Pedro, and Geoff Hulten. \"Mining high-speed data streams.\" In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 71-80. 2000. \u21a9","title":"References"},{"location":"api/synth/SEA/","text":"SEA \u00b6 SEA synthetic dataset. Implementation of the data stream with abrupt drift described in 1 . Each observation is composed of 3 features. Only the first two features are relevant. The target is binary, and is positive if the sum of the features exceeds a certain threshold. There are 4 thresholds to choose from. Concept drift can be introduced by switching the threshold anytime during the stream. Variant 0 : True if \\(att1 + att2 > 8\\) Variant 1 : True if \\(att1 + att2 > 9\\) Variant 2 : True if \\(att1 + att2 > 7\\) Variant 3 : True if \\(att1 + att2 > 9.5\\) Parameters \u00b6 variant \u2013 defaults to 0 Determines the classification function to use. Possible choices are 0, 1, 2, 3. noise \u2013 defaults to 0.0 Determines the amount of observations for which the target sign will be flipped. seed ( int ) \u2013 defaults to None Random seed number used for reproducibility. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . SEA ( variant = 0 , seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 6.39426 , 1 : 0.25010 , 2 : 2.75029 } False { 0 : 2.23210 , 1 : 7.36471 , 2 : 6.76699 } True { 0 : 8.92179 , 1 : 0.86938 , 2 : 4.21921 } True { 0 : 0.29797 , 1 : 2.18637 , 2 : 5.05355 } False { 0 : 0.26535 , 1 : 1.98837 , 2 : 6.49884 } False Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) References \u00b6 A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification \u21a9","title":"SEA"},{"location":"api/synth/SEA/#sea","text":"SEA synthetic dataset. Implementation of the data stream with abrupt drift described in 1 . Each observation is composed of 3 features. Only the first two features are relevant. The target is binary, and is positive if the sum of the features exceeds a certain threshold. There are 4 thresholds to choose from. Concept drift can be introduced by switching the threshold anytime during the stream. Variant 0 : True if \\(att1 + att2 > 8\\) Variant 1 : True if \\(att1 + att2 > 9\\) Variant 2 : True if \\(att1 + att2 > 7\\) Variant 3 : True if \\(att1 + att2 > 9.5\\)","title":"SEA"},{"location":"api/synth/SEA/#parameters","text":"variant \u2013 defaults to 0 Determines the classification function to use. Possible choices are 0, 1, 2, 3. noise \u2013 defaults to 0.0 Determines the amount of observations for which the target sign will be flipped. seed ( int ) \u2013 defaults to None Random seed number used for reproducibility.","title":"Parameters"},{"location":"api/synth/SEA/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/SEA/#examples","text":">>> from river import synth >>> dataset = synth . SEA ( variant = 0 , seed = 42 ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 6.39426 , 1 : 0.25010 , 2 : 2.75029 } False { 0 : 2.23210 , 1 : 7.36471 , 2 : 6.76699 } True { 0 : 8.92179 , 1 : 0.86938 , 2 : 4.21921 } True { 0 : 0.29797 , 1 : 2.18637 , 2 : 5.05355 } False { 0 : 0.26535 , 1 : 1.98837 , 2 : 6.49884 } False","title":"Examples"},{"location":"api/synth/SEA/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/SEA/#references","text":"A Streaming Ensemble Algorithm (SEA) for Large-Scale Classification \u21a9","title":"References"},{"location":"api/synth/STAGGER/","text":"STAGGER \u00b6 STAGGER concepts stream generator. This generator is an implementation of the dara stream with abrupt concept drift, as described in 1 . The STAGGER concepts are boolean functions f with three features describing objects: size (small, medium and large), shape (circle, square and triangle) and colour (red, blue and green). f options: True if the size is small and the color is red. True if the color is green or the shape is a circle. True if the size is medium or large Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream . One important feature is the possibility to balance classes, which means the class distribution will tend to a uniform one. Parameters \u00b6 classification_function ( int ) \u2013 defaults to 0 Classification functions to use. From 0 to 2. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to an uniform distribution. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . STAGGER ( classification_function = 2 , seed = 112 , ... balance_classes = False ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'size' : 0 , 'color' : 0 , 'shape' : 2 } 0 { 'size' : 1 , 'color' : 0 , 'shape' : 1 } 1 { 'size' : 0 , 'color' : 0 , 'shape' : 0 } 0 { 'size' : 1 , 'color' : 2 , 'shape' : 0 } 1 { 'size' : 1 , 'color' : 0 , 'shape' : 2 } 1 Methods \u00b6 generate_drift Generate drift by switching the classification function at random. take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 The sample generation works as follows: The 3 attributes are generated with the random number generator. The classification function defines whether to classify the instance as class 0 or class 1. Finally, data is balanced, if this option is set by the user. References \u00b6 Schlimmer, J. C., & Granger, R. H. (1986). Incremental learning from noisy data. Machine learning, 1(3), 317-354. \u21a9","title":"STAGGER"},{"location":"api/synth/STAGGER/#stagger","text":"STAGGER concepts stream generator. This generator is an implementation of the dara stream with abrupt concept drift, as described in 1 . The STAGGER concepts are boolean functions f with three features describing objects: size (small, medium and large), shape (circle, square and triangle) and colour (red, blue and green). f options: True if the size is small and the color is red. True if the color is green or the shape is a circle. True if the size is medium or large Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream . One important feature is the possibility to balance classes, which means the class distribution will tend to a uniform one.","title":"STAGGER"},{"location":"api/synth/STAGGER/#parameters","text":"classification_function ( int ) \u2013 defaults to 0 Classification functions to use. From 0 to 2. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to an uniform distribution.","title":"Parameters"},{"location":"api/synth/STAGGER/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/STAGGER/#examples","text":">>> from river import synth >>> dataset = synth . STAGGER ( classification_function = 2 , seed = 112 , ... balance_classes = False ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 'size' : 0 , 'color' : 0 , 'shape' : 2 } 0 { 'size' : 1 , 'color' : 0 , 'shape' : 1 } 1 { 'size' : 0 , 'color' : 0 , 'shape' : 0 } 0 { 'size' : 1 , 'color' : 2 , 'shape' : 0 } 1 { 'size' : 1 , 'color' : 0 , 'shape' : 2 } 1","title":"Examples"},{"location":"api/synth/STAGGER/#methods","text":"generate_drift Generate drift by switching the classification function at random. take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/STAGGER/#notes","text":"The sample generation works as follows: The 3 attributes are generated with the random number generator. The classification function defines whether to classify the instance as class 0 or class 1. Finally, data is balanced, if this option is set by the user.","title":"Notes"},{"location":"api/synth/STAGGER/#references","text":"Schlimmer, J. C., & Granger, R. H. (1986). Incremental learning from noisy data. Machine learning, 1(3), 317-354. \u21a9","title":"References"},{"location":"api/synth/Sine/","text":"Sine \u00b6 Sine generator. This generator is an implementation of the dara stream with abrupt concept drift, as described in Gama, Joao, et al. 1 . It generates up to 4 relevant numerical features, that vary from 0 to 1, where only 2 of them are relevant to the classification task and the other 2 are optionally added by as noise. A classification function is chosen among four options: SINE1 . Abrupt concept drift, noise-free examples. It has two relevant attributes. Each attributes has values uniformly distributed in [0, 1]. In the first context all points below the curve \\(y = sin(x)\\) are classified as positive. Reversed SINE1 . The reversed classification of SINE1 . SINE2 . The same two relevant attributes. The classification function is \\(y < 0.5 + 0.3 sin(3 \\pi x)\\) . Reversed SINE2 . The reversed classification of SINE2 . Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream . Two important features are the possibility to balance classes, which means the class distribution will tend to a uniform one, and the possibility to add noise, which will, add two non relevant attributes. Parameters \u00b6 classification_function ( int ) \u2013 defaults to 0 Classification functions to use. From 0 to 3. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to an uniform distribution. has_noise ( bool ) \u2013 defaults to False Adds 2 non relevant features to the stream. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Sine ( classification_function = 2 , seed = 112 , ... balance_classes = False , has_noise = True ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.3750 , 1 : 0.6403 , 2 : 0.9500 , 3 : 0.0756 } 1 { 0 : 0.7769 , 1 : 0.8327 , 2 : 0.0548 , 3 : 0.8176 } 1 { 0 : 0.8853 , 1 : 0.7223 , 2 : 0.0025 , 3 : 0.9811 } 0 { 0 : 0.3434 , 1 : 0.0947 , 2 : 0.3946 , 3 : 0.0049 } 1 { 0 : 0.7367 , 1 : 0.9558 , 2 : 0.8206 , 3 : 0.3449 } 0 Methods \u00b6 generate_drift Generate drift by switching the classification function at random. take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 The sample generation works as follows: The two attributes are generated with the random number generator. The classification function defines whether to classify the instance as class 0 or class 1. Finally, data is balanced and noise is added, if these options are set by the user. The generated sample will have 2 relevant features, and an additional two noise features if has_noise is set. References \u00b6 Gama, Joao, et al.'s 'Learning with drift detection.' Advances in artificial intelligence\u2013SBIA 2004. Springer Berlin Heidelberg, 2004. 286-295.\" \u21a9","title":"Sine"},{"location":"api/synth/Sine/#sine","text":"Sine generator. This generator is an implementation of the dara stream with abrupt concept drift, as described in Gama, Joao, et al. 1 . It generates up to 4 relevant numerical features, that vary from 0 to 1, where only 2 of them are relevant to the classification task and the other 2 are optionally added by as noise. A classification function is chosen among four options: SINE1 . Abrupt concept drift, noise-free examples. It has two relevant attributes. Each attributes has values uniformly distributed in [0, 1]. In the first context all points below the curve \\(y = sin(x)\\) are classified as positive. Reversed SINE1 . The reversed classification of SINE1 . SINE2 . The same two relevant attributes. The classification function is \\(y < 0.5 + 0.3 sin(3 \\pi x)\\) . Reversed SINE2 . The reversed classification of SINE2 . Concept drift can be introduced by changing the classification function. This can be done manually or using ConceptDriftStream . Two important features are the possibility to balance classes, which means the class distribution will tend to a uniform one, and the possibility to add noise, which will, add two non relevant attributes.","title":"Sine"},{"location":"api/synth/Sine/#parameters","text":"classification_function ( int ) \u2013 defaults to 0 Classification functions to use. From 0 to 3. seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . balance_classes ( bool ) \u2013 defaults to False Whether to balance classes or not. If balanced, the class distribution will converge to an uniform distribution. has_noise ( bool ) \u2013 defaults to False Adds 2 non relevant features to the stream.","title":"Parameters"},{"location":"api/synth/Sine/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Sine/#examples","text":">>> from river import synth >>> dataset = synth . Sine ( classification_function = 2 , seed = 112 , ... balance_classes = False , has_noise = True ) >>> for x , y in dataset . take ( 5 ): ... print ( x , y ) { 0 : 0.3750 , 1 : 0.6403 , 2 : 0.9500 , 3 : 0.0756 } 1 { 0 : 0.7769 , 1 : 0.8327 , 2 : 0.0548 , 3 : 0.8176 } 1 { 0 : 0.8853 , 1 : 0.7223 , 2 : 0.0025 , 3 : 0.9811 } 0 { 0 : 0.3434 , 1 : 0.0947 , 2 : 0.3946 , 3 : 0.0049 } 1 { 0 : 0.7367 , 1 : 0.9558 , 2 : 0.8206 , 3 : 0.3449 } 0","title":"Examples"},{"location":"api/synth/Sine/#methods","text":"generate_drift Generate drift by switching the classification function at random. take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Sine/#notes","text":"The sample generation works as follows: The two attributes are generated with the random number generator. The classification function defines whether to classify the instance as class 0 or class 1. Finally, data is balanced and noise is added, if these options are set by the user. The generated sample will have 2 relevant features, and an additional two noise features if has_noise is set.","title":"Notes"},{"location":"api/synth/Sine/#references","text":"Gama, Joao, et al.'s 'Learning with drift detection.' Advances in artificial intelligence\u2013SBIA 2004. Springer Berlin Heidelberg, 2004. 286-295.\" \u21a9","title":"References"},{"location":"api/synth/Waveform/","text":"Waveform \u00b6 Waveform stream generator. Generates samples with 21 numeric features and 3 classes, based on a random differentiation of some base waveforms. Supports noise addition, in this case the samples will have 40 features. Parameters \u00b6 seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . has_noise ( bool ) \u2013 defaults to False Adds 19 unrelated features to the stream. Attributes \u00b6 desc Return the description from the docstring. Examples \u00b6 >>> from river import synth >>> dataset = synth . Waveform ( seed = 42 , has_noise = True ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.5437 , - 0.6154 , - 1.1978 , 2.1417 , - 0.0946 , - 0.7254 , - 0.4783 , 0.1982 , 0.3312 , 1.9780 , 3.0469 , 3.5249 , 5.4624 , 6.1318 , 2.7471 , 4.7896 , 2.9351 , 2.2258 , 0.1168 , 2.2835 , - 0.0245 , 0.3556 , 0.4170 , 0.8325 , - 0.2934 , - 0.0298 , 0.0951 , 0.6647 , - 0.1402 , - 0.0332 , - 0.7491 , - 0.7784 , 0.9488 , 1.5809 , - 0.3682 , 0.3756 , - 1.1932 , - 0.4091 , - 0.4467 , 1.5242 ] 2 [ 0.2186 , 1.4285 , 0.0843 , 0.0568 , 2.9605 , 2.6487 , 2.8402 , 3.2128 , 2.8694 , 4.0410 , 4.3953 , 3.7009 , 2.7075 , 2.1149 , 0.6994 , - 0.1702 , - 1.5082 , 1.0996 , - 0.1777 , - 0.4104 , 1.1797 , - 0.8982 , 0.8348 , 0.2966 , - 1.0378 , - 0.0758 , 0.9730 , 0.7956 , 1.4954 , 0.3382 , 3.3723 , - 0.9204 , - 0.3986 , - 0.0609 , - 1.4188 , 1.0425 , 0.9035 , 0.0190 , - 0.5344 , - 1.4951 ] 1 [ 0.1358 , 0.6081 , 0.7050 , 0.3609 , - 1.4670 , 1.6896 , 1.4886 , 1.4355 , 2.7730 , 2.7890 , 4.8437 , 5.3447 , 3.6724 , 2.5445 , 2.5541 , 2.2732 , - 0.5371 , - 0.4099 , 0.5331 , - 1.0464 , 1.9451 , - 0.1533 , - 0.9070 , - 0.8174 , - 0.4831 , - 0.5698 , - 2.0916 , 1.2637 , - 0.0155 , - 0.0274 , 0.8179 , - 1.0546 , - 0.7583 , 0.4574 , - 0.0644 , 0.3449 , - 0.0801 , - 0.2414 , 1.4335 , 1.0658 ] 2 [ 1.1428 , 1.2414 , 1.7699 , 0.5590 , 3.3606 , 1.0454 , 3.5236 , 4.6377 , 0.9673 , 1.4126 , 2.0997 , 1.5176 , 0.4915 , 2.6213 , 2.0010 , 3.0263 , 1.1228 , 3.0816 , 0.2378 , 0.1885 , 0.8135 , - 1.2309 , 0.2275 , 1.3071 , - 1.6075 , 0.1846 , 0.2599 , 0.7818 , - 1.2370 , - 1.3205 , 0.5219 , 0.2970 , 0.2505 , 0.3464 , - 0.6800 , 0.2323 , 0.2931 , - 0.7144 , 1.8658 , 0.4738 ] 0 [ - 0.9747 , 1.0114 , 1.6071 , - 0.1479 , 1.8605 , 1.5341 , 2.1677 , 3.0181 , 0.6517 , 0.6948 , 1.1105 , 1.7357 , 3.0258 , 4.2198 , 4.9311 , 4.7058 , 3.1159 , 3.7807 , 1.2868 , 3.4959 , 0.6257 , - 0.8572 , - 1.0709 , 0.4825 , - 0.2235 , 0.7140 , 0.4732 , - 0.0728 , - 0.8468 , - 1.5148 , - 0.4465 , 0.8564 , 0.2141 , - 1.2457 , 0.1732 , 0.3853 , - 0.8839 , 0.1537 , 0.0582 , - 1.1430 ] 0 Methods \u00b6 take Iterate over the k samples. Parameters k ( int ) Notes \u00b6 An instance is generated based on the parameters passed. The generator will randomly choose one of the hard coded waveforms, as well as random multipliers. For each feature, the actual value generated will be a a combination of the hard coded functions, with the multipliers and a random value. If noise is added then the features 21 to 40 will be replaced with a random normal value.","title":"Waveform"},{"location":"api/synth/Waveform/#waveform","text":"Waveform stream generator. Generates samples with 21 numeric features and 3 classes, based on a random differentiation of some base waveforms. Supports noise addition, in this case the samples will have 40 features.","title":"Waveform"},{"location":"api/synth/Waveform/#parameters","text":"seed ( int ) \u2013 defaults to None If int, seed is used to seed the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . has_noise ( bool ) \u2013 defaults to False Adds 19 unrelated features to the stream.","title":"Parameters"},{"location":"api/synth/Waveform/#attributes","text":"desc Return the description from the docstring.","title":"Attributes"},{"location":"api/synth/Waveform/#examples","text":">>> from river import synth >>> dataset = synth . Waveform ( seed = 42 , has_noise = True ) >>> for x , y in dataset . take ( 5 ): ... print ( list ( x . values ()), y ) [ 0.5437 , - 0.6154 , - 1.1978 , 2.1417 , - 0.0946 , - 0.7254 , - 0.4783 , 0.1982 , 0.3312 , 1.9780 , 3.0469 , 3.5249 , 5.4624 , 6.1318 , 2.7471 , 4.7896 , 2.9351 , 2.2258 , 0.1168 , 2.2835 , - 0.0245 , 0.3556 , 0.4170 , 0.8325 , - 0.2934 , - 0.0298 , 0.0951 , 0.6647 , - 0.1402 , - 0.0332 , - 0.7491 , - 0.7784 , 0.9488 , 1.5809 , - 0.3682 , 0.3756 , - 1.1932 , - 0.4091 , - 0.4467 , 1.5242 ] 2 [ 0.2186 , 1.4285 , 0.0843 , 0.0568 , 2.9605 , 2.6487 , 2.8402 , 3.2128 , 2.8694 , 4.0410 , 4.3953 , 3.7009 , 2.7075 , 2.1149 , 0.6994 , - 0.1702 , - 1.5082 , 1.0996 , - 0.1777 , - 0.4104 , 1.1797 , - 0.8982 , 0.8348 , 0.2966 , - 1.0378 , - 0.0758 , 0.9730 , 0.7956 , 1.4954 , 0.3382 , 3.3723 , - 0.9204 , - 0.3986 , - 0.0609 , - 1.4188 , 1.0425 , 0.9035 , 0.0190 , - 0.5344 , - 1.4951 ] 1 [ 0.1358 , 0.6081 , 0.7050 , 0.3609 , - 1.4670 , 1.6896 , 1.4886 , 1.4355 , 2.7730 , 2.7890 , 4.8437 , 5.3447 , 3.6724 , 2.5445 , 2.5541 , 2.2732 , - 0.5371 , - 0.4099 , 0.5331 , - 1.0464 , 1.9451 , - 0.1533 , - 0.9070 , - 0.8174 , - 0.4831 , - 0.5698 , - 2.0916 , 1.2637 , - 0.0155 , - 0.0274 , 0.8179 , - 1.0546 , - 0.7583 , 0.4574 , - 0.0644 , 0.3449 , - 0.0801 , - 0.2414 , 1.4335 , 1.0658 ] 2 [ 1.1428 , 1.2414 , 1.7699 , 0.5590 , 3.3606 , 1.0454 , 3.5236 , 4.6377 , 0.9673 , 1.4126 , 2.0997 , 1.5176 , 0.4915 , 2.6213 , 2.0010 , 3.0263 , 1.1228 , 3.0816 , 0.2378 , 0.1885 , 0.8135 , - 1.2309 , 0.2275 , 1.3071 , - 1.6075 , 0.1846 , 0.2599 , 0.7818 , - 1.2370 , - 1.3205 , 0.5219 , 0.2970 , 0.2505 , 0.3464 , - 0.6800 , 0.2323 , 0.2931 , - 0.7144 , 1.8658 , 0.4738 ] 0 [ - 0.9747 , 1.0114 , 1.6071 , - 0.1479 , 1.8605 , 1.5341 , 2.1677 , 3.0181 , 0.6517 , 0.6948 , 1.1105 , 1.7357 , 3.0258 , 4.2198 , 4.9311 , 4.7058 , 3.1159 , 3.7807 , 1.2868 , 3.4959 , 0.6257 , - 0.8572 , - 1.0709 , 0.4825 , - 0.2235 , 0.7140 , 0.4732 , - 0.0728 , - 0.8468 , - 1.5148 , - 0.4465 , 0.8564 , 0.2141 , - 1.2457 , 0.1732 , 0.3853 , - 0.8839 , 0.1537 , 0.0582 , - 1.1430 ] 0","title":"Examples"},{"location":"api/synth/Waveform/#methods","text":"take Iterate over the k samples. Parameters k ( int )","title":"Methods"},{"location":"api/synth/Waveform/#notes","text":"An instance is generated based on the parameters passed. The generator will randomly choose one of the hard coded waveforms, as well as random multipliers. For each feature, the actual value generated will be a a combination of the hard coded functions, with the multipliers and a random value. If noise is added then the features 21 to 40 will be replaced with a random normal value.","title":"Notes"},{"location":"api/time-series/Detrender/","text":"Detrender \u00b6 A linear detrender which centers the target in zero. At each learn_one step, the current mean of y is subtracted from y before being fed to the provided regression model. During the predict_one step, the current mean is added to the prediction of the regression model. Parameters \u00b6 regressor ( base.Regressor ) window_size ( int ) \u2013 defaults to None Window size used for calculating the rolling mean. If None , then a mean over the whole target data will instead be used. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Detrender"},{"location":"api/time-series/Detrender/#detrender","text":"A linear detrender which centers the target in zero. At each learn_one step, the current mean of y is subtracted from y before being fed to the provided regression model. During the predict_one step, the current mean is added to the prediction of the regression model.","title":"Detrender"},{"location":"api/time-series/Detrender/#parameters","text":"regressor ( base.Regressor ) window_size ( int ) \u2013 defaults to None Window size used for calculating the rolling mean. If None , then a mean over the whole target data will instead be used.","title":"Parameters"},{"location":"api/time-series/Detrender/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/time-series/GroupDetrender/","text":"GroupDetrender \u00b6 Removes the trend of the target inside each group. Parameters \u00b6 regressor ( base.Regressor ) by ( str ) window_size ( int ) \u2013 defaults to None Window size used for calculating each rolling mean. If None , then a mean over the whole target data will instead be used. Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"GroupDetrender"},{"location":"api/time-series/GroupDetrender/#groupdetrender","text":"Removes the trend of the target inside each group.","title":"GroupDetrender"},{"location":"api/time-series/GroupDetrender/#parameters","text":"regressor ( base.Regressor ) by ( str ) window_size ( int ) \u2013 defaults to None Window size used for calculating each rolling mean. If None , then a mean over the whole target data will instead be used.","title":"Parameters"},{"location":"api/time-series/GroupDetrender/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. learn_one Fits to a set of features x and a real-valued target y . Parameters x ( dict ) y ( numbers.Number ) Returns Regressor : self predict_one Predicts the target value of a set of features x . Parameters x ( dict ) Returns Number : The prediction.","title":"Methods"},{"location":"api/time-series/SNARIMAX/","text":"SNARIMAX \u00b6 SNARIMAX model. SNARIMAX stands for (S)easonal (N)on-linear (A)uto(R)egressive (I)ntegrated (M)oving-(A)verage with e(X)ogenous inputs model. This model generalizes many established time series models in a single interface that can be trained online. It assumes that the provided training data is ordered in time and is uniformly spaced. It is made up of the following components: S (Seasonal) - N (Non-linear): Any online regression model can be used, not necessarily a linear regression as is done in textbooks. - AR (Autoregressive): Lags of the target variable are used as features. - I (Integrated): The model can be fitted on a differenced version of a time series. In this context, integration is the reverse of differencing. - MA (Moving average): Lags of the errors are used as features. - X (Exogenous): Users can provide additional features. Care has to be taken to include features that will be available both at training and prediction time. Each of these components can be switched on and off by specifying the appropriate parameters. Classical time series models such as AR, MA, ARMA, and ARIMA can thus be seen as special parametrizations of the SNARIMAX model. This model is tailored for time series that are homoskedastic. In other words, it might not work well if the variance of the time series varies widely along time. Parameters \u00b6 p ( int ) Order of the autoregressive part. This is the number of past target values that will be included as features. d ( int ) Differencing order. q ( int ) Order of the moving average part. This is the number of past error terms that will be included as features. m ( int ) \u2013 defaults to 1 Season length used for extracting seasonal features. If you believe your data has a seasonal pattern, then set this accordingly. For instance, if the data seems to exhibit a yearly seasonality, and that your data is spaced by month, then you should set this to 12. Note that for this parameter to have any impact you should also set at least one of the p , d , and q parameters. sp ( int ) \u2013 defaults to 0 Seasonal order of the autoregressive part. This is the number of past target values that will be included as features. sd ( int ) \u2013 defaults to 0 Seasonal differencing order. sq ( int ) \u2013 defaults to 0 Seasonal order of the moving average part. This is the number of past error terms that will be included as features. regressor ( base.Regressor ) \u2013 defaults to None The online regression model to use. By default, a preprocessing.StandardScaler piped with a linear_model.LinearRegression will be used. Attributes \u00b6 differencer ( Differencer ) y_trues ( collections.deque ) The p past target values. errors ( collections.deque ) The q past error values. Examples \u00b6 >>> import calendar >>> import datetime as dt >>> from river import compose >>> from river import datasets >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> from river import time_series >>> def get_month_distances ( x ): ... return { ... calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) ... for month in range ( 1 , 13 ) ... } >>> def get_ordinal_date ( x ): ... return { 'ordinal_date' : x [ 'month' ] . toordinal ()} >>> extract_features = compose . TransformerUnion ( ... get_ordinal_date , ... get_month_distances ... ) >>> model = ( ... extract_features | ... time_series . SNARIMAX ( ... p = 0 , ... d = 0 , ... q = 0 , ... m = 12 , ... sp = 3 , ... sq = 6 , ... regressor = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( ... intercept_init = 110 , ... optimizer = optim . SGD ( 0.01 ), ... intercept_lr = 0.3 ... ) ... ) ... ) ... ) >>> metric = metrics . Rolling ( metrics . MAE (), 12 ) >>> for x , y in datasets . AirlinePassengers (): ... y_pred = model . forecast ( horizon = 1 , xs = [ x ]) ... model = model . learn_one ( x , y ) ... metric = metric . update ( y , y_pred [ 0 ]) >>> metric MAE : 11.636563 ( rolling 12 ) >>> horizon = 12 >>> future = [ ... { 'month' : dt . date ( year = 1961 , month = m , day = 1 )} ... for m in range ( 1 , horizon + 1 ) ... ] >>> forecast = model . forecast ( horizon = horizon , xs = future ) >>> for x , y_pred in zip ( future , forecast ): ... print ( x [ 'month' ], f ' { y_pred : .3f } ' ) 1961 - 01 - 01 442.554 1961 - 02 - 01 427.305 1961 - 03 - 01 471.861 1961 - 04 - 01 483.978 1961 - 05 - 01 489.995 1961 - 06 - 01 544.270 1961 - 07 - 01 632.882 1961 - 08 - 01 633.229 1961 - 09 - 01 531.349 1961 - 10 - 01 457.258 1961 - 11 - 01 405.978 1961 - 12 - 01 439.674 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. forecast Makes forecast at each step of the given horizon. Parameters horizon ( int ) xs ( list ) \u2013 defaults to None learn_one Updates the model. Parameters y ( float ) x ( dict ) \u2013 defaults to None References \u00b6 Wikipedia page on ARMA \u21a9 Wikipedia page on NARX \u21a9 ARIMA models \u21a9 Anava, O., Hazan, E., Mannor, S. and Shamir, O., 2013, June. Online learning for time series prediction. In Conference on learning theory (pp. 172-184) \u21a9","title":"SNARIMAX"},{"location":"api/time-series/SNARIMAX/#snarimax","text":"SNARIMAX model. SNARIMAX stands for (S)easonal (N)on-linear (A)uto(R)egressive (I)ntegrated (M)oving-(A)verage with e(X)ogenous inputs model. This model generalizes many established time series models in a single interface that can be trained online. It assumes that the provided training data is ordered in time and is uniformly spaced. It is made up of the following components: S (Seasonal) - N (Non-linear): Any online regression model can be used, not necessarily a linear regression as is done in textbooks. - AR (Autoregressive): Lags of the target variable are used as features. - I (Integrated): The model can be fitted on a differenced version of a time series. In this context, integration is the reverse of differencing. - MA (Moving average): Lags of the errors are used as features. - X (Exogenous): Users can provide additional features. Care has to be taken to include features that will be available both at training and prediction time. Each of these components can be switched on and off by specifying the appropriate parameters. Classical time series models such as AR, MA, ARMA, and ARIMA can thus be seen as special parametrizations of the SNARIMAX model. This model is tailored for time series that are homoskedastic. In other words, it might not work well if the variance of the time series varies widely along time.","title":"SNARIMAX"},{"location":"api/time-series/SNARIMAX/#parameters","text":"p ( int ) Order of the autoregressive part. This is the number of past target values that will be included as features. d ( int ) Differencing order. q ( int ) Order of the moving average part. This is the number of past error terms that will be included as features. m ( int ) \u2013 defaults to 1 Season length used for extracting seasonal features. If you believe your data has a seasonal pattern, then set this accordingly. For instance, if the data seems to exhibit a yearly seasonality, and that your data is spaced by month, then you should set this to 12. Note that for this parameter to have any impact you should also set at least one of the p , d , and q parameters. sp ( int ) \u2013 defaults to 0 Seasonal order of the autoregressive part. This is the number of past target values that will be included as features. sd ( int ) \u2013 defaults to 0 Seasonal differencing order. sq ( int ) \u2013 defaults to 0 Seasonal order of the moving average part. This is the number of past error terms that will be included as features. regressor ( base.Regressor ) \u2013 defaults to None The online regression model to use. By default, a preprocessing.StandardScaler piped with a linear_model.LinearRegression will be used.","title":"Parameters"},{"location":"api/time-series/SNARIMAX/#attributes","text":"differencer ( Differencer ) y_trues ( collections.deque ) The p past target values. errors ( collections.deque ) The q past error values.","title":"Attributes"},{"location":"api/time-series/SNARIMAX/#examples","text":">>> import calendar >>> import datetime as dt >>> from river import compose >>> from river import datasets >>> from river import linear_model >>> from river import metrics >>> from river import optim >>> from river import preprocessing >>> from river import time_series >>> def get_month_distances ( x ): ... return { ... calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) ... for month in range ( 1 , 13 ) ... } >>> def get_ordinal_date ( x ): ... return { 'ordinal_date' : x [ 'month' ] . toordinal ()} >>> extract_features = compose . TransformerUnion ( ... get_ordinal_date , ... get_month_distances ... ) >>> model = ( ... extract_features | ... time_series . SNARIMAX ( ... p = 0 , ... d = 0 , ... q = 0 , ... m = 12 , ... sp = 3 , ... sq = 6 , ... regressor = ( ... preprocessing . StandardScaler () | ... linear_model . LinearRegression ( ... intercept_init = 110 , ... optimizer = optim . SGD ( 0.01 ), ... intercept_lr = 0.3 ... ) ... ) ... ) ... ) >>> metric = metrics . Rolling ( metrics . MAE (), 12 ) >>> for x , y in datasets . AirlinePassengers (): ... y_pred = model . forecast ( horizon = 1 , xs = [ x ]) ... model = model . learn_one ( x , y ) ... metric = metric . update ( y , y_pred [ 0 ]) >>> metric MAE : 11.636563 ( rolling 12 ) >>> horizon = 12 >>> future = [ ... { 'month' : dt . date ( year = 1961 , month = m , day = 1 )} ... for m in range ( 1 , horizon + 1 ) ... ] >>> forecast = model . forecast ( horizon = horizon , xs = future ) >>> for x , y_pred in zip ( future , forecast ): ... print ( x [ 'month' ], f ' { y_pred : .3f } ' ) 1961 - 01 - 01 442.554 1961 - 02 - 01 427.305 1961 - 03 - 01 471.861 1961 - 04 - 01 483.978 1961 - 05 - 01 489.995 1961 - 06 - 01 544.270 1961 - 07 - 01 632.882 1961 - 08 - 01 633.229 1961 - 09 - 01 531.349 1961 - 10 - 01 457.258 1961 - 11 - 01 405.978 1961 - 12 - 01 439.674","title":"Examples"},{"location":"api/time-series/SNARIMAX/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. forecast Makes forecast at each step of the given horizon. Parameters horizon ( int ) xs ( list ) \u2013 defaults to None learn_one Updates the model. Parameters y ( float ) x ( dict ) \u2013 defaults to None","title":"Methods"},{"location":"api/time-series/SNARIMAX/#references","text":"Wikipedia page on ARMA \u21a9 Wikipedia page on NARX \u21a9 ARIMA models \u21a9 Anava, O., Hazan, E., Mannor, S. and Shamir, O., 2013, June. Online learning for time series prediction. In Conference on learning theory (pp. 172-184) \u21a9","title":"References"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/","text":"ExtremelyFastDecisionTreeClassifier \u00b6 Extremely Fast Decision Tree classifier. Also referred to as Hoeffding AnyTime Tree (HATT) classifier. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. min_samples_reevaluate ( int ) \u2013 defaults to 20 Number of instances a node should observe before reevaluating the best split. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . Agrawal ( classification_function = 0 , seed = 42 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . ExtremelyFastDecisionTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... nominal_attributes = [ 'elevel' , 'car' , 'zipcode' ], ... min_samples_reevaluate = 100 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 87.89 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Incrementally train the model Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df Notes \u00b6 The Extremely Fast Decision Tree (EFDT) 1 constructs a tree incrementally. The EFDT seeks to select and deploy a split as soon as it is confident the split is useful, and then revisits that decision, replacing the split if it subsequently becomes evident that a better split is available. The EFDT learns rapidly from a stationary distribution and eventually it learns the asymptotic batch tree if the distribution from which the data are drawn is stationary. References \u00b6 C. Manapragada, G. Webb, and M. Salehi. Extremely Fast Decision Tree. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '18). ACM, New York, NY, USA, 1953-1962. DOI: https://doi.org/10.1145/3219819.3220005 \u21a9","title":"ExtremelyFastDecisionTreeClassifier"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#extremelyfastdecisiontreeclassifier","text":"Extremely Fast Decision Tree classifier. Also referred to as Hoeffding AnyTime Tree (HATT) classifier.","title":"ExtremelyFastDecisionTreeClassifier"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. min_samples_reevaluate ( int ) \u2013 defaults to 20 Number of instances a node should observe before reevaluating the best split. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning.","title":"Parameters"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#examples","text":">>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . Agrawal ( classification_function = 0 , seed = 42 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . ExtremelyFastDecisionTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... nominal_attributes = [ 'elevel' , 'car' , 'zipcode' ], ... min_samples_reevaluate = 100 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 87.89 %","title":"Examples"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Incrementally train the model Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#notes","text":"The Extremely Fast Decision Tree (EFDT) 1 constructs a tree incrementally. The EFDT seeks to select and deploy a split as soon as it is confident the split is useful, and then revisits that decision, replacing the split if it subsequently becomes evident that a better split is available. The EFDT learns rapidly from a stationary distribution and eventually it learns the asymptotic batch tree if the distribution from which the data are drawn is stationary.","title":"Notes"},{"location":"api/tree/ExtremelyFastDecisionTreeClassifier/#references","text":"C. Manapragada, G. Webb, and M. Salehi. Extremely Fast Decision Tree. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '18). ACM, New York, NY, USA, 1953-1962. DOI: https://doi.org/10.1145/3219819.3220005 \u21a9","title":"References"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/","text":"HoeffdingAdaptiveTreeClassifier \u00b6 Hoeffding Adaptive Tree classifier. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . bootstrap_sampling ( bool ) \u2013 defaults to True If True, perform bootstrap sampling in the leaf nodes. drift_window_threshold ( int ) \u2013 defaults to 300 Minimum number of examples an alternate tree must observe before being considered as a potential replacement to the current one. adwin_confidence ( float ) \u2013 defaults to 0.002 The delta parameter used in the nodes' ADWIN drift detectors. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. seed \u2013 defaults to None If int, seed is the seed used by the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Only used when bootstrap_sampling=True to direct the bootstrap sampling. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_alternate_trees n_branches n_inactive_leaves n_leaves n_nodes n_pruned_alternate_trees n_switch_alternate_trees split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . ConceptDriftStream ( stream = synth . SEA ( seed = 42 , variant = 0 ), ... drift_stream = synth . SEA ( seed = 42 , variant = 1 ), ... seed = 1 , position = 500 , width = 50 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . HoeffdingAdaptiveTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... leaf_prediction = 'nb' , ... nb_threshold = 10 , ... seed = 0 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 91.09 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the model on instance x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df Notes \u00b6 The Hoeffding Adaptive Tree 1 uses ADWIN 2 to monitor performance of branches on the tree and to replace them with new branches when their accuracy decreases if the new branches are more accurate. The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree algorithm. It is enabled by default since, in general, it results in better performance. References \u00b6 Bifet, Albert, and Ricard Gavald\u00e0. \"Adaptive learning from evolving data streams.\" In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin, Heidelberg, 2009. \u21a9 Bifet, Albert, and Ricard Gavald\u00e0. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"HoeffdingAdaptiveTreeClassifier"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#hoeffdingadaptivetreeclassifier","text":"Hoeffding Adaptive Tree classifier.","title":"HoeffdingAdaptiveTreeClassifier"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . bootstrap_sampling ( bool ) \u2013 defaults to True If True, perform bootstrap sampling in the leaf nodes. drift_window_threshold ( int ) \u2013 defaults to 300 Minimum number of examples an alternate tree must observe before being considered as a potential replacement to the current one. adwin_confidence ( float ) \u2013 defaults to 0.002 The delta parameter used in the nodes' ADWIN drift detectors. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. seed \u2013 defaults to None If int, seed is the seed used by the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Only used when bootstrap_sampling=True to direct the bootstrap sampling.","title":"Parameters"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_alternate_trees n_branches n_inactive_leaves n_leaves n_nodes n_pruned_alternate_trees n_switch_alternate_trees split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#examples","text":">>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . ConceptDriftStream ( stream = synth . SEA ( seed = 42 , variant = 0 ), ... drift_stream = synth . SEA ( seed = 42 , variant = 1 ), ... seed = 1 , position = 500 , width = 50 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . HoeffdingAdaptiveTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... leaf_prediction = 'nb' , ... nb_threshold = 10 , ... seed = 0 ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 91.09 %","title":"Examples"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the model on instance x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#notes","text":"The Hoeffding Adaptive Tree 1 uses ADWIN 2 to monitor performance of branches on the tree and to replace them with new branches when their accuracy decreases if the new branches are more accurate. The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree algorithm. It is enabled by default since, in general, it results in better performance.","title":"Notes"},{"location":"api/tree/HoeffdingAdaptiveTreeClassifier/#references","text":"Bifet, Albert, and Ricard Gavald\u00e0. \"Adaptive learning from evolving data streams.\" In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin, Heidelberg, 2009. \u21a9 Bifet, Albert, and Ricard Gavald\u00e0. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"References"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/","text":"HoeffdingAdaptiveTreeRegressor \u00b6 Hoeffding Adaptive Tree regressor (HATR). This class implements a regression version of the Hoeffding Adaptive Tree Classifier. Hence, it also uses an ADWIN concept-drift detector instance at each decision node to monitor possible changes in the data distribution. If a drift is detected in a node, an alternate tree begins to be induced in the background. When enough information is gathered, HATR swaps the node where the change was detected by its alternate tree. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None The regression model used to provide responses if leaf_prediction='model' . If not provided an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. bootstrap_sampling ( bool ) \u2013 defaults to True If True, perform bootstrap sampling in the leaf nodes. drift_window_threshold ( int ) \u2013 defaults to 300 Minimum number of examples an alternate tree must observe before being considered as a potential replacement to the current one. adwin_confidence ( float ) \u2013 defaults to 0.002 The delta parameter used in the nodes' ADWIN drift detectors. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. seed \u2013 defaults to None If int, seed is the seed used by the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Only used when bootstrap_sampling=True to direct the bootstrap sampling. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_alternate_trees n_branches n_inactive_leaves n_leaves n_nodes n_pruned_alternate_trees n_switch_alternate_trees split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... tree . HoeffdingAdaptiveTreeRegressor ( ... grace_period = 50 , ... leaf_prediction = 'adaptive' , ... model_selector_decay = 0.3 , ... seed = 0 ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.78838 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the tree model on sample x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_one Predict the target value using one of the leaf prediction strategies. Parameters x Returns Predicted target value. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df Notes \u00b6 The Hoeffding Adaptive Tree 1 uses ADWIN 2 to monitor performance of branches on the tree and to replace them with new branches when their accuracy decreases if the new branches are more accurate. The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree algorithm. It is enabled by default since, in general, it results in better performance. To cope with ADWIN's requirements of bounded input data, HATR uses a novel error normalization strategy based on the empiral rule of Gaussian distributions. We assume the deviations of the predictions from the expected values follow a normal distribution. Hence, we subject these errors to a min-max normalization assuming that most of the data lies in the \\(\\left[-3\\sigma, 3\\sigma\\right]\\) range. These normalized errors are passed to the ADWIN instances. This is the same strategy used by Adaptive Random Forest Regressor. References \u00b6 Bifet, Albert, and Ricard Gavald\u00e0. \"Adaptive learning from evolving data streams.\" In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin, Heidelberg, 2009. \u21a9 Bifet, Albert, and Ricard Gavald\u00e0. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"HoeffdingAdaptiveTreeRegressor"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#hoeffdingadaptivetreeregressor","text":"Hoeffding Adaptive Tree regressor (HATR). This class implements a regression version of the Hoeffding Adaptive Tree Classifier. Hence, it also uses an ADWIN concept-drift detector instance at each decision node to monitor possible changes in the data distribution. If a drift is detected in a node, an alternate tree begins to be induced in the background. When enough information is gathered, HATR swaps the node where the change was detected by its alternate tree.","title":"HoeffdingAdaptiveTreeRegressor"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None The regression model used to provide responses if leaf_prediction='model' . If not provided an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. bootstrap_sampling ( bool ) \u2013 defaults to True If True, perform bootstrap sampling in the leaf nodes. drift_window_threshold ( int ) \u2013 defaults to 300 Minimum number of examples an alternate tree must observe before being considered as a potential replacement to the current one. adwin_confidence ( float ) \u2013 defaults to 0.002 The delta parameter used in the nodes' ADWIN drift detectors. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. seed \u2013 defaults to None If int, seed is the seed used by the random number generator; If RandomState instance, seed is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Only used when bootstrap_sampling=True to direct the bootstrap sampling.","title":"Parameters"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_alternate_trees n_branches n_inactive_leaves n_leaves n_nodes n_pruned_alternate_trees n_switch_alternate_trees split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... tree . HoeffdingAdaptiveTreeRegressor ( ... grace_period = 50 , ... leaf_prediction = 'adaptive' , ... model_selector_decay = 0.3 , ... seed = 0 ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.78838","title":"Examples"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the tree model on sample x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_one Predict the target value using one of the leaf prediction strategies. Parameters x Returns Predicted target value. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#notes","text":"The Hoeffding Adaptive Tree 1 uses ADWIN 2 to monitor performance of branches on the tree and to replace them with new branches when their accuracy decreases if the new branches are more accurate. The bootstrap sampling strategy is an improvement over the original Hoeffding Adaptive Tree algorithm. It is enabled by default since, in general, it results in better performance. To cope with ADWIN's requirements of bounded input data, HATR uses a novel error normalization strategy based on the empiral rule of Gaussian distributions. We assume the deviations of the predictions from the expected values follow a normal distribution. Hence, we subject these errors to a min-max normalization assuming that most of the data lies in the \\(\\left[-3\\sigma, 3\\sigma\\right]\\) range. These normalized errors are passed to the ADWIN instances. This is the same strategy used by Adaptive Random Forest Regressor.","title":"Notes"},{"location":"api/tree/HoeffdingAdaptiveTreeRegressor/#references","text":"Bifet, Albert, and Ricard Gavald\u00e0. \"Adaptive learning from evolving data streams.\" In International Symposium on Intelligent Data Analysis, pp. 249-260. Springer, Berlin, Heidelberg, 2009. \u21a9 Bifet, Albert, and Ricard Gavald\u00e0. \"Learning from time-changing data with adaptive windowing.\" In Proceedings of the 2007 SIAM international conference on data mining, pp. 443-448. Society for Industrial and Applied Mathematics, 2007. \u21a9","title":"References"},{"location":"api/tree/HoeffdingTreeClassifier/","text":"HoeffdingTreeClassifier \u00b6 Hoeffding Tree or Very Fast Decision Tree classifier. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . Agrawal ( classification_function = 0 , seed = 42 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . HoeffdingTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... nominal_attributes = [ 'elevel' , 'car' , 'zipcode' ] ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 83.78 % Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the model on instance x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df Notes \u00b6 A Hoeffding Tree 1 is an incremental, anytime decision tree induction algorithm that is capable of learning from massive data streams, assuming that the distribution generating examples does not change over time. Hoeffding trees exploit the fact that a small sample can often be enough to choose an optimal splitting attribute. This idea is supported mathematically by the Hoeffding bound, which quantifies the number of observations (in our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute). A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. Implementation based on MOA 2 . References \u00b6 G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In KDD\u201901, pages 97\u2013106, San Francisco, CA, 2001. ACM Press. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"HoeffdingTreeClassifier"},{"location":"api/tree/HoeffdingTreeClassifier/#hoeffdingtreeclassifier","text":"Hoeffding Tree or Very Fast Decision Tree classifier.","title":"HoeffdingTreeClassifier"},{"location":"api/tree/HoeffdingTreeClassifier/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning.","title":"Parameters"},{"location":"api/tree/HoeffdingTreeClassifier/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/HoeffdingTreeClassifier/#examples","text":">>> from river import synth >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> gen = synth . Agrawal ( classification_function = 0 , seed = 42 ) >>> # Take 1000 instances from the infinite data generator >>> dataset = iter ( gen . take ( 1000 )) >>> model = tree . HoeffdingTreeClassifier ( ... grace_period = 100 , ... split_confidence = 1e-5 , ... nominal_attributes = [ 'elevel' , 'car' , 'zipcode' ] ... ) >>> metric = metrics . Accuracy () >>> evaluate . progressive_val_score ( dataset , model , metric ) Accuracy : 83.78 %","title":"Examples"},{"location":"api/tree/HoeffdingTreeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the model on instance x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the label of a set of features x . Parameters x ( dict ) Returns typing.Union[bool, str, int] : The predicted label. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/HoeffdingTreeClassifier/#notes","text":"A Hoeffding Tree 1 is an incremental, anytime decision tree induction algorithm that is capable of learning from massive data streams, assuming that the distribution generating examples does not change over time. Hoeffding trees exploit the fact that a small sample can often be enough to choose an optimal splitting attribute. This idea is supported mathematically by the Hoeffding bound, which quantifies the number of observations (in our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute). A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. Implementation based on MOA 2 .","title":"Notes"},{"location":"api/tree/HoeffdingTreeClassifier/#references","text":"G. Hulten, L. Spencer, and P. Domingos. Mining time-changing data streams. In KDD\u201901, pages 97\u2013106, San Francisco, CA, 2001. ACM Press. \u21a9 Albert Bifet, Geoff Holmes, Richard Kirkby, Bernhard Pfahringer. MOA: Massive Online Analysis; Journal of Machine Learning Research 11: 1601-1604, 2010. \u21a9","title":"References"},{"location":"api/tree/HoeffdingTreeRegressor/","text":"HoeffdingTreeRegressor \u00b6 Hoeffding Tree regressor. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None The regression model used to provide responses if leaf_prediction='model' . If not provided an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... tree . HoeffdingTreeRegressor ( ... grace_period = 100 , ... leaf_prediction = 'adaptive' , ... model_selector_decay = 0.9 ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.852902 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the tree model on sample x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_one Predict the target value using one of the leaf prediction strategies. Parameters x Returns Predicted target value. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df Notes \u00b6 The Hoeffding Tree Regressor (HTR) is an adaptation of the incremental tree algorithm of the same name for classification. Similarly to its classification counterpart, HTR uses the Hoeffding bound to control its split decisions. Differently from the classification algorithm, HTR relies on calculating the reduction of variance in the target space to decide among the split candidates. The smallest the variance at its leaf nodes, the more homogeneous the partitions are. At its leaf nodes, HTR fits either linear models or uses the target average as the predictor.","title":"HoeffdingTreeRegressor"},{"location":"api/tree/HoeffdingTreeRegressor/#hoeffdingtreeregressor","text":"Hoeffding Tree regressor.","title":"HoeffdingTreeRegressor"},{"location":"api/tree/HoeffdingTreeRegressor/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( base.Regressor ) \u2013 defaults to None The regression model used to provide responses if leaf_prediction='model' . If not provided an instance of river.linear_model.LinearRegression with the default hyperparameters is used. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning.","title":"Parameters"},{"location":"api/tree/HoeffdingTreeRegressor/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/HoeffdingTreeRegressor/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> from river import preprocessing >>> dataset = datasets . TrumpApproval () >>> model = ( ... preprocessing . StandardScaler () | ... tree . HoeffdingTreeRegressor ( ... grace_period = 100 , ... leaf_prediction = 'adaptive' , ... model_selector_decay = 0.9 ... ) ... ) >>> metric = metrics . MAE () >>> evaluate . progressive_val_score ( dataset , model , metric ) MAE : 0.852902","title":"Examples"},{"location":"api/tree/HoeffdingTreeRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Train the tree model on sample x and corresponding target y. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_one Predict the target value using one of the leaf prediction strategies. Parameters x Returns Predicted target value. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/HoeffdingTreeRegressor/#notes","text":"The Hoeffding Tree Regressor (HTR) is an adaptation of the incremental tree algorithm of the same name for classification. Similarly to its classification counterpart, HTR uses the Hoeffding bound to control its split decisions. Differently from the classification algorithm, HTR relies on calculating the reduction of variance in the target space to decide among the split candidates. The smallest the variance at its leaf nodes, the more homogeneous the partitions are. At its leaf nodes, HTR fits either linear models or uses the target average as the predictor.","title":"Notes"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/","text":"LabelCombinationHoeffdingTreeClassifier \u00b6 Label Combination Hoeffding Tree for multi-label classification. Label combination transforms the problem from multi-label to multi-class. For each unique combination of labels it assigns a class and proceeds with training the hoeffding tree normally. The transformation is done by changing the label set which could be seen as a binary number to an int which will represent the class, and after the prediction the int is converted back to a binary number which is the predicted label-set. Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> dataset = iter ( datasets . Music () . take ( 200 )) >>> model = tree . LabelCombinationHoeffdingTreeClassifier ( ... split_confidence = 1e-5 , ... grace_period = 50 ... ) >>> metric = metrics . Hamming () >>> evaluate . progressive_val_score ( dataset , model , metric ) Hamming : 0.154104 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Update the Multi-label Hoeffding Tree Classifier. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the labels of an instance. Parameters x ( dict ) Returns typing.Union[bool, str, int] : Predicted labels. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"LabelCombinationHoeffdingTreeClassifier"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/#labelcombinationhoeffdingtreeclassifier","text":"Label Combination Hoeffding Tree for multi-label classification. Label combination transforms the problem from multi-label to multi-class. For each unique combination of labels it assigns a class and proceeds with training the hoeffding tree normally. The transformation is done by changing the label set which could be seen as a binary number to an int which will represent the class, and after the prediction the int is converted back to a binary number which is the predicted label-set.","title":"LabelCombinationHoeffdingTreeClassifier"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_criterion ( str ) \u2013 defaults to info_gain Split criterion to use. - 'gini' - Gini - 'info_gain' - Information Gain - 'hellinger' - Helinger Distance split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to nba Prediction mechanism used at leafs. - 'mc' - Majority Class - 'nb' - Naive Bayes - 'nba' - Naive Bayes Adaptive nb_threshold ( int ) \u2013 defaults to 0 Number of instances a leaf should observe before allowing Naive Bayes. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.GaussianSplitter is used if splitter is None . binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 100 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning.","title":"Parameters"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/#examples","text":">>> from river import datasets >>> from river import evaluate >>> from river import metrics >>> from river import tree >>> dataset = iter ( datasets . Music () . take ( 200 )) >>> model = tree . LabelCombinationHoeffdingTreeClassifier ( ... split_confidence = 1e-5 , ... grace_period = 50 ... ) >>> metric = metrics . Hamming () >>> evaluate . progressive_val_score ( dataset , model , metric ) Hamming : 0.154104","title":"Examples"},{"location":"api/tree/LabelCombinationHoeffdingTreeClassifier/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Update the Multi-label Hoeffding Tree Classifier. Parameters x y sample_weight \u2013 defaults to 1.0 Returns self predict_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns Series : Series of predicted labels. predict_one Predict the labels of an instance. Parameters x ( dict ) Returns typing.Union[bool, str, int] : Predicted labels. predict_proba_many Predict the labels of a DataFrame X . Parameters X ( pandas.core.frame.DataFrame ) Returns DataFrame : DataFrame that associate probabilities which each label as columns. predict_proba_one Predict the probability of each label for a dictionary of features x . Parameters x Returns A dictionary that associates a probability which each label. to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/iSOUPTreeRegressor/","text":"iSOUPTreeRegressor \u00b6 Incremental Structured Output Prediction Tree (iSOUP-Tree) for multi-target regression. This is an implementation of the iSOUP-Tree proposed by A. Osojnik, P. Panov, and S. D\u017eeroski 1 . Parameters \u00b6 grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( Union[ base.Regressor , Dict] ) \u2013 defaults to None The regression model(s) used to provide responses if leaf_prediction='model' . It can be either a regressor (in which case it is going to be replicated to all the targets) or a dictionary whose keys are target identifiers, and the values are instances of river.base.Regressor. If not provided, instances of river.linear_model.LinearRegression with the default hyperparameters are used for all the targets. If a dictionary is passed and not all target models are specified, copies from the first model match in the dictionary will be used to the remaining targets. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning. Attributes \u00b6 height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer. Examples \u00b6 >>> import numbers >>> from river import compose >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> from river import tree >>> dataset = datasets . SolarFlare () >>> num = compose . SelectType ( numbers . Number ) | preprocessing . MinMaxScaler () >>> cat = compose . SelectType ( str ) | preprocessing . OneHotEncoder ( sparse = False ) >>> model = tree . iSOUPTreeRegressor ( ... grace_period = 100 , ... leaf_prediction = 'model' , ... leaf_model = { ... 'c-class-flares' : linear_model . LinearRegression ( l2 = 0.02 ), ... 'm-class-flares' : linear_model . PARegressor (), ... 'x-class-flares' : linear_model . LinearRegression ( l2 = 0.1 ) ... } ... ) >>> pipeline = ( num + cat ) | model >>> metric = metrics . RegressionMultiOutput ( metrics . MAE ()) >>> evaluate . progressive_val_score ( dataset , pipeline , metric ) MAE : 0.425929 Methods \u00b6 clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Incrementally train the model with one sample. Training tasks: * If the tree is empty, create a leaf node as the root. * If the tree is already initialized, find the corresponding leaf for the instance and update the leaf node statistics. * If growth is allowed and the number of instances that the leaf has observed between split attempts exceed the grace period then attempt to split. Parameters x ( dict ) y ( Dict[Hashable, numbers.Number] ) sample_weight ( float ) \u2013 defaults to 1.0 predict_one Predict the target values for a given instance. Parameters x ( dict ) Returns typing.Dict[typing.Hashable, numbers.Number] : dict to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df References \u00b6 Alja\u017e Osojnik, Pan\u010de Panov, and Sa\u0161o D\u017eeroski. \"Tree-based methods for online multi-target regression.\" Journal of Intelligent Information Systems 50.2 (2018): 315-339. \u21a9","title":"iSOUPTreeRegressor"},{"location":"api/tree/iSOUPTreeRegressor/#isouptreeregressor","text":"Incremental Structured Output Prediction Tree (iSOUP-Tree) for multi-target regression. This is an implementation of the iSOUP-Tree proposed by A. Osojnik, P. Panov, and S. D\u017eeroski 1 .","title":"iSOUPTreeRegressor"},{"location":"api/tree/iSOUPTreeRegressor/#parameters","text":"grace_period ( int ) \u2013 defaults to 200 Number of instances a leaf should observe between split attempts. max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. split_confidence ( float ) \u2013 defaults to 1e-07 Allowed error in split decision, a value closer to 0 takes longer to decide. tie_threshold ( float ) \u2013 defaults to 0.05 Threshold below which a split will be forced to break ties. leaf_prediction ( str ) \u2013 defaults to model Prediction mechanism used at leafs. - 'mean' - Target mean - 'model' - Uses the model defined in leaf_model - 'adaptive' - Chooses between 'mean' and 'model' dynamically leaf_model ( Union[ base.Regressor , Dict] ) \u2013 defaults to None The regression model(s) used to provide responses if leaf_prediction='model' . It can be either a regressor (in which case it is going to be replicated to all the targets) or a dictionary whose keys are target identifiers, and the values are instances of river.base.Regressor. If not provided, instances of river.linear_model.LinearRegression with the default hyperparameters are used for all the targets. If a dictionary is passed and not all target models are specified, copies from the first model match in the dictionary will be used to the remaining targets. model_selector_decay ( float ) \u2013 defaults to 0.95 The exponential decaying factor applied to the learning models' squared errors, that are monitored if leaf_prediction='adaptive' . Must be between 0 and 1 . The closer to 1 , the more importance is going to be given to past observations. On the other hand, if its value approaches 0 , the recent observed errors are going to have more influence on the final decision. nominal_attributes ( list ) \u2013 defaults to None List of Nominal attributes identifiers. If empty, then assume that all numeric attributes should be treated as continuous. splitter ( river.tree.splitter.base_splitter.Splitter ) \u2013 defaults to None The Splitter or Attribute Observer (AO) used to monitor the class statistics of numeric features and perform splits. Splitters are available in the tree.splitter module. Different splitters are available for classification and regression tasks. Classification and regression splitters can be distinguished by their property is_target_class . This is an advanced option. Special care must be taken when choosing different splitters. By default, tree.splitter.EBSTSplitter is used if splitter is None . min_samples_split ( int ) \u2013 defaults to 5 The minimum number of samples every branch resulting from a split candidate must have to be considered valid. binary_split ( bool ) \u2013 defaults to False If True, only allow binary splits. max_size ( int ) \u2013 defaults to 500 The max size of the tree, in Megabytes (MB). memory_estimate_period ( int ) \u2013 defaults to 1000000 Interval (number of processed instances) between memory consumption checks. stop_mem_management ( bool ) \u2013 defaults to False If True, stop growing as soon as memory limit is hit. remove_poor_attrs ( bool ) \u2013 defaults to False If True, disable poor attributes to reduce memory usage. merit_preprune ( bool ) \u2013 defaults to True If True, enable merit-based tree pre-pruning.","title":"Parameters"},{"location":"api/tree/iSOUPTreeRegressor/#attributes","text":"height leaf_prediction Return the prediction strategy used by the tree at its leaves. max_size Max allowed size tree can reach (in MB). n_active_leaves n_branches n_inactive_leaves n_leaves n_nodes split_criterion Return a string with the name of the split criterion being used by the tree. summary Collect metrics corresponding to the current status of the tree in a string buffer.","title":"Attributes"},{"location":"api/tree/iSOUPTreeRegressor/#examples","text":">>> import numbers >>> from river import compose >>> from river import datasets >>> from river import evaluate >>> from river import linear_model >>> from river import metrics >>> from river import preprocessing >>> from river import tree >>> dataset = datasets . SolarFlare () >>> num = compose . SelectType ( numbers . Number ) | preprocessing . MinMaxScaler () >>> cat = compose . SelectType ( str ) | preprocessing . OneHotEncoder ( sparse = False ) >>> model = tree . iSOUPTreeRegressor ( ... grace_period = 100 , ... leaf_prediction = 'model' , ... leaf_model = { ... 'c-class-flares' : linear_model . LinearRegression ( l2 = 0.02 ), ... 'm-class-flares' : linear_model . PARegressor (), ... 'x-class-flares' : linear_model . LinearRegression ( l2 = 0.1 ) ... } ... ) >>> pipeline = ( num + cat ) | model >>> metric = metrics . RegressionMultiOutput ( metrics . MAE ()) >>> evaluate . progressive_val_score ( dataset , pipeline , metric ) MAE : 0.425929","title":"Examples"},{"location":"api/tree/iSOUPTreeRegressor/#methods","text":"clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. debug_one Print an explanation of how x is predicted. Parameters x ( dict ) Returns typing.Union[str, NoneType] : A representation of the path followed by the tree to predict x ; None if draw Draw the tree using the graphviz library. Since the tree is drawn without passing incoming samples, classification trees will show the majority class in their leaves, whereas regression trees will use the target mean. Parameters max_depth ( int ) \u2013 defaults to None The maximum depth a tree can reach. If None , the tree will grow indefinitely. learn_one Incrementally train the model with one sample. Training tasks: * If the tree is empty, create a leaf node as the root. * If the tree is already initialized, find the corresponding leaf for the instance and update the leaf node statistics. * If growth is allowed and the number of instances that the leaf has observed between split attempts exceed the grace period then attempt to split. Parameters x ( dict ) y ( Dict[Hashable, numbers.Number] ) sample_weight ( float ) \u2013 defaults to 1.0 predict_one Predict the target values for a given instance. Parameters x ( dict ) Returns typing.Dict[typing.Hashable, numbers.Number] : dict to_dataframe Return a representation of the current tree structure organized in a pandas.DataFrame object. In case the tree is empty or it only contains a single node (a leaf), None is returned. Returns df","title":"Methods"},{"location":"api/tree/iSOUPTreeRegressor/#references","text":"Alja\u017e Osojnik, Pan\u010de Panov, and Sa\u0161o D\u017eeroski. \"Tree-based methods for online multi-target regression.\" Journal of Intelligent Information Systems 50.2 (2018): 315-339. \u21a9","title":"References"},{"location":"api/tree/splitter/EBSTSplitter/","text":"EBSTSplitter \u00b6 iSOUP-Tree's Extended Binary Search Tree (E-BST). This class implements the Extended Binary Search Tree 1 (E-BST) structure, using the variant employed by Osojnik et al. 2 in the iSOUP-Tree algorithm. This structure is employed to observe the target space distribution. Proposed along with Fast Incremental Model Tree with Drift Detection 1 (FIMT-DD), E-BST was the first attribute observer (AO) proposed for incremental Hoeffding Tree regressors. This AO works by storing all observations between splits in an extended binary search tree structure. E-BST stores the input feature realizations and statistics of the target(s) that enable calculating the split heuristic at any time. To alleviate time and memory costs, E-BST implements a memory management routine, where the worst split candidates are pruned from the binary tree. In this variant, only the left branch statistics are stored and the complete split-enabling statistics are calculated with an in-order traversal of the binary search tree. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Not implemented in regression splitters. Parameters att_val target_val ( Union[bool, str, int] ) remove_bad_splits Remove bad splits. Based on FIMT-DD's 1 procedure to remove bad split candidates from the E-BST. This mechanism is triggered every time a split attempt fails. The rationale is to remove points whose split merit is much worse than the best candidate overall (for which the growth decision already failed). Let \\(m_1\\) be the merit of the best split point and \\(m_2\\) be the merit of the second best split candidate. The ratio \\(r = m_2/m_1\\) along with the Hoeffding bound ( \\(\\epsilon\\) ) are used to decide upon creating a split. A split occurs when \\(r < 1 - \\epsilon\\) . A split candidate, with merit \\(m_i\\) , is considered badr if \\(m_i / m_1 < r - 2\\epsilon\\) . The rationale is the following: if the merit ratio for this point is smaller than the lower bound of \\(r\\) , then the true merit of that split relative to the best one is small. Hence, this candidate can be safely removed. To avoid excessive and costly manipulations of the E-BST to update the stored statistics, only the nodes whose children are all bad split points are pruned, as defined in 1 . Parameters criterion last_check_ratio ( float ) last_check_vr ( float ) last_check_e ( float ) pre_split_dist ( Union[List, Dict] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float ) References \u00b6 Ikonomovska, E., Gama, J., & D\u017eeroski, S. (2011). Learning model trees from evolving data streams. Data mining and knowledge discovery, 23(1), 128-168. \u21a9 \u21a9 \u21a9 \u21a9 Osojnik, Alja\u017e. 2017. Structured output prediction on Data Streams (Doctoral Dissertation) \u21a9","title":"EBSTSplitter"},{"location":"api/tree/splitter/EBSTSplitter/#ebstsplitter","text":"iSOUP-Tree's Extended Binary Search Tree (E-BST). This class implements the Extended Binary Search Tree 1 (E-BST) structure, using the variant employed by Osojnik et al. 2 in the iSOUP-Tree algorithm. This structure is employed to observe the target space distribution. Proposed along with Fast Incremental Model Tree with Drift Detection 1 (FIMT-DD), E-BST was the first attribute observer (AO) proposed for incremental Hoeffding Tree regressors. This AO works by storing all observations between splits in an extended binary search tree structure. E-BST stores the input feature realizations and statistics of the target(s) that enable calculating the split heuristic at any time. To alleviate time and memory costs, E-BST implements a memory management routine, where the worst split candidates are pruned from the binary tree. In this variant, only the left branch statistics are stored and the complete split-enabling statistics are calculated with an in-order traversal of the binary search tree.","title":"EBSTSplitter"},{"location":"api/tree/splitter/EBSTSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/EBSTSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Not implemented in regression splitters. Parameters att_val target_val ( Union[bool, str, int] ) remove_bad_splits Remove bad splits. Based on FIMT-DD's 1 procedure to remove bad split candidates from the E-BST. This mechanism is triggered every time a split attempt fails. The rationale is to remove points whose split merit is much worse than the best candidate overall (for which the growth decision already failed). Let \\(m_1\\) be the merit of the best split point and \\(m_2\\) be the merit of the second best split candidate. The ratio \\(r = m_2/m_1\\) along with the Hoeffding bound ( \\(\\epsilon\\) ) are used to decide upon creating a split. A split occurs when \\(r < 1 - \\epsilon\\) . A split candidate, with merit \\(m_i\\) , is considered badr if \\(m_i / m_1 < r - 2\\epsilon\\) . The rationale is the following: if the merit ratio for this point is smaller than the lower bound of \\(r\\) , then the true merit of that split relative to the best one is small. Hence, this candidate can be safely removed. To avoid excessive and costly manipulations of the E-BST to update the stored statistics, only the nodes whose children are all bad split points are pruned, as defined in 1 . Parameters criterion last_check_ratio ( float ) last_check_vr ( float ) last_check_e ( float ) pre_split_dist ( Union[List, Dict] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/EBSTSplitter/#references","text":"Ikonomovska, E., Gama, J., & D\u017eeroski, S. (2011). Learning model trees from evolving data streams. Data mining and knowledge discovery, 23(1), 128-168. \u21a9 \u21a9 \u21a9 \u21a9 Osojnik, Alja\u017e. 2017. Structured output prediction on Data Streams (Doctoral Dissertation) \u21a9","title":"References"},{"location":"api/tree/splitter/ExhaustiveSplitter/","text":"ExhaustiveSplitter \u00b6 Numeric attribute observer for classification tasks that is based on a Binary Search Tree. This algorithm 1 is also referred to as exhaustive attribute observer, since it ends up storing all the observations between split attempts 2 . This splitter cannot perform probability density estimations, so it does not work well when coupled with tree leaves using naive bayes models. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba The underlying data structure used to monitor the input does not allow probability density estimations. Hence, it always returns zero for any given input. Parameters att_val target_val ( Union[bool, str, int] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float ) References \u00b6 Domingos, P. and Hulten, G., 2000, August. Mining high-speed data streams. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 71-80). \u21a9 Pfahringer, B., Holmes, G. and Kirkby, R., 2008, May. Handling numeric attributes in hoeffding trees. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 296-307). Springer, Berlin, Heidelberg. \u21a9","title":"ExhaustiveSplitter"},{"location":"api/tree/splitter/ExhaustiveSplitter/#exhaustivesplitter","text":"Numeric attribute observer for classification tasks that is based on a Binary Search Tree. This algorithm 1 is also referred to as exhaustive attribute observer, since it ends up storing all the observations between split attempts 2 . This splitter cannot perform probability density estimations, so it does not work well when coupled with tree leaves using naive bayes models.","title":"ExhaustiveSplitter"},{"location":"api/tree/splitter/ExhaustiveSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/ExhaustiveSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba The underlying data structure used to monitor the input does not allow probability density estimations. Hence, it always returns zero for any given input. Parameters att_val target_val ( Union[bool, str, int] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/ExhaustiveSplitter/#references","text":"Domingos, P. and Hulten, G., 2000, August. Mining high-speed data streams. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 71-80). \u21a9 Pfahringer, B., Holmes, G. and Kirkby, R., 2008, May. Handling numeric attributes in hoeffding trees. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 296-307). Springer, Berlin, Heidelberg. \u21a9","title":"References"},{"location":"api/tree/splitter/GaussianSplitter/","text":"GaussianSplitter \u00b6 Numeric attribute observer for classification tasks that is based on Gaussian estimators. The distribution of each class is approximated using a Gaussian distribution. Hence, the probability density function can be easily calculated. Parameters \u00b6 n_splits ( int ) \u2013 defaults to 10 The number of partitions to consider when querying for split candidates. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"GaussianSplitter"},{"location":"api/tree/splitter/GaussianSplitter/#gaussiansplitter","text":"Numeric attribute observer for classification tasks that is based on Gaussian estimators. The distribution of each class is approximated using a Gaussian distribution. Hence, the probability density function can be easily calculated.","title":"GaussianSplitter"},{"location":"api/tree/splitter/GaussianSplitter/#parameters","text":"n_splits ( int ) \u2013 defaults to 10 The number of partitions to consider when querying for split candidates.","title":"Parameters"},{"location":"api/tree/splitter/GaussianSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/GaussianSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/HistogramSplitter/","text":"HistogramSplitter \u00b6 Numeric attribute observer for classification tasks that discretizes features using histograms. Parameters \u00b6 n_bins ( int ) \u2013 defaults to 256 The maximum number of bins in the histogram. n_splits ( int ) \u2013 defaults to 32 The number of split points to evaluate when querying for the best split candidate. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"HistogramSplitter"},{"location":"api/tree/splitter/HistogramSplitter/#histogramsplitter","text":"Numeric attribute observer for classification tasks that discretizes features using histograms.","title":"HistogramSplitter"},{"location":"api/tree/splitter/HistogramSplitter/#parameters","text":"n_bins ( int ) \u2013 defaults to 256 The maximum number of bins in the histogram. n_splits ( int ) \u2013 defaults to 32 The number of split points to evaluate when querying for the best split candidate.","title":"Parameters"},{"location":"api/tree/splitter/HistogramSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/HistogramSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/QOSplitter/","text":"QOSplitter \u00b6 Quantization observer (QO). This splitter utilizes a hash-based quantization algorithm to keep track of the target statistics and evaluate split candidates. QO, relies on the radius parameter to define discretization intervals for each incoming feature. Split candidates are defined as the midpoints between two consecutive hash slots. Both binary splits and multi-way splits can be created by this attribute observer. This class implements the algorithm described in 1 . The smaller the quantization radius, the more hash slots will be created to accommodate the discretized data. Hence, both the running time and memory consumption increase, but the resulting splits ought to be closer to the ones obtained by a batch exhaustive approach. On the other hand, if the radius is too large, fewer slots will be created, less memory and running time will be required, but at the cost of coarse split suggestions. QO assumes that all features have the same range. It is always advised to scale the features to apply this splitter. That can be done using the preprocessing module. A good \"rule of thumb\" is to scale data using preprocessing.StandardScaler and define the radius as a proportion of the features' standard deviation. For instance, the default radius value would correspond to one quarter of the normalized features' standard deviation (since the scaled data has zero mean and unit variance). If the features come from normal distributions, by following the empirical rule, roughly 32 hash slots will be created. Parameters \u00b6 radius ( float ) \u2013 defaults to 0.25 The quantization radius. QO discretizes the incoming feature in intervals of equal length that are defined by this parameter. allow_multiway_splits \u2013 defaults to False Whether or not allow that multiway splits are evaluated. Numeric multi-way splits use the same quantization strategy of QO to create multiple tree branches. The same quantization radius is used, and each stored slot represents the split enabling statistics of one branch. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float ) References \u00b6 Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization to perform split attempts in online tree regressors. Pattern Recognition Letters. \u21a9","title":"QOSplitter"},{"location":"api/tree/splitter/QOSplitter/#qosplitter","text":"Quantization observer (QO). This splitter utilizes a hash-based quantization algorithm to keep track of the target statistics and evaluate split candidates. QO, relies on the radius parameter to define discretization intervals for each incoming feature. Split candidates are defined as the midpoints between two consecutive hash slots. Both binary splits and multi-way splits can be created by this attribute observer. This class implements the algorithm described in 1 . The smaller the quantization radius, the more hash slots will be created to accommodate the discretized data. Hence, both the running time and memory consumption increase, but the resulting splits ought to be closer to the ones obtained by a batch exhaustive approach. On the other hand, if the radius is too large, fewer slots will be created, less memory and running time will be required, but at the cost of coarse split suggestions. QO assumes that all features have the same range. It is always advised to scale the features to apply this splitter. That can be done using the preprocessing module. A good \"rule of thumb\" is to scale data using preprocessing.StandardScaler and define the radius as a proportion of the features' standard deviation. For instance, the default radius value would correspond to one quarter of the normalized features' standard deviation (since the scaled data has zero mean and unit variance). If the features come from normal distributions, by following the empirical rule, roughly 32 hash slots will be created.","title":"QOSplitter"},{"location":"api/tree/splitter/QOSplitter/#parameters","text":"radius ( float ) \u2013 defaults to 0.25 The quantization radius. QO discretizes the incoming feature in intervals of equal length that are defined by this parameter. allow_multiway_splits \u2013 defaults to False Whether or not allow that multiway splits are evaluated. Numeric multi-way splits use the same quantization strategy of QO to create multiple tree branches. The same quantization radius is used, and each stored slot represents the split enabling statistics of one branch.","title":"Parameters"},{"location":"api/tree/splitter/QOSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/QOSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/QOSplitter/#references","text":"Mastelini, S.M. and de Leon Ferreira, A.C.P., 2021. Using dynamical quantization to perform split attempts in online tree regressors. Pattern Recognition Letters. \u21a9","title":"References"},{"location":"api/tree/splitter/Splitter/","text":"Splitter \u00b6 Base class for the tree splitters. Each Attribute Observer (AO) or Splitter monitors one input feature and finds the best split point for this attribute. AOs can also perform other tasks related to the monitored feature, such as estimating its probability density function (classification case). This class should not be instantiated, as none of its methods are implemented. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Splitter"},{"location":"api/tree/splitter/Splitter/#splitter","text":"Base class for the tree splitters. Each Attribute Observer (AO) or Splitter monitors one input feature and finds the best split point for this attribute. AOs can also perform other tasks related to the monitored feature, such as estimating its probability density function (classification case). This class should not be instantiated, as none of its methods are implemented.","title":"Splitter"},{"location":"api/tree/splitter/Splitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/Splitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Get the probability for an attribute value given a class. Parameters att_val target_val ( Union[bool, str, int] ) Returns float : Probability for an attribute value given a class. update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/tree/splitter/TEBSTSplitter/","text":"TEBSTSplitter \u00b6 Truncated E-BST. Variation of E-BST that rounds the incoming feature values before passing them to the binary search tree (BST). By doing so, the attribute observer might reduce its processing time and memory usage since small variations in the input values will end up being mapped to the same BST node. Parameters \u00b6 digits ( int ) \u2013 defaults to 3 The number of decimal places used to round the input feature values. Attributes \u00b6 is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees. Methods \u00b6 best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Not implemented in regression splitters. Parameters att_val target_val ( Union[bool, str, int] ) remove_bad_splits Remove bad splits. Based on FIMT-DD's [^1] procedure to remove bad split candidates from the E-BST. This mechanism is triggered every time a split attempt fails. The rationale is to remove points whose split merit is much worse than the best candidate overall (for which the growth decision already failed). Let \\(m_1\\) be the merit of the best split point and \\(m_2\\) be the merit of the second best split candidate. The ratio \\(r = m_2/m_1\\) along with the Hoeffding bound ( \\(\\epsilon\\) ) are used to decide upon creating a split. A split occurs when \\(r < 1 - \\epsilon\\) . A split candidate, with merit \\(m_i\\) , is considered badr if \\(m_i / m_1 < r - 2\\epsilon\\) . The rationale is the following: if the merit ratio for this point is smaller than the lower bound of \\(r\\) , then the true merit of that split relative to the best one is small. Hence, this candidate can be safely removed. To avoid excessive and costly manipulations of the E-BST to update the stored statistics, only the nodes whose children are all bad split points are pruned, as defined in [^1]. Parameters criterion last_check_ratio ( float ) last_check_vr ( float ) last_check_e ( float ) pre_split_dist ( Union[List, Dict] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"TEBSTSplitter"},{"location":"api/tree/splitter/TEBSTSplitter/#tebstsplitter","text":"Truncated E-BST. Variation of E-BST that rounds the incoming feature values before passing them to the binary search tree (BST). By doing so, the attribute observer might reduce its processing time and memory usage since small variations in the input values will end up being mapped to the same BST node.","title":"TEBSTSplitter"},{"location":"api/tree/splitter/TEBSTSplitter/#parameters","text":"digits ( int ) \u2013 defaults to 3 The number of decimal places used to round the input feature values.","title":"Parameters"},{"location":"api/tree/splitter/TEBSTSplitter/#attributes","text":"is_numeric Determine whether or not the splitter works with numerical features. is_target_class Check on which kind of learning task the splitter is designed to work. If True , the splitter works with classification trees, otherwise it is designed for regression trees.","title":"Attributes"},{"location":"api/tree/splitter/TEBSTSplitter/#methods","text":"best_evaluated_split_suggestion Get the best split suggestion given a criterion and the target's statistics. Parameters criterion ( river.tree.split_criterion.base_split_criterion.SplitCriterion ) pre_split_dist ( Union[List, Dict] ) att_idx ( Hashable ) binary_only ( bool ) \u2013 defaults to True Returns BranchFactory : Suggestion of the best attribute split. clone Return a fresh estimator with the same parameters. The clone has the same parameters but has not been updated with any data. This works by looking at the parameters from the class signature. Each parameter is either - recursively cloned if it's a River classes. - deep-copied via copy.deepcopy if not. If the calling object is stochastic (i.e. it accepts a seed parameter) and has not been seeded, then the clone will not be idempotent. Indeed, this method's purpose if simply to return a new instance with the same input parameters. cond_proba Not implemented in regression splitters. Parameters att_val target_val ( Union[bool, str, int] ) remove_bad_splits Remove bad splits. Based on FIMT-DD's [^1] procedure to remove bad split candidates from the E-BST. This mechanism is triggered every time a split attempt fails. The rationale is to remove points whose split merit is much worse than the best candidate overall (for which the growth decision already failed). Let \\(m_1\\) be the merit of the best split point and \\(m_2\\) be the merit of the second best split candidate. The ratio \\(r = m_2/m_1\\) along with the Hoeffding bound ( \\(\\epsilon\\) ) are used to decide upon creating a split. A split occurs when \\(r < 1 - \\epsilon\\) . A split candidate, with merit \\(m_i\\) , is considered badr if \\(m_i / m_1 < r - 2\\epsilon\\) . The rationale is the following: if the merit ratio for this point is smaller than the lower bound of \\(r\\) , then the true merit of that split relative to the best one is small. Hence, this candidate can be safely removed. To avoid excessive and costly manipulations of the E-BST to update the stored statistics, only the nodes whose children are all bad split points are pruned, as defined in [^1]. Parameters criterion last_check_ratio ( float ) last_check_vr ( float ) last_check_e ( float ) pre_split_dist ( Union[List, Dict] ) update Update statistics of this observer given an attribute value, its target value and the weight of the instance observed. Parameters att_val target_val ( Union[bool, str, int, numbers.Number] ) sample_weight ( float )","title":"Methods"},{"location":"api/utils/Histogram/","text":"Histogram \u00b6 Streaming histogram. Parameters \u00b6 max_bins \u2013 defaults to 256 Maximal number of bins. Attributes \u00b6 n Total number of seen values. Examples \u00b6 >>> from river import utils >>> import matplotlib.pyplot as plt >>> import numpy as np >>> np . random . seed ( 42 ) >>> values = np . hstack (( ... np . random . normal ( - 3 , 1 , 1000 ), ... np . random . normal ( 3 , 1 , 1000 ), ... )) >>> hist = utils . Histogram ( max_bins = 60 ) >>> for x in values : ... hist = hist . update ( x ) >>> ax = plt . bar ( ... x = [( b . left + b . right ) / 2 for b in hist ], ... height = [ b . count for b in hist ], ... width = [( b . right - b . left ) / 2 for b in hist ] ... ) .. image:: ../../docs/img/histogram_docstring.svg :align: center Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters item cdf Cumulative distribution function. Example: >>> from river import utils >>> hist = utils.Histogram() >>> for x in range(4): ... hist = hist.update(x) >>> print(hist) [0.00000, 0.00000]: 1 [1.00000, 1.00000]: 1 [2.00000, 2.00000]: 1 [3.00000, 3.00000]: 1 >>> hist.cdf(-1) 0.0 >>> hist.cdf(0) 0.25 >>> hist.cdf(.5) 0.25 >>> hist.cdf(1) 0.5 >>> hist.cdf(2.5) 0.75 >>> hist.cdf(3.5) 1.0 Parameters x clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item iter_cdf Yields CDF values for a sorted iterable of values. This is faster than calling cdf with many values. Example: >>> from river import utils >>> hist = utils.Histogram() >>> for x in range(4): ... hist = hist.update(x) >>> print(hist) [0.00000, 0.00000]: 1 [1.00000, 1.00000]: 1 [2.00000, 2.00000]: 1 [3.00000, 3.00000]: 1 >>> X = [-1, 0, .5, 1, 2.5, 3.5] >>> for x, cdf in zip(X, hist.iter_cdf(X)): ... print(x, cdf) -1 0.0 0 0.25 0.5 0.25 1 0.5 2.5 0.75 3.5 1.0 Parameters X verbose \u2013 defaults to False pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort update References \u00b6 Ben-Haim, Y. and Tom-Tov, E., 2010. A streaming parallel decision tree algorithm. Journal of Machine Learning Research, 11(Feb), pp.849-872. \u21a9 Go implementation \u21a9","title":"Histogram"},{"location":"api/utils/Histogram/#histogram","text":"Streaming histogram.","title":"Histogram"},{"location":"api/utils/Histogram/#parameters","text":"max_bins \u2013 defaults to 256 Maximal number of bins.","title":"Parameters"},{"location":"api/utils/Histogram/#attributes","text":"n Total number of seen values.","title":"Attributes"},{"location":"api/utils/Histogram/#examples","text":">>> from river import utils >>> import matplotlib.pyplot as plt >>> import numpy as np >>> np . random . seed ( 42 ) >>> values = np . hstack (( ... np . random . normal ( - 3 , 1 , 1000 ), ... np . random . normal ( 3 , 1 , 1000 ), ... )) >>> hist = utils . Histogram ( max_bins = 60 ) >>> for x in values : ... hist = hist . update ( x ) >>> ax = plt . bar ( ... x = [( b . left + b . right ) / 2 for b in hist ], ... height = [ b . count for b in hist ], ... width = [( b . right - b . left ) / 2 for b in hist ] ... ) .. image:: ../../docs/img/histogram_docstring.svg :align: center","title":"Examples"},{"location":"api/utils/Histogram/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters item cdf Cumulative distribution function. Example: >>> from river import utils >>> hist = utils.Histogram() >>> for x in range(4): ... hist = hist.update(x) >>> print(hist) [0.00000, 0.00000]: 1 [1.00000, 1.00000]: 1 [2.00000, 2.00000]: 1 [3.00000, 3.00000]: 1 >>> hist.cdf(-1) 0.0 >>> hist.cdf(0) 0.25 >>> hist.cdf(.5) 0.25 >>> hist.cdf(1) 0.5 >>> hist.cdf(2.5) 0.75 >>> hist.cdf(3.5) 1.0 Parameters x clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item iter_cdf Yields CDF values for a sorted iterable of values. This is faster than calling cdf with many values. Example: >>> from river import utils >>> hist = utils.Histogram() >>> for x in range(4): ... hist = hist.update(x) >>> print(hist) [0.00000, 0.00000]: 1 [1.00000, 1.00000]: 1 [2.00000, 2.00000]: 1 [3.00000, 3.00000]: 1 >>> X = [-1, 0, .5, 1, 2.5, 3.5] >>> for x, cdf in zip(X, hist.iter_cdf(X)): ... print(x, cdf) -1 0.0 0 0.25 0.5 0.25 1 0.5 2.5 0.75 3.5 1.0 Parameters X verbose \u2013 defaults to False pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort update","title":"Methods"},{"location":"api/utils/Histogram/#references","text":"Ben-Haim, Y. and Tom-Tov, E., 2010. A streaming parallel decision tree algorithm. Journal of Machine Learning Research, 11(Feb), pp.849-872. \u21a9 Go implementation \u21a9","title":"References"},{"location":"api/utils/SDFT/","text":"SDFT \u00b6 Sliding Discrete Fourier Transform (SDFT). Initially, the coefficients are all equal to 0, up until enough values have been seen. A call to numpy.fft.fft is triggered once window_size values have been seen. Subsequent values will update the coefficients online. This is much faster than recomputing an FFT from scratch for every new value. Parameters \u00b6 window_size The size of the window. Attributes \u00b6 window ( utils.Window ) The window of values. Examples \u00b6 >>> from river import utils >>> X = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] >>> window_size = 5 >>> sdft = utils . SDFT ( window_size ) >>> for i , x in enumerate ( X ): ... sdft = sdft . update ( x ) ... ... if i + 1 >= window_size : ... assert np . allclose ( sdft , np . fft . fft ( X [ i + 1 - window_size : i + 1 ])) Methods \u00b6 append extend popleft update References \u00b6 Jacobsen, E. and Lyons, R., 2003. The sliding DFT. IEEE Signal Processing Magazine, 20(2), pp.74-80. <https://www.comm.utoronto.ca/~dimitris/ece431/slidingdft.pdf> _ \u21a9 Understanding and Implementing the Sliding DFT <https://www.dsprelated.com/showarticle/776.php> _ \u21a9","title":"SDFT"},{"location":"api/utils/SDFT/#sdft","text":"Sliding Discrete Fourier Transform (SDFT). Initially, the coefficients are all equal to 0, up until enough values have been seen. A call to numpy.fft.fft is triggered once window_size values have been seen. Subsequent values will update the coefficients online. This is much faster than recomputing an FFT from scratch for every new value.","title":"SDFT"},{"location":"api/utils/SDFT/#parameters","text":"window_size The size of the window.","title":"Parameters"},{"location":"api/utils/SDFT/#attributes","text":"window ( utils.Window ) The window of values.","title":"Attributes"},{"location":"api/utils/SDFT/#examples","text":">>> from river import utils >>> X = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] >>> window_size = 5 >>> sdft = utils . SDFT ( window_size ) >>> for i , x in enumerate ( X ): ... sdft = sdft . update ( x ) ... ... if i + 1 >= window_size : ... assert np . allclose ( sdft , np . fft . fft ( X [ i + 1 - window_size : i + 1 ]))","title":"Examples"},{"location":"api/utils/SDFT/#methods","text":"append extend popleft update","title":"Methods"},{"location":"api/utils/SDFT/#references","text":"Jacobsen, E. and Lyons, R., 2003. The sliding DFT. IEEE Signal Processing Magazine, 20(2), pp.74-80. <https://www.comm.utoronto.ca/~dimitris/ece431/slidingdft.pdf> _ \u21a9 Understanding and Implementing the Sliding DFT <https://www.dsprelated.com/showarticle/776.php> _ \u21a9","title":"References"},{"location":"api/utils/Skyline/","text":"Skyline \u00b6 A skyline is set of points which is not dominated by any other point. This implementation uses a block nested loop. Identical observations are all part of the skyline if applicable. Parameters \u00b6 minimize ( list ) \u2013 defaults to None A list of features for which the values need to be minimized. Can be omitted as long as maximize is specified. maximize ( list ) \u2013 defaults to None A list of features for which the values need to be maximized. Can be omitted as long as minimize is specified. Examples \u00b6 Here is an example taken from this blog post. >>> import random >>> from river import utils >>> import matplotlib.pyplot as plt >>> city_prices = { ... 'Bordeaux' : 4045 , ... 'Lyon' : 4547 , ... 'Toulouse' : 3278 ... } >>> def random_house (): ... city = random . choice ([ 'Bordeaux' , 'Lyon' , 'Toulouse' ]) ... size = round ( random . gauss ( 200 , 50 )) ... price = round ( random . uniform ( 0.8 , 1.2 ) * city_prices [ city ] * size ) ... return { 'city' : city , 'size' : size , 'price' : price } >>> skyline = utils . Skyline ( minimize = [ 'price' ], maximize = [ 'size' ]) >>> random . seed ( 42 ) >>> for _ in range ( 100 ): ... house = random_house () ... skyline = skyline . update ( house ) >>> print ( len ( skyline )) 13 >>> print ( skyline [ 0 ]) { 'city' : 'Toulouse' , 'size' : 280 , 'price' : 763202 } >>> fig , ax = plt . subplots () >>> scatter = ax . scatter ( ... x = [ h [ 'size' ] for h in skyline ], ... y = [ h [ 'price' ] for h in skyline ] ... ) >>> grid = ax . grid () >>> title = ax . set_title ( 'Houses skyline' ) >>> xlabel = ax . set_xlabel ( 'Size' ) >>> ylabel = ax . set_ylabel ( 'Price' ) .. image:: ../../docs/img/skyline_docstring.svg :align: center Here is another example using the kart data from Mario Kart: Double Dash!! . >>> import collections >>> from river import utils >>> Kart = collections . namedtuple ( ... 'Kart' , ... 'name speed off_road acceleration weight turbo' ... ) >>> karts = [ ... Kart ( 'Red Fire' , 5 , 4 , 4 , 5 , 2 ), ... Kart ( 'Green Fire' , 7 , 3 , 3 , 4 , 2 ), ... Kart ( 'Heart Coach' , 4 , 6 , 6 , 5 , 2 ), ... Kart ( 'Bloom Coach' , 6 , 4 , 5 , 3 , 2 ), ... Kart ( 'Turbo Yoshi' , 4 , 5 , 6 , 6 , 2 ), ... Kart ( 'Turbo Birdo' , 6 , 4 , 4 , 7 , 2 ), ... Kart ( 'Goo-Goo Buggy' , 1 , 9 , 9 , 2 , 3 ), ... Kart ( 'Rattle Buggy' , 2 , 9 , 8 , 2 , 3 ), ... Kart ( 'Toad Kart' , 3 , 9 , 7 , 2 , 3 ), ... Kart ( 'Toadette Kart' , 1 , 9 , 9 , 2 , 3 ), ... Kart ( 'Koopa Dasher' , 2 , 8 , 8 , 3 , 3 ), ... Kart ( 'Para-Wing' , 1 , 8 , 9 , 3 , 3 ), ... Kart ( 'DK Jumbo' , 8 , 2 , 2 , 8 , 1 ), ... Kart ( 'Barrel Train' , 8 , 7 , 3 , 5 , 3 ), ... Kart ( 'Koopa King' , 9 , 1 , 1 , 9 , 1 ), ... Kart ( 'Bullet Blaster' , 8 , 1 , 4 , 1 , 3 ), ... Kart ( 'Wario Car' , 7 , 3 , 3 , 7 , 1 ), ... Kart ( 'Waluigi Racer' , 5 , 9 , 5 , 6 , 2 ), ... Kart ( 'Piranha Pipes' , 8 , 7 , 2 , 9 , 1 ), ... Kart ( 'Boo Pipes' , 2 , 9 , 8 , 9 , 1 ), ... Kart ( 'Parade Kart' , 7 , 3 , 4 , 7 , 3 ) ... ] >>> skyline = utils . Skyline ( ... maximize = [ 'speed' , 'off_road' , 'acceleration' , 'turbo' ], ... minimize = [ 'weight' ] ... ) >>> for kart in karts : ... skyline = skyline . update ( kart . _asdict ()) >>> best_cart_names = [ kart [ 'name' ] for kart in skyline ] >>> for name in best_cart_names : ... print ( f '- { name } ' ) - Green Fire - Heart Coach - Bloom Coach - Goo - Goo Buggy - Rattle Buggy - Toad Kart - Toadette Kart - Barrel Train - Koopa King - Bullet Blaster - Waluigi Racer - Parade Kart >>> for name in sorted ( set ( kart . name for kart in karts ) - set ( best_cart_names )): ... print ( f '- { name } ' ) - Boo Pipes - DK Jumbo - Koopa Dasher - Para - Wing - Piranha Pipes - Red Fire - Turbo Birdo - Turbo Yoshi - Wario Car Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters item clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort update References \u00b6 Skyline queries in Python \u21a9 Borzsony, S., Kossmann, D. and Stocker, K., 2001, April. The skyline operator. In Proceedings 17th international conference on data engineering (pp. 421-430). IEEE. \u21a9 Tao, Y. and Papadias, D., 2006. Maintaining sliding window skylines on data streams. IEEE Transactions on Knowledge and Data Engineering, 18(3), pp.377-391. \u21a9","title":"Skyline"},{"location":"api/utils/Skyline/#skyline","text":"A skyline is set of points which is not dominated by any other point. This implementation uses a block nested loop. Identical observations are all part of the skyline if applicable.","title":"Skyline"},{"location":"api/utils/Skyline/#parameters","text":"minimize ( list ) \u2013 defaults to None A list of features for which the values need to be minimized. Can be omitted as long as maximize is specified. maximize ( list ) \u2013 defaults to None A list of features for which the values need to be maximized. Can be omitted as long as minimize is specified.","title":"Parameters"},{"location":"api/utils/Skyline/#examples","text":"Here is an example taken from this blog post. >>> import random >>> from river import utils >>> import matplotlib.pyplot as plt >>> city_prices = { ... 'Bordeaux' : 4045 , ... 'Lyon' : 4547 , ... 'Toulouse' : 3278 ... } >>> def random_house (): ... city = random . choice ([ 'Bordeaux' , 'Lyon' , 'Toulouse' ]) ... size = round ( random . gauss ( 200 , 50 )) ... price = round ( random . uniform ( 0.8 , 1.2 ) * city_prices [ city ] * size ) ... return { 'city' : city , 'size' : size , 'price' : price } >>> skyline = utils . Skyline ( minimize = [ 'price' ], maximize = [ 'size' ]) >>> random . seed ( 42 ) >>> for _ in range ( 100 ): ... house = random_house () ... skyline = skyline . update ( house ) >>> print ( len ( skyline )) 13 >>> print ( skyline [ 0 ]) { 'city' : 'Toulouse' , 'size' : 280 , 'price' : 763202 } >>> fig , ax = plt . subplots () >>> scatter = ax . scatter ( ... x = [ h [ 'size' ] for h in skyline ], ... y = [ h [ 'price' ] for h in skyline ] ... ) >>> grid = ax . grid () >>> title = ax . set_title ( 'Houses skyline' ) >>> xlabel = ax . set_xlabel ( 'Size' ) >>> ylabel = ax . set_ylabel ( 'Price' ) .. image:: ../../docs/img/skyline_docstring.svg :align: center Here is another example using the kart data from Mario Kart: Double Dash!! . >>> import collections >>> from river import utils >>> Kart = collections . namedtuple ( ... 'Kart' , ... 'name speed off_road acceleration weight turbo' ... ) >>> karts = [ ... Kart ( 'Red Fire' , 5 , 4 , 4 , 5 , 2 ), ... Kart ( 'Green Fire' , 7 , 3 , 3 , 4 , 2 ), ... Kart ( 'Heart Coach' , 4 , 6 , 6 , 5 , 2 ), ... Kart ( 'Bloom Coach' , 6 , 4 , 5 , 3 , 2 ), ... Kart ( 'Turbo Yoshi' , 4 , 5 , 6 , 6 , 2 ), ... Kart ( 'Turbo Birdo' , 6 , 4 , 4 , 7 , 2 ), ... Kart ( 'Goo-Goo Buggy' , 1 , 9 , 9 , 2 , 3 ), ... Kart ( 'Rattle Buggy' , 2 , 9 , 8 , 2 , 3 ), ... Kart ( 'Toad Kart' , 3 , 9 , 7 , 2 , 3 ), ... Kart ( 'Toadette Kart' , 1 , 9 , 9 , 2 , 3 ), ... Kart ( 'Koopa Dasher' , 2 , 8 , 8 , 3 , 3 ), ... Kart ( 'Para-Wing' , 1 , 8 , 9 , 3 , 3 ), ... Kart ( 'DK Jumbo' , 8 , 2 , 2 , 8 , 1 ), ... Kart ( 'Barrel Train' , 8 , 7 , 3 , 5 , 3 ), ... Kart ( 'Koopa King' , 9 , 1 , 1 , 9 , 1 ), ... Kart ( 'Bullet Blaster' , 8 , 1 , 4 , 1 , 3 ), ... Kart ( 'Wario Car' , 7 , 3 , 3 , 7 , 1 ), ... Kart ( 'Waluigi Racer' , 5 , 9 , 5 , 6 , 2 ), ... Kart ( 'Piranha Pipes' , 8 , 7 , 2 , 9 , 1 ), ... Kart ( 'Boo Pipes' , 2 , 9 , 8 , 9 , 1 ), ... Kart ( 'Parade Kart' , 7 , 3 , 4 , 7 , 3 ) ... ] >>> skyline = utils . Skyline ( ... maximize = [ 'speed' , 'off_road' , 'acceleration' , 'turbo' ], ... minimize = [ 'weight' ] ... ) >>> for kart in karts : ... skyline = skyline . update ( kart . _asdict ()) >>> best_cart_names = [ kart [ 'name' ] for kart in skyline ] >>> for name in best_cart_names : ... print ( f '- { name } ' ) - Green Fire - Heart Coach - Bloom Coach - Goo - Goo Buggy - Rattle Buggy - Toad Kart - Toadette Kart - Barrel Train - Koopa King - Bullet Blaster - Waluigi Racer - Parade Kart >>> for name in sorted ( set ( kart . name for kart in karts ) - set ( best_cart_names )): ... print ( f '- { name } ' ) - Boo Pipes - DK Jumbo - Koopa Dasher - Para - Wing - Piranha Pipes - Red Fire - Turbo Birdo - Turbo Yoshi - Wario Car","title":"Examples"},{"location":"api/utils/Skyline/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters item clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort update","title":"Methods"},{"location":"api/utils/Skyline/#references","text":"Skyline queries in Python \u21a9 Borzsony, S., Kossmann, D. and Stocker, K., 2001, April. The skyline operator. In Proceedings 17th international conference on data engineering (pp. 421-430). IEEE. \u21a9 Tao, Y. and Papadias, D., 2006. Maintaining sliding window skylines on data streams. IEEE Transactions on Knowledge and Data Engineering, 18(3), pp.377-391. \u21a9","title":"References"},{"location":"api/utils/SortedWindow/","text":"SortedWindow \u00b6 Sorted running window data structure. Parameters \u00b6 size ( int ) Size of the window to compute the rolling quantile. Attributes \u00b6 size Examples \u00b6 >>> from river import utils >>> window = utils . SortedWindow ( size = 3 ) >>> for i in reversed ( range ( 9 )): ... print ( window . append ( i )) [ 8 ] [ 7 , 8 ] [ 6 , 7 , 8 ] [ 5 , 6 , 7 ] [ 4 , 5 , 6 ] [ 3 , 4 , 5 ] [ 2 , 3 , 4 ] [ 1 , 2 , 3 ] [ 0 , 1 , 2 ] Methods \u00b6 append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort References \u00b6 Left sorted inserts in Python \u21a9","title":"SortedWindow"},{"location":"api/utils/SortedWindow/#sortedwindow","text":"Sorted running window data structure.","title":"SortedWindow"},{"location":"api/utils/SortedWindow/#parameters","text":"size ( int ) Size of the window to compute the rolling quantile.","title":"Parameters"},{"location":"api/utils/SortedWindow/#attributes","text":"size","title":"Attributes"},{"location":"api/utils/SortedWindow/#examples","text":">>> from river import utils >>> window = utils . SortedWindow ( size = 3 ) >>> for i in reversed ( range ( 9 )): ... print ( window . append ( i )) [ 8 ] [ 7 , 8 ] [ 6 , 7 , 8 ] [ 5 , 6 , 7 ] [ 4 , 5 , 6 ] [ 3 , 4 , 5 ] [ 2 , 3 , 4 ] [ 1 , 2 , 3 ] [ 0 , 1 , 2 ]","title":"Examples"},{"location":"api/utils/SortedWindow/#methods","text":"append S.append(value) -- append value to the end of the sequence Parameters x clear S.clear() -> None -- remove all items from S copy count S.count(value) -> integer -- return number of occurrences of value Parameters item extend S.extend(iterable) -- extend sequence by appending elements from the iterable Parameters other index S.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. Supporting start and stop arguments is optional, but recommended. Parameters item args insert S.insert(index, value) -- insert value before index Parameters i item pop S.pop([index]) -> item -- remove and return item at index (default last). Raise IndexError if list is empty or index is out of range. Parameters i \u2013 defaults to -1 remove S.remove(value) -- remove first occurrence of value. Raise ValueError if the value is not present. Parameters item reverse S.reverse() -- reverse IN PLACE sort","title":"Methods"},{"location":"api/utils/SortedWindow/#references","text":"Left sorted inserts in Python \u21a9","title":"References"},{"location":"api/utils/VectorDict/","text":"VectorDict \u00b6 Methods \u00b6 abs clear get Parameters key args kwargs items keys max maximum Parameters other min minimum Parameters other pop Parameters args kwargs popitem setdefault Parameters key args kwargs to_dict to_numpy Parameters fields update Parameters args kwargs values with_mask Parameters mask copy","title":"VectorDict"},{"location":"api/utils/VectorDict/#vectordict","text":"","title":"VectorDict"},{"location":"api/utils/VectorDict/#methods","text":"abs clear get Parameters key args kwargs items keys max maximum Parameters other min minimum Parameters other pop Parameters args kwargs popitem setdefault Parameters key args kwargs to_dict to_numpy Parameters fields update Parameters args kwargs values with_mask Parameters mask copy","title":"Methods"},{"location":"api/utils/Window/","text":"Window \u00b6 Running window data structure. This is just a convenience layer on top of a collections.deque . The only reason this exists is that deepcopying a class which inherits from collections.deque seems to bug out when the class has a parameter with no default value. Parameters \u00b6 size ( int ) Size of the rolling window. Attributes \u00b6 size Examples \u00b6 >>> from river import utils >>> window = utils . Window ( size = 2 ) >>> for x in [ 1 , 2 , 3 , 4 , 5 , 6 ]: ... print ( window . append ( x )) [ 1 ] [ 1 , 2 ] [ 2 , 3 ] [ 3 , 4 ] [ 4 , 5 ] [ 5 , 6 ] Methods \u00b6 append extend popleft","title":"Window"},{"location":"api/utils/Window/#window","text":"Running window data structure. This is just a convenience layer on top of a collections.deque . The only reason this exists is that deepcopying a class which inherits from collections.deque seems to bug out when the class has a parameter with no default value.","title":"Window"},{"location":"api/utils/Window/#parameters","text":"size ( int ) Size of the rolling window.","title":"Parameters"},{"location":"api/utils/Window/#attributes","text":"size","title":"Attributes"},{"location":"api/utils/Window/#examples","text":">>> from river import utils >>> window = utils . Window ( size = 2 ) >>> for x in [ 1 , 2 , 3 , 4 , 5 , 6 ]: ... print ( window . append ( x )) [ 1 ] [ 1 , 2 ] [ 2 , 3 ] [ 3 , 4 ] [ 4 , 5 ] [ 5 , 6 ]","title":"Examples"},{"location":"api/utils/Window/#methods","text":"append extend popleft","title":"Methods"},{"location":"api/utils/check-estimator/","text":"check_estimator \u00b6 Check if a model adheres to river 's conventions. This will run a series of unit tests. The nature of the unit tests depends on the type of model. Parameters \u00b6 model","title":"check_estimator"},{"location":"api/utils/check-estimator/#check_estimator","text":"Check if a model adheres to river 's conventions. This will run a series of unit tests. The nature of the unit tests depends on the type of model.","title":"check_estimator"},{"location":"api/utils/check-estimator/#parameters","text":"model","title":"Parameters"},{"location":"api/utils/dict2numpy/","text":"dict2numpy \u00b6 Convert a dictionary containing data to a numpy array. There is not restriction to the type of keys in data , but values must be strictly numeric. To make sure random permutations of the features do not impact on the learning algorithms, keys are first converted to strings and then sorted prior to the conversion. Parameters \u00b6 data A dictionary whose keys represent input attributes and the values represent their observed contents. Examples \u00b6 >>> from river.utils import dict2numpy >>> dict2numpy ({ 'a' : 1 , 'b' : 2 , 3 : 3 }) array ([ 3 , 1 , 2 ])","title":"dict2numpy"},{"location":"api/utils/dict2numpy/#dict2numpy","text":"Convert a dictionary containing data to a numpy array. There is not restriction to the type of keys in data , but values must be strictly numeric. To make sure random permutations of the features do not impact on the learning algorithms, keys are first converted to strings and then sorted prior to the conversion.","title":"dict2numpy"},{"location":"api/utils/dict2numpy/#parameters","text":"data A dictionary whose keys represent input attributes and the values represent their observed contents.","title":"Parameters"},{"location":"api/utils/dict2numpy/#examples","text":">>> from river.utils import dict2numpy >>> dict2numpy ({ 'a' : 1 , 'b' : 2 , 3 : 3 }) array ([ 3 , 1 , 2 ])","title":"Examples"},{"location":"api/utils/expand-param-grid/","text":"expand_param_grid \u00b6 Expands a grid of parameters. This method can be used to generate a list of model parametrizations from a dictionary where each parameter is associated with a list of possible parameters. In other words, it expands a grid of parameters. Typically, this method can be used to create copies of a given model with different parameter choices. The models can then be used as part of a model selection process, such as a expert.SuccessiveHalvingClassifier or a expert.EWARegressor . The syntax for the parameter grid is quite flexible. It allows nesting parameters and can therefore be used to generate parameters for a pipeline. Parameters \u00b6 model ( base.Estimator ) grid ( dict ) The grid of parameters to expand. The provided dictionary can be nested. The only requirement is that the values at the leaves need to be lists. Examples \u00b6 As an initial example, we can expand a grid of parameters for a single model. >>> from river import linear_model >>> from river import optim >>> from river import utils >>> model = linear_model . LinearRegression () >>> grid = { 'optimizer' : [ optim . SGD ( .1 ), optim . SGD ( .01 ), optim . SGD ( .001 )]} >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 3 >>> models [ 0 ] LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.1 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) You can expand parameters for multiple choices like so: >>> grid = { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'lr' : [ .1 , .01 , .01 ]}) ... ] ... } >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 6 You may specify a grid of parameters for a pipeline via nesting: >>> from river import feature_extraction >>> model = ( ... feature_extraction . BagOfWords () | ... linear_model . LinearRegression () ... ) >>> grid = { ... 'BagOfWords' : { ... 'strip_accents' : [ False , True ] ... }, ... 'LinearRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 ]}), ... ( optim . Adam , { 'lr' : [ .1 , .01 ]}) ... ] ... } ... } >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 8","title":"expand_param_grid"},{"location":"api/utils/expand-param-grid/#expand_param_grid","text":"Expands a grid of parameters. This method can be used to generate a list of model parametrizations from a dictionary where each parameter is associated with a list of possible parameters. In other words, it expands a grid of parameters. Typically, this method can be used to create copies of a given model with different parameter choices. The models can then be used as part of a model selection process, such as a expert.SuccessiveHalvingClassifier or a expert.EWARegressor . The syntax for the parameter grid is quite flexible. It allows nesting parameters and can therefore be used to generate parameters for a pipeline.","title":"expand_param_grid"},{"location":"api/utils/expand-param-grid/#parameters","text":"model ( base.Estimator ) grid ( dict ) The grid of parameters to expand. The provided dictionary can be nested. The only requirement is that the values at the leaves need to be lists.","title":"Parameters"},{"location":"api/utils/expand-param-grid/#examples","text":"As an initial example, we can expand a grid of parameters for a single model. >>> from river import linear_model >>> from river import optim >>> from river import utils >>> model = linear_model . LinearRegression () >>> grid = { 'optimizer' : [ optim . SGD ( .1 ), optim . SGD ( .01 ), optim . SGD ( .001 )]} >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 3 >>> models [ 0 ] LinearRegression ( optimizer = SGD ( lr = Constant ( learning_rate = 0.1 ) ) loss = Squared () l2 = 0. intercept_init = 0. intercept_lr = Constant ( learning_rate = 0.01 ) clip_gradient = 1e+12 initializer = Zeros () ) You can expand parameters for multiple choices like so: >>> grid = { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 , .001 ]}), ... ( optim . Adam , { 'lr' : [ .1 , .01 , .01 ]}) ... ] ... } >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 6 You may specify a grid of parameters for a pipeline via nesting: >>> from river import feature_extraction >>> model = ( ... feature_extraction . BagOfWords () | ... linear_model . LinearRegression () ... ) >>> grid = { ... 'BagOfWords' : { ... 'strip_accents' : [ False , True ] ... }, ... 'LinearRegression' : { ... 'optimizer' : [ ... ( optim . SGD , { 'lr' : [ .1 , .01 ]}), ... ( optim . Adam , { 'lr' : [ .1 , .01 ]}) ... ] ... } ... } >>> models = utils . expand_param_grid ( model , grid ) >>> len ( models ) 8","title":"Examples"},{"location":"api/utils/numpy2dict/","text":"numpy2dict \u00b6 Convert a numpy array to a dictionary. Parameters \u00b6 data ( numpy.ndarray ) An one-dimensional numpy.array. Examples \u00b6 >>> import numpy as np >>> from river.utils import numpy2dict >>> numpy2dict ( np . array ([ 1.0 , 2.0 , 3.0 ])) { 0 : 1.0 , 1 : 2.0 , 2 : 3.0 }","title":"numpy2dict"},{"location":"api/utils/numpy2dict/#numpy2dict","text":"Convert a numpy array to a dictionary.","title":"numpy2dict"},{"location":"api/utils/numpy2dict/#parameters","text":"data ( numpy.ndarray ) An one-dimensional numpy.array.","title":"Parameters"},{"location":"api/utils/numpy2dict/#examples","text":">>> import numpy as np >>> from river.utils import numpy2dict >>> numpy2dict ( np . array ([ 1.0 , 2.0 , 3.0 ])) { 0 : 1.0 , 1 : 2.0 , 2 : 3.0 }","title":"Examples"},{"location":"api/utils/math/argmax/","text":"argmax \u00b6 Argmax function. Parameters \u00b6 lst ( list )","title":"argmax"},{"location":"api/utils/math/argmax/#argmax","text":"Argmax function.","title":"argmax"},{"location":"api/utils/math/argmax/#parameters","text":"lst ( list )","title":"Parameters"},{"location":"api/utils/math/chain-dot/","text":"chain_dot \u00b6 Returns the dot product of multiple vectors represented as dicts. Parameters \u00b6 xs Examples \u00b6 >>> from river import utils >>> x = { 'x0' : 1 , 'x1' : 2 , 'x2' : 1 } >>> y = { 'x1' : 21 , 'x2' : 3 } >>> z = { 'x1' : 2 , 'x2' : 1 / 3 } >>> utils . math . chain_dot ( x , y , z ) 85.0","title":"chain_dot"},{"location":"api/utils/math/chain-dot/#chain_dot","text":"Returns the dot product of multiple vectors represented as dicts.","title":"chain_dot"},{"location":"api/utils/math/chain-dot/#parameters","text":"xs","title":"Parameters"},{"location":"api/utils/math/chain-dot/#examples","text":">>> from river import utils >>> x = { 'x0' : 1 , 'x1' : 2 , 'x2' : 1 } >>> y = { 'x1' : 21 , 'x2' : 3 } >>> z = { 'x1' : 2 , 'x2' : 1 / 3 } >>> utils . math . chain_dot ( x , y , z ) 85.0","title":"Examples"},{"location":"api/utils/math/clamp/","text":"clamp \u00b6 Clamp a number. This is a synonym of clipping. Parameters \u00b6 x ( float ) minimum \u2013 defaults to 0.0 maximum \u2013 defaults to 1.0","title":"clamp"},{"location":"api/utils/math/clamp/#clamp","text":"Clamp a number. This is a synonym of clipping.","title":"clamp"},{"location":"api/utils/math/clamp/#parameters","text":"x ( float ) minimum \u2013 defaults to 0.0 maximum \u2013 defaults to 1.0","title":"Parameters"},{"location":"api/utils/math/dot/","text":"dot \u00b6 Returns the dot product of two vectors represented as dicts. Parameters \u00b6 x ( dict ) y ( dict ) Examples \u00b6 >>> from river import utils >>> x = { 'x0' : 1 , 'x1' : 2 } >>> y = { 'x1' : 21 , 'x2' : 3 } >>> utils . math . dot ( x , y ) 42","title":"dot"},{"location":"api/utils/math/dot/#dot","text":"Returns the dot product of two vectors represented as dicts.","title":"dot"},{"location":"api/utils/math/dot/#parameters","text":"x ( dict ) y ( dict )","title":"Parameters"},{"location":"api/utils/math/dot/#examples","text":">>> from river import utils >>> x = { 'x0' : 1 , 'x1' : 2 } >>> y = { 'x1' : 21 , 'x2' : 3 } >>> utils . math . dot ( x , y ) 42","title":"Examples"},{"location":"api/utils/math/dotvecmat/","text":"dotvecmat \u00b6 Vectors times matrix. Parameters \u00b6 x A","title":"dotvecmat"},{"location":"api/utils/math/dotvecmat/#dotvecmat","text":"Vectors times matrix.","title":"dotvecmat"},{"location":"api/utils/math/dotvecmat/#parameters","text":"x A","title":"Parameters"},{"location":"api/utils/math/matmul2d/","text":"matmul2d \u00b6 Multiplication for 2D matrices. Parameters \u00b6 A B Examples \u00b6 >>> import pprint >>> from river import utils >>> A = { ... ( 0 , 0 ): 2 , ( 0 , 1 ): 0 , ( 0 , 2 ): 4 , ... ( 1 , 0 ): 5 , ( 1 , 1 ): 6 , ( 1 , 2 ): 0 ... } >>> B = { ... ( 0 , 0 ): 1 , ( 0 , 1 ): 1 , ( 0 , 2 ): 0 , ( 0 , 3 ): 0 , ... ( 1 , 0 ): 2 , ( 1 , 1 ): 0 , ( 1 , 2 ): 1 , ( 1 , 3 ): 3 , ... ( 2 , 0 ): 4 , ( 2 , 1 ): 0 , ( 2 , 2 ): 0 , ( 2 , 3 ): 0 ... } >>> C = matmul2d ( A , B ) >>> pprint . pprint ( C ) {( 0 , 0 ): 18.0 , ( 0 , 1 ): 2.0 , ( 0 , 2 ): 0.0 , ( 0 , 3 ): 0.0 , ( 1 , 0 ): 17.0 , ( 1 , 1 ): 5.0 , ( 1 , 2 ): 6.0 , ( 1 , 3 ): 18.0 }","title":"matmul2d"},{"location":"api/utils/math/matmul2d/#matmul2d","text":"Multiplication for 2D matrices.","title":"matmul2d"},{"location":"api/utils/math/matmul2d/#parameters","text":"A B","title":"Parameters"},{"location":"api/utils/math/matmul2d/#examples","text":">>> import pprint >>> from river import utils >>> A = { ... ( 0 , 0 ): 2 , ( 0 , 1 ): 0 , ( 0 , 2 ): 4 , ... ( 1 , 0 ): 5 , ( 1 , 1 ): 6 , ( 1 , 2 ): 0 ... } >>> B = { ... ( 0 , 0 ): 1 , ( 0 , 1 ): 1 , ( 0 , 2 ): 0 , ( 0 , 3 ): 0 , ... ( 1 , 0 ): 2 , ( 1 , 1 ): 0 , ( 1 , 2 ): 1 , ( 1 , 3 ): 3 , ... ( 2 , 0 ): 4 , ( 2 , 1 ): 0 , ( 2 , 2 ): 0 , ( 2 , 3 ): 0 ... } >>> C = matmul2d ( A , B ) >>> pprint . pprint ( C ) {( 0 , 0 ): 18.0 , ( 0 , 1 ): 2.0 , ( 0 , 2 ): 0.0 , ( 0 , 3 ): 0.0 , ( 1 , 0 ): 17.0 , ( 1 , 1 ): 5.0 , ( 1 , 2 ): 6.0 , ( 1 , 3 ): 18.0 }","title":"Examples"},{"location":"api/utils/math/minkowski-distance/","text":"minkowski_distance \u00b6 Minkowski distance. Parameters \u00b6 a ( dict ) b ( dict ) p ( int ) Parameter for the Minkowski distance. When p=1 , this is equivalent to using the Manhattan distance. When p=2 , this is equivalent to using the Euclidean distance.","title":"minkowski_distance"},{"location":"api/utils/math/minkowski-distance/#minkowski_distance","text":"Minkowski distance.","title":"minkowski_distance"},{"location":"api/utils/math/minkowski-distance/#parameters","text":"a ( dict ) b ( dict ) p ( int ) Parameter for the Minkowski distance. When p=1 , this is equivalent to using the Manhattan distance. When p=2 , this is equivalent to using the Euclidean distance.","title":"Parameters"},{"location":"api/utils/math/norm/","text":"norm \u00b6 Compute the norm of a dictionaries values. Parameters \u00b6 x ( dict ) order \u2013 defaults to None","title":"norm"},{"location":"api/utils/math/norm/#norm","text":"Compute the norm of a dictionaries values.","title":"norm"},{"location":"api/utils/math/norm/#parameters","text":"x ( dict ) order \u2013 defaults to None","title":"Parameters"},{"location":"api/utils/math/outer/","text":"outer \u00b6 Outer-product between two vectors. Parameters \u00b6 u ( dict ) v ( dict ) Examples \u00b6 >>> import pprint >>> from river import utils >>> u = dict ( enumerate (( 1 , 2 , 3 ))) >>> v = dict ( enumerate (( 2 , 4 , 8 ))) >>> uTv = utils . math . outer ( u , v ) >>> pprint . pprint ( uTv ) {( 0 , 0 ): 2 , ( 0 , 1 ): 4 , ( 0 , 2 ): 8 , ( 1 , 0 ): 4 , ( 1 , 1 ): 8 , ( 1 , 2 ): 16 , ( 2 , 0 ): 6 , ( 2 , 1 ): 12 , ( 2 , 2 ): 24 }","title":"outer"},{"location":"api/utils/math/outer/#outer","text":"Outer-product between two vectors.","title":"outer"},{"location":"api/utils/math/outer/#parameters","text":"u ( dict ) v ( dict )","title":"Parameters"},{"location":"api/utils/math/outer/#examples","text":">>> import pprint >>> from river import utils >>> u = dict ( enumerate (( 1 , 2 , 3 ))) >>> v = dict ( enumerate (( 2 , 4 , 8 ))) >>> uTv = utils . math . outer ( u , v ) >>> pprint . pprint ( uTv ) {( 0 , 0 ): 2 , ( 0 , 1 ): 4 , ( 0 , 2 ): 8 , ( 1 , 0 ): 4 , ( 1 , 1 ): 8 , ( 1 , 2 ): 16 , ( 2 , 0 ): 6 , ( 2 , 1 ): 12 , ( 2 , 2 ): 24 }","title":"Examples"},{"location":"api/utils/math/prod/","text":"prod \u00b6 Product function. Parameters \u00b6 iterable","title":"prod"},{"location":"api/utils/math/prod/#prod","text":"Product function.","title":"prod"},{"location":"api/utils/math/prod/#parameters","text":"iterable","title":"Parameters"},{"location":"api/utils/math/sherman-morrison/","text":"sherman_morrison \u00b6 Sherman\u2013Morrison formula. This modifies A_inv inplace. Parameters \u00b6 A_inv ( dict ) u ( dict ) v ( dict ) Examples \u00b6 >>> import pprint >>> from river import utils >>> A_inv = { ... ( 0 , 0 ): 0.2 , ... ( 1 , 1 ): 1 , ... ( 2 , 2 ): 1 ... } >>> u = { 0 : 1 , 1 : 2 , 2 : 3 } >>> v = { 0 : 4 } >>> inv = sherman_morrison ( A_inv , u , v ) >>> pprint . pprint ( inv ) {( 0 , 0 ): 0.111111 , ( 1 , 0 ): - 0.888888 , ( 1 , 1 ): 1 , ( 2 , 0 ): - 1.333333 , ( 2 , 2 ): 1 } References \u00b6 Wikipedia article on the Sherman-Morrison formula s \u21a9","title":"sherman_morrison"},{"location":"api/utils/math/sherman-morrison/#sherman_morrison","text":"Sherman\u2013Morrison formula. This modifies A_inv inplace.","title":"sherman_morrison"},{"location":"api/utils/math/sherman-morrison/#parameters","text":"A_inv ( dict ) u ( dict ) v ( dict )","title":"Parameters"},{"location":"api/utils/math/sherman-morrison/#examples","text":">>> import pprint >>> from river import utils >>> A_inv = { ... ( 0 , 0 ): 0.2 , ... ( 1 , 1 ): 1 , ... ( 2 , 2 ): 1 ... } >>> u = { 0 : 1 , 1 : 2 , 2 : 3 } >>> v = { 0 : 4 } >>> inv = sherman_morrison ( A_inv , u , v ) >>> pprint . pprint ( inv ) {( 0 , 0 ): 0.111111 , ( 1 , 0 ): - 0.888888 , ( 1 , 1 ): 1 , ( 2 , 0 ): - 1.333333 , ( 2 , 2 ): 1 }","title":"Examples"},{"location":"api/utils/math/sherman-morrison/#references","text":"Wikipedia article on the Sherman-Morrison formula s \u21a9","title":"References"},{"location":"api/utils/math/sigmoid/","text":"sigmoid \u00b6 Sigmoid function. Parameters \u00b6 x ( float )","title":"sigmoid"},{"location":"api/utils/math/sigmoid/#sigmoid","text":"Sigmoid function.","title":"sigmoid"},{"location":"api/utils/math/sigmoid/#parameters","text":"x ( float )","title":"Parameters"},{"location":"api/utils/math/sign/","text":"sign \u00b6 Sign function. Parameters \u00b6 x ( float )","title":"sign"},{"location":"api/utils/math/sign/#sign","text":"Sign function.","title":"sign"},{"location":"api/utils/math/sign/#parameters","text":"x ( float )","title":"Parameters"},{"location":"api/utils/math/softmax/","text":"softmax \u00b6 Normalizes a dictionary of predicted probabilities, in-place. Parameters \u00b6 y_pred ( dict )","title":"softmax"},{"location":"api/utils/math/softmax/#softmax","text":"Normalizes a dictionary of predicted probabilities, in-place.","title":"softmax"},{"location":"api/utils/math/softmax/#parameters","text":"y_pred ( dict )","title":"Parameters"},{"location":"api/utils/pretty/humanize-bytes/","text":"humanize_bytes \u00b6 Returns a human-friendly byte size. Parameters \u00b6 n_bytes ( int )","title":"humanize_bytes"},{"location":"api/utils/pretty/humanize-bytes/#humanize_bytes","text":"Returns a human-friendly byte size.","title":"humanize_bytes"},{"location":"api/utils/pretty/humanize-bytes/#parameters","text":"n_bytes ( int )","title":"Parameters"},{"location":"api/utils/pretty/print-table/","text":"print_table \u00b6 Pretty-prints a table. Parameters \u00b6 headers ( List[str] ) The column names. columns ( List[List[str]] ) The column values. order ( List[int] ) \u2013 defaults to None Order in which to print the column the values. Defaults to the order in which the values are given.","title":"print_table"},{"location":"api/utils/pretty/print-table/#print_table","text":"Pretty-prints a table.","title":"print_table"},{"location":"api/utils/pretty/print-table/#parameters","text":"headers ( List[str] ) The column names. columns ( List[List[str]] ) The column values. order ( List[int] ) \u2013 defaults to None Order in which to print the column the values. Defaults to the order in which the values are given.","title":"Parameters"},{"location":"examples/batch-to-online/","text":"From batch to online/stream \u00b6 A quick overview of batch learning \u00b6 If you've already delved into machine learning, then you shouldn't have any difficulty in getting to use incremental learning. If you are somewhat new to machine learning, then do not worry! The point of this notebook in particular is to introduce simple notions. We'll also start to show how river fits in and explain how to use it. The whole point of machine learning is to learn from data . In supervised learning you want to learn how to predict a target \\(y\\) given a set of features \\(X\\) . Meanwhile in an unsupervised learning there is no target, and the goal is rather to identify patterns and trends in the features \\(X\\) . At this point most people tend to imagine \\(X\\) as a somewhat big table where each row is an observation and each column is a feature, and they would be quite right. Learning from tabular data is part of what's called batch learning , which basically that all of the data is available to our learning algorithm at once. Multiple libraries have been created to handle the batch learning regime, with one of the most prominent being Python's scikit-learn . As a simple example of batch learning let's say we want to learn to predict if a women has breast cancer or not. We'll use the breast cancer dataset available with scikit-learn . We'll learn to map a set of features to a binary decision using a logistic regression . Like many other models based on numerical weights, logistic regression is sensitive to the scale of the features. Rescaling the data so that each feature has mean 0 and variance 1 is generally considered good practice. We can apply the rescaling and fit the logistic regression sequentially in an elegant manner using a Pipeline . To measure the performance of the model we'll evaluate the average ROC AUC score using a 5 fold cross-validation . from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn import pipeline from sklearn import preprocessing # Load the data dataset = datasets . load_breast_cancer () X , y = dataset . data , dataset . target # Define the steps of the model model = pipeline . Pipeline ([ ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LogisticRegression ( solver = 'lbfgs' )) ]) # Define a determistic cross-validation procedure cv = model_selection . KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Compute the MSE values scorer = metrics . make_scorer ( metrics . roc_auc_score ) scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.975 (\u00b1 0.011) This might be a lot to take in if you're not accustomed to scikit-learn, but it probably isn't if you are. Batch learning basically boils down to: Loading (and preprocessing) the data Fitting a model to the data Computing the performance of the model on unseen data This is pretty standard and is maybe how most people imagine a machine learning pipeline. However, this way of proceeding has certain downsides. First of all your laptop would crash if the load_boston function returned a dataset who's size exceeds your available amount of RAM. Sometimes you can use some tricks to get around this. For example by optimizing the data types and by using sparse representations when applicable you can potentially save precious gigabytes of RAM. However, like many tricks this only goes so far. If your dataset weighs hundreds of gigabytes then you won't go far without some special hardware. One solution is to do out-of-core learning; that is, algorithms that can learn by being presented the data in chunks or mini-batches. If you want to go down this road then take a look at Dask and Spark's MLlib . Another issue with the batch learning regime is that it can't elegantly learn from new data. Indeed if new data is made available, then the model has to learn from scratch with a new dataset composed of the old data and the new data. This is particularly annoying in a real situation where you might have new incoming data every week, day, hour, minute, or even setting. For example if you're building a recommendation engine for an e-commerce app, then you're probably training your model from 0 every week or so. As your app grows in popularity, so does the dataset you're training on. This will lead to longer and longer training times and might require a hardware upgrade. A final downside that isn't very easy to grasp concerns the manner in which features are extracted. Every time you want to train your model you first have to extract features. The trick is that some features might not be accessible at the particular point in time you are at. For example maybe that some attributes in your data warehouse get overwritten with time. In other words maybe that all the features pertaining to a particular observations are not available, whereas they were a week ago. This happens more often than not in real scenarios, and apart if you have a sophisticated data engineering pipeline then you will encounter these issues at some point. A hands-on introduction to incremental learning \u00b6 Incremental learning is also often called online learning or stream learning , but if you google online learning a lot of the results will point to educational websites. Hence, the terms \"incremental learning\" and \"stream learning\" (from which river derives it's name) are prefered. The point of incremental learning is to fit a model to a stream of data. In other words, the data isn't available in it's entirety, but rather the observations are provided one by one. As an example let's stream through the dataset used previously. for xi , yi in zip ( X , y ): # This is where the model learns pass In this case we're iterating over a dataset that is already in memory, but we could just as well stream from a CSV file, a Kafka stream, an SQL query, etc. If we look at xi we can notice that it is a numpy.ndarray . xi array([7.760e+00, 2.454e+01, 4.792e+01, 1.810e+02, 5.263e-02, 4.362e-02, 0.000e+00, 0.000e+00, 1.587e-01, 5.884e-02, 3.857e-01, 1.428e+00, 2.548e+00, 1.915e+01, 7.189e-03, 4.660e-03, 0.000e+00, 0.000e+00, 2.676e-02, 2.783e-03, 9.456e+00, 3.037e+01, 5.916e+01, 2.686e+02, 8.996e-02, 6.444e-02, 0.000e+00, 0.000e+00, 2.871e-01, 7.039e-02]) river by design works with dict s. We believe that dict s are more enjoyable to program with than numpy.ndarray s, at least for when single observations are concerned. dict 's bring the added benefit that each feature can be accessed by name rather than by position. for xi , yi in zip ( X , y ): xi = dict ( zip ( dataset . feature_names , xi )) pass xi {'mean radius': 7.76, 'mean texture': 24.54, 'mean perimeter': 47.92, 'mean area': 181.0, 'mean smoothness': 0.05263, 'mean compactness': 0.04362, 'mean concavity': 0.0, 'mean concave points': 0.0, 'mean symmetry': 0.1587, 'mean fractal dimension': 0.05884, 'radius error': 0.3857, 'texture error': 1.428, 'perimeter error': 2.548, 'area error': 19.15, 'smoothness error': 0.007189, 'compactness error': 0.00466, 'concavity error': 0.0, 'concave points error': 0.0, 'symmetry error': 0.02676, 'fractal dimension error': 0.002783, 'worst radius': 9.456, 'worst texture': 30.37, 'worst perimeter': 59.16, 'worst area': 268.6, 'worst smoothness': 0.08996, 'worst compactness': 0.06444, 'worst concavity': 0.0, 'worst concave points': 0.0, 'worst symmetry': 0.2871, 'worst fractal dimension': 0.07039} Conveniently, river 's stream module has an iter_sklearn_dataset method that we can use instead. from river import stream for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): pass The simple fact that we are getting the data as a stream means that we can't do a lot of things the same way as in a batch setting. For example let's say we want to scale the data so that it has mean 0 and variance 1, as we did earlier. To do so we simply have to subtract the mean of each feature to each value and then divide the result by the standard deviation of the feature. The problem is that we can't possible known the values of the mean and the standard deviation before actually going through all the data! One way to proceed would be to do a first pass over the data to compute the necessary values and then scale the values during a second pass. The problem is that this defeats our purpose, which is to learn by only looking at the data once. Although this might seem rather restrictive, it reaps sizable benefits down the road. The way we do feature scaling in river involves computing running statistics (also know as moving statistics ). The idea is that we use a data structure that estimates the mean and updates itself when it is provided with a value. The same goes for the variance (and thus the standard deviation). For example, if we denote \\(\\mu_t\\) the mean and \\(n_t\\) the count at any moment \\(t\\) , then updating the mean can be done as so: \\[ \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} \\] Likewise, the running variance can be computed as so: \\[ \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} \\] where \\(s_t\\) is a running sum of squares and \\(\\sigma_t\\) is the running variance at time \\(t\\) . This might seem a tad more involved than the batch algorithms you learn in school, but it is rather elegant. Implementing this in Python is not too difficult. For example let's compute the running mean and variance of the 'mean area' variable. n , mean , sum_of_squares , variance = 0 , 0 , 0 , 0 for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): n += 1 old_mean = mean mean += ( xi [ 'mean area' ] - mean ) / n sum_of_squares += ( xi [ 'mean area' ] - old_mean ) * ( xi [ 'mean area' ] - mean ) variance = sum_of_squares / n print ( f 'Running mean: { mean : .3f } ' ) print ( f 'Running variance: { variance : .3f } ' ) Running mean: 654.889 Running variance: 123625.903 Let's compare this with numpy . But remember, numpy requires access to \"all\" the data. import numpy as np i = list ( dataset . feature_names ) . index ( 'mean area' ) print ( f 'True mean: { np . mean ( X [:, i ]) : .3f } ' ) print ( f 'True variance: { np . var ( X [:, i ]) : .3f } ' ) True mean: 654.889 True variance: 123625.903 The results seem to be exactly the same! The twist is that the running statistics won't be very accurate for the first few observations. In general though this doesn't matter too much. Some would even go as far as to say that this descrepancy is beneficial and acts as some sort of regularization... Now the idea is that we can compute the running statistics of each feature and scale them as they come along. The way to do this with river is to use the StandardScaler class from the preprocessing module, as so: from river import preprocessing scaler = preprocessing . StandardScaler () for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): scaler = scaler . learn_one ( xi ) Now that we are scaling the data, we can start doing some actual machine learning. We're going to implement an online linear regression task. Because all the data isn't available at once, we are obliged to do what is called stochastic gradient descent , which is a popular research topic and has a lot of variants. SGD is commonly used to train neural networks. The idea is that at each step we compute the loss between the target prediction and the truth. We then calculate the gradient, which is simply a set of derivatives with respect to each weight from the linear regression. Once we have obtained the gradient, we can update the weights by moving them in the opposite direction of the gradient. The amount by which the weights are moved typically depends on a learning rate , which is typically set by the user. Different optimizers have different ways of managing the weight update, and some handle the learning rate implicitly. Online linear regression can be done in river with the LinearRegression class from the linear_model module. We'll be using plain and simple SGD using the SGD optimizer from the optim module. During training we'll measure the squared error between the truth and the predictions. from river import linear_model from river import optim scaler = preprocessing . StandardScaler () optimizer = optim . SGD ( lr = 0.01 ) log_reg = linear_model . LogisticRegression ( optimizer ) y_true = [] y_pred = [] for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer (), shuffle = True , seed = 42 ): # Scale the features xi_scaled = scaler . learn_one ( xi ) . transform_one ( xi ) # Test the current model on the new \"unobserved\" sample yi_pred = log_reg . predict_proba_one ( xi_scaled ) # Train the model with the new sample log_reg . learn_one ( xi_scaled , yi ) # Store the truth and the prediction y_true . append ( yi ) y_pred . append ( yi_pred [ True ]) print ( f 'ROC AUC: { metrics . roc_auc_score ( y_true , y_pred ) : .3f } ' ) ROC AUC: 0.990 The ROC AUC is significantly better than the one obtained from the cross-validation of scikit-learn's logisitic regression. However to make things really comparable it would be nice to compare with the same cross-validation procedure. river has a compat module that contains utilities for making river compatible with other Python libraries. Because we're doing regression we'll be using the SKLRegressorWrapper . We'll also be using Pipeline to encapsulate the logic of the StandardScaler and the LogisticRegression in one single object. from river import compat from river import compose # We define a Pipeline, exactly like we did earlier for sklearn model = compose . Pipeline ( ( 'scale' , preprocessing . StandardScaler ()), ( 'log_reg' , linear_model . LogisticRegression ()) ) # We make the Pipeline compatible with sklearn model = compat . convert_river_to_sklearn ( model ) # We compute the CV scores using the same CV scheme and the same scoring scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.964 (\u00b1 0.016) This time the ROC AUC score is lower, which is what we would expect. Indeed online learning isn't as accurate as batch learning. However it all depends in what you're interested in. If you're only interested in predicting the next observation then the online learning regime would be better. That's why it's a bit hard to compare both approaches: they're both suited to different scenarios. Going further \u00b6 Here a few resources if you want to do some reading: Online learning -- Wikipedia What is online machine learning? -- Max Pagels Introduction to Online Learning -- USC course Online Methods in Machine Learning -- MIT course Online Learning: A Comprehensive Survey Streaming 101: The world beyond batch Machine learning for data streams Data Stream Mining: A Practical Approach","title":"From batch to online/stream"},{"location":"examples/batch-to-online/#from-batch-to-onlinestream","text":"","title":"From batch to online/stream"},{"location":"examples/batch-to-online/#a-quick-overview-of-batch-learning","text":"If you've already delved into machine learning, then you shouldn't have any difficulty in getting to use incremental learning. If you are somewhat new to machine learning, then do not worry! The point of this notebook in particular is to introduce simple notions. We'll also start to show how river fits in and explain how to use it. The whole point of machine learning is to learn from data . In supervised learning you want to learn how to predict a target \\(y\\) given a set of features \\(X\\) . Meanwhile in an unsupervised learning there is no target, and the goal is rather to identify patterns and trends in the features \\(X\\) . At this point most people tend to imagine \\(X\\) as a somewhat big table where each row is an observation and each column is a feature, and they would be quite right. Learning from tabular data is part of what's called batch learning , which basically that all of the data is available to our learning algorithm at once. Multiple libraries have been created to handle the batch learning regime, with one of the most prominent being Python's scikit-learn . As a simple example of batch learning let's say we want to learn to predict if a women has breast cancer or not. We'll use the breast cancer dataset available with scikit-learn . We'll learn to map a set of features to a binary decision using a logistic regression . Like many other models based on numerical weights, logistic regression is sensitive to the scale of the features. Rescaling the data so that each feature has mean 0 and variance 1 is generally considered good practice. We can apply the rescaling and fit the logistic regression sequentially in an elegant manner using a Pipeline . To measure the performance of the model we'll evaluate the average ROC AUC score using a 5 fold cross-validation . from sklearn import datasets from sklearn import linear_model from sklearn import metrics from sklearn import model_selection from sklearn import pipeline from sklearn import preprocessing # Load the data dataset = datasets . load_breast_cancer () X , y = dataset . data , dataset . target # Define the steps of the model model = pipeline . Pipeline ([ ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LogisticRegression ( solver = 'lbfgs' )) ]) # Define a determistic cross-validation procedure cv = model_selection . KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Compute the MSE values scorer = metrics . make_scorer ( metrics . roc_auc_score ) scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.975 (\u00b1 0.011) This might be a lot to take in if you're not accustomed to scikit-learn, but it probably isn't if you are. Batch learning basically boils down to: Loading (and preprocessing) the data Fitting a model to the data Computing the performance of the model on unseen data This is pretty standard and is maybe how most people imagine a machine learning pipeline. However, this way of proceeding has certain downsides. First of all your laptop would crash if the load_boston function returned a dataset who's size exceeds your available amount of RAM. Sometimes you can use some tricks to get around this. For example by optimizing the data types and by using sparse representations when applicable you can potentially save precious gigabytes of RAM. However, like many tricks this only goes so far. If your dataset weighs hundreds of gigabytes then you won't go far without some special hardware. One solution is to do out-of-core learning; that is, algorithms that can learn by being presented the data in chunks or mini-batches. If you want to go down this road then take a look at Dask and Spark's MLlib . Another issue with the batch learning regime is that it can't elegantly learn from new data. Indeed if new data is made available, then the model has to learn from scratch with a new dataset composed of the old data and the new data. This is particularly annoying in a real situation where you might have new incoming data every week, day, hour, minute, or even setting. For example if you're building a recommendation engine for an e-commerce app, then you're probably training your model from 0 every week or so. As your app grows in popularity, so does the dataset you're training on. This will lead to longer and longer training times and might require a hardware upgrade. A final downside that isn't very easy to grasp concerns the manner in which features are extracted. Every time you want to train your model you first have to extract features. The trick is that some features might not be accessible at the particular point in time you are at. For example maybe that some attributes in your data warehouse get overwritten with time. In other words maybe that all the features pertaining to a particular observations are not available, whereas they were a week ago. This happens more often than not in real scenarios, and apart if you have a sophisticated data engineering pipeline then you will encounter these issues at some point.","title":"A quick overview of batch learning"},{"location":"examples/batch-to-online/#a-hands-on-introduction-to-incremental-learning","text":"Incremental learning is also often called online learning or stream learning , but if you google online learning a lot of the results will point to educational websites. Hence, the terms \"incremental learning\" and \"stream learning\" (from which river derives it's name) are prefered. The point of incremental learning is to fit a model to a stream of data. In other words, the data isn't available in it's entirety, but rather the observations are provided one by one. As an example let's stream through the dataset used previously. for xi , yi in zip ( X , y ): # This is where the model learns pass In this case we're iterating over a dataset that is already in memory, but we could just as well stream from a CSV file, a Kafka stream, an SQL query, etc. If we look at xi we can notice that it is a numpy.ndarray . xi array([7.760e+00, 2.454e+01, 4.792e+01, 1.810e+02, 5.263e-02, 4.362e-02, 0.000e+00, 0.000e+00, 1.587e-01, 5.884e-02, 3.857e-01, 1.428e+00, 2.548e+00, 1.915e+01, 7.189e-03, 4.660e-03, 0.000e+00, 0.000e+00, 2.676e-02, 2.783e-03, 9.456e+00, 3.037e+01, 5.916e+01, 2.686e+02, 8.996e-02, 6.444e-02, 0.000e+00, 0.000e+00, 2.871e-01, 7.039e-02]) river by design works with dict s. We believe that dict s are more enjoyable to program with than numpy.ndarray s, at least for when single observations are concerned. dict 's bring the added benefit that each feature can be accessed by name rather than by position. for xi , yi in zip ( X , y ): xi = dict ( zip ( dataset . feature_names , xi )) pass xi {'mean radius': 7.76, 'mean texture': 24.54, 'mean perimeter': 47.92, 'mean area': 181.0, 'mean smoothness': 0.05263, 'mean compactness': 0.04362, 'mean concavity': 0.0, 'mean concave points': 0.0, 'mean symmetry': 0.1587, 'mean fractal dimension': 0.05884, 'radius error': 0.3857, 'texture error': 1.428, 'perimeter error': 2.548, 'area error': 19.15, 'smoothness error': 0.007189, 'compactness error': 0.00466, 'concavity error': 0.0, 'concave points error': 0.0, 'symmetry error': 0.02676, 'fractal dimension error': 0.002783, 'worst radius': 9.456, 'worst texture': 30.37, 'worst perimeter': 59.16, 'worst area': 268.6, 'worst smoothness': 0.08996, 'worst compactness': 0.06444, 'worst concavity': 0.0, 'worst concave points': 0.0, 'worst symmetry': 0.2871, 'worst fractal dimension': 0.07039} Conveniently, river 's stream module has an iter_sklearn_dataset method that we can use instead. from river import stream for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): pass The simple fact that we are getting the data as a stream means that we can't do a lot of things the same way as in a batch setting. For example let's say we want to scale the data so that it has mean 0 and variance 1, as we did earlier. To do so we simply have to subtract the mean of each feature to each value and then divide the result by the standard deviation of the feature. The problem is that we can't possible known the values of the mean and the standard deviation before actually going through all the data! One way to proceed would be to do a first pass over the data to compute the necessary values and then scale the values during a second pass. The problem is that this defeats our purpose, which is to learn by only looking at the data once. Although this might seem rather restrictive, it reaps sizable benefits down the road. The way we do feature scaling in river involves computing running statistics (also know as moving statistics ). The idea is that we use a data structure that estimates the mean and updates itself when it is provided with a value. The same goes for the variance (and thus the standard deviation). For example, if we denote \\(\\mu_t\\) the mean and \\(n_t\\) the count at any moment \\(t\\) , then updating the mean can be done as so: \\[ \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\end{cases} \\] Likewise, the running variance can be computed as so: \\[ \\begin{cases} n_{t+1} = n_t + 1 \\\\ \\mu_{t+1} = \\mu_t + \\frac{x - \\mu_t}{n_{t+1}} \\\\ s_{t+1} = s_t + (x - \\mu_t) \\times (x - \\mu_{t+1}) \\\\ \\sigma_{t+1} = \\frac{s_{t+1}}{n_{t+1}} \\end{cases} \\] where \\(s_t\\) is a running sum of squares and \\(\\sigma_t\\) is the running variance at time \\(t\\) . This might seem a tad more involved than the batch algorithms you learn in school, but it is rather elegant. Implementing this in Python is not too difficult. For example let's compute the running mean and variance of the 'mean area' variable. n , mean , sum_of_squares , variance = 0 , 0 , 0 , 0 for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): n += 1 old_mean = mean mean += ( xi [ 'mean area' ] - mean ) / n sum_of_squares += ( xi [ 'mean area' ] - old_mean ) * ( xi [ 'mean area' ] - mean ) variance = sum_of_squares / n print ( f 'Running mean: { mean : .3f } ' ) print ( f 'Running variance: { variance : .3f } ' ) Running mean: 654.889 Running variance: 123625.903 Let's compare this with numpy . But remember, numpy requires access to \"all\" the data. import numpy as np i = list ( dataset . feature_names ) . index ( 'mean area' ) print ( f 'True mean: { np . mean ( X [:, i ]) : .3f } ' ) print ( f 'True variance: { np . var ( X [:, i ]) : .3f } ' ) True mean: 654.889 True variance: 123625.903 The results seem to be exactly the same! The twist is that the running statistics won't be very accurate for the first few observations. In general though this doesn't matter too much. Some would even go as far as to say that this descrepancy is beneficial and acts as some sort of regularization... Now the idea is that we can compute the running statistics of each feature and scale them as they come along. The way to do this with river is to use the StandardScaler class from the preprocessing module, as so: from river import preprocessing scaler = preprocessing . StandardScaler () for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer ()): scaler = scaler . learn_one ( xi ) Now that we are scaling the data, we can start doing some actual machine learning. We're going to implement an online linear regression task. Because all the data isn't available at once, we are obliged to do what is called stochastic gradient descent , which is a popular research topic and has a lot of variants. SGD is commonly used to train neural networks. The idea is that at each step we compute the loss between the target prediction and the truth. We then calculate the gradient, which is simply a set of derivatives with respect to each weight from the linear regression. Once we have obtained the gradient, we can update the weights by moving them in the opposite direction of the gradient. The amount by which the weights are moved typically depends on a learning rate , which is typically set by the user. Different optimizers have different ways of managing the weight update, and some handle the learning rate implicitly. Online linear regression can be done in river with the LinearRegression class from the linear_model module. We'll be using plain and simple SGD using the SGD optimizer from the optim module. During training we'll measure the squared error between the truth and the predictions. from river import linear_model from river import optim scaler = preprocessing . StandardScaler () optimizer = optim . SGD ( lr = 0.01 ) log_reg = linear_model . LogisticRegression ( optimizer ) y_true = [] y_pred = [] for xi , yi in stream . iter_sklearn_dataset ( datasets . load_breast_cancer (), shuffle = True , seed = 42 ): # Scale the features xi_scaled = scaler . learn_one ( xi ) . transform_one ( xi ) # Test the current model on the new \"unobserved\" sample yi_pred = log_reg . predict_proba_one ( xi_scaled ) # Train the model with the new sample log_reg . learn_one ( xi_scaled , yi ) # Store the truth and the prediction y_true . append ( yi ) y_pred . append ( yi_pred [ True ]) print ( f 'ROC AUC: { metrics . roc_auc_score ( y_true , y_pred ) : .3f } ' ) ROC AUC: 0.990 The ROC AUC is significantly better than the one obtained from the cross-validation of scikit-learn's logisitic regression. However to make things really comparable it would be nice to compare with the same cross-validation procedure. river has a compat module that contains utilities for making river compatible with other Python libraries. Because we're doing regression we'll be using the SKLRegressorWrapper . We'll also be using Pipeline to encapsulate the logic of the StandardScaler and the LogisticRegression in one single object. from river import compat from river import compose # We define a Pipeline, exactly like we did earlier for sklearn model = compose . Pipeline ( ( 'scale' , preprocessing . StandardScaler ()), ( 'log_reg' , linear_model . LogisticRegression ()) ) # We make the Pipeline compatible with sklearn model = compat . convert_river_to_sklearn ( model ) # We compute the CV scores using the same CV scheme and the same scoring scores = model_selection . cross_val_score ( model , X , y , scoring = scorer , cv = cv ) # Display the average score and it's standard deviation print ( f 'ROC AUC: { scores . mean () : .3f } (\u00b1 { scores . std () : .3f } )' ) ROC AUC: 0.964 (\u00b1 0.016) This time the ROC AUC score is lower, which is what we would expect. Indeed online learning isn't as accurate as batch learning. However it all depends in what you're interested in. If you're only interested in predicting the next observation then the online learning regime would be better. That's why it's a bit hard to compare both approaches: they're both suited to different scenarios.","title":"A hands-on introduction to incremental learning"},{"location":"examples/batch-to-online/#going-further","text":"Here a few resources if you want to do some reading: Online learning -- Wikipedia What is online machine learning? -- Max Pagels Introduction to Online Learning -- USC course Online Methods in Machine Learning -- MIT course Online Learning: A Comprehensive Survey Streaming 101: The world beyond batch Machine learning for data streams Data Stream Mining: A Practical Approach","title":"Going further"},{"location":"examples/bike-sharing-forecasting/","text":"Bike-sharing forecasting \u00b6 In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data. from pprint import pprint from river import datasets X_y = datasets . Bikes () for x , y in X_y : pprint ( x ) print ( f 'Number of available bikes: { y } ' ) break {'clouds': 75, 'description': 'light rain', 'humidity': 81, 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'pressure': 1017.0, 'station': 'metro-canal-du-midi', 'temperature': 6.54, 'wind': 9.3} Number of available bikes: 1 Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a Select . Linear regression is very likely to go haywire if we don't scale the data, so we'll use a StandardScaler to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations. from river import compose from river import linear_model from river import metrics from river import evaluate from river import preprocessing from river import optim X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 4.912727 [40,000] MAE: 5.333554 [60,000] MAE: 5.330948 [80,000] MAE: 5.392313 [100,000] MAE: 5.423059 [120,000] MAE: 5.541223 [140,000] MAE: 5.613023 [160,000] MAE: 5.622428 [180,000] MAE: 5.567824 MAE: 5.563893 The model doesn't seem to be doing that well, but then again we didn't provide a lot of features. Generally, a good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the moment field. We can then use a TargetAgg to aggregate the values of the target. from river import feature_extraction from river import stats X_y = iter ( datasets . Bikes ()) def get_hour ( x ): x [ 'hour' ] = x [ 'moment' ] . hour return x model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 3.721246 [40,000] MAE: 3.829972 [60,000] MAE: 3.845068 [80,000] MAE: 3.910259 [100,000] MAE: 3.888652 [120,000] MAE: 3.923727 [140,000] MAE: 3.980953 [160,000] MAE: 3.950034 [180,000] MAE: 3.934545 MAE: 3.933498 By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully river has some ways to relieve the pain. The first thing we can do it to visualize the pipeline, to get an idea of how the data flows through it. model ['clouds', 'humidity', 'pressure', 'temperature', 'wind'] {'whitelist': {'pressure', 'wind', 'humidity', 'temperature', 'clouds'}} get_hour def get_hour(x): x['hour'] = x['moment'].hour return x target_mean_by_station_and_hour {'by': ['station', 'hour'], 'feature_name': 'target_mean_by_station_and_hour', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7feefcfb3700>, Mean: 0.), {'metro-canal-du-midi_0': Mean: 7.93981, 'metro-canal-du-midi_1': Mean: 8.179704, 'metro-canal-du-midi_10': Mean: 12.486815, 'metro-canal-du-midi_11': Mean: 11.675479, 'metro-canal-du-midi_12': Mean: 10.197409, 'metro-canal-du-midi_13': Mean: 10.650855, 'metro-canal-du-midi_14': Mean: 11.109123, 'metro-canal-du-midi_15': Mean: 11.068934, 'metro-canal-du-midi_16': Mean: 11.274958, 'metro-canal-du-midi_17': Mean: 8.459136, 'metro-canal-du-midi_18': Mean: 7.587469, 'metro-canal-du-midi_19': Mean: 7.734677, 'metro-canal-du-midi_2': Mean: 8.35824, 'metro-canal-du-midi_20': Mean: 7.582465, 'metro-canal-du-midi_21': Mean: 7.190665, 'metro-canal-du-midi_22': Mean: 7.486895, 'metro-canal-du-midi_23': Mean: 7.840791, 'metro-canal-du-midi_3': Mean: 8.656051, 'metro-canal-du-midi_4': Mean: 8.868445, 'metro-canal-du-midi_5': Mean: 8.99656, 'metro-canal-du-midi_6': Mean: 9.09966, 'metro-canal-du-midi_7': Mean: 8.852642, 'metro-canal-du-midi_8': Mean: 12.66712, 'metro-canal-du-midi_9': Mean: 13.412186, 'place-des-carmes_0': Mean: 4.720696, 'place-des-carmes_1': Mean: 3.390295, 'place-des-carmes_10': Mean: 8.575303, 'place-des-carmes_11': Mean: 9.321546, 'place-des-carmes_12': Mean: 10.511931, 'place-des-carmes_13': Mean: 11.392745, 'place-des-carmes_14': Mean: 10.735003, 'place-des-carmes_15': Mean: 10.198787, 'place-des-carmes_16': Mean: 9.941479, 'place-des-carmes_17': Mean: 9.125579, 'place-des-carmes_18': Mean: 7.660775, 'place-des-carmes_19': Mean: 6.847649, 'place-des-carmes_2': Mean: 2.232181, 'place-des-carmes_20': Mean: 9.626876, 'place-des-carmes_21': Mean: 11.602929, 'place-des-carmes_22': Mean: 10.405537, 'place-des-carmes_23': Mean: 7.700904, 'place-des-carmes_3': Mean: 1.371981, 'place-des-carmes_4': Mean: 1.051665, 'place-des-carmes_5': Mean: 0.984993, 'place-des-carmes_6': Mean: 2.039947, 'place-des-carmes_7': Mean: 3.850369, 'place-des-carmes_8': Mean: 3.792624, 'place-des-carmes_9': Mean: 5.957182, 'place-esquirol_0': Mean: 7.415789, 'place-esquirol_1': Mean: 5.244396, 'place-esquirol_10': Mean: 19.465005, 'place-esquirol_11': Mean: 22.976512, 'place-esquirol_12': Mean: 25.324159, 'place-esquirol_13': Mean: 25.428847, 'place-esquirol_14': Mean: 24.57762, 'place-esquirol_15': Mean: 24.416851, 'place-esquirol_16': Mean: 23.555125, 'place-esquirol_17': Mean: 22.062564, 'place-esquirol_18': Mean: 18.10623, 'place-esquirol_19': Mean: 11.916638, 'place-esquirol_2': Mean: 2.858635, 'place-esquirol_20': Mean: 13.346362, 'place-esquirol_21': Mean: 16.743318, 'place-esquirol_22': Mean: 15.562088, 'place-esquirol_23': Mean: 10.911134, 'place-esquirol_3': Mean: 1.155929, 'place-esquirol_4': Mean: 0.73306, 'place-esquirol_5': Mean: 0.668546, 'place-esquirol_6': Mean: 1.21265, 'place-esquirol_7': Mean: 3.107535, 'place-esquirol_8': Mean: 8.518696, 'place-esquirol_9': Mean: 15.470588, 'place-jeanne-darc_0': Mean: 6.541667, 'place-jeanne-darc_1': Mean: 5.99892, 'place-jeanne-darc_10': Mean: 5.735553, 'place-jeanne-darc_11': Mean: 5.616142, 'place-jeanne-darc_12': Mean: 5.787478, 'place-jeanne-darc_13': Mean: 5.817699, 'place-jeanne-darc_14': Mean: 5.657546, 'place-jeanne-darc_15': Mean: 6.224604, 'place-jeanne-darc_16': Mean: 5.796141, 'place-jeanne-darc_17': Mean: 5.743089, 'place-jeanne-darc_18': Mean: 5.674784, 'place-jeanne-darc_19': Mean: 5.833068, 'place-jeanne-darc_2': Mean: 5.598169, 'place-jeanne-darc_20': Mean: 6.015755, 'place-jeanne-darc_21': Mean: 6.242541, 'place-jeanne-darc_22': Mean: 6.141509, 'place-jeanne-darc_23': Mean: 6.493028, 'place-jeanne-darc_3': Mean: 5.180556, 'place-jeanne-darc_4': Mean: 4.779626, 'place-jeanne-darc_5': Mean: 4.67063, 'place-jeanne-darc_6': Mean: 4.611995, 'place-jeanne-darc_7': Mean: 4.960718, 'place-jeanne-darc_8': Mean: 5.552273, 'place-jeanne-darc_9': Mean: 6.249573, 'pomme_0': Mean: 3.301532, 'pomme_1': Mean: 2.312914, 'pomme_10': Mean: 11.791436, 'pomme_11': Mean: 12.976854, 'pomme_12': Mean: 13.962654, 'pomme_13': Mean: 11.692257, 'pomme_14': Mean: 11.180851, 'pomme_15': Mean: 11.939586, 'pomme_16': Mean: 12.267051, 'pomme_17': Mean: 12.132993, 'pomme_18': Mean: 11.399108, 'pomme_19': Mean: 6.37021, 'pomme_2': Mean: 2.144453, 'pomme_20': Mean: 5.279234, 'pomme_21': Mean: 6.254257, 'pomme_22': Mean: 6.568678, 'pomme_23': Mean: 5.235756, 'pomme_3': Mean: 1.563622, 'pomme_4': Mean: 0.947328, 'pomme_5': Mean: 0.924175, 'pomme_6': Mean: 1.287805, 'pomme_7': Mean: 1.299456, 'pomme_8': Mean: 2.94988, 'pomme_9': Mean: 7.89396}), 'how': Mean: 0., 'target_name': 'target'} StandardScaler {'counts': Counter({'target_mean_by_station_and_hour': 182470, 'pressure': 182470, 'wind': 182470, 'humidity': 182470, 'temperature': 182470, 'clouds': 182470}), 'means': defaultdict(<class 'float'>, {'clouds': 30.315131254453505, 'humidity': 62.24244533347998, 'pressure': 1017.0563060996391, 'target_mean_by_station_and_hour': 9.468200635816528, 'temperature': 20.50980692716619, 'wind': 3.4184331122924543}), 'vars': defaultdict(<class 'float'>, {'clouds': 1389.0025610928221, 'humidity': 349.59967918503554, 'pressure': 33.298307526514115, 'target_mean_by_station_and_hour': 33.720872727055365, 'temperature': 34.70701720774977, 'wind': 4.473627075744674})} LinearRegression {'_weights': {'target_mean_by_station_and_hour': 3.871921823842874, 'pressure': 2.1481948468090253, 'wind': -0.26121936045271543, 'humidity': 3.881792987494524, 'temperature': -2.7950777648287284, 'clouds': -0.6106470619450347}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 6.096564954881427, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.001}), 'n_iterations': 182470})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } We can also use the debug_one method to see what happens to one particular instance. Let's train the model on the first 10,000 observations and then call debug_one on the next one. To do this, we will turn the Bike object into a Python generator with iter() function. The Pythonic way to read the first 10,000 elements of a generator is to use itertools.islice . import itertools X_y = iter ( datasets . Bikes ()) model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () for x , y in itertools . islice ( X_y , 10000 ): y_pred = model . predict_one ( x ) model . learn_one ( x , y ) x , y = next ( X_y ) print ( model . debug_one ( x )) 0. Input -------- clouds: 0 (int) description: clear sky (str) humidity: 52 (int) moment: 2016-04-10 19:03:27 (datetime) pressure: 1,001.00000 (float) station: place-esquirol (str) temperature: 19.00000 (float) wind: 7.70000 (float) 1. Transformer union -------------------- 1.0 Select ---------- clouds: 0 (int) humidity: 52 (int) pressure: 1,001.00000 (float) temperature: 19.00000 (float) wind: 7.70000 (float) 1.1 get_hour | target_mean_by_station_and_hour ---------------------------------------------- target_mean_by_station_and_hour: 7.97175 (float) clouds: 0 (int) humidity: 52 (int) pressure: 1,001.00000 (float) target_mean_by_station_and_hour: 7.97175 (float) temperature: 19.00000 (float) wind: 7.70000 (float) 2. StandardScaler ----------------- clouds: -1.36138 (float) humidity: -1.73083 (float) pressure: -1.26076 (float) target_mean_by_station_and_hour: 0.05496 (float) temperature: 1.76232 (float) wind: 1.45841 (float) 3. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 6.58252 6.58252 temperature 1.76232 2.47030 4.35345 clouds -1.36138 -1.92255 2.61732 target_mean_by_station_and_hour 0.05496 0.54167 0.02977 wind 1.45841 -0.77720 -1.13348 humidity -1.73083 1.44921 -2.50833 pressure -1.26076 3.78529 -4.77234 Prediction: 5.16889 The debug_one method shows what happens to an input set of features, step by step. And now comes the catch. Up until now we've been using the progressive_val_score method from the evaluate module. What this does it that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of doing is often used for evaluating online learning models, but in some cases it is the wrong approach. The following paragraph is extremely important. When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago. If you think about, this is exactly how a real-time machine learning system should work. The problem is that this isn't what the progressive_val_score method is emulating, indeed it is simply asking the model to predict the next observation, which is only a few minutes ahead, and then updates the model immediately. We can prove that this is flawed by adding a feature that measures a running average of the very recent values. X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 20.159286 [40,000] MAE: 10.458898 [60,000] MAE: 7.2759 [80,000] MAE: 5.715397 [100,000] MAE: 4.775094 [120,000] MAE: 4.138421 [140,000] MAE: 3.682591 [160,000] MAE: 3.35015 [180,000] MAE: 3.091398 MAE: 3.06414 The score we got is too good to be true. This is simply because the problem is too easy. What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the moment and delay parameters in the progressive_val_score method. The idea is that each observation of the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The moment parameter determines which variable should be used as a timestamp, while the delay parameter controls the duration to wait before revealing the true values to the model. import datetime as dt model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( dataset = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.24812 [40,000] MAE: 2.240287 [60,000] MAE: 2.270287 [80,000] MAE: 2.28649 [100,000] MAE: 2.294264 [120,000] MAE: 2.275891 [140,000] MAE: 2.261411 [160,000] MAE: 2.285978 [180,000] MAE: 2.289353 MAE: 2.29304 The score we now have is much more realistic, as it is comparable with the related data science competition . Moreover, we can see that the model gets better with time, which feels better than the previous situations. The point is that progressive_val_score method can be used to simulate a production scenario, and is thus extremely valuable. Now that we have a working pipeline in place, we can attempt to make it more accurate. As a simple example, we'll using a EWARegressor from the expert module to combine 3 linear regression model trained with different optimizers. The EWARegressor will run the 3 models in parallel and assign weights to each model based on their individual performance. from river import expert from river import optim model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model += feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) model |= preprocessing . StandardScaler () model |= expert . EWARegressor ([ linear_model . LinearRegression ( optim . SGD ()), linear_model . LinearRegression ( optim . RMSProp ()), linear_model . LinearRegression ( optim . Adam ()) ]) evaluate . progressive_val_score ( dataset = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.253263 [40,000] MAE: 2.242859 [60,000] MAE: 2.272001 [80,000] MAE: 2.287776 [100,000] MAE: 2.295292 [120,000] MAE: 2.276748 [140,000] MAE: 2.262146 [160,000] MAE: 2.286621 [180,000] MAE: 2.289925 MAE: 2.293604","title":"Bike-sharing forecasting"},{"location":"examples/bike-sharing-forecasting/#bike-sharing-forecasting","text":"In this tutorial we're going to forecast the number of bikes in 5 bike stations from the city of Toulouse. We'll do so by building a simple model step by step. The dataset contains 182,470 observations. Let's first take a peak at the data. from pprint import pprint from river import datasets X_y = datasets . Bikes () for x , y in X_y : pprint ( x ) print ( f 'Number of available bikes: { y } ' ) break {'clouds': 75, 'description': 'light rain', 'humidity': 81, 'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'pressure': 1017.0, 'station': 'metro-canal-du-midi', 'temperature': 6.54, 'wind': 9.3} Number of available bikes: 1 Let's start by using a simple linear regression on the numeric features. We can select the numeric features and discard the rest of the features using a Select . Linear regression is very likely to go haywire if we don't scale the data, so we'll use a StandardScaler to do just that. We'll evaluate the model by measuring the mean absolute error. Finally we'll print the score every 20,000 observations. from river import compose from river import linear_model from river import metrics from river import evaluate from river import preprocessing from river import optim X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 4.912727 [40,000] MAE: 5.333554 [60,000] MAE: 5.330948 [80,000] MAE: 5.392313 [100,000] MAE: 5.423059 [120,000] MAE: 5.541223 [140,000] MAE: 5.613023 [160,000] MAE: 5.622428 [180,000] MAE: 5.567824 MAE: 5.563893 The model doesn't seem to be doing that well, but then again we didn't provide a lot of features. Generally, a good idea for this kind of problem is to look at an average of the previous values. For example, for each station we can look at the average number of bikes per hour. To do so we first have to extract the hour from the moment field. We can then use a TargetAgg to aggregate the values of the target. from river import feature_extraction from river import stats X_y = iter ( datasets . Bikes ()) def get_hour ( x ): x [ 'hour' ] = x [ 'moment' ] . hour return x model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression ( optimizer = optim . SGD ( 0.001 )) metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 3.721246 [40,000] MAE: 3.829972 [60,000] MAE: 3.845068 [80,000] MAE: 3.910259 [100,000] MAE: 3.888652 [120,000] MAE: 3.923727 [140,000] MAE: 3.980953 [160,000] MAE: 3.950034 [180,000] MAE: 3.934545 MAE: 3.933498 By adding a single feature, we've managed to significantly reduce the mean absolute error. At this point you might think that the model is getting slightly complex, and is difficult to understand and test. Pipelines have the advantage of being terse, but they aren't always to debug. Thankfully river has some ways to relieve the pain. The first thing we can do it to visualize the pipeline, to get an idea of how the data flows through it. model ['clouds', 'humidity', 'pressure', 'temperature', 'wind'] {'whitelist': {'pressure', 'wind', 'humidity', 'temperature', 'clouds'}} get_hour def get_hour(x): x['hour'] = x['moment'].hour return x target_mean_by_station_and_hour {'by': ['station', 'hour'], 'feature_name': 'target_mean_by_station_and_hour', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7feefcfb3700>, Mean: 0.), {'metro-canal-du-midi_0': Mean: 7.93981, 'metro-canal-du-midi_1': Mean: 8.179704, 'metro-canal-du-midi_10': Mean: 12.486815, 'metro-canal-du-midi_11': Mean: 11.675479, 'metro-canal-du-midi_12': Mean: 10.197409, 'metro-canal-du-midi_13': Mean: 10.650855, 'metro-canal-du-midi_14': Mean: 11.109123, 'metro-canal-du-midi_15': Mean: 11.068934, 'metro-canal-du-midi_16': Mean: 11.274958, 'metro-canal-du-midi_17': Mean: 8.459136, 'metro-canal-du-midi_18': Mean: 7.587469, 'metro-canal-du-midi_19': Mean: 7.734677, 'metro-canal-du-midi_2': Mean: 8.35824, 'metro-canal-du-midi_20': Mean: 7.582465, 'metro-canal-du-midi_21': Mean: 7.190665, 'metro-canal-du-midi_22': Mean: 7.486895, 'metro-canal-du-midi_23': Mean: 7.840791, 'metro-canal-du-midi_3': Mean: 8.656051, 'metro-canal-du-midi_4': Mean: 8.868445, 'metro-canal-du-midi_5': Mean: 8.99656, 'metro-canal-du-midi_6': Mean: 9.09966, 'metro-canal-du-midi_7': Mean: 8.852642, 'metro-canal-du-midi_8': Mean: 12.66712, 'metro-canal-du-midi_9': Mean: 13.412186, 'place-des-carmes_0': Mean: 4.720696, 'place-des-carmes_1': Mean: 3.390295, 'place-des-carmes_10': Mean: 8.575303, 'place-des-carmes_11': Mean: 9.321546, 'place-des-carmes_12': Mean: 10.511931, 'place-des-carmes_13': Mean: 11.392745, 'place-des-carmes_14': Mean: 10.735003, 'place-des-carmes_15': Mean: 10.198787, 'place-des-carmes_16': Mean: 9.941479, 'place-des-carmes_17': Mean: 9.125579, 'place-des-carmes_18': Mean: 7.660775, 'place-des-carmes_19': Mean: 6.847649, 'place-des-carmes_2': Mean: 2.232181, 'place-des-carmes_20': Mean: 9.626876, 'place-des-carmes_21': Mean: 11.602929, 'place-des-carmes_22': Mean: 10.405537, 'place-des-carmes_23': Mean: 7.700904, 'place-des-carmes_3': Mean: 1.371981, 'place-des-carmes_4': Mean: 1.051665, 'place-des-carmes_5': Mean: 0.984993, 'place-des-carmes_6': Mean: 2.039947, 'place-des-carmes_7': Mean: 3.850369, 'place-des-carmes_8': Mean: 3.792624, 'place-des-carmes_9': Mean: 5.957182, 'place-esquirol_0': Mean: 7.415789, 'place-esquirol_1': Mean: 5.244396, 'place-esquirol_10': Mean: 19.465005, 'place-esquirol_11': Mean: 22.976512, 'place-esquirol_12': Mean: 25.324159, 'place-esquirol_13': Mean: 25.428847, 'place-esquirol_14': Mean: 24.57762, 'place-esquirol_15': Mean: 24.416851, 'place-esquirol_16': Mean: 23.555125, 'place-esquirol_17': Mean: 22.062564, 'place-esquirol_18': Mean: 18.10623, 'place-esquirol_19': Mean: 11.916638, 'place-esquirol_2': Mean: 2.858635, 'place-esquirol_20': Mean: 13.346362, 'place-esquirol_21': Mean: 16.743318, 'place-esquirol_22': Mean: 15.562088, 'place-esquirol_23': Mean: 10.911134, 'place-esquirol_3': Mean: 1.155929, 'place-esquirol_4': Mean: 0.73306, 'place-esquirol_5': Mean: 0.668546, 'place-esquirol_6': Mean: 1.21265, 'place-esquirol_7': Mean: 3.107535, 'place-esquirol_8': Mean: 8.518696, 'place-esquirol_9': Mean: 15.470588, 'place-jeanne-darc_0': Mean: 6.541667, 'place-jeanne-darc_1': Mean: 5.99892, 'place-jeanne-darc_10': Mean: 5.735553, 'place-jeanne-darc_11': Mean: 5.616142, 'place-jeanne-darc_12': Mean: 5.787478, 'place-jeanne-darc_13': Mean: 5.817699, 'place-jeanne-darc_14': Mean: 5.657546, 'place-jeanne-darc_15': Mean: 6.224604, 'place-jeanne-darc_16': Mean: 5.796141, 'place-jeanne-darc_17': Mean: 5.743089, 'place-jeanne-darc_18': Mean: 5.674784, 'place-jeanne-darc_19': Mean: 5.833068, 'place-jeanne-darc_2': Mean: 5.598169, 'place-jeanne-darc_20': Mean: 6.015755, 'place-jeanne-darc_21': Mean: 6.242541, 'place-jeanne-darc_22': Mean: 6.141509, 'place-jeanne-darc_23': Mean: 6.493028, 'place-jeanne-darc_3': Mean: 5.180556, 'place-jeanne-darc_4': Mean: 4.779626, 'place-jeanne-darc_5': Mean: 4.67063, 'place-jeanne-darc_6': Mean: 4.611995, 'place-jeanne-darc_7': Mean: 4.960718, 'place-jeanne-darc_8': Mean: 5.552273, 'place-jeanne-darc_9': Mean: 6.249573, 'pomme_0': Mean: 3.301532, 'pomme_1': Mean: 2.312914, 'pomme_10': Mean: 11.791436, 'pomme_11': Mean: 12.976854, 'pomme_12': Mean: 13.962654, 'pomme_13': Mean: 11.692257, 'pomme_14': Mean: 11.180851, 'pomme_15': Mean: 11.939586, 'pomme_16': Mean: 12.267051, 'pomme_17': Mean: 12.132993, 'pomme_18': Mean: 11.399108, 'pomme_19': Mean: 6.37021, 'pomme_2': Mean: 2.144453, 'pomme_20': Mean: 5.279234, 'pomme_21': Mean: 6.254257, 'pomme_22': Mean: 6.568678, 'pomme_23': Mean: 5.235756, 'pomme_3': Mean: 1.563622, 'pomme_4': Mean: 0.947328, 'pomme_5': Mean: 0.924175, 'pomme_6': Mean: 1.287805, 'pomme_7': Mean: 1.299456, 'pomme_8': Mean: 2.94988, 'pomme_9': Mean: 7.89396}), 'how': Mean: 0., 'target_name': 'target'} StandardScaler {'counts': Counter({'target_mean_by_station_and_hour': 182470, 'pressure': 182470, 'wind': 182470, 'humidity': 182470, 'temperature': 182470, 'clouds': 182470}), 'means': defaultdict(<class 'float'>, {'clouds': 30.315131254453505, 'humidity': 62.24244533347998, 'pressure': 1017.0563060996391, 'target_mean_by_station_and_hour': 9.468200635816528, 'temperature': 20.50980692716619, 'wind': 3.4184331122924543}), 'vars': defaultdict(<class 'float'>, {'clouds': 1389.0025610928221, 'humidity': 349.59967918503554, 'pressure': 33.298307526514115, 'target_mean_by_station_and_hour': 33.720872727055365, 'temperature': 34.70701720774977, 'wind': 4.473627075744674})} LinearRegression {'_weights': {'target_mean_by_station_and_hour': 3.871921823842874, 'pressure': 2.1481948468090253, 'wind': -0.26121936045271543, 'humidity': 3.881792987494524, 'temperature': -2.7950777648287284, 'clouds': -0.6106470619450347}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 6.096564954881427, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.001}), 'n_iterations': 182470})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } We can also use the debug_one method to see what happens to one particular instance. Let's train the model on the first 10,000 observations and then call debug_one on the next one. To do this, we will turn the Bike object into a Python generator with iter() function. The Pythonic way to read the first 10,000 elements of a generator is to use itertools.islice . import itertools X_y = iter ( datasets . Bikes ()) model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () for x , y in itertools . islice ( X_y , 10000 ): y_pred = model . predict_one ( x ) model . learn_one ( x , y ) x , y = next ( X_y ) print ( model . debug_one ( x )) 0. Input -------- clouds: 0 (int) description: clear sky (str) humidity: 52 (int) moment: 2016-04-10 19:03:27 (datetime) pressure: 1,001.00000 (float) station: place-esquirol (str) temperature: 19.00000 (float) wind: 7.70000 (float) 1. Transformer union -------------------- 1.0 Select ---------- clouds: 0 (int) humidity: 52 (int) pressure: 1,001.00000 (float) temperature: 19.00000 (float) wind: 7.70000 (float) 1.1 get_hour | target_mean_by_station_and_hour ---------------------------------------------- target_mean_by_station_and_hour: 7.97175 (float) clouds: 0 (int) humidity: 52 (int) pressure: 1,001.00000 (float) target_mean_by_station_and_hour: 7.97175 (float) temperature: 19.00000 (float) wind: 7.70000 (float) 2. StandardScaler ----------------- clouds: -1.36138 (float) humidity: -1.73083 (float) pressure: -1.26076 (float) target_mean_by_station_and_hour: 0.05496 (float) temperature: 1.76232 (float) wind: 1.45841 (float) 3. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 6.58252 6.58252 temperature 1.76232 2.47030 4.35345 clouds -1.36138 -1.92255 2.61732 target_mean_by_station_and_hour 0.05496 0.54167 0.02977 wind 1.45841 -0.77720 -1.13348 humidity -1.73083 1.44921 -2.50833 pressure -1.26076 3.78529 -4.77234 Prediction: 5.16889 The debug_one method shows what happens to an input set of features, step by step. And now comes the catch. Up until now we've been using the progressive_val_score method from the evaluate module. What this does it that it sequentially predicts the output of an observation and updates the model immediately afterwards. This way of doing is often used for evaluating online learning models, but in some cases it is the wrong approach. The following paragraph is extremely important. When evaluating a machine learning model, the goal is to simulate production conditions in order to get a trust-worthy assessment of the performance of the model. In our case, we typically want to forecast the number of bikes available in a station, say, 30 minutes ahead. Then, once the 30 minutes have passed, the true number of available bikes will be available and we will be able to update the model using the features available 30 minutes ago. If you think about, this is exactly how a real-time machine learning system should work. The problem is that this isn't what the progressive_val_score method is emulating, indeed it is simply asking the model to predict the next observation, which is only a few minutes ahead, and then updates the model immediately. We can prove that this is flawed by adding a feature that measures a running average of the very recent values. X_y = datasets . Bikes () model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () evaluate . progressive_val_score ( X_y , model , metric , print_every = 20_000 ) [20,000] MAE: 20.159286 [40,000] MAE: 10.458898 [60,000] MAE: 7.2759 [80,000] MAE: 5.715397 [100,000] MAE: 4.775094 [120,000] MAE: 4.138421 [140,000] MAE: 3.682591 [160,000] MAE: 3.35015 [180,000] MAE: 3.091398 MAE: 3.06414 The score we got is too good to be true. This is simply because the problem is too easy. What we really want is to evaluate the model by forecasting 30 minutes ahead and only updating the model once the true values are available. This can be done using the moment and delay parameters in the progressive_val_score method. The idea is that each observation of the stream of the data is shown twice to the model: once for making a prediction, and once for updating the model when the true value is revealed. The moment parameter determines which variable should be used as a timestamp, while the delay parameter controls the duration to wait before revealing the true values to the model. import datetime as dt model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( dataset = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.24812 [40,000] MAE: 2.240287 [60,000] MAE: 2.270287 [80,000] MAE: 2.28649 [100,000] MAE: 2.294264 [120,000] MAE: 2.275891 [140,000] MAE: 2.261411 [160,000] MAE: 2.285978 [180,000] MAE: 2.289353 MAE: 2.29304 The score we now have is much more realistic, as it is comparable with the related data science competition . Moreover, we can see that the model gets better with time, which feels better than the previous situations. The point is that progressive_val_score method can be used to simulate a production scenario, and is thus extremely valuable. Now that we have a working pipeline in place, we can attempt to make it more accurate. As a simple example, we'll using a EWARegressor from the expert module to combine 3 linear regression model trained with different optimizers. The EWARegressor will run the 3 models in parallel and assign weights to each model based on their individual performance. from river import expert from river import optim model = compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) model += ( get_hour | feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) ) model += feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ( 0.5 )) model |= preprocessing . StandardScaler () model |= expert . EWARegressor ([ linear_model . LinearRegression ( optim . SGD ()), linear_model . LinearRegression ( optim . RMSProp ()), linear_model . LinearRegression ( optim . Adam ()) ]) evaluate . progressive_val_score ( dataset = datasets . Bikes (), model = model , metric = metrics . MAE (), moment = 'moment' , delay = dt . timedelta ( minutes = 30 ), print_every = 20_000 ) [20,000] MAE: 2.253263 [40,000] MAE: 2.242859 [60,000] MAE: 2.272001 [80,000] MAE: 2.287776 [100,000] MAE: 2.295292 [120,000] MAE: 2.276748 [140,000] MAE: 2.262146 [160,000] MAE: 2.286621 [180,000] MAE: 2.289925 MAE: 2.293604","title":"Bike-sharing forecasting"},{"location":"examples/building-a-simple-time-series-model/","text":"Building a simple time series model \u00b6 % matplotlib inline We'll be using the international airline passenger data available from here . This particular dataset is included with river in the datasets module. from river import datasets for x , y in datasets . AirlinePassengers (): print ( x , y ) break {'month': datetime.datetime(1949, 1, 1, 0, 0)} 112 The data is as simple as can be: it consists of a sequence of months and values representing the total number of international airline passengers per month. Our goal is going to be to predict the number of passengers for the next month at each step. Notice that because the dataset is small -- which is usually the case for time series -- we could just fit a model from scratch each month. However for the sake of example we're going to train a single model online. Although the overall performance might be potentially weaker, training a time series model online has the benefit of being scalable if, say, you have have thousands of time series to manage . We'll start with a very simple model where the only feature will be the ordinal date of each month. This should be able to capture some of the underlying trend. from river import compose from river import linear_model from river import preprocessing def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) We'll write down a function to evaluate the model. This will go through each observation in the dataset and update the model as it goes on. The prior predictions will be stored along with the true values and will be plotted together. from river import metrics import matplotlib.pyplot as plt def evaluate_model ( model ): metric = metrics . Rolling ( metrics . MAE (), 12 ) dates = [] y_trues = [] y_preds = [] for x , y in datasets . AirlinePassengers (): # Obtain the prior prediction and update the model in one go y_pred = model . predict_one ( x ) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_pred ) # Store the true value and the prediction dates . append ( x [ 'month' ]) y_trues . append ( y ) y_preds . append ( y_pred ) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Ground truth' ) ax . plot ( dates , y_preds , lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . legend () ax . set_title ( metric ) Let's evaluate our first model. evaluate_model ( model ) /Users/max.halford/anaconda3/envs/river/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 9 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/max.halford/anaconda3/envs/river/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 9 missing from current font. font.set_text(s, 0, flags=flags) The model has captured a trend but not the right one. Indeed it thinks the trend is linear whereas we can visually see that the growth of the data increases with time. In other words the second derivative of the series is positive. This is a well know problem in time series forecasting and there are thus many ways to handle it; for example by using a Box-Cox transform . However we are going to do something a bit different, and instead linearly detrend the series using a Detrender . We'll set window_size to 12 in order to use a rolling mean of size 12 for detrending. The Detrender will center the target in 0, which means that we don't need an intercept in our linear regression. We can thus set intercept_lr to 0. from river import stats from river import time_series model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )), ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) Now let's try and capture the monthly trend by one-hot encoding the month name. import calendar def get_month ( x ): return { calendar . month_name [ month ]: month == x [ 'month' ] . month for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This seems pretty decent. We can take a look at the weights of the linear regression to get an idea of the importance of each feature. model . regressor [ 'lin_reg' ] . weights {'January': -9.469216150196292, 'February': -14.616539494085554, 'March': -3.543831579749453, 'April': -2.766323821459067, 'May': -0.6224556651312263, 'June': 13.478341968972643, 'July': 28.772701948655836, 'August': 26.88560005378563, 'September': 4.707302312777268, 'October': -8.705184336916485, 'November': -22.48440069047625, 'December': -13.372426538578132, 'ordinal_date': 13.829969822169433} As could be expected the months of July and August have the highest weights because these are the months where people typically go on holiday abroad. The month of December has a low weight because this is a month of festivities in most of the Western world where people usually stay at home. Our model seems to understand which months are important, but it fails to see that the importance of each month grows multiplicatively as the years go on. In other words our model is too shy. We can fix this by increasing the learning rate of the LinearRegression 's optimizer. from river import optim model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This is starting to look good! Naturally in production we would tune the learning rate, ideally in real-time. Before finishing, we're going to introduce a cool feature extraction trick based on radial basis function kernels . The one-hot encoding we did on the month is a good idea but if you think about it is a bit rigid. Indeed the value of each feature is going to be 0 or 1, depending on the month of each observation. We're basically saying that the month of September is as distant to the month of August as it is to the month of March. Of course this isn't true, and it would be nice if our features would reflect this. To do so we can simply calculate the distance between the month of each observation and all the months in the calendar. Instead of simply computing the distance linearly, we're going to use a so-called Gaussian radial basic function kernel . This is a bit of a mouthful but for us it boils down to a simple formula, which is: \\[d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2})\\] Intuitively this computes a similarity between two months -- denoted by \\(i\\) and \\(j\\) -- which decreases the further apart they are from each other. The \\(sigma\\) parameter can be seen as a hyperparameter than can be tuned -- in the following snippet we'll simply ignore it. The thing to take away is that this results in smoother predictions than when using a one-hot encoding scheme, which is often a desirable property. You can also see trick in action in this nice presentation . import math def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month_distances' , compose . FuncTransformer ( get_month_distances )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) We've managed to get a good looking prediction curve with a reasonably simple model. What's more our model has the advantage of being interpretable and easy to debug. There surely are more rocks to squeeze (e.g. tune the hyperparameters, use an ensemble model, etc.) but we'll leave that as an exercice to the reader. As a finishing touch we'll rewrite our pipeline using the | operator, which is called a \"pipe\". extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model )","title":"Building a simple time series model"},{"location":"examples/building-a-simple-time-series-model/#building-a-simple-time-series-model","text":"% matplotlib inline We'll be using the international airline passenger data available from here . This particular dataset is included with river in the datasets module. from river import datasets for x , y in datasets . AirlinePassengers (): print ( x , y ) break {'month': datetime.datetime(1949, 1, 1, 0, 0)} 112 The data is as simple as can be: it consists of a sequence of months and values representing the total number of international airline passengers per month. Our goal is going to be to predict the number of passengers for the next month at each step. Notice that because the dataset is small -- which is usually the case for time series -- we could just fit a model from scratch each month. However for the sake of example we're going to train a single model online. Although the overall performance might be potentially weaker, training a time series model online has the benefit of being scalable if, say, you have have thousands of time series to manage . We'll start with a very simple model where the only feature will be the ordinal date of each month. This should be able to capture some of the underlying trend. from river import compose from river import linear_model from river import preprocessing def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) We'll write down a function to evaluate the model. This will go through each observation in the dataset and update the model as it goes on. The prior predictions will be stored along with the true values and will be plotted together. from river import metrics import matplotlib.pyplot as plt def evaluate_model ( model ): metric = metrics . Rolling ( metrics . MAE (), 12 ) dates = [] y_trues = [] y_preds = [] for x , y in datasets . AirlinePassengers (): # Obtain the prior prediction and update the model in one go y_pred = model . predict_one ( x ) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_pred ) # Store the true value and the prediction dates . append ( x [ 'month' ]) y_trues . append ( y ) y_preds . append ( y_pred ) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Ground truth' ) ax . plot ( dates , y_preds , lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . legend () ax . set_title ( metric ) Let's evaluate our first model. evaluate_model ( model ) /Users/max.halford/anaconda3/envs/river/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 9 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/max.halford/anaconda3/envs/river/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 9 missing from current font. font.set_text(s, 0, flags=flags) The model has captured a trend but not the right one. Indeed it thinks the trend is linear whereas we can visually see that the growth of the data increases with time. In other words the second derivative of the series is positive. This is a well know problem in time series forecasting and there are thus many ways to handle it; for example by using a Box-Cox transform . However we are going to do something a bit different, and instead linearly detrend the series using a Detrender . We'll set window_size to 12 in order to use a rolling mean of size 12 for detrending. The Detrender will center the target in 0, which means that we don't need an intercept in our linear regression. We can thus set intercept_lr to 0. from river import stats from river import time_series model = compose . Pipeline ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )), ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) Now let's try and capture the monthly trend by one-hot encoding the month name. import calendar def get_month ( x ): return { calendar . month_name [ month ]: month == x [ 'month' ] . month for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This seems pretty decent. We can take a look at the weights of the linear regression to get an idea of the importance of each feature. model . regressor [ 'lin_reg' ] . weights {'January': -9.469216150196292, 'February': -14.616539494085554, 'March': -3.543831579749453, 'April': -2.766323821459067, 'May': -0.6224556651312263, 'June': 13.478341968972643, 'July': 28.772701948655836, 'August': 26.88560005378563, 'September': 4.707302312777268, 'October': -8.705184336916485, 'November': -22.48440069047625, 'December': -13.372426538578132, 'ordinal_date': 13.829969822169433} As could be expected the months of July and August have the highest weights because these are the months where people typically go on holiday abroad. The month of December has a low weight because this is a month of festivities in most of the Western world where people usually stay at home. Our model seems to understand which months are important, but it fails to see that the importance of each month grows multiplicatively as the years go on. In other words our model is too shy. We can fix this by increasing the learning rate of the LinearRegression 's optimizer. from river import optim model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month' , compose . FuncTransformer ( get_month )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) This is starting to look good! Naturally in production we would tune the learning rate, ideally in real-time. Before finishing, we're going to introduce a cool feature extraction trick based on radial basis function kernels . The one-hot encoding we did on the month is a good idea but if you think about it is a bit rigid. Indeed the value of each feature is going to be 0 or 1, depending on the month of each observation. We're basically saying that the month of September is as distant to the month of August as it is to the month of March. Of course this isn't true, and it would be nice if our features would reflect this. To do so we can simply calculate the distance between the month of each observation and all the months in the calendar. Instead of simply computing the distance linearly, we're going to use a so-called Gaussian radial basic function kernel . This is a bit of a mouthful but for us it boils down to a simple formula, which is: \\[d(i, j) = exp(-\\frac{(i - j)^2}{2\\sigma^2})\\] Intuitively this computes a similarity between two months -- denoted by \\(i\\) and \\(j\\) -- which decreases the further apart they are from each other. The \\(sigma\\) parameter can be seen as a hyperparameter than can be tuned -- in the following snippet we'll simply ignore it. The thing to take away is that this results in smoother predictions than when using a one-hot encoding scheme, which is often a desirable property. You can also see trick in action in this nice presentation . import math def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'ordinal_date' , compose . FuncTransformer ( get_ordinal_date )), ( 'month_distances' , compose . FuncTransformer ( get_month_distances )), )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) )) ) model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model ) We've managed to get a good looking prediction curve with a reasonably simple model. What's more our model has the advantage of being interpretable and easy to debug. There surely are more rocks to squeeze (e.g. tune the hyperparameters, use an ensemble model, etc.) but we'll leave that as an exercice to the reader. As a finishing touch we'll rewrite our pipeline using the | operator, which is called a \"pipe\". extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 0.03 ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) evaluate_model ( model )","title":"Building a simple time series model"},{"location":"examples/concept-drift-detection/","text":"Concept Drift \u00b6 In the context of data streams, it is assumed that data can change over time. The change in the relationship between the data (features) and the target to learn is known as Concept Drift . As examples we can mention, the electricity demand across the year, the stock market, and the likelihood of a new movie to be successful. Let's consider the movie example: Two movies can have similar features such as popular actors/directors, storyline, production budget, marketing campaigns, etc. yet it is not certain that both will be similarly successful. What the target audience considers worth watching (and their money) is constantly changing and production companies must adapt accordingly to avoid \"box office flops\". Impact of drift on learning \u00b6 Concept drift can have a significant impact on predictive performance if not handled properly. Most batch learning models will fail in the presence of concept drift as they are essentially trained on different data. On the other hand, stream learning methods continuously update themselves and adapt to new concepts. Furthermore, drift-aware methods use change detection methods (a.k.a. drift detectors) to trigger mitigation mechanisms if a change in performance is detected. Detecting concept drift \u00b6 Multiple drift detection methods have been proposed. The goal of a drift detector is to signal an alarm in the presence of drift. A good drift detector maximizes the number of true positives while keeping the number of false positives to a minimum. It must also be resource-wise efficient to work in the context of infinite data streams. For this example, we will generate a synthetic data stream by concatenating 3 distributions of 1000 samples each: \\(dist_a\\) : \\(\\mu=0.8\\) , \\(\\sigma=0.05\\) \\(dist_b\\) : \\(\\mu=0.4\\) , \\(\\sigma=0.02\\) \\(dist_c\\) : \\(\\mu=0.6\\) , \\(\\sigma=0.1\\) . import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Generate data for 3 distributions random_state = np . random . RandomState ( seed = 42 ) dist_a = random_state . normal ( 0.8 , 0.05 , 1000 ) dist_b = random_state . normal ( 0.4 , 0.02 , 1000 ) dist_c = random_state . normal ( 0.6 , 0.1 , 1000 ) # Concatenate data to simulate a data stream with 2 drifts stream = np . concatenate (( dist_a , dist_b , dist_c )) # Auxiliary function to plot the data def plot_data ( dist_a , dist_b , dist_c , drifts = None ): fig = plt . figure ( figsize = ( 7 , 3 ), tight_layout = True ) gs = gridspec . GridSpec ( 1 , 2 , width_ratios = [ 3 , 1 ]) ax1 , ax2 = plt . subplot ( gs [ 0 ]), plt . subplot ( gs [ 1 ]) ax1 . grid () ax1 . plot ( stream , label = 'Stream' ) ax2 . grid ( axis = 'y' ) ax2 . hist ( dist_a , label = r '$dist_a$' ) ax2 . hist ( dist_b , label = r '$dist_b$' ) ax2 . hist ( dist_c , label = r '$dist_c$' ) if drifts is not None : for drift_detected in drifts : ax1 . axvline ( drift_detected , color = 'red' ) plt . show () plot_data ( dist_a , dist_b , dist_c ) Drift detection test \u00b6 We will use the ADaptive WINdowing ( ADWIN ) drift detection method. Remember that the goal is to indicate that drift has occurred after samples 1000 and 2000 in the synthetic data stream. from river import drift drift_detector = drift . ADWIN () drifts = [] for i , val in enumerate ( stream ): drift_detector . update ( val ) # Data is processed one sample at a time if drift_detector . change_detected : # The drift detector indicates after each sample if there is a drift in the data print ( f 'Change detected at index { i } ' ) drifts . append ( i ) drift_detector . reset () # As a best practice, we reset the detector plot_data ( dist_a , dist_b , dist_c , drifts ) Change detected at index 1055 Change detected at index 1087 Change detected at index 1151 Change detected at index 1183 Change detected at index 2111 Change detected at index 2143 Change detected at index 2207 Change detected at index 2335 We see that ADWIN successfully indicates the presence of drift (red vertical lines) close to the begining of a new data distribution. We conclude this example with some remarks regarding concept drift detectors and their usage: In practice, drift detectors provide stream learning methods with robustness against concept drift. Drift detectors monitor the model usually through a performance metric. Drift detectors work on univariate data. This is why they are used to monitor a model's performance and not the data itself. Remember that concept drift is defined as a change in the relationship between data and the target to learn (in supervised learning). Drift detectors define their expectations regarding input data. It is important to know these expectations to feed a given drift detector with the correct data.","title":"Concept Drift"},{"location":"examples/concept-drift-detection/#concept-drift","text":"In the context of data streams, it is assumed that data can change over time. The change in the relationship between the data (features) and the target to learn is known as Concept Drift . As examples we can mention, the electricity demand across the year, the stock market, and the likelihood of a new movie to be successful. Let's consider the movie example: Two movies can have similar features such as popular actors/directors, storyline, production budget, marketing campaigns, etc. yet it is not certain that both will be similarly successful. What the target audience considers worth watching (and their money) is constantly changing and production companies must adapt accordingly to avoid \"box office flops\".","title":"Concept Drift"},{"location":"examples/concept-drift-detection/#impact-of-drift-on-learning","text":"Concept drift can have a significant impact on predictive performance if not handled properly. Most batch learning models will fail in the presence of concept drift as they are essentially trained on different data. On the other hand, stream learning methods continuously update themselves and adapt to new concepts. Furthermore, drift-aware methods use change detection methods (a.k.a. drift detectors) to trigger mitigation mechanisms if a change in performance is detected.","title":"Impact of drift on learning"},{"location":"examples/concept-drift-detection/#detecting-concept-drift","text":"Multiple drift detection methods have been proposed. The goal of a drift detector is to signal an alarm in the presence of drift. A good drift detector maximizes the number of true positives while keeping the number of false positives to a minimum. It must also be resource-wise efficient to work in the context of infinite data streams. For this example, we will generate a synthetic data stream by concatenating 3 distributions of 1000 samples each: \\(dist_a\\) : \\(\\mu=0.8\\) , \\(\\sigma=0.05\\) \\(dist_b\\) : \\(\\mu=0.4\\) , \\(\\sigma=0.02\\) \\(dist_c\\) : \\(\\mu=0.6\\) , \\(\\sigma=0.1\\) . import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec # Generate data for 3 distributions random_state = np . random . RandomState ( seed = 42 ) dist_a = random_state . normal ( 0.8 , 0.05 , 1000 ) dist_b = random_state . normal ( 0.4 , 0.02 , 1000 ) dist_c = random_state . normal ( 0.6 , 0.1 , 1000 ) # Concatenate data to simulate a data stream with 2 drifts stream = np . concatenate (( dist_a , dist_b , dist_c )) # Auxiliary function to plot the data def plot_data ( dist_a , dist_b , dist_c , drifts = None ): fig = plt . figure ( figsize = ( 7 , 3 ), tight_layout = True ) gs = gridspec . GridSpec ( 1 , 2 , width_ratios = [ 3 , 1 ]) ax1 , ax2 = plt . subplot ( gs [ 0 ]), plt . subplot ( gs [ 1 ]) ax1 . grid () ax1 . plot ( stream , label = 'Stream' ) ax2 . grid ( axis = 'y' ) ax2 . hist ( dist_a , label = r '$dist_a$' ) ax2 . hist ( dist_b , label = r '$dist_b$' ) ax2 . hist ( dist_c , label = r '$dist_c$' ) if drifts is not None : for drift_detected in drifts : ax1 . axvline ( drift_detected , color = 'red' ) plt . show () plot_data ( dist_a , dist_b , dist_c )","title":"Detecting concept drift"},{"location":"examples/concept-drift-detection/#drift-detection-test","text":"We will use the ADaptive WINdowing ( ADWIN ) drift detection method. Remember that the goal is to indicate that drift has occurred after samples 1000 and 2000 in the synthetic data stream. from river import drift drift_detector = drift . ADWIN () drifts = [] for i , val in enumerate ( stream ): drift_detector . update ( val ) # Data is processed one sample at a time if drift_detector . change_detected : # The drift detector indicates after each sample if there is a drift in the data print ( f 'Change detected at index { i } ' ) drifts . append ( i ) drift_detector . reset () # As a best practice, we reset the detector plot_data ( dist_a , dist_b , dist_c , drifts ) Change detected at index 1055 Change detected at index 1087 Change detected at index 1151 Change detected at index 1183 Change detected at index 2111 Change detected at index 2143 Change detected at index 2207 Change detected at index 2335 We see that ADWIN successfully indicates the presence of drift (red vertical lines) close to the begining of a new data distribution. We conclude this example with some remarks regarding concept drift detectors and their usage: In practice, drift detectors provide stream learning methods with robustness against concept drift. Drift detectors monitor the model usually through a performance metric. Drift detectors work on univariate data. This is why they are used to monitor a model's performance and not the data itself. Remember that concept drift is defined as a change in the relationship between data and the target to learn (in supervised learning). Drift detectors define their expectations regarding input data. It is important to know these expectations to feed a given drift detector with the correct data.","title":"Drift detection test"},{"location":"examples/debugging-a-pipeline/","text":"Debugging a pipeline \u00b6 river encourages users to make use of pipelines. The biggest pain point of pipelines is that it can be hard to understand what's happening to the data, especially when the pipeline is complex. Fortunately the Pipeline class has a debug_one method that can help out. Let's look at a fairly complex pipeline for predicting the number of bikes in 5 bike stations from the city of Toulouse. It doesn't matter if you understand the pipeline or not; the point of this notebook is to learn how to introspect a pipeline. import datetime as dt from river import compose from river import datasets from river import feature_extraction from river import linear_model from river import metrics from river import preprocessing from river import stats from river import stream X_y = datasets . Bikes () X_y = stream . simulate_qa ( X_y , moment = 'moment' , delay = dt . timedelta ( minutes = 30 )) def add_time_features ( x ): return { ** x , 'hour' : x [ 'moment' ] . hour , 'day' : x [ 'moment' ] . weekday () } model = add_time_features model |= ( compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) + feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () questions = {} for i , x , y in X_y : # Question is_question = y is None if is_question : y_pred = model . predict_one ( x ) questions [ i ] = y_pred # Answer else : metric . update ( y , questions [ i ]) model = model . learn_one ( x , y ) if i >= 30000 and i % 30000 == 0 : print ( i , metric ) 30000 MAE: 2.220942 60000 MAE: 2.270271 90000 MAE: 2.301302 120000 MAE: 2.275876 150000 MAE: 2.275224 180000 MAE: 2.289347 Let's start by looking at the pipeline. You can click each cell to display the current state for each step of the pipeline. model add_time_features def add_time_features(x): return { **x, 'hour': x['moment'].hour, 'day': x['moment'].weekday() } ['clouds', 'humidity', 'pressure', 'temperature', 'wind'] {'whitelist': {'wind', 'humidity', 'pressure', 'clouds', 'temperature'}} target_mean_by_station_and_hour {'by': ['station', 'hour'], 'feature_name': 'target_mean_by_station_and_hour', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f8a927b2700>, Mean: 0.), {'metro-canal-du-midi_0': Mean: 7.93981, 'metro-canal-du-midi_1': Mean: 8.179704, 'metro-canal-du-midi_10': Mean: 12.486815, 'metro-canal-du-midi_11': Mean: 11.675479, 'metro-canal-du-midi_12': Mean: 10.197409, 'metro-canal-du-midi_13': Mean: 10.650855, 'metro-canal-du-midi_14': Mean: 11.109123, 'metro-canal-du-midi_15': Mean: 11.068934, 'metro-canal-du-midi_16': Mean: 11.274958, 'metro-canal-du-midi_17': Mean: 8.459136, 'metro-canal-du-midi_18': Mean: 7.587469, 'metro-canal-du-midi_19': Mean: 7.734677, 'metro-canal-du-midi_2': Mean: 8.35824, 'metro-canal-du-midi_20': Mean: 7.582465, 'metro-canal-du-midi_21': Mean: 7.190665, 'metro-canal-du-midi_22': Mean: 7.486895, 'metro-canal-du-midi_23': Mean: 7.840791, 'metro-canal-du-midi_3': Mean: 8.656051, 'metro-canal-du-midi_4': Mean: 8.868445, 'metro-canal-du-midi_5': Mean: 8.99656, 'metro-canal-du-midi_6': Mean: 9.09966, 'metro-canal-du-midi_7': Mean: 8.852642, 'metro-canal-du-midi_8': Mean: 12.66712, 'metro-canal-du-midi_9': Mean: 13.412186, 'place-des-carmes_0': Mean: 4.720696, 'place-des-carmes_1': Mean: 3.390295, 'place-des-carmes_10': Mean: 8.575303, 'place-des-carmes_11': Mean: 9.321546, 'place-des-carmes_12': Mean: 10.511931, 'place-des-carmes_13': Mean: 11.392745, 'place-des-carmes_14': Mean: 10.735003, 'place-des-carmes_15': Mean: 10.198787, 'place-des-carmes_16': Mean: 9.941479, 'place-des-carmes_17': Mean: 9.125579, 'place-des-carmes_18': Mean: 7.660775, 'place-des-carmes_19': Mean: 6.847649, 'place-des-carmes_2': Mean: 2.232181, 'place-des-carmes_20': Mean: 9.626876, 'place-des-carmes_21': Mean: 11.602929, 'place-des-carmes_22': Mean: 10.405537, 'place-des-carmes_23': Mean: 7.700904, 'place-des-carmes_3': Mean: 1.371981, 'place-des-carmes_4': Mean: 1.051665, 'place-des-carmes_5': Mean: 0.984993, 'place-des-carmes_6': Mean: 2.039947, 'place-des-carmes_7': Mean: 3.850369, 'place-des-carmes_8': Mean: 3.792624, 'place-des-carmes_9': Mean: 5.957182, 'place-esquirol_0': Mean: 7.415789, 'place-esquirol_1': Mean: 5.244396, 'place-esquirol_10': Mean: 19.465005, 'place-esquirol_11': Mean: 22.976512, 'place-esquirol_12': Mean: 25.324159, 'place-esquirol_13': Mean: 25.428847, 'place-esquirol_14': Mean: 24.57762, 'place-esquirol_15': Mean: 24.416851, 'place-esquirol_16': Mean: 23.555125, 'place-esquirol_17': Mean: 22.062564, 'place-esquirol_18': Mean: 18.10623, 'place-esquirol_19': Mean: 11.916638, 'place-esquirol_2': Mean: 2.858635, 'place-esquirol_20': Mean: 13.346362, 'place-esquirol_21': Mean: 16.743318, 'place-esquirol_22': Mean: 15.562088, 'place-esquirol_23': Mean: 10.911134, 'place-esquirol_3': Mean: 1.155929, 'place-esquirol_4': Mean: 0.73306, 'place-esquirol_5': Mean: 0.668546, 'place-esquirol_6': Mean: 1.21265, 'place-esquirol_7': Mean: 3.107535, 'place-esquirol_8': Mean: 8.518696, 'place-esquirol_9': Mean: 15.470588, 'place-jeanne-darc_0': Mean: 6.541667, 'place-jeanne-darc_1': Mean: 5.99892, 'place-jeanne-darc_10': Mean: 5.735553, 'place-jeanne-darc_11': Mean: 5.616142, 'place-jeanne-darc_12': Mean: 5.787478, 'place-jeanne-darc_13': Mean: 5.817699, 'place-jeanne-darc_14': Mean: 5.657546, 'place-jeanne-darc_15': Mean: 6.224604, 'place-jeanne-darc_16': Mean: 5.796141, 'place-jeanne-darc_17': Mean: 5.743089, 'place-jeanne-darc_18': Mean: 5.674784, 'place-jeanne-darc_19': Mean: 5.833068, 'place-jeanne-darc_2': Mean: 5.598169, 'place-jeanne-darc_20': Mean: 6.015755, 'place-jeanne-darc_21': Mean: 6.242541, 'place-jeanne-darc_22': Mean: 6.141509, 'place-jeanne-darc_23': Mean: 6.493028, 'place-jeanne-darc_3': Mean: 5.180556, 'place-jeanne-darc_4': Mean: 4.779626, 'place-jeanne-darc_5': Mean: 4.67063, 'place-jeanne-darc_6': Mean: 4.611995, 'place-jeanne-darc_7': Mean: 4.960718, 'place-jeanne-darc_8': Mean: 5.552273, 'place-jeanne-darc_9': Mean: 6.249573, 'pomme_0': Mean: 3.301532, 'pomme_1': Mean: 2.312914, 'pomme_10': Mean: 11.791436, 'pomme_11': Mean: 12.976854, 'pomme_12': Mean: 13.962654, 'pomme_13': Mean: 11.692257, 'pomme_14': Mean: 11.180851, 'pomme_15': Mean: 11.939586, 'pomme_16': Mean: 12.267051, 'pomme_17': Mean: 12.132993, 'pomme_18': Mean: 11.399108, 'pomme_19': Mean: 6.37021, 'pomme_2': Mean: 2.144453, 'pomme_20': Mean: 5.279234, 'pomme_21': Mean: 6.254257, 'pomme_22': Mean: 6.568678, 'pomme_23': Mean: 5.235756, 'pomme_3': Mean: 1.563622, 'pomme_4': Mean: 0.947328, 'pomme_5': Mean: 0.924175, 'pomme_6': Mean: 1.287805, 'pomme_7': Mean: 1.299456, 'pomme_8': Mean: 2.94988, 'pomme_9': Mean: 7.89396}), 'how': Mean: 0., 'target_name': 'target'} target_ewm_0.5_by_station {'by': ['station'], 'feature_name': 'target_ewm_0.5_by_station', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f8a927b2700>, EWMean: 0.), {'metro-canal-du-midi': EWMean: 4.690531, 'place-des-carmes': EWMean: 3.295317, 'place-esquirol': EWMean: 31.539759, 'place-jeanne-darc': EWMean: 22.449934, 'pomme': EWMean: 11.803716}), 'how': EWMean: 0., 'target_name': 'target'} StandardScaler {'counts': Counter({'target_ewm_0.5_by_station': 182470, 'target_mean_by_station_and_hour': 182470, 'wind': 182470, 'humidity': 182470, 'pressure': 182470, 'clouds': 182470, 'temperature': 182470}), 'means': defaultdict(<class 'float'>, {'clouds': 30.315131254453505, 'humidity': 62.24244533347998, 'pressure': 1017.0563060996391, 'target_ewm_0.5_by_station': 10.08331958752748, 'target_mean_by_station_and_hour': 9.410348580619415, 'temperature': 20.50980692716619, 'wind': 3.4184331122924543}), 'vars': defaultdict(<class 'float'>, {'clouds': 1389.0025610928221, 'humidity': 349.59967918503554, 'pressure': 33.298307526514115, 'target_ewm_0.5_by_station': 80.17355266024735, 'target_mean_by_station_and_hour': 33.98249801051089, 'temperature': 34.70701720774977, 'wind': 4.473627075744674})} LinearRegression {'_weights': {'target_ewm_0.5_by_station': 9.264175276315454, 'target_mean_by_station_and_hour': 0.1980140007049783, 'wind': -0.040879547751792, 'humidity': 1.0125248437612904, 'pressure': 0.18137498909137345, 'clouds': -0.3269694794458286, 'temperature': -0.42112178062192257}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 9.223158690689178, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 182470})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } As mentioned above the Pipeline class has a debug_one method. You can use this at any point you want to visualize what happen to an input x . For example, let's see what happens to the last seen x . print ( model . debug_one ( x )) 0. Input -------- clouds: 88 (int) description: overcast clouds (str) humidity: 84 (int) moment: 2016-10-05 09:57:18 (datetime) pressure: 1,017.34000 (float) station: pomme (str) temperature: 17.45000 (float) wind: 1.95000 (float) 1. add_time_features -------------------- clouds: 88 (int) day: 2 (int) description: overcast clouds (str) hour: 9 (int) humidity: 84 (int) moment: 2016-10-05 09:57:18 (datetime) pressure: 1,017.34000 (float) station: pomme (str) temperature: 17.45000 (float) wind: 1.95000 (float) 2. Transformer union -------------------- 2.0 Select ---------- clouds: 88 (int) humidity: 84 (int) pressure: 1,017.34000 (float) temperature: 17.45000 (float) wind: 1.95000 (float) 2.1 TargetAgg ------------- target_mean_by_station_and_hour: 7.89396 (float) 2.2 TargetAgg1 -------------- target_ewm_0.5_by_station: 11.80372 (float) clouds: 88 (int) humidity: 84 (int) pressure: 1,017.34000 (float) target_ewm_0.5_by_station: 11.80372 (float) target_mean_by_station_and_hour: 7.89396 (float) temperature: 17.45000 (float) wind: 1.95000 (float) 3. StandardScaler ----------------- clouds: 1.54778 (float) humidity: 1.16366 (float) pressure: 0.04916 (float) target_ewm_0.5_by_station: 0.19214 (float) target_mean_by_station_and_hour: -0.26013 (float) temperature: -0.51938 (float) wind: -0.69426 (float) 4. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 9.22316 9.22316 target_ewm_0.5_by_station 0.19214 9.26418 1.78000 humidity 1.16366 1.01252 1.17823 temperature -0.51938 -0.42112 0.21872 wind -0.69426 -0.04088 0.02838 pressure 0.04916 0.18137 0.00892 target_mean_by_station_and_hour -0.26013 0.19801 -0.05151 clouds 1.54778 -0.32697 -0.50608 Prediction: 11.87982 The pipeline does quite a few things, but using debug_one shows what happens step by step. This is really useful for checking that the pipeline is behaving as you're expecting it too. Remember that you can debug_one whenever you wish, be it before, during, or after training a model.","title":"Debugging a pipeline"},{"location":"examples/debugging-a-pipeline/#debugging-a-pipeline","text":"river encourages users to make use of pipelines. The biggest pain point of pipelines is that it can be hard to understand what's happening to the data, especially when the pipeline is complex. Fortunately the Pipeline class has a debug_one method that can help out. Let's look at a fairly complex pipeline for predicting the number of bikes in 5 bike stations from the city of Toulouse. It doesn't matter if you understand the pipeline or not; the point of this notebook is to learn how to introspect a pipeline. import datetime as dt from river import compose from river import datasets from river import feature_extraction from river import linear_model from river import metrics from river import preprocessing from river import stats from river import stream X_y = datasets . Bikes () X_y = stream . simulate_qa ( X_y , moment = 'moment' , delay = dt . timedelta ( minutes = 30 )) def add_time_features ( x ): return { ** x , 'hour' : x [ 'moment' ] . hour , 'day' : x [ 'moment' ] . weekday () } model = add_time_features model |= ( compose . Select ( 'clouds' , 'humidity' , 'pressure' , 'temperature' , 'wind' ) + feature_extraction . TargetAgg ( by = [ 'station' , 'hour' ], how = stats . Mean ()) + feature_extraction . TargetAgg ( by = 'station' , how = stats . EWMean ()) ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () metric = metrics . MAE () questions = {} for i , x , y in X_y : # Question is_question = y is None if is_question : y_pred = model . predict_one ( x ) questions [ i ] = y_pred # Answer else : metric . update ( y , questions [ i ]) model = model . learn_one ( x , y ) if i >= 30000 and i % 30000 == 0 : print ( i , metric ) 30000 MAE: 2.220942 60000 MAE: 2.270271 90000 MAE: 2.301302 120000 MAE: 2.275876 150000 MAE: 2.275224 180000 MAE: 2.289347 Let's start by looking at the pipeline. You can click each cell to display the current state for each step of the pipeline. model add_time_features def add_time_features(x): return { **x, 'hour': x['moment'].hour, 'day': x['moment'].weekday() } ['clouds', 'humidity', 'pressure', 'temperature', 'wind'] {'whitelist': {'wind', 'humidity', 'pressure', 'clouds', 'temperature'}} target_mean_by_station_and_hour {'by': ['station', 'hour'], 'feature_name': 'target_mean_by_station_and_hour', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f8a927b2700>, Mean: 0.), {'metro-canal-du-midi_0': Mean: 7.93981, 'metro-canal-du-midi_1': Mean: 8.179704, 'metro-canal-du-midi_10': Mean: 12.486815, 'metro-canal-du-midi_11': Mean: 11.675479, 'metro-canal-du-midi_12': Mean: 10.197409, 'metro-canal-du-midi_13': Mean: 10.650855, 'metro-canal-du-midi_14': Mean: 11.109123, 'metro-canal-du-midi_15': Mean: 11.068934, 'metro-canal-du-midi_16': Mean: 11.274958, 'metro-canal-du-midi_17': Mean: 8.459136, 'metro-canal-du-midi_18': Mean: 7.587469, 'metro-canal-du-midi_19': Mean: 7.734677, 'metro-canal-du-midi_2': Mean: 8.35824, 'metro-canal-du-midi_20': Mean: 7.582465, 'metro-canal-du-midi_21': Mean: 7.190665, 'metro-canal-du-midi_22': Mean: 7.486895, 'metro-canal-du-midi_23': Mean: 7.840791, 'metro-canal-du-midi_3': Mean: 8.656051, 'metro-canal-du-midi_4': Mean: 8.868445, 'metro-canal-du-midi_5': Mean: 8.99656, 'metro-canal-du-midi_6': Mean: 9.09966, 'metro-canal-du-midi_7': Mean: 8.852642, 'metro-canal-du-midi_8': Mean: 12.66712, 'metro-canal-du-midi_9': Mean: 13.412186, 'place-des-carmes_0': Mean: 4.720696, 'place-des-carmes_1': Mean: 3.390295, 'place-des-carmes_10': Mean: 8.575303, 'place-des-carmes_11': Mean: 9.321546, 'place-des-carmes_12': Mean: 10.511931, 'place-des-carmes_13': Mean: 11.392745, 'place-des-carmes_14': Mean: 10.735003, 'place-des-carmes_15': Mean: 10.198787, 'place-des-carmes_16': Mean: 9.941479, 'place-des-carmes_17': Mean: 9.125579, 'place-des-carmes_18': Mean: 7.660775, 'place-des-carmes_19': Mean: 6.847649, 'place-des-carmes_2': Mean: 2.232181, 'place-des-carmes_20': Mean: 9.626876, 'place-des-carmes_21': Mean: 11.602929, 'place-des-carmes_22': Mean: 10.405537, 'place-des-carmes_23': Mean: 7.700904, 'place-des-carmes_3': Mean: 1.371981, 'place-des-carmes_4': Mean: 1.051665, 'place-des-carmes_5': Mean: 0.984993, 'place-des-carmes_6': Mean: 2.039947, 'place-des-carmes_7': Mean: 3.850369, 'place-des-carmes_8': Mean: 3.792624, 'place-des-carmes_9': Mean: 5.957182, 'place-esquirol_0': Mean: 7.415789, 'place-esquirol_1': Mean: 5.244396, 'place-esquirol_10': Mean: 19.465005, 'place-esquirol_11': Mean: 22.976512, 'place-esquirol_12': Mean: 25.324159, 'place-esquirol_13': Mean: 25.428847, 'place-esquirol_14': Mean: 24.57762, 'place-esquirol_15': Mean: 24.416851, 'place-esquirol_16': Mean: 23.555125, 'place-esquirol_17': Mean: 22.062564, 'place-esquirol_18': Mean: 18.10623, 'place-esquirol_19': Mean: 11.916638, 'place-esquirol_2': Mean: 2.858635, 'place-esquirol_20': Mean: 13.346362, 'place-esquirol_21': Mean: 16.743318, 'place-esquirol_22': Mean: 15.562088, 'place-esquirol_23': Mean: 10.911134, 'place-esquirol_3': Mean: 1.155929, 'place-esquirol_4': Mean: 0.73306, 'place-esquirol_5': Mean: 0.668546, 'place-esquirol_6': Mean: 1.21265, 'place-esquirol_7': Mean: 3.107535, 'place-esquirol_8': Mean: 8.518696, 'place-esquirol_9': Mean: 15.470588, 'place-jeanne-darc_0': Mean: 6.541667, 'place-jeanne-darc_1': Mean: 5.99892, 'place-jeanne-darc_10': Mean: 5.735553, 'place-jeanne-darc_11': Mean: 5.616142, 'place-jeanne-darc_12': Mean: 5.787478, 'place-jeanne-darc_13': Mean: 5.817699, 'place-jeanne-darc_14': Mean: 5.657546, 'place-jeanne-darc_15': Mean: 6.224604, 'place-jeanne-darc_16': Mean: 5.796141, 'place-jeanne-darc_17': Mean: 5.743089, 'place-jeanne-darc_18': Mean: 5.674784, 'place-jeanne-darc_19': Mean: 5.833068, 'place-jeanne-darc_2': Mean: 5.598169, 'place-jeanne-darc_20': Mean: 6.015755, 'place-jeanne-darc_21': Mean: 6.242541, 'place-jeanne-darc_22': Mean: 6.141509, 'place-jeanne-darc_23': Mean: 6.493028, 'place-jeanne-darc_3': Mean: 5.180556, 'place-jeanne-darc_4': Mean: 4.779626, 'place-jeanne-darc_5': Mean: 4.67063, 'place-jeanne-darc_6': Mean: 4.611995, 'place-jeanne-darc_7': Mean: 4.960718, 'place-jeanne-darc_8': Mean: 5.552273, 'place-jeanne-darc_9': Mean: 6.249573, 'pomme_0': Mean: 3.301532, 'pomme_1': Mean: 2.312914, 'pomme_10': Mean: 11.791436, 'pomme_11': Mean: 12.976854, 'pomme_12': Mean: 13.962654, 'pomme_13': Mean: 11.692257, 'pomme_14': Mean: 11.180851, 'pomme_15': Mean: 11.939586, 'pomme_16': Mean: 12.267051, 'pomme_17': Mean: 12.132993, 'pomme_18': Mean: 11.399108, 'pomme_19': Mean: 6.37021, 'pomme_2': Mean: 2.144453, 'pomme_20': Mean: 5.279234, 'pomme_21': Mean: 6.254257, 'pomme_22': Mean: 6.568678, 'pomme_23': Mean: 5.235756, 'pomme_3': Mean: 1.563622, 'pomme_4': Mean: 0.947328, 'pomme_5': Mean: 0.924175, 'pomme_6': Mean: 1.287805, 'pomme_7': Mean: 1.299456, 'pomme_8': Mean: 2.94988, 'pomme_9': Mean: 7.89396}), 'how': Mean: 0., 'target_name': 'target'} target_ewm_0.5_by_station {'by': ['station'], 'feature_name': 'target_ewm_0.5_by_station', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f8a927b2700>, EWMean: 0.), {'metro-canal-du-midi': EWMean: 4.690531, 'place-des-carmes': EWMean: 3.295317, 'place-esquirol': EWMean: 31.539759, 'place-jeanne-darc': EWMean: 22.449934, 'pomme': EWMean: 11.803716}), 'how': EWMean: 0., 'target_name': 'target'} StandardScaler {'counts': Counter({'target_ewm_0.5_by_station': 182470, 'target_mean_by_station_and_hour': 182470, 'wind': 182470, 'humidity': 182470, 'pressure': 182470, 'clouds': 182470, 'temperature': 182470}), 'means': defaultdict(<class 'float'>, {'clouds': 30.315131254453505, 'humidity': 62.24244533347998, 'pressure': 1017.0563060996391, 'target_ewm_0.5_by_station': 10.08331958752748, 'target_mean_by_station_and_hour': 9.410348580619415, 'temperature': 20.50980692716619, 'wind': 3.4184331122924543}), 'vars': defaultdict(<class 'float'>, {'clouds': 1389.0025610928221, 'humidity': 349.59967918503554, 'pressure': 33.298307526514115, 'target_ewm_0.5_by_station': 80.17355266024735, 'target_mean_by_station_and_hour': 33.98249801051089, 'temperature': 34.70701720774977, 'wind': 4.473627075744674})} LinearRegression {'_weights': {'target_ewm_0.5_by_station': 9.264175276315454, 'target_mean_by_station_and_hour': 0.1980140007049783, 'wind': -0.040879547751792, 'humidity': 1.0125248437612904, 'pressure': 0.18137498909137345, 'clouds': -0.3269694794458286, 'temperature': -0.42112178062192257}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 9.223158690689178, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 182470})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } As mentioned above the Pipeline class has a debug_one method. You can use this at any point you want to visualize what happen to an input x . For example, let's see what happens to the last seen x . print ( model . debug_one ( x )) 0. Input -------- clouds: 88 (int) description: overcast clouds (str) humidity: 84 (int) moment: 2016-10-05 09:57:18 (datetime) pressure: 1,017.34000 (float) station: pomme (str) temperature: 17.45000 (float) wind: 1.95000 (float) 1. add_time_features -------------------- clouds: 88 (int) day: 2 (int) description: overcast clouds (str) hour: 9 (int) humidity: 84 (int) moment: 2016-10-05 09:57:18 (datetime) pressure: 1,017.34000 (float) station: pomme (str) temperature: 17.45000 (float) wind: 1.95000 (float) 2. Transformer union -------------------- 2.0 Select ---------- clouds: 88 (int) humidity: 84 (int) pressure: 1,017.34000 (float) temperature: 17.45000 (float) wind: 1.95000 (float) 2.1 TargetAgg ------------- target_mean_by_station_and_hour: 7.89396 (float) 2.2 TargetAgg1 -------------- target_ewm_0.5_by_station: 11.80372 (float) clouds: 88 (int) humidity: 84 (int) pressure: 1,017.34000 (float) target_ewm_0.5_by_station: 11.80372 (float) target_mean_by_station_and_hour: 7.89396 (float) temperature: 17.45000 (float) wind: 1.95000 (float) 3. StandardScaler ----------------- clouds: 1.54778 (float) humidity: 1.16366 (float) pressure: 0.04916 (float) target_ewm_0.5_by_station: 0.19214 (float) target_mean_by_station_and_hour: -0.26013 (float) temperature: -0.51938 (float) wind: -0.69426 (float) 4. LinearRegression ------------------- Name Value Weight Contribution Intercept 1.00000 9.22316 9.22316 target_ewm_0.5_by_station 0.19214 9.26418 1.78000 humidity 1.16366 1.01252 1.17823 temperature -0.51938 -0.42112 0.21872 wind -0.69426 -0.04088 0.02838 pressure 0.04916 0.18137 0.00892 target_mean_by_station_and_hour -0.26013 0.19801 -0.05151 clouds 1.54778 -0.32697 -0.50608 Prediction: 11.87982 The pipeline does quite a few things, but using debug_one shows what happens step by step. This is really useful for checking that the pipeline is behaving as you're expecting it too. Remember that you can debug_one whenever you wish, be it before, during, or after training a model.","title":"Debugging a pipeline"},{"location":"examples/imbalanced-learning/","text":"Working with imbalanced data \u00b6 In machine learning it is quite usual to have to deal with imbalanced dataset. This is particularly true in online learning for tasks such as fraud detection and spam classification. In these two cases, which are binary classification problems, there are usually many more 0s than 1s, which generally hinders the performance of the classifiers we thrown at them. As an example we'll use the credit card dataset available in river . We'll first use a collections.Counter to count the number of 0s and 1s in order to get an idea of the class balance. import collections from river import datasets X_y = datasets . CreditCard () counts = collections . Counter ( y for _ , y in X_y ) for c , count in counts . items (): print ( f ' { c } : { count } ( { count / sum ( counts . values ()) : .5% } )' ) 0: 284315 (99.82725%) 1: 492 (0.17275%) Baseline \u00b6 The dataset is quite unbalanced. For each 1 there are about 578 0s. Let's now train a logistic regression with default parameters and see how well it does. We'll measure the ROC AUC score. from river import linear_model from river import metrics from river import evaluate from river import preprocessing X_y = datasets . CreditCard () model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression () ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.891072 Importance weighting \u00b6 The performance is already quite acceptable, but as we will now see we can do even better. The first thing we can do is to add weight to the 1s by using the weight_pos argument of the Log loss function. from river import optim model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.914269 Focal loss \u00b6 The deep learning for object detection community has produced a special loss function for imbalaced learning called focal loss . We are doing binary classification, so we can plug the binary version of focal loss into our logistic regression and see how well it fairs. model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . BinaryFocalLoss ( 2 , 1 )) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.913072 Under-sampling the majority class \u00b6 Adding importance weights only works with gradient-based models (which includes neural networks). A more generic, and potentially more effective approach, is to use undersamplig and oversampling. As an example, we'll under-sample the stream so that our logistic regression encounter 20% of 1s and 80% of 0s. Under-sampling has the additional benefit of requiring less training steps, and thus reduces the total training time. from river import imblearn model = ( preprocessing . StandardScaler () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.948824 The RandomUnderSampler class is a wrapper for classifiers. This is represented by a rectangle around the logistic regression bubble when we visualize the model. model StandardScaler {'counts': Counter({'Time': 284807, 'V1': 284807, 'V2': 284807, 'V3': 284807, 'V4': 284807, 'V5': 284807, 'V6': 284807, 'V7': 284807, 'V8': 284807, 'V9': 284807, 'V10': 284807, 'V11': 284807, 'V12': 284807, 'V13': 284807, 'V14': 284807, 'V15': 284807, 'V16': 284807, 'V17': 284807, 'V18': 284807, 'V19': 284807, 'V20': 284807, 'V21': 284807, 'V22': 284807, 'V23': 284807, 'V24': 284807, 'V25': 284807, 'V26': 284807, 'V27': 284807, 'V28': 284807, 'Amount': 284807}), 'means': defaultdict(<class 'float'>, {'Amount': 88.34961925093155, 'Time': 94813.8595750808, 'V1': 2.9277520180090704e-15, 'V10': 2.419775348112352e-15, 'V11': 2.6777824308789593e-15, 'V12': -2.2140916080800113e-15, 'V13': 8.342900777166882e-16, 'V14': 1.903846574088133e-15, 'V15': 8.581815631423259e-15, 'V16': 1.4766213137618707e-15, 'V17': -1.6801787893383664e-16, 'V18': 5.854597499006342e-16, 'V19': 1.0841438330912623e-15, 'V2': 5.886023480140661e-16, 'V20': 7.744049542249276e-16, 'V21': 2.332071227413037e-16, 'V22': 4.956273530422241e-16, 'V23': -2.4249219202998693e-16, 'V24': 4.437131669261511e-15, 'V25': -6.981503896318856e-16, 'V26': 1.6805599541646309e-15, 'V27': -3.266881107112892e-16, 'V28': -1.173670292237036e-16, 'V3': -1.2140654523102711e-15, 'V4': 3.4083746059071583e-15, 'V5': 3.0974740213536643e-15, 'V6': 1.6259034591771526e-15, 'V7': -1.293283785185756e-16, 'V8': 3.1643541546820877e-16, 'V9': -1.6996522885539796e-15}), 'vars': defaultdict(<class 'float'>, {'Amount': 62559.84938856013, 'Time': 2255116088.124347, 'V1': 3.8364757815609964, 'V10': 1.1855896488198305, 'V11': 1.041851426830977, 'V12': 0.9983999112951535, 'V13': 0.9905673151089326, 'V14': 0.9189023195064231, 'V15': 0.837800459457307, 'V16': 0.7678164267285925, 'V17': 0.721370914880897, 'V18': 0.7025368914993138, 'V19': 0.6626596101863256, 'V2': 2.726810450381156, 'V20': 0.594323307231822, 'V21': 0.5395236333332668, 'V22': 0.5266409057048476, 'V23': 0.3899492915994535, 'V24': 0.3668070828485584, 'V25': 0.27172987273928645, 'V26': 0.23254207582578096, 'V27': 0.16291861895803472, 'V28': 0.10895457872151114, 'V3': 2.2990211684909436, 'V4': 2.004676782760293, 'V5': 1.905074357779823, 'V6': 1.7749400245019011, 'V7': 1.5303951971990823, 'V8': 1.426473847533605, 'V9': 1.2069882295421888})} RandomUnderSampler(LogisticRegression) {'_actual_dist': Counter({0: 284315, 1: 492}), '_pivot': 1, '_rng': RandomState(MT19937) at 0x7FFBF2E57A40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.01 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.8, 1: 0.2}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } Over-sampling the minority class \u00b6 We can also attain the same class distribution by over-sampling the minority class. This will come at cost of having to train with more samples. model = ( preprocessing . StandardScaler () | imblearn . RandomOverSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.918082 Sampling with a desired sample size \u00b6 The downside of both RandomUnderSampler and RandomOverSampler is that you don't have any control on the amount of data the classifier trains on. The number of samples is adjusted so that the target distribution can be attained, either by under-sampling or over-sampling. However, you can do both at the same time and choose how much data the classifier will see. To do so, we can use the RandomSampler class. In addition to the desired class distribution, we can specify how much data to train on. The samples will both be under-sampled and over-sampled in order to fit your constraints. This is powerful because it allows you to control both the class distribution and the size of the training data (and thus the training time). In the following example we'll set it so that the model will train with 1 percent of the data. model = ( preprocessing . StandardScaler () | imblearn . RandomSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, sampling_rate = .01 , seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.951296 Hybrid approach \u00b6 As you might have guessed by now, nothing is stopping you from mixing imbalanced learning methods together. As an example, let's combine sampling.RandomUnderSampler and the weight_pos parameter from the optim.losses.Log loss function. model = ( preprocessing . StandardScaler () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.968289","title":"Working with imbalanced data"},{"location":"examples/imbalanced-learning/#working-with-imbalanced-data","text":"In machine learning it is quite usual to have to deal with imbalanced dataset. This is particularly true in online learning for tasks such as fraud detection and spam classification. In these two cases, which are binary classification problems, there are usually many more 0s than 1s, which generally hinders the performance of the classifiers we thrown at them. As an example we'll use the credit card dataset available in river . We'll first use a collections.Counter to count the number of 0s and 1s in order to get an idea of the class balance. import collections from river import datasets X_y = datasets . CreditCard () counts = collections . Counter ( y for _ , y in X_y ) for c , count in counts . items (): print ( f ' { c } : { count } ( { count / sum ( counts . values ()) : .5% } )' ) 0: 284315 (99.82725%) 1: 492 (0.17275%)","title":"Working with imbalanced data"},{"location":"examples/imbalanced-learning/#baseline","text":"The dataset is quite unbalanced. For each 1 there are about 578 0s. Let's now train a logistic regression with default parameters and see how well it does. We'll measure the ROC AUC score. from river import linear_model from river import metrics from river import evaluate from river import preprocessing X_y = datasets . CreditCard () model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression () ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.891072","title":"Baseline"},{"location":"examples/imbalanced-learning/#importance-weighting","text":"The performance is already quite acceptable, but as we will now see we can do even better. The first thing we can do is to add weight to the 1s by using the weight_pos argument of the Log loss function. from river import optim model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.914269","title":"Importance weighting"},{"location":"examples/imbalanced-learning/#focal-loss","text":"The deep learning for object detection community has produced a special loss function for imbalaced learning called focal loss . We are doing binary classification, so we can plug the binary version of focal loss into our logistic regression and see how well it fairs. model = ( preprocessing . StandardScaler () | linear_model . LogisticRegression ( loss = optim . losses . BinaryFocalLoss ( 2 , 1 )) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.913072","title":"Focal loss"},{"location":"examples/imbalanced-learning/#under-sampling-the-majority-class","text":"Adding importance weights only works with gradient-based models (which includes neural networks). A more generic, and potentially more effective approach, is to use undersamplig and oversampling. As an example, we'll under-sample the stream so that our logistic regression encounter 20% of 1s and 80% of 0s. Under-sampling has the additional benefit of requiring less training steps, and thus reduces the total training time. from river import imblearn model = ( preprocessing . StandardScaler () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.948824 The RandomUnderSampler class is a wrapper for classifiers. This is represented by a rectangle around the logistic regression bubble when we visualize the model. model StandardScaler {'counts': Counter({'Time': 284807, 'V1': 284807, 'V2': 284807, 'V3': 284807, 'V4': 284807, 'V5': 284807, 'V6': 284807, 'V7': 284807, 'V8': 284807, 'V9': 284807, 'V10': 284807, 'V11': 284807, 'V12': 284807, 'V13': 284807, 'V14': 284807, 'V15': 284807, 'V16': 284807, 'V17': 284807, 'V18': 284807, 'V19': 284807, 'V20': 284807, 'V21': 284807, 'V22': 284807, 'V23': 284807, 'V24': 284807, 'V25': 284807, 'V26': 284807, 'V27': 284807, 'V28': 284807, 'Amount': 284807}), 'means': defaultdict(<class 'float'>, {'Amount': 88.34961925093155, 'Time': 94813.8595750808, 'V1': 2.9277520180090704e-15, 'V10': 2.419775348112352e-15, 'V11': 2.6777824308789593e-15, 'V12': -2.2140916080800113e-15, 'V13': 8.342900777166882e-16, 'V14': 1.903846574088133e-15, 'V15': 8.581815631423259e-15, 'V16': 1.4766213137618707e-15, 'V17': -1.6801787893383664e-16, 'V18': 5.854597499006342e-16, 'V19': 1.0841438330912623e-15, 'V2': 5.886023480140661e-16, 'V20': 7.744049542249276e-16, 'V21': 2.332071227413037e-16, 'V22': 4.956273530422241e-16, 'V23': -2.4249219202998693e-16, 'V24': 4.437131669261511e-15, 'V25': -6.981503896318856e-16, 'V26': 1.6805599541646309e-15, 'V27': -3.266881107112892e-16, 'V28': -1.173670292237036e-16, 'V3': -1.2140654523102711e-15, 'V4': 3.4083746059071583e-15, 'V5': 3.0974740213536643e-15, 'V6': 1.6259034591771526e-15, 'V7': -1.293283785185756e-16, 'V8': 3.1643541546820877e-16, 'V9': -1.6996522885539796e-15}), 'vars': defaultdict(<class 'float'>, {'Amount': 62559.84938856013, 'Time': 2255116088.124347, 'V1': 3.8364757815609964, 'V10': 1.1855896488198305, 'V11': 1.041851426830977, 'V12': 0.9983999112951535, 'V13': 0.9905673151089326, 'V14': 0.9189023195064231, 'V15': 0.837800459457307, 'V16': 0.7678164267285925, 'V17': 0.721370914880897, 'V18': 0.7025368914993138, 'V19': 0.6626596101863256, 'V2': 2.726810450381156, 'V20': 0.594323307231822, 'V21': 0.5395236333332668, 'V22': 0.5266409057048476, 'V23': 0.3899492915994535, 'V24': 0.3668070828485584, 'V25': 0.27172987273928645, 'V26': 0.23254207582578096, 'V27': 0.16291861895803472, 'V28': 0.10895457872151114, 'V3': 2.2990211684909436, 'V4': 2.004676782760293, 'V5': 1.905074357779823, 'V6': 1.7749400245019011, 'V7': 1.5303951971990823, 'V8': 1.426473847533605, 'V9': 1.2069882295421888})} RandomUnderSampler(LogisticRegression) {'_actual_dist': Counter({0: 284315, 1: 492}), '_pivot': 1, '_rng': RandomState(MT19937) at 0x7FFBF2E57A40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.01 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.8, 1: 0.2}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; }","title":"Under-sampling the majority class"},{"location":"examples/imbalanced-learning/#over-sampling-the-minority-class","text":"We can also attain the same class distribution by over-sampling the minority class. This will come at cost of having to train with more samples. model = ( preprocessing . StandardScaler () | imblearn . RandomOverSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.918082","title":"Over-sampling the minority class"},{"location":"examples/imbalanced-learning/#sampling-with-a-desired-sample-size","text":"The downside of both RandomUnderSampler and RandomOverSampler is that you don't have any control on the amount of data the classifier trains on. The number of samples is adjusted so that the target distribution can be attained, either by under-sampling or over-sampling. However, you can do both at the same time and choose how much data the classifier will see. To do so, we can use the RandomSampler class. In addition to the desired class distribution, we can specify how much data to train on. The samples will both be under-sampled and over-sampled in order to fit your constraints. This is powerful because it allows you to control both the class distribution and the size of the training data (and thus the training time). In the following example we'll set it so that the model will train with 1 percent of the data. model = ( preprocessing . StandardScaler () | imblearn . RandomSampler ( classifier = linear_model . LogisticRegression (), desired_dist = { 0 : .8 , 1 : .2 }, sampling_rate = .01 , seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.951296","title":"Sampling with a desired sample size"},{"location":"examples/imbalanced-learning/#hybrid-approach","text":"As you might have guessed by now, nothing is stopping you from mixing imbalanced learning methods together. As an example, let's combine sampling.RandomUnderSampler and the weight_pos parameter from the optim.losses.Log loss function. model = ( preprocessing . StandardScaler () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( loss = optim . losses . Log ( weight_pos = 5 ) ), desired_dist = { 0 : .8 , 1 : .2 }, seed = 42 ) ) metric = metrics . ROCAUC () evaluate . progressive_val_score ( X_y , model , metric ) ROCAUC: 0.968289","title":"Hybrid approach"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/","text":"Matrix Factorization for Recommender Systems - Part 1 \u00b6 Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning Introduction \u00b6 A recommender system is a software tool designed to generate and suggest items or entities to the users. Popular large scale examples include: Amazon (suggesting products) Facebook (suggesting posts in users' news feeds) Spotify (suggesting music) Social recommendation from graph (mostly used by social networks) are not covered in river . We focus on the general case, item recommendation. This problem can be represented with the user-item matrix: \\[ \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} \\] Where \\(U\\) and \\(I\\) are the number of user and item of the system, respectively. A matrix entry represents a user's preference for an item, it can be a rating, a like or dislike, etc. Because of the huge number of users and items compared to the number of observed entries, those matrices are very sparsed (usually less than 1% filled). Matrix Factorization (MF) is a class of collaborative filtering algorithms derived from Singular Value Decomposition (SVD) . MF strength lies in its capacity to able to model high cardinality categorical variables interactions. This subfield boomed during the famous Netflix Prize contest in 2006, when numerous novel variants has been invented and became popular thanks to their attractive accuracy and scalability. MF approach seeks to fill the user-item matrix considering the problem as a matrix completion one. MF core idea assume a latent model learning its own representation of the users and the items in a lower latent dimensional space by factorizing the observed parts of the matrix. A factorized user or item is represented as a vector \\(\\mathbf{v}_u\\) or \\(\\mathbf{v}_i\\) composed of \\(k\\) latent factors, with \\(k << U, I\\) . Those learnt latent variables represent, for an item the various aspects describing it, and for a user its interests in terms of those aspects. The model then assume a user's choice or fondness is composed of a sum of preferences about the various aspects of the concerned item. This sum being the dot product between the latent vectors of a given user-item pair: \\[ \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} \\] MF models weights are learnt in an online fashion, often with stochastic gradient descent as it provides relatively fast running time and good accuracy. There is a great and widely popular library named surprise that implements MF models (and others) but in contrast with river doesn't follow a pure online philosophy (all the data have to be loaded in memory and the API doesn't allow you to update your model with new data). Notes: In recent years, proposed deep learning techniques for recommendation tasks claim state of the art results. However, recent work (August 2019) showed that those promises can't be taken for granted and traditional MF methods are still relevant today. For more information about how the business value of recommender systems is measured and why they are one of the main success stories of machine learning, see the following literature survey (December 2019). Let's start \u00b6 In this tutorial, we are going to explore MF algorithms available in river and test them on a movie recommendation problem with the MovieLens 100K dataset. This latter is a collection of movie ratings (from 1 to 5) that includes various information about both the items and the users. We can access it from the river.datasets module: import json from river import datasets for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break Downloading https://maxhalford.github.io/files/datasets/ml_100k.zip (1.83 MB) Uncompressing into /Users/max.halford/river_data/MovieLens100K x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 Let's define a routine to evaluate our different models on MovieLens 100K. Mean Absolute Error and Root Mean Squared Error will be our metrics printed alongside model's computation time and memory usage: from river import metrics from river.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) Naive prediction \u00b6 It's good practice in machine learning to start with a naive baseline and then iterate from simple things to complex ones observing progress incrementally. Let's start by predicing the target running mean as a first shot: from river import stats mean = stats . Mean () metric = metrics . MAE () + metrics . RMSE () for i , x_y in enumerate ( datasets . MovieLens100K (), start = 1 ): _ , y = x_y metric . update ( y , mean . get ()) mean . update ( y ) if not i % 25_000 : print ( f '[ { i : ,d } ] { metric } ' ) [25,000] MAE: 0.934259, RMSE: 1.124469 [50,000] MAE: 0.923893, RMSE: 1.105 [75,000] MAE: 0.937359, RMSE: 1.123696 [100,000] MAE: 0.942162, RMSE: 1.125783 Baseline model \u00b6 Now we can do machine learning and explore available models in river.reco module starting with the baseline model. It extends our naive prediction by adding to the global running mean two bias terms characterizing the user and the item discrepancy from the general tendency. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} \\] This baseline model can be viewed as a linear regression where the intercept is replaced by the target running mean with the users and the items one hot encoded. All machine learning models in river expect dicts as input with feature names as keys and feature values as values. Specifically, models from river.reco expect a 'user' and an 'item' entries without any type constraint on their values (i.e. can be strings or numbers), e.g.: x = { 'user' : 'Guido' , 'item' : \"Monty Python's Flying Circus\" } Other entries, if exist, are simply ignored. This is quite useful as we don't need to spend time and storage doing one hot encoding. from river import meta from river import optim from river import reco baseline_params = { 'optimizer' : optim . SGD ( 0.025 ), 'l2' : 0. , 'initializer' : optim . initializers . Zeros () } model = meta . PredClipper ( regressor = reco . Baseline ( ** baseline_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761844, RMSE: 0.960972 \u2013 0:00:00.962592 \u2013 169.99 KB [50,000] MAE: 0.753292, RMSE: 0.951223 \u2013 0:00:02.222282 \u2013 238.63 KB [75,000] MAE: 0.754177, RMSE: 0.953376 \u2013 0:00:03.443442 \u2013 282.43 KB [100,000] MAE: 0.754651, RMSE: 0.954148 \u2013 0:00:04.465794 \u2013 306.03 KB We won two tenth of MAE compared to our naive prediction (0.7546 vs 0.9421) meaning that significant information has been learnt by the model. Funk Matrix Factorization (FunkMF) \u00b6 It's the pure form of matrix factorization consisting of only learning the users and items latent representations as discussed in introduction. Simon Funk popularized its stochastic gradient descent optimization in 2006 during the Netflix Prize. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\] Note: FunkMF is sometimes referred as Probabilistic Matrix Factorization which is an extended probabilistic version. funk_mf_params = { 'n_factors' : 10 , 'optimizer' : optim . SGD ( 0.05 ), 'l2' : 0.1 , 'initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ) } model = meta . PredClipper ( regressor = reco . FunkMF ( ** funk_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 1.070136, RMSE: 1.397014 \u2013 0:00:01.990650 \u2013 938.01 KB [50,000] MAE: 0.99174, RMSE: 1.290666 \u2013 0:00:04.081581 \u2013 1.13 MB [75,000] MAE: 0.961072, RMSE: 1.250842 \u2013 0:00:06.099587 \u2013 1.33 MB [100,000] MAE: 0.944883, RMSE: 1.227688 \u2013 0:00:08.139818 \u2013 1.5 MB Results are equivalent to our naive prediction (0.9448 vs 0.9421). By only focusing on the users preferences and the items characteristics, the model is limited in his ability to capture different views of the problem. Despite its poor performance alone, this algorithm is quite useful combined in other models or when we need to build dense representations for other tasks. Biased Matrix Factorization (BiasedMF) \u00b6 It's the combination of the Baseline model and FunkMF. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\] Note: Biased Matrix Factorization name is used by some people but some others refer to it by SVD or Funk SVD . It's the case of Yehuda Koren and Robert Bell in Recommender Systems Handbook (Chapter 5 Advances in Collaborative Filtering ) and of surprise library. Nevertheless, SVD could be confused with the original Singular Value Decomposition from which it's derived from, and Funk SVD could also be misleading because of the biased part of the model equation which doesn't come from Simon Funk's work. For those reasons, we chose to side with Biased Matrix Factorization which fits more naturally to it. biased_mf_params = { 'n_factors' : 10 , 'bias_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), 'l2_bias' : 0. , 'l2_latent' : 0. } model = meta . PredClipper ( regressor = reco . BiasedMF ( ** biased_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761818, RMSE: 0.961057 \u2013 0:00:02.215027 \u2013 1.01 MB [50,000] MAE: 0.751667, RMSE: 0.949443 \u2013 0:00:04.477109 \u2013 1.28 MB [75,000] MAE: 0.749653, RMSE: 0.948723 \u2013 0:00:06.553924 \u2013 1.51 MB [100,000] MAE: 0.748559, RMSE: 0.947854 \u2013 0:00:08.727386 \u2013 1.69 MB Results improved (0.7485 vs 0.7546) demonstrating that users and items latent representations bring additional information. To conclude this first tutorial about factorization models, let's review the important parameters to tune when dealing with this family of methods: n_factors : the number of latent factors. The more you set, the more items aspects and users preferences you are going to learn. Too many will cause overfitting, l2 regularization could help. *_optimizer : the optimizers. Classic stochastic gradient descent performs well, finding the good learning rate will make the difference. initializer : the latent weights initialization. Latent vectors have to be initialized with non-constant values. We generally sample them from a zero-mean normal distribution with small standard deviation.","title":"Matrix Factorization for Recommender Systems - Part 1"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#matrix-factorization-for-recommender-systems-part-1","text":"Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning","title":"Matrix Factorization for Recommender Systems - Part 1"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#introduction","text":"A recommender system is a software tool designed to generate and suggest items or entities to the users. Popular large scale examples include: Amazon (suggesting products) Facebook (suggesting posts in users' news feeds) Spotify (suggesting music) Social recommendation from graph (mostly used by social networks) are not covered in river . We focus on the general case, item recommendation. This problem can be represented with the user-item matrix: \\[ \\normalsize \\begin{matrix} & \\begin{matrix} _1 & _\\cdots & _\\cdots & _\\cdots & _I \\end{matrix} \\\\ \\begin{matrix} _1 \\\\ _\\vdots \\\\ _\\vdots \\\\ _\\vdots \\\\ _U \\end{matrix} & \\begin{bmatrix} {\\color{Red} ?} & 2 & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & 4.5 \\\\ \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\ 3 & {\\color{Red} ?} & \\cdots & {\\color{Red} ?} & {\\color{Red} ?} \\\\ {\\color{Red} ?} & {\\color{Red} ?} & \\cdots & 5 & {\\color{Red} ?} \\end{bmatrix} \\end{matrix} \\] Where \\(U\\) and \\(I\\) are the number of user and item of the system, respectively. A matrix entry represents a user's preference for an item, it can be a rating, a like or dislike, etc. Because of the huge number of users and items compared to the number of observed entries, those matrices are very sparsed (usually less than 1% filled). Matrix Factorization (MF) is a class of collaborative filtering algorithms derived from Singular Value Decomposition (SVD) . MF strength lies in its capacity to able to model high cardinality categorical variables interactions. This subfield boomed during the famous Netflix Prize contest in 2006, when numerous novel variants has been invented and became popular thanks to their attractive accuracy and scalability. MF approach seeks to fill the user-item matrix considering the problem as a matrix completion one. MF core idea assume a latent model learning its own representation of the users and the items in a lower latent dimensional space by factorizing the observed parts of the matrix. A factorized user or item is represented as a vector \\(\\mathbf{v}_u\\) or \\(\\mathbf{v}_i\\) composed of \\(k\\) latent factors, with \\(k << U, I\\) . Those learnt latent variables represent, for an item the various aspects describing it, and for a user its interests in terms of those aspects. The model then assume a user's choice or fondness is composed of a sum of preferences about the various aspects of the concerned item. This sum being the dot product between the latent vectors of a given user-item pair: \\[ \\normalsize \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{u, f} \\cdot \\mathbf{v}_{i, f} \\] MF models weights are learnt in an online fashion, often with stochastic gradient descent as it provides relatively fast running time and good accuracy. There is a great and widely popular library named surprise that implements MF models (and others) but in contrast with river doesn't follow a pure online philosophy (all the data have to be loaded in memory and the API doesn't allow you to update your model with new data). Notes: In recent years, proposed deep learning techniques for recommendation tasks claim state of the art results. However, recent work (August 2019) showed that those promises can't be taken for granted and traditional MF methods are still relevant today. For more information about how the business value of recommender systems is measured and why they are one of the main success stories of machine learning, see the following literature survey (December 2019).","title":"Introduction"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#lets-start","text":"In this tutorial, we are going to explore MF algorithms available in river and test them on a movie recommendation problem with the MovieLens 100K dataset. This latter is a collection of movie ratings (from 1 to 5) that includes various information about both the items and the users. We can access it from the river.datasets module: import json from river import datasets for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break Downloading https://maxhalford.github.io/files/datasets/ml_100k.zip (1.83 MB) Uncompressing into /Users/max.halford/river_data/MovieLens100K x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 Let's define a routine to evaluate our different models on MovieLens 100K. Mean Absolute Error and Root Mean Squared Error will be our metrics printed alongside model's computation time and memory usage: from river import metrics from river.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True )","title":"Let's start"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#naive-prediction","text":"It's good practice in machine learning to start with a naive baseline and then iterate from simple things to complex ones observing progress incrementally. Let's start by predicing the target running mean as a first shot: from river import stats mean = stats . Mean () metric = metrics . MAE () + metrics . RMSE () for i , x_y in enumerate ( datasets . MovieLens100K (), start = 1 ): _ , y = x_y metric . update ( y , mean . get ()) mean . update ( y ) if not i % 25_000 : print ( f '[ { i : ,d } ] { metric } ' ) [25,000] MAE: 0.934259, RMSE: 1.124469 [50,000] MAE: 0.923893, RMSE: 1.105 [75,000] MAE: 0.937359, RMSE: 1.123696 [100,000] MAE: 0.942162, RMSE: 1.125783","title":"Naive prediction"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#baseline-model","text":"Now we can do machine learning and explore available models in river.reco module starting with the baseline model. It extends our naive prediction by adding to the global running mean two bias terms characterizing the user and the item discrepancy from the general tendency. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} \\] This baseline model can be viewed as a linear regression where the intercept is replaced by the target running mean with the users and the items one hot encoded. All machine learning models in river expect dicts as input with feature names as keys and feature values as values. Specifically, models from river.reco expect a 'user' and an 'item' entries without any type constraint on their values (i.e. can be strings or numbers), e.g.: x = { 'user' : 'Guido' , 'item' : \"Monty Python's Flying Circus\" } Other entries, if exist, are simply ignored. This is quite useful as we don't need to spend time and storage doing one hot encoding. from river import meta from river import optim from river import reco baseline_params = { 'optimizer' : optim . SGD ( 0.025 ), 'l2' : 0. , 'initializer' : optim . initializers . Zeros () } model = meta . PredClipper ( regressor = reco . Baseline ( ** baseline_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761844, RMSE: 0.960972 \u2013 0:00:00.962592 \u2013 169.99 KB [50,000] MAE: 0.753292, RMSE: 0.951223 \u2013 0:00:02.222282 \u2013 238.63 KB [75,000] MAE: 0.754177, RMSE: 0.953376 \u2013 0:00:03.443442 \u2013 282.43 KB [100,000] MAE: 0.754651, RMSE: 0.954148 \u2013 0:00:04.465794 \u2013 306.03 KB We won two tenth of MAE compared to our naive prediction (0.7546 vs 0.9421) meaning that significant information has been learnt by the model.","title":"Baseline model"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#funk-matrix-factorization-funkmf","text":"It's the pure form of matrix factorization consisting of only learning the users and items latent representations as discussed in introduction. Simon Funk popularized its stochastic gradient descent optimization in 2006 during the Netflix Prize. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\] Note: FunkMF is sometimes referred as Probabilistic Matrix Factorization which is an extended probabilistic version. funk_mf_params = { 'n_factors' : 10 , 'optimizer' : optim . SGD ( 0.05 ), 'l2' : 0.1 , 'initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ) } model = meta . PredClipper ( regressor = reco . FunkMF ( ** funk_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 1.070136, RMSE: 1.397014 \u2013 0:00:01.990650 \u2013 938.01 KB [50,000] MAE: 0.99174, RMSE: 1.290666 \u2013 0:00:04.081581 \u2013 1.13 MB [75,000] MAE: 0.961072, RMSE: 1.250842 \u2013 0:00:06.099587 \u2013 1.33 MB [100,000] MAE: 0.944883, RMSE: 1.227688 \u2013 0:00:08.139818 \u2013 1.5 MB Results are equivalent to our naive prediction (0.9448 vs 0.9421). By only focusing on the users preferences and the items characteristics, the model is limited in his ability to capture different views of the problem. Despite its poor performance alone, this algorithm is quite useful combined in other models or when we need to build dense representations for other tasks.","title":"Funk Matrix Factorization (FunkMF)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-1/#biased-matrix-factorization-biasedmf","text":"It's the combination of the Baseline model and FunkMF. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = \\bar{y} + bu_{u} + bi_{i} + \\langle \\mathbf{v}_u, \\mathbf{v}_i \\rangle \\] Note: Biased Matrix Factorization name is used by some people but some others refer to it by SVD or Funk SVD . It's the case of Yehuda Koren and Robert Bell in Recommender Systems Handbook (Chapter 5 Advances in Collaborative Filtering ) and of surprise library. Nevertheless, SVD could be confused with the original Singular Value Decomposition from which it's derived from, and Funk SVD could also be misleading because of the biased part of the model equation which doesn't come from Simon Funk's work. For those reasons, we chose to side with Biased Matrix Factorization which fits more naturally to it. biased_mf_params = { 'n_factors' : 10 , 'bias_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), 'l2_bias' : 0. , 'l2_latent' : 0. } model = meta . PredClipper ( regressor = reco . BiasedMF ( ** biased_mf_params ), y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761818, RMSE: 0.961057 \u2013 0:00:02.215027 \u2013 1.01 MB [50,000] MAE: 0.751667, RMSE: 0.949443 \u2013 0:00:04.477109 \u2013 1.28 MB [75,000] MAE: 0.749653, RMSE: 0.948723 \u2013 0:00:06.553924 \u2013 1.51 MB [100,000] MAE: 0.748559, RMSE: 0.947854 \u2013 0:00:08.727386 \u2013 1.69 MB Results improved (0.7485 vs 0.7546) demonstrating that users and items latent representations bring additional information. To conclude this first tutorial about factorization models, let's review the important parameters to tune when dealing with this family of methods: n_factors : the number of latent factors. The more you set, the more items aspects and users preferences you are going to learn. Too many will cause overfitting, l2 regularization could help. *_optimizer : the optimizers. Classic stochastic gradient descent performs well, finding the good learning rate will make the difference. initializer : the latent weights initialization. Latent vectors have to be initialized with non-constant values. We generally sample them from a zero-mean normal distribution with small standard deviation.","title":"Biased Matrix Factorization (BiasedMF)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/","text":"Matrix Factorization for Recommender Systems - Part 2 \u00b6 As seen in Part 1 , strength of Matrix Factorization (MF) lies in its ability to deal with sparse and high cardinality categorical variables. In this second tutorial we will have a look at Factorization Machines (FM) algorithm and study how it generalizes the power of MF. Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning Factorization Machines \u00b6 Steffen Rendel came up in 2010 with Factorization Machines , an algorithm able to handle any real valued feature vector, combining the advantages of general predictors with factorization models. It became quite popular in the field of online advertising, notably after winning several Kaggle competitions. The modeling technique starts with a linear regression to capture the effects of each variable individually: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} \\] Then are added interaction terms to learn features relations. Instead of learning a single and specific weight per interaction (as in polynomial regression ), a set of latent factors is learnt per feature (as in MF). An interaction is calculated by multiplying involved features product with their latent vectors dot product. The degree of factorization \u2014 or model order \u2014 represents the maximum number of features per interaction considered. The model equation for a factorization machine of degree \\(d\\) = 2 is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] Where \\(\\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle\\) is the dot product of \\(j\\) and \\(j'\\) latent vectors: \\[ \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} \\] Higher-order FM will be covered in a following section, just note that factorization models express their power in sparse settings, which is also where higher-order interactions are hard to estimate. Strong emphasis must be placed on feature engineering as it allows FM to mimic most factorization models and significantly impact its performance. High cardinality categorical variables one hot encoding is the most frequent step before feeding the model with data. For more efficiency, river FM implementation considers string values as categorical variables and automatically one hot encode them. FM models have their own module river.facto . ## Mimic Biased Matrix Factorization (BiasedMF) Let's start with a simple example where we want to reproduce the Biased Matrix Factorization model we trained in the previous tutorial. For a fair comparison with Part 1 example , let's set the same evaluation framework: from river import datasets from river import metrics from river.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) In order to build an equivalent model we need to use the same hyper-parameters. As we can't replace FM intercept by the global running mean we won't be able to build the exact same model: from river import compose from river import facto from river import meta from river import optim from river import stats fm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'sample_normalization' : False , 'l1_weight' : 0. , 'l2_weight' : 0. , 'l1_latent' : 0. , 'l2_latent' : 0. , 'intercept' : 3 , 'intercept_lr' : .01 , 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761761, RMSE: 0.960662 \u2013 0:00:04.465706 \u2013 1.16 MB [50,000] MAE: 0.751922, RMSE: 0.949783 \u2013 0:00:08.982324 \u2013 1.36 MB [75,000] MAE: 0.749822, RMSE: 0.948634 \u2013 0:00:13.391947 \u2013 1.58 MB [100,000] MAE: 0.748393, RMSE: 0.94776 \u2013 0:00:17.715558 \u2013 1.77 MB Both MAE are very close to each other (0.7486 vs 0.7485) showing that we almost reproduced reco.BiasedMF algorithm. The cost is a naturally slower running time as FM implementation offers more flexibility. Feature engineering for FM models \u00b6 Let's study the basics of how to properly encode data for FM models. We are going to keep using MovieLens 100K as it provides various feature types: import json for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 The features we are going to add to our model don't improve its predictive power. Nevertheless, they are useful to illustrate different methods of data encoding: Set-categorical variables We have seen that categorical variables are one hot encoded automatically if set to strings, in the other hand, set-categorical variables must be encoded explicitly by the user. A good way of doing so is to assign them a value of \\(1/m\\) , where \\(m\\) is the number of elements of the sample set. It gives the feature a constant \"weight\" across all samples preserving model's stability. Let's create a routine to encode movies genres this way: def split_genres ( x ): genres = x [ 'genres' ] . split ( ', ' ) return { f 'genre_ { genre } ' : 1 / len ( genres ) for genre in genres } Numerical variables In practice, transforming numerical features into categorical ones works better in most cases. Feature binning is the natural way, but finding good bins is sometimes more an art than a science. Let's encode users age with something simple: def bin_age ( x ): if x [ 'age' ] <= 18 : return { 'age_0-18' : 1 } elif x [ 'age' ] <= 32 : return { 'age_19-32' : 1 } elif x [ 'age' ] < 55 : return { 'age_33-54' : 1 } else : return { 'age_55-100' : 1 } Let's put everything together: fm_params = { 'n_factors' : 14 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.760059, RMSE: 0.961415 \u2013 0:00:10.728660 \u2013 1.43 MB [50,000] MAE: 0.751429, RMSE: 0.951504 \u2013 0:00:21.400132 \u2013 1.68 MB [75,000] MAE: 0.750568, RMSE: 0.951592 \u2013 0:00:31.539616 \u2013 1.95 MB [100,000] MAE: 0.75018, RMSE: 0.951622 \u2013 0:00:44.264960 \u2013 2.2 MB Note that using more variables involves factorizing a larger latent space, then increasing the number of latent factors \\(k\\) often helps capturing more information. Some other feature engineering tips from 3 idiots' winning solution for Kaggle Criteo display ads competition in 2014: Infrequent modalities often bring noise and little information, transforming them into a special tag can help In some cases, sample-wise normalization seems to make the optimization problem easier to be solved Higher-Order Factorization Machines (HOFM) \u00b6 The model equation generalized to any order \\(d \\geq 2\\) is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) \\] hofm_params = { 'degree' : 3 , 'n_factors' : 12 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . HOFMRegressor ( ** hofm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761379, RMSE: 0.96214 \u2013 0:00:52.504063 \u2013 2.61 MB [50,000] MAE: 0.751998, RMSE: 0.951589 \u2013 0:01:39.173239 \u2013 3.08 MB [75,000] MAE: 0.750994, RMSE: 0.951616 \u2013 0:02:24.711378 \u2013 3.6 MB [100,000] MAE: 0.750849, RMSE: 0.952142 \u2013 0:03:10.028292 \u2013 4.07 MB As said previously, high-order interactions are often hard to estimate due to too much sparsity, that's why we won't spend too much time here. Field-aware Factorization Machines (FFM) \u00b6 Field-aware variant of FM (FFM) improved the original method by adding the notion of \" fields \". A \" field \" is a group of features that belong to a specific domain (e.g. the \" users \" field, the \" items \" field, or the \" movie genres \" field). FFM restricts itself to pairwise interactions and factorizes separated latent spaces \u2014 one per combination of fields (e.g. users/items, users/movie genres, or items/movie genres) \u2014 instead of a common one shared by all fields. Therefore, each feature has one latent vector per field it can interact with \u2014 so that it can learn the specific effect with each different field. The model equation is defined by: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} \\] Where \\(f_j\\) and \\(f_{j'}\\) are the fields corresponding to \\(j\\) and \\(j'\\) features, respectively. ffm_params = { 'n_factors' : 8 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FFMRegressor ( ** ffm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.758339, RMSE: 0.959047 \u2013 0:00:13.815202 \u2013 3.04 MB [50,000] MAE: 0.749833, RMSE: 0.948531 \u2013 0:00:27.627278 \u2013 3.59 MB [75,000] MAE: 0.749631, RMSE: 0.949418 \u2013 0:00:42.088746 \u2013 4.19 MB [100,000] MAE: 0.749776, RMSE: 0.950131 \u2013 0:00:56.502844 \u2013 4.75 MB Note that FFM usually needs to learn smaller number of latent factors \\(k\\) than FM as each latent vector only deals with one field. Field-weighted Factorization Machines (FwFM) \u00b6 Field-weighted Factorization Machines (FwFM) address FFM memory issues caused by its large number of parameters, which is in the order of feature number times field number . As FFM, FwFM is an extension of FM restricted to pairwise interactions, but instead of factorizing separated latent spaces, it learns a specific weight \\(r_{f_j, f_{j'}}\\) for each field combination modelling the interaction strength. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] fwfm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'seed' : 73 , } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FwFMRegressor ( ** fwfm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761435, RMSE: 0.962211 \u2013 0:00:18.568712 \u2013 1.18 MB [50,000] MAE: 0.754063, RMSE: 0.953248 \u2013 0:00:37.070571 \u2013 1.38 MB [75,000] MAE: 0.754729, RMSE: 0.95507 \u2013 0:00:55.700074 \u2013 1.6 MB [100,000] MAE: 0.755697, RMSE: 0.956542 \u2013 0:01:14.242048 \u2013 1.79 MB","title":"Matrix Factorization for Recommender Systems - Part 2"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#matrix-factorization-for-recommender-systems-part-2","text":"As seen in Part 1 , strength of Matrix Factorization (MF) lies in its ability to deal with sparse and high cardinality categorical variables. In this second tutorial we will have a look at Factorization Machines (FM) algorithm and study how it generalizes the power of MF. Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning","title":"Matrix Factorization for Recommender Systems - Part 2"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#factorization-machines","text":"Steffen Rendel came up in 2010 with Factorization Machines , an algorithm able to handle any real valued feature vector, combining the advantages of general predictors with factorization models. It became quite popular in the field of online advertising, notably after winning several Kaggle competitions. The modeling technique starts with a linear regression to capture the effects of each variable individually: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} \\] Then are added interaction terms to learn features relations. Instead of learning a single and specific weight per interaction (as in polynomial regression ), a set of latent factors is learnt per feature (as in MF). An interaction is calculated by multiplying involved features product with their latent vectors dot product. The degree of factorization \u2014 or model order \u2014 represents the maximum number of features per interaction considered. The model equation for a factorization machine of degree \\(d\\) = 2 is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] Where \\(\\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle\\) is the dot product of \\(j\\) and \\(j'\\) latent vectors: \\[ \\normalsize \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle = \\sum_{f=1}^{k} \\mathbf{v}_{j, f} \\cdot \\mathbf{v}_{j', f} \\] Higher-order FM will be covered in a following section, just note that factorization models express their power in sparse settings, which is also where higher-order interactions are hard to estimate. Strong emphasis must be placed on feature engineering as it allows FM to mimic most factorization models and significantly impact its performance. High cardinality categorical variables one hot encoding is the most frequent step before feeding the model with data. For more efficiency, river FM implementation considers string values as categorical variables and automatically one hot encode them. FM models have their own module river.facto . ## Mimic Biased Matrix Factorization (BiasedMF) Let's start with a simple example where we want to reproduce the Biased Matrix Factorization model we trained in the previous tutorial. For a fair comparison with Part 1 example , let's set the same evaluation framework: from river import datasets from river import metrics from river.evaluate import progressive_val_score def evaluate ( model ): X_y = datasets . MovieLens100K () metric = metrics . MAE () + metrics . RMSE () _ = progressive_val_score ( X_y , model , metric , print_every = 25_000 , show_time = True , show_memory = True ) In order to build an equivalent model we need to use the same hyper-parameters. As we can't replace FM intercept by the global running mean we won't be able to build the exact same model: from river import compose from river import facto from river import meta from river import optim from river import stats fm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.025 ), 'latent_optimizer' : optim . SGD ( 0.05 ), 'sample_normalization' : False , 'l1_weight' : 0. , 'l2_weight' : 0. , 'l1_latent' : 0. , 'l2_latent' : 0. , 'intercept' : 3 , 'intercept_lr' : .01 , 'weight_initializer' : optim . initializers . Zeros (), 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.1 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761761, RMSE: 0.960662 \u2013 0:00:04.465706 \u2013 1.16 MB [50,000] MAE: 0.751922, RMSE: 0.949783 \u2013 0:00:08.982324 \u2013 1.36 MB [75,000] MAE: 0.749822, RMSE: 0.948634 \u2013 0:00:13.391947 \u2013 1.58 MB [100,000] MAE: 0.748393, RMSE: 0.94776 \u2013 0:00:17.715558 \u2013 1.77 MB Both MAE are very close to each other (0.7486 vs 0.7485) showing that we almost reproduced reco.BiasedMF algorithm. The cost is a naturally slower running time as FM implementation offers more flexibility.","title":"Factorization Machines"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#feature-engineering-for-fm-models","text":"Let's study the basics of how to properly encode data for FM models. We are going to keep using MovieLens 100K as it provides various feature types: import json for x , y in datasets . MovieLens100K (): print ( f 'x = { json . dumps ( x , indent = 4 ) } \\n y = { y } ' ) break x = { \"user\": \"259\", \"item\": \"255\", \"timestamp\": 874731910000000000, \"title\": \"My Best Friend's Wedding (1997)\", \"release_date\": 866764800000000000, \"genres\": \"comedy, romance\", \"age\": 21.0, \"gender\": \"M\", \"occupation\": \"student\", \"zip_code\": \"48823\" } y = 4.0 The features we are going to add to our model don't improve its predictive power. Nevertheless, they are useful to illustrate different methods of data encoding: Set-categorical variables We have seen that categorical variables are one hot encoded automatically if set to strings, in the other hand, set-categorical variables must be encoded explicitly by the user. A good way of doing so is to assign them a value of \\(1/m\\) , where \\(m\\) is the number of elements of the sample set. It gives the feature a constant \"weight\" across all samples preserving model's stability. Let's create a routine to encode movies genres this way: def split_genres ( x ): genres = x [ 'genres' ] . split ( ', ' ) return { f 'genre_ { genre } ' : 1 / len ( genres ) for genre in genres } Numerical variables In practice, transforming numerical features into categorical ones works better in most cases. Feature binning is the natural way, but finding good bins is sometimes more an art than a science. Let's encode users age with something simple: def bin_age ( x ): if x [ 'age' ] <= 18 : return { 'age_0-18' : 1 } elif x [ 'age' ] <= 32 : return { 'age_19-32' : 1 } elif x [ 'age' ] < 55 : return { 'age_33-54' : 1 } else : return { 'age_55-100' : 1 } Let's put everything together: fm_params = { 'n_factors' : 14 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FMRegressor ( ** fm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.760059, RMSE: 0.961415 \u2013 0:00:10.728660 \u2013 1.43 MB [50,000] MAE: 0.751429, RMSE: 0.951504 \u2013 0:00:21.400132 \u2013 1.68 MB [75,000] MAE: 0.750568, RMSE: 0.951592 \u2013 0:00:31.539616 \u2013 1.95 MB [100,000] MAE: 0.75018, RMSE: 0.951622 \u2013 0:00:44.264960 \u2013 2.2 MB Note that using more variables involves factorizing a larger latent space, then increasing the number of latent factors \\(k\\) often helps capturing more information. Some other feature engineering tips from 3 idiots' winning solution for Kaggle Criteo display ads competition in 2014: Infrequent modalities often bring noise and little information, transforming them into a special tag can help In some cases, sample-wise normalization seems to make the optimization problem easier to be solved","title":"Feature engineering for FM models"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#higher-order-factorization-machines-hofm","text":"The model equation generalized to any order \\(d \\geq 2\\) is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{l=2}^{d} \\sum_{j_1=1}^{p} \\cdots \\sum_{j_l=j_{l-1}+1}^{p} \\left(\\prod_{j'=1}^{l} x_{j_{j'}} \\right) \\left(\\sum_{f=1}^{k_l} \\prod_{j'=1}^{l} v_{j_{j'}, f}^{(l)} \\right) \\] hofm_params = { 'degree' : 3 , 'n_factors' : 12 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . HOFMRegressor ( ** hofm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761379, RMSE: 0.96214 \u2013 0:00:52.504063 \u2013 2.61 MB [50,000] MAE: 0.751998, RMSE: 0.951589 \u2013 0:01:39.173239 \u2013 3.08 MB [75,000] MAE: 0.750994, RMSE: 0.951616 \u2013 0:02:24.711378 \u2013 3.6 MB [100,000] MAE: 0.750849, RMSE: 0.952142 \u2013 0:03:10.028292 \u2013 4.07 MB As said previously, high-order interactions are often hard to estimate due to too much sparsity, that's why we won't spend too much time here.","title":"Higher-Order Factorization Machines (HOFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#field-aware-factorization-machines-ffm","text":"Field-aware variant of FM (FFM) improved the original method by adding the notion of \" fields \". A \" field \" is a group of features that belong to a specific domain (e.g. the \" users \" field, the \" items \" field, or the \" movie genres \" field). FFM restricts itself to pairwise interactions and factorizes separated latent spaces \u2014 one per combination of fields (e.g. users/items, users/movie genres, or items/movie genres) \u2014 instead of a common one shared by all fields. Therefore, each feature has one latent vector per field it can interact with \u2014 so that it can learn the specific effect with each different field. The model equation is defined by: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_{j}} \\rangle x_{j} x_{j'} \\] Where \\(f_j\\) and \\(f_{j'}\\) are the fields corresponding to \\(j\\) and \\(j'\\) features, respectively. ffm_params = { 'n_factors' : 8 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'latent_initializer' : optim . initializers . Normal ( mu = 0. , sigma = 0.05 , seed = 73 ), } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FFMRegressor ( ** ffm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.758339, RMSE: 0.959047 \u2013 0:00:13.815202 \u2013 3.04 MB [50,000] MAE: 0.749833, RMSE: 0.948531 \u2013 0:00:27.627278 \u2013 3.59 MB [75,000] MAE: 0.749631, RMSE: 0.949418 \u2013 0:00:42.088746 \u2013 4.19 MB [100,000] MAE: 0.749776, RMSE: 0.950131 \u2013 0:00:56.502844 \u2013 4.75 MB Note that FFM usually needs to learn smaller number of latent factors \\(k\\) than FM as each latent vector only deals with one field.","title":"Field-aware Factorization Machines (FFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-2/#field-weighted-factorization-machines-fwfm","text":"Field-weighted Factorization Machines (FwFM) address FFM memory issues caused by its large number of parameters, which is in the order of feature number times field number . As FFM, FwFM is an extension of FM restricted to pairwise interactions, but instead of factorizing separated latent spaces, it learns a specific weight \\(r_{f_j, f_{j'}}\\) for each field combination modelling the interaction strength. The model equation is defined as: \\[ \\normalsize \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j} + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} r_{f_j, f_{j'}} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] fwfm_params = { 'n_factors' : 10 , 'weight_optimizer' : optim . SGD ( 0.01 ), 'latent_optimizer' : optim . SGD ( 0.025 ), 'intercept' : 3 , 'seed' : 73 , } regressor = compose . Select ( 'user' , 'item' ) regressor += ( compose . Select ( 'genres' ) | compose . FuncTransformer ( split_genres ) ) regressor += ( compose . Select ( 'age' ) | compose . FuncTransformer ( bin_age ) ) regressor |= facto . FwFMRegressor ( ** fwfm_params ) model = meta . PredClipper ( regressor = regressor , y_min = 1 , y_max = 5 ) evaluate ( model ) [25,000] MAE: 0.761435, RMSE: 0.962211 \u2013 0:00:18.568712 \u2013 1.18 MB [50,000] MAE: 0.754063, RMSE: 0.953248 \u2013 0:00:37.070571 \u2013 1.38 MB [75,000] MAE: 0.754729, RMSE: 0.95507 \u2013 0:00:55.700074 \u2013 1.6 MB [100,000] MAE: 0.755697, RMSE: 0.956542 \u2013 0:01:14.242048 \u2013 1.79 MB","title":"Field-weighted Factorization Machines (FwFM)"},{"location":"examples/matrix-factorization-for-recommender-systems-part-3/","text":"Matrix Factorization for Recommender Systems - Part 3 \u00b6 Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning To do.","title":"Matrix Factorization for Recommender Systems - Part 3"},{"location":"examples/matrix-factorization-for-recommender-systems-part-3/#matrix-factorization-for-recommender-systems-part-3","text":"Table of contents of this tutorial series on matrix factorization for recommender systems: Part 1 - Traditional Matrix Factorization methods for Recommender Systems Part 2 - Factorization Machines and Field-aware Factorization Machines Part 3 - Large scale learning and better predictive power with multiple pass learning To do.","title":"Matrix Factorization for Recommender Systems - Part 3"},{"location":"examples/quantile-regression-uncertainty/","text":"Handling uncertainty with quantile regression \u00b6 % matplotlib inline Quantile regression is useful when you're not so much interested in the accuracy of your model, but rather you want your model to be good at ranking observations correctly. The typical way to perform quantile regression is to use a special loss function, namely the quantile loss. The quantile loss takes a parameter, \\(\\alpha\\) (alpha), which indicates which quantile the model should be targeting. In the case of \\(\\alpha = 0.5\\) , then this is equivalent to asking the model to predict the median value of the target, and not the most likely value which would be the mean. A nice thing we can do with quantile regression is to produce a prediction interval for each prediction. Indeed, if we predict the lower and upper quantiles of the target then we will be able to obtain a \"trust region\" in between which the true value is likely to belong. Of course, the likeliness will depend on the chosen quantiles. For a slightly more detailed explanation see this blog post. As an example, let us take the simple time series model we built in another notebook . Instead of predicting the mean value of the target distribution, we will predict the 5th, 50th, 95th quantiles. This will require training three separate models, so we will encapsulate the model building logic in a function called make_model . We also have to slightly adapt the training loop, but not by much. Finally, we will draw the prediction interval along with the predictions from for 50th quantile (i.e. the median) and the true values. import calendar import math import matplotlib.pyplot as plt from river import compose from river import datasets from river import linear_model from river import metrics from river import optim from river import preprocessing from river import stats from river import time_series def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } def make_model ( alpha ): extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 3 ), loss = optim . losses . Quantile ( alpha = alpha ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) return model metric = metrics . MAE () models = { 'lower' : make_model ( alpha = 0.05 ), 'center' : make_model ( alpha = 0.5 ), 'upper' : make_model ( alpha = 0.95 ) } dates = [] y_trues = [] y_preds = { 'lower' : [], 'center' : [], 'upper' : [] } for x , y in datasets . AirlinePassengers (): y_trues . append ( y ) dates . append ( x [ 'month' ]) for name , model in models . items (): y_preds [ name ] . append ( model . predict_one ( x )) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_preds [ 'center' ][ - 1 ]) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Truth' ) ax . plot ( dates , y_preds [ 'center' ], lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . fill_between ( dates , y_preds [ 'lower' ], y_preds [ 'upper' ], color = '#e74c3c' , alpha = 0.3 , label = 'Prediction interval' ) ax . legend () ax . set_title ( metric ); Text(0.5, 1.0, 'MAE: 15.634343') An important thing to note is that the prediction interval we obtained should not be confused with a confidence interval. Simply put, a prediction interval represents uncertainty for where the true value lies, whereas a confidence interval encapsulates the uncertainty on the prediction. You can find out more by reading this CrossValidated post.","title":"Handling uncertainty with quantile regression"},{"location":"examples/quantile-regression-uncertainty/#handling-uncertainty-with-quantile-regression","text":"% matplotlib inline Quantile regression is useful when you're not so much interested in the accuracy of your model, but rather you want your model to be good at ranking observations correctly. The typical way to perform quantile regression is to use a special loss function, namely the quantile loss. The quantile loss takes a parameter, \\(\\alpha\\) (alpha), which indicates which quantile the model should be targeting. In the case of \\(\\alpha = 0.5\\) , then this is equivalent to asking the model to predict the median value of the target, and not the most likely value which would be the mean. A nice thing we can do with quantile regression is to produce a prediction interval for each prediction. Indeed, if we predict the lower and upper quantiles of the target then we will be able to obtain a \"trust region\" in between which the true value is likely to belong. Of course, the likeliness will depend on the chosen quantiles. For a slightly more detailed explanation see this blog post. As an example, let us take the simple time series model we built in another notebook . Instead of predicting the mean value of the target distribution, we will predict the 5th, 50th, 95th quantiles. This will require training three separate models, so we will encapsulate the model building logic in a function called make_model . We also have to slightly adapt the training loop, but not by much. Finally, we will draw the prediction interval along with the predictions from for 50th quantile (i.e. the median) and the true values. import calendar import math import matplotlib.pyplot as plt from river import compose from river import datasets from river import linear_model from river import metrics from river import optim from river import preprocessing from river import stats from river import time_series def get_ordinal_date ( x ): return { 'ordinal_date' : x [ 'month' ] . toordinal ()} def get_month_distances ( x ): return { calendar . month_name [ month ]: math . exp ( - ( x [ 'month' ] . month - month ) ** 2 ) for month in range ( 1 , 13 ) } def make_model ( alpha ): extract_features = compose . TransformerUnion ( get_ordinal_date , get_month_distances ) scale = preprocessing . StandardScaler () learn = linear_model . LinearRegression ( intercept_lr = 0 , optimizer = optim . SGD ( 3 ), loss = optim . losses . Quantile ( alpha = alpha ) ) model = extract_features | scale | learn model = time_series . Detrender ( regressor = model , window_size = 12 ) return model metric = metrics . MAE () models = { 'lower' : make_model ( alpha = 0.05 ), 'center' : make_model ( alpha = 0.5 ), 'upper' : make_model ( alpha = 0.95 ) } dates = [] y_trues = [] y_preds = { 'lower' : [], 'center' : [], 'upper' : [] } for x , y in datasets . AirlinePassengers (): y_trues . append ( y ) dates . append ( x [ 'month' ]) for name , model in models . items (): y_preds [ name ] . append ( model . predict_one ( x )) model . learn_one ( x , y ) # Update the error metric metric . update ( y , y_preds [ 'center' ][ - 1 ]) # Plot the results fig , ax = plt . subplots ( figsize = ( 10 , 6 )) ax . grid ( alpha = 0.75 ) ax . plot ( dates , y_trues , lw = 3 , color = '#2ecc71' , alpha = 0.8 , label = 'Truth' ) ax . plot ( dates , y_preds [ 'center' ], lw = 3 , color = '#e74c3c' , alpha = 0.8 , label = 'Prediction' ) ax . fill_between ( dates , y_preds [ 'lower' ], y_preds [ 'upper' ], color = '#e74c3c' , alpha = 0.3 , label = 'Prediction interval' ) ax . legend () ax . set_title ( metric ); Text(0.5, 1.0, 'MAE: 15.634343') An important thing to note is that the prediction interval we obtained should not be confused with a confidence interval. Simply put, a prediction interval represents uncertainty for where the true value lies, whereas a confidence interval encapsulates the uncertainty on the prediction. You can find out more by reading this CrossValidated post.","title":"Handling uncertainty with quantile regression"},{"location":"examples/sentence_classification/","text":"Sentence classification \u00b6 In this tutorial we will try to predict whether an SMS is a spam or not. To train our model, we will use the SMSSpam dataset. This dataset is unbalanced, there is only 13.4% spam. Let's look at the data: from river import datasets datasets . SMSSpam () SMS Spam Collection dataset. The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent 13.4% of the dataset. The goal is to predict whether an SMS is a spam or not. Name SMSSpam Task Binary classification Samples 5,574 Features 1 Sparse False Path /Users/max.halford/river_data/SMSSpam/SMSSpamCollection URL https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip Size 466.71 KB Downloaded True from pprint import pprint X_y = datasets . SMSSpam () for x , y in X_y : pprint ( x ) print ( f 'Spam: { y } ' ) break {'body': 'Go until jurong point, crazy.. Available only in bugis n great world ' 'la e buffet... Cine there got amore wat...\\n'} Spam: False Let's start by building a simple model like a Naive Bayes classifier. We will first preprocess the sentences with a TF-IDF transform that our model can consume. Then, we will measure the accuracy of our model with the AUC metric. This is the right metric to use when the classes are not balanced. In addition, the Naive Bayes models can perform very well on unbalanced datasets and can be used for both binary and multi-class classification problems. from river import feature_extraction from river import naive_bayes from river import metrics def extract_body ( x ): \"\"\"Extract the body of the sms.\"\"\" return x [ 'body' ] X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | naive_bayes . BernoulliNB ( alpha = 0 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) if y_pred is not None : metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.929296 The confusion matrix: cm False True False 4809 17 True 103 644 The results are quite good with this first model. Since we are working with an unbalanced dataset, we can use the imblearn module to rebalance the classes of our dataset. For more information about the imblearn module, you can find a dedicated tutorial here . from river import imblearn X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | imblearn . RandomUnderSampler ( classifier = naive_bayes . BernoulliNB ( alpha = 0 ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) if y_pred is not None : metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.951728 The imblearn module improved our results. Not bad! We can visualize the pipeline to understand how the data is processed. The confusion matrix: cm False True False 4624 201 True 41 706 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] TFIDF {'dfs': Counter({'to': 1687, 'you': 1591, 'the': 1035, 'in': 810, 'and': 795, 'is': 752, 'me': 690, 'for': 624, 'it': 614, 'my': 613, 'your': 587, 'call': 551, 'of': 550, 'have': 531, 'that': 512, 'on': 488, 'now': 481, 'are': 445, 'so': 433, 'can': 425, 'but': 422, 'not': 418, 'or': 398, 'at': 378, 'get': 368, 'just': 365, 'do': 359, 'be': 355, 'will': 354, 'with': 350, 'if': 350, 'we': 350, 'no': 343, 'this': 319, 'ur': 310, 'up': 297, 'how': 293, 'ok': 280, 'what': 279, 'from': 273, 'when': 272, 'go': 264, 'out': 262, 'all': 261, 'll': 258, 'know': 247, 'gt': 242, 'lt': 242, 'good': 233, 'then': 232, 'got': 229, 'free': 229, 'like': 229, 'come': 217, 'there': 214, 'am': 214, 'day': 212, 'only': 211, 'its': 210, 'time': 210, 'was': 196, 'send': 195, 'want': 188, 'love': 178, 'text': 174, 'he': 171, 'going': 167, 'txt': 165, 'home': 165, 'one': 163, 'by': 162, 'need': 162, 'today': 159, 'see': 158, 'as': 156, 'still': 155, 'about': 151, 'sorry': 151, 'don': 148, 'back': 147, 'lor': 145, 'our': 143, 'take': 140, 'dont': 139, 'stop': 138, 'reply': 135, 'da': 134, 'tell': 134, 'new': 133, 'later': 132, 'please': 131, 'any': 131, 'think': 129, 'mobile': 128, 'been': 126, 'phone': 124, 'here': 122, 'hi': 120, 'some': 119, 'did': 119, 'well': 119, 'she': 117, 'they': 116, 're': 114, 'where': 114, 'hope': 112, 'hey': 111, 'dear': 111, 'week': 110, 'oh': 110, 'pls': 110, 'has': 109, 'much': 109, 'night': 109, 'great': 108, 'claim': 108, 'an': 108, 'too': 107, 'msg': 106, 'more': 103, 'yes': 103, 'wat': 102, 'had': 101, 'him': 101, 'www': 100, 'her': 100, 'make': 99, 'work': 98, 'give': 98, 'way': 97, 've': 95, 'won': 95, 'who': 95, 'message': 93, 'number': 92, 'tomorrow': 90, 'after': 90, 'say': 89, 'already': 89, 'should': 89, 'doing': 88, 'right': 86, 'yeah': 86, 'happy': 86, 'prize': 84, 'why': 84, 'really': 83, 'ask': 83, 'find': 81, 'meet': 80, 'said': 80, 'cash': 78, 'im': 78, 'last': 78, 'let': 77, 'morning': 76, 'babe': 76, 'them': 76, 'lol': 74, 'thanks': 74, 'miss': 73, 'cos': 73, 'care': 73, 'anything': 72, '150p': 71, 'amp': 71, 'uk': 71, 'pick': 71, 'also': 71, 'win': 70, 'very': 70, 'urgent': 69, 'com': 69, 'would': 69, 'sure': 68, 'something': 68, 'contact': 68, 'over': 67, 'life': 67, 'sent': 67, 'again': 66, 'keep': 66, 'wait': 65, 'every': 65, 'his': 64, 'cant': 64, 'first': 62, 'us': 62, 'buy': 62, 'gud': 62, 'before': 62, 'thing': 62, 'even': 61, 'min': 61, 'soon': 60, 'next': 60, 'place': 60, 'off': 60, '50': 59, 'nice': 59, 'which': 59, 'customer': 58, 'tonight': 58, 'always': 58, 'service': 58, 'around': 57, 'per': 57, 'were': 57, 'late': 57, 'someone': 57, 'gonna': 56, 'feel': 56, 'could': 56, 'money': 56, 'help': 55, 'down': 55, 'sms': 55, 'sleep': 55, 'leave': 55, 'co': 54, '16': 54, 'other': 54, 'many': 54, 'wan': 54, 'nokia': 53, 'went': 53, 'told': 53, 'friends': 52, 'try': 52, 'waiting': 52, 'dun': 51, '18': 51, 'chat': 51, 'friend': 51, 'may': 50, 'fine': 50, 'guaranteed': 50, 'ya': 50, 'coming': 50, 'getting': 49, 'done': 49, 'special': 49, 'yet': 49, 'people': 49, 'haha': 49, 'use': 48, 'year': 48, 'same': 48, 'mins': 48, 'wish': 48, 'didn': 47, 'things': 47, 'holiday': 47, 'thk': 47, 'name': 46, 'man': 46, 'best': 46, 'thought': 46, 'talk': 45, 'bit': 45, 'hello': 44, 'draw': 44, 'few': 44, '500': 44, 'person': 44, 'cs': 44, 'stuff': 43, 'yup': 43, 'trying': 43, 'meeting': 43, 'thats': 43, 'job': 43, 'line': 43, 'heart': 43, 'being': 42, 'class': 42, 'never': 42, 'cool': 42, 'long': 42, 'better': 42, 'ill': 42, 'having': 42, 'days': 42, 'tone': 42, 'cost': 41, '100': 41, 'car': 41, 'live': 41, '1000': 41, 'house': 41, 'ready': 41, 'mind': 41, 'finish': 40, 'enjoy': 40, 'lunch': 39, 'half': 39, 'play': 39, 'check': 39, '10': 39, 'real': 39, 'lot': 39, 'dat': 39, 'chance': 39, 'god': 39, 'word': 38, 'awarded': 38, 'wanna': 38, 'box': 38, 'nothing': 38, 'guess': 38, 'sir': 38, 'lar': 37, 'latest': 37, 'end': 37, 'another': 37, 'liao': 37, 'guys': 37, 'than': 37, 'dinner': 36, 'month': 36, 'sweet': 36, 'ah': 36, 'shows': 36, 'big': 36, 'into': 36, 'shit': 36, '1st': 36, 'world': 35, 'xxx': 35, 'eat': 35, 'po': 35, 'account': 35, 'bt': 35, 'might': 35, 'problem': 35, 'quite': 35, 'receive': 34, 'camera': 34, 'watching': 34, 'smile': 34, '150ppm': 34, 'landline': 34, 'because': 34, 'start': 34, 'speak': 33, 'wont': 33, 'room': 33, 'yo': 33, 'wk': 33, 'aight': 33, 'luv': 33, 'tv': 33, 'offer': 33, 'called': 33, 'two': 33, 'probably': 33, 'rate': 32, 'apply': 32, 'remember': 32, 'left': 32, 'weekend': 32, 'once': 32, 'forgot': 32, 'jus': 32, 'watch': 32, 'plan': 32, 'actually': 32, 'bad': 32, 'princess': 32, 'early': 31, 'code': 31, 'does': 31, 'look': 31, 'maybe': 31, 'hear': 31, 'between': 31, 'easy': 31, 'reach': 31, 'thanx': 31, 'video': 31, 'shopping': 31, 'shall': 31, 'dunno': 31, 'minutes': 31, 'office': 31, 'fun': 30, '2nd': 30, 'part': 30, 'anyway': 30, 'didnt': 30, 'hour': 30, 'baby': 30, 'ever': 30, 'sat': 30, 'network': 29, 'selected': 29, 'enough': 29, 'thank': 29, 'bus': 29, 'ringtone': 29, 'pa': 29, 'looking': 29, 'bed': 29, 'birthday': 29, 'girl': 29, 'little': 29, 'working': 29, 'leh': 29, 'made': 29, 'orange': 29, 'put': 29, 'dad': 29, 'town': 29, 'pay': 28, 'calls': 28, 'afternoon': 28, 'those': 28, 'evening': 28, 'collect': 28, 'everything': 28, 'asked': 28, 'true': 28, 'texts': 28, 'den': 28, 'while': 28, 'kiss': 28, 'until': 27, 'though': 27, '000': 27, 'since': 27, 'pain': 27, 'dis': 27, 'came': 27, 'okay': 27, 'must': 27, 'join': 27, 'tmr': 27, 'details': 27, 'fuck': 27, 'wif': 26, 'wanted': 26, 'most': 26, 'means': 26, 'says': 26, 'mail': 26, 'able': 26, 'important': 26, 'wake': 26, 'collection': 26, 'goes': 25, 'times': 25, 'mob': 25, 'haven': 25, '5000': 25, 'show': 25, 'price': 25, 'school': 25, '2000': 25, 'sexy': 25, 'til': 25, 'guy': 25, 'away': 25, 'valid': 24, 'alright': 24, 'messages': 24, 'missed': 24, 'saw': 24, 'yesterday': 24, 'wen': 24, 'havent': 24, 'abt': 24, 'else': 24, 'juz': 24, 'years': 24, 'hav': 24, 'weekly': 24, 'wot': 24, 'bring': 24, 'attempt': 24, 'yours': 23, 'run': 23, 'making': 23, 'worry': 23, 'haf': 23, 'coz': 23, 'id': 23, 'oso': 23, '10p': 23, 'music': 23, 'stay': 23, 'bored': 23, 'these': 23, 'wife': 23, 'gift': 23, 'plus': 23, 'lei': 23, 'question': 22, 'colour': 22, 'net': 22, 'words': 22, 'national': 22, 'tried': 22, 'yourself': 22, 'address': 22, 'food': 22, 'top': 22, 'without': 22, 'boy': 22, 'decimal': 22, 'shop': 22, 'nite': 22, 'hot': 22, 'book': 22, 'friendship': 22, 'dude': 22, 'change': 22, 'feeling': 22, 'either': 22, '800': 22, 'online': 22, 'family': 22, 'de': 22, 'entry': 21, 'hours': 21, 'http': 21, 'ard': 21, 'till': 21, 'delivery': 21, 'hair': 21, 'bonus': 21, 'test': 21, 'driving': 21, 'busy': 21, 'todays': 21, 'answer': 21, 'nt': 21, 'xmas': 21, 'both': 21, 'vouchers': 21, 'full': 21, 'plz': 21, 'tones': 21, 'calling': 21, 'tot': 21, 'sae': 21, 'together': 21, 'wants': 21, 'goin': 21, 'sad': 21, 'brother': 20, 'set': 20, 'date': 20, 'trip': 20, 'comes': 20, 'movie': 20, 'mean': 20, 'old': 20, 'points': 20, 'award': 20, 'leaving': 20, 'order': 20, 'believe': 20, 'story': 20, 'sleeping': 20, 'noe': 20, 'happen': 20, 'face': 20, 'wid': 20, 'ring': 20, 'huh': 20, 'sch': 20, 'game': 20, 'makes': 20, 'await': 20, 'pounds': 19, 'news': 19, 'aft': 19, 'doesn': 19, 'tomo': 19, 'congrats': 19, 'took': 19, 'double': 19, 'finished': 19, 'started': 19, 'private': 19, 'gr8': 19, 'minute': 19, 'awesome': 19, 'wil': 19, '750': 19, '86688': 19, 'hurt': 19, 'row': 19, 'pm': 19, 'head': 19, 'eve': 19, 'beautiful': 19, 'thinking': 19, 'mum': 19, 'saying': 19, 'rite': 19, 'available': 18, 'final': 18, 'update': 18, 'tho': 18, 'xx': 18, 'close': 18, 'cause': 18, 'services': 18, 'taking': 18, 'missing': 18, 'touch': 18, 'walk': 18, 'unsubscribe': 18, 'charge': 18, 'lets': 18, 'post': 18, 'drink': 18, '250': 18, 'land': 18, 'gd': 18, '150': 18, 'mine': 18, 'pics': 18, 'pub': 18, 'email': 18, 'drive': 18, 'drop': 18, 'dreams': 18, '11': 17, 'lesson': 17, 'second': 17, 'lucky': 17, 'search': 17, '12hrs': 17, 'statement': 17, 'expires': 17, 'msgs': 17, 'open': 17, 'whats': 17, 'lots': 17, 'everyone': 17, 'carlos': 17, 'worth': 17, 'sis': 17, 'sounds': 17, 'company': 17, 'choose': 17, 'club': 17, 'okie': 17, 'card': 17, 'sister': 17, 'chikku': 17, 'poly': 17, 'dating': 17, 'opt': 17, 'neva': 17, 'anyone': 17, 'loving': 17, 'alone': 17, 'treat': 16, 'winner': 16, 'info': 16, 'pobox': 16, 'saturday': 16, 'decided': 16, 'forget': 16, '08000930705': 16, 'girls': 16, 'smiling': 16, 'prob': 16, 'gone': 16, 'happened': 16, 'identifier': 16, 'type': 16, 'ni8': 16, 'ltd': 16, 'hard': 16, 'each': 16, 'boytoy': 16, 'found': 16, 'college': 16, 'break': 16, 'anytime': 16, 'far': 16, 'games': 16, 'mobileupd8': 16, 'bout': 16, 'kind': 16, 'visit': 16, 'fast': 16, 'voucher': 16, 'sun': 16, '8007': 16, 'hows': 16, 'wonderful': 15, 'smth': 15, 'mom': 15, 'camcorder': 15, 'used': 15, 'hit': 15, 'operator': 15, 'friday': 15, 'quiz': 15, 'player': 15, 'parents': 15, 'frnd': 15, 'finally': 15, 'darlin': 15, 'goodmorning': 15, 'oredi': 15, 'secret': 15, 'congratulations': 15, 'hold': 15, 'takes': 15, 'read': 15, 'suite342': 15, '2lands': 15, '08000839402': 15, 'fucking': 15, 'nope': 15, 'outside': 15, 'pretty': 15, 'sea': 15, 'whatever': 15, 'weeks': 15, 'lovely': 15, 'mates': 15, 'wrong': 15, 'party': 15, 'nyt': 15, 'pic': 15, 'crazy': 14, 'wkly': 14, 'freemsg': 14, 'credit': 14, 'seeing': 14, 'whole': 14, 'frnds': 14, 'isn': 14, 'hmm': 14, 'mu': 14, 'their': 14, 'content': 14, 'fancy': 14, 'log': 14, 'course': 14, 'mrng': 14, 'tc': 14, 'thinks': 14, 'case': 14, 'tel': 14, 'meant': 14, 'fr': 14, 'angry': 14, 'light': 14, 'jay': 14, 'project': 14, 'reason': 14, 'ten': 14, 'welcome': 14, 'cum': 14, 'b4': 14, 'mate': 14, 'least': 14, 'earlier': 14, 'chennai': 14, '30': 14, 'point': 13, 'press': 13, 'valued': 13, 'hungry': 13, 'almost': 13, 'hee': 13, '0800': 13, 'felt': 13, 'invited': 13, '03': 13, 'caller': 13, 'numbers': 13, 'yr': 13, 'tired': 13, 'wit': 13, 'needs': 13, 'hmmm': 13, 'mr': 13, 'smoke': 13, 'balance': 13, 'march': 13, 'side': 13, '87066': 13, 'dnt': 13, 'unlimited': 13, 'fone': 13, 'stupid': 13, 'bslvyl': 13, 'lost': 13, 'reading': 13, 'txts': 13, 'ago': 13, 'currently': 13, 'motorola': 13, 'talking': 13, 'couple': 13, 'phones': 13, 'ass': 13, 'park': 13, 'frm': 13, 'fri': 13, 'offers': 13, 'within': 13, '2003': 13, 'un': 13, 'listen': 13, 'yar': 13, 'knw': 13, 'sex': 13, 'mayb': 13, 'understand': 13, 'knew': 13, 'gas': 13, 'comp': 12, '12': 12, 'mobiles': 12, '20': 12, 'eh': 12, 'confirm': 12, 'telling': 12, 'wow': 12, 'correct': 12, 'pass': 12, 'etc': 12, 'complimentary': 12, 'gotta': 12, 'loads': 12, 'computer': 12, 'mah': 12, 'askd': 12, 'uncle': 12, 'sending': 12, 'direct': 12, 'age': 12, 'hand': 12, 'bank': 12, 'bcoz': 12, 'laptop': 12, 'questions': 12, 'swing': 12, 'ge': 12, 'ends': 12, 'die': 12, '200': 12, 'via': 12, 'met': 12, 'call2optout': 12, 'seen': 12, 'rental': 12, 'india': 12, 'doin': 12, 'lose': 12, 'ipod': 12, '04': 12, 'redeemed': 12, 'through': 12, 'gym': 12, 'happiness': 12, 'snow': 12, 'area': 12, 'sound': 12, 'picking': 12, 'ugh': 12, 'extra': 12, 'heard': 12, 'support': 12, 'surprise': 12, 'information': 12, 'grins': 12, 'luck': 12, 'enter': 12, 'auction': 12, 'difficult': 12, 'wasn': 12, 'std': 11, 'usf': 11, 'sunday': 11, 'eg': 11, 'comin': 11, 'charged': 11, 'abiola': 11, 'crave': 11, 'gets': 11, 'ac': 11, 'move': 11, 'checking': 11, 'cut': 11, 'rply': 11, 'download': 11, 'shower': 11, 'entered': 11, 'match': 11, '350': 11, 'txting': 11, 'lovable': 11, 'wine': 11, 'safe': 11, 'orchard': 11, 'kate': 11, 'rs': 11, 'semester': 11, 'wana': 11, 'somebody': 11, 'rest': 11, 'christmas': 11, 'pete': 11, 'plans': 11, 'small': 11, 'ex': 11, 'w1j6hl': 11, 'hg': 11, 'discount': 11, 'slow': 11, 'yep': 11, 'th': 11, 'supposed': 11, 'asking': 11, 'remove': 11, 'monday': 11, 'simple': 11, 'noon': 11, 'darren': 11, 'ans': 11, 'store': 11, 'wonder': 11, 'sort': 11, 'asap': 11, 'na': 11, 'nobody': 11, 'nah': 10, '900': 10, 'months': 10, 'link': 10, 'ha': 10, 'worried': 10, 'myself': 10, 'knows': 10, 'oops': 10, 'hospital': 10, 'red': 10, 'reached': 10, 'forever': 10, 'song': 10, 'save': 10, 'tickets': 10, 'il': 10, 'representative': 10, 'gave': 10, 'rates': 10, 'del': 10, 'sony': 10, 'pray': 10, 'dream': 10, 'spend': 10, 'muz': 10, 'bath': 10, 'bathe': 10, 'study': 10, 'exam': 10, 'street': 10, 'reveal': 10, 'admirer': 10, 'deep': 10, 'own': 10, 'leaves': 10, 'blue': 10, 'usual': 10, 'somewhere': 10, 'normal': 10, 'merry': 10, 'immediately': 10, 'custcare': 10, 'weed': 10, 'rakhesh': 10, 'moment': 10, 'st': 10, 'woke': 10, 'mm': 10, 'voice': 10, 'ldn': 10, 'booked': 10, 'different': 10, 'terms': 10, 'water': 10, 'sub': 10, '00': 10, 'across': 10, 'warm': 10, 'cheap': 10, 'clean': 10, 'em': 10, 'ts': 10, 'drugs': 10, 'laugh': 10, 'fantastic': 10, 'glad': 10, 'wishing': 10, 'getzed': 10, 'whenever': 10, 'otherwise': 10, 'ntt': 10, 'truth': 10, 'gn': 10, 'convey': 10, 'film': 10, '2nite': 10, 'write': 10, 'fact': 10, 'loved': 10, 'slowly': 10, 'cup': 9, 'copy': 9, 'reward': 9, 'england': 9, 'seriously': 9, 'sick': 9, 'catch': 9, 'decide': 9, 'ice': 9, 'situation': 9, 'short': 9, 'rain': 9, 'coffee': 9, 'men': 9, 'boss': 9, 'specially': 9, 'ending': 9, 'sunshine': 9, 'lazy': 9, 'completely': 9, 'staying': 9, 'doesnt': 9, 'especially': 9, 'studying': 9, 'trust': 9, 'using': 9, 'deal': 9, 'itself': 9, 'dead': 9, 'mrt': 9, 'bill': 9, 'lessons': 9, 'goodnight': 9, 'cd': 9, 'ldew': 9, 'lover': 9, 'disturb': 9, 'credits': 9, 'worries': 9, 'tonite': 9, 'unless': 9, '4u': 9, '2day': 9, '11mths': 9, 'valentines': 9, 'urself': 9, 'bluetooth': 9, 'rock': 9, 'starts': 9, 'kinda': 9, 'loan': 9, 'meh': 9, 'near': 9, 'rent': 9, 'silent': 9, 'less': 9, 'children': 9, 'hoping': 9, 'age16': 9, 'self': 9, 'train': 9, 'forwarded': 9, 'starting': 9, 'paper': 9, 'seems': 9, 'sell': 9, 'eyes': 9, 'possible': 9, 'ones': 9, 'gettin': 9, 'poor': 9, 'tampa': 9, 'user': 9, 'mo': 9, 'against': 9, 'hiya': 9, 'doctor': 9, 'mon': 9, 'john': 9, 'mode': 9, 'others': 9, 'wondering': 9, 'ringtones': 9, 'bb': 9, 'tht': 9, '20p': 9, 'moral': 9, 'excellent': 9, 'father': 9, 'thinkin': 9, 'sitting': 9, 'sofa': 9, 'request': 8, 'entitled': 8, 'anymore': 8, 'promise': 8, 'wap': 8, 'pizza': 8, 'mark': 8, 'cheers': 8, 'quick': 8, 'replying': 8, 'nigeria': 8, 'cinema': 8, 'ip4': 8, '5we': 8, 'stand': 8, 'spent': 8, 'loves': 8, 'hurts': 8, 'trouble': 8, 'planning': 8, 'ave': 8, 'wishes': 8, 'weekends': 8, 'apartment': 8, 'inc': 8, 'paying': 8, '2004': 8, 'buying': 8, 'bak': 8, 'sp': 8, 'dvd': 8, 'dogging': 8, 'swt': 8, 'joy': 8, 'goto': 8, 'freephone': 8, 'joined': 8, 'however': 8, 'ive': 8, 'slept': 8, 'sign': 8, 'kick': 8, 'lemme': 8, 'rose': 8, 'cake': 8, 'fixed': 8, 'rcvd': 8, 'interested': 8, 'round': 8, 'figure': 8, 'reference': 8, 'mistake': 8, 'facebook': 8, 'al': 8, 'yahoo': 8, 'aha': 8, '3030': 8, 'funny': 8, 'giving': 8, 'din': 8, 'thru': 8, 'style': 8, 'opinion': 8, '02': 8, 'savamob': 8, 'member': 8, 'fingers': 8, '50p': 8, 'blood': 8, 'daddy': 8, 'door': 8, 'kids': 8, 'pound': 8, 'ar': 8, 'alex': 8, 'longer': 8, '25p': 8, 'pc': 8, 'xy': 8, 'bedroom': 8, 'king': 8, 'idea': 8, 'add': 8, 'library': 8, 'slave': 8, 'omg': 8, 'no1': 8, 'polys': 8, 'moon': 8, 'training': 8, 'gay': 8, 'sale': 8, '08712460324': 8, 'registered': 8, 'miracle': 8, 'during': 8, 'movies': 8, 'digital': 8, 'black': 8, 'awaiting': 8, 'cancel': 8, 'cute': 8, 'energy': 8, 'complete': 8, 'honey': 8, 'picked': 8, 'vl': 8, 'frens': 8, 'reaching': 8, '0870': 8, 'cover': 8, '06': 8, 'south': 8, 'inside': 8, 'hw': 8, 'wednesday': 8, 'pix': 8, 'mood': 8, 'la': 7, 'bugis': 7, 'cine': 7, 'naughty': 7, 'sucks': 7, 'tea': 7, 'eating': 7, 'learn': 7, 'ahead': 7, 'kept': 7, 'liked': 7, 'bx420': 7, 'joke': 7, 'wun': 7, 'following': 7, 'ta': 7, 'pleasure': 7, '10am': 7, 'password': 7, 'cuz': 7, 'page': 7, 'umma': 7, 'weight': 7, 'bother': 7, 'country': 7, '82277': 7, 'yijue': 7, 'lect': 7, 'persons': 7, 'sometimes': 7, 'become': 7, '62468': 7, 'internet': 7, 'waste': 7, 'hell': 7, 'experience': 7, 'towards': 7, 'bucks': 7, 'past': 7, 'biz': 7, 'appreciate': 7, 'road': 7, 'battery': 7, '25': 7, 'kallis': 7, 'cal': 7, 'showing': 7, 'naked': 7, 'horny': 7, 'quality': 7, 'definitely': 7, 'sense': 7, 'sim': 7, 'loyalty': 7, 'high': 7, 'advance': 7, 'power': 7, 'return': 7, 'access': 7, '08718720201': 7, 'wiv': 7, 'fault': 7, 'maximize': 7, 'cold': 7, 'forward': 7, 'happening': 7, 'lift': 7, 'tough': 7, 'tenerife': 7, 'notice': 7, '8th': 7, 'depends': 7, 'realy': 7, 'mp3': 7, '85023': 7, 'unsub': 7, 'single': 7, 'fat': 7, 'married': 7, 'rather': 7, 'hotel': 7, 'omw': 7, 'hurry': 7, 'workin': 7, 'gee': 7, 'izzit': 7, 'spree': 7, 'present': 7, 'valentine': 7, 'future': 7, 'shuhui': 7, 'weather': 7, 'login': 7, 'tuesday': 7, 'ho': 7, 'awake': 7, 'bold': 7, 'looks': 7, 'dey': 7, 'sit': 7, '7pm': 7, 'holla': 7, 'summer': 7, 'damn': 7, 'space': 7, '36504': 7, 'bag': 7, 'model': 7, 'mother': 7, 'yrs': 7, 'mid': 7, 'midnight': 7, 'january': 7, 'iam': 7, 'photo': 7, 'sk38xh': 7, 'recently': 7, 'feels': 7, 'heavy': 7, 'nxt': 7, '3g': 7, 'o2': 7, 'onto': 7, 'station': 7, 'tuition': 7, 'strong': 7, 'cell': 7, 'dog': 7, 'alrite': 7, 'shd': 7, 'croydon': 7, 'cr9': 7, '5wb': 7, '1327': 7, 'meaning': 7, 'players': 7, 'share': 7, 'lmao': 7, 'except': 7, 'arrive': 7, 'instead': 7, 'holding': 7, 'list': 7, 'thnk': 7, 'excuse': 7, 'costa': 7, 'sol': 7, 'including': 7, 'vikky': 7, 'colleagues': 7, 'tear': 7, 'worse': 7, 'murderer': 7, 'maid': 7, 'murdered': 7, 'happens': 7, 'feb': 7, 'planned': 7, 'joking': 6, 'hl': 6, 'click': 6, 'team': 6, 'texting': 6, 'tyler': 6, 'usually': 6, 'fyi': 6, '150pm': 6, 'review': 6, 'pleased': 6, 'kano': 6, 'simply': 6, 'changed': 6, 'eatin': 6, 'flights': 6, 'directly': 6, 'informed': 6, 'app': 6, 'standard': 6, '08712300220': 6, 'shouldn': 6, 'replied': 6, 'local': 6, 'qatar': 6, 'arrange': 6, 'inviting': 6, 'turns': 6, 'spoke': 6, 'personal': 6, 'nights': 6, 'system': 6, 'partner': 6, 'died': 6, 'website': 6, 'tncs': 6, 'childish': 6, 'handset': 6, 'dint': 6, 'sunny': 6, 'ended': 6, 'anybody': 6, 'imagine': 6, 'babes': 6, 'sport': 6, 'accept': 6, 'kb': 6, 'yoga': 6, 'track': 6, 'ish': 6, 'cc': 6, 'posted': 6, 'air': 6, 'willing': 6, 'body': 6, 'relax': 6, 'pilates': 6, 'putting': 6, 'fullonsms': 6, 'competition': 6, 'aathi': 6, 'wnt': 6, 'vry': 6, 'vary': 6, 'askin': 6, 'group': 6, 'ttyl': 6, 'isnt': 6, 'gives': 6, 'moan': 6, 'fb': 6, 'activate': 6, 'character': 6, 'jst': 6, 'tat': 6, '40gb': 6, 'pin': 6, 'campus': 6, 'lady': 6, 'l8r': 6, 'aiyo': 6, 'barely': 6, 'scream': 6, 'marriage': 6, 'announcement': 6, 'indian': 6, 'ladies': 6, '28': 6, 'imma': 6, 'daily': 6, 'paid': 6, 'vodafone': 6, 'matches': 6, 'holder': 6, 'evng': 6, 'earth': 6, 'under': 6, 'torch': 6, 'aftr': 6, 'exactly': 6, 'yay': 6, 'txtauction': 6, 'yest': 6, 'closed': 6, 'wats': 6, 'couldn': 6, 'pobox84': 6, 'norm150p': 6, 'w45wq': 6, 'hunny': 6, 'boo': 6, 'teasing': 6, 'zed': 6, 'green': 6, 'surely': 6, 'five': 6, 'wed': 6, 'fall': 6, 'sup': 6, 'murder': 6, 'due': 6, 'teach': 6, 'ate': 6, 'wherever': 6, 'medical': 6, 'brand': 6, 'contract': 6, 'kerala': 6, 'asleep': 6, 'loverboy': 6, 'serious': 6, 'april': 6, 'flower': 6, 'process': 6, 'works': 6, 'regards': 6, 'sipix': 6, 'aiyah': 6, 'urawinner': 6, 'gal': 6, 'howz': 6, 'raining': 6, 'thts': 6, 'tour': 6, 'super': 6, 'marry': 6, 'problems': 6, 'fantasies': 6, '08707509020': 6, 'walking': 6, 'cafe': 6, 'bought': 6, '4th': 6, 'nature': 6, 'keeping': 6, 'screaming': 6, '86021': 6, 'london': 6, 'lookin': 6, 'exciting': 6, 'toclaim': 6, 'max10mins': 6, 'pobox334': 6, '09050090044': 6, 'stockport': 6, 'theatre': 6, 'ahmad': 6, 'official': 6, 'armand': 6, 'nimya': 6, 'sed': 6, 'role': 6, 'checked': 6, 'added': 6, 'pussy': 6, 'budget': 6, 'random': 6, 'er': 6, 'hr': 6, 'hrs': 6, 'cancer': 6, 'tariffs': 6, 'meds': 6, 'darling': 5, 'callers': 5, 'callertune': 5, 'searching': 5, 'wet': 5, '87077': 5, 'stock': 5, 'egg': 5, 'subscription': 5, 'roommate': 5, 'hopefully': 5, 'ride': 5, 'respect': 5, 'urgnt': 5, '530': 5, 'truly': 5, 'scared': 5, 'cabin': 5, 'voda': 5, 'quoting': 5, 'ec2a': 5, 'laid': 5, 'locations': 5, 'rooms': 5, 'begin': 5, 'shirt': 5, 'menu': 5, 'hop': 5, 'discuss': 5, 'bye': 5, '9am': 5, 'transaction': 5, 'cannot': 5, 'straight': 5, 'connection': 5, 'sen': 5, 'atm': 5, 'romantic': 5, '2optout': 5, 'flirt': 5, 'sam': 5, 'argument': 5, 'wins': 5, 'fix': 5, 'singles': 5, 'rays': 5, 'bf': 5, '21': 5, 'themob': 5, 'selection': 5, 'aren': 5, 'pongal': 5, 'december': 5, 'ppl': 5, 'cud': 5, 'report': 5, 'surfing': 5, 'num': 5, 'basically': 5, 'allah': 5, 'sonyericsson': 5, 'geeee': 5, 'sighs': 5, 'brings': 5, 'guide': 5, 'intro': 5, 'current': 5, 'pictures': 5, 'none': 5, 'yan': 5, 'jiu': 5, 'logo': 5, 'pobox36504w45wq': 5, 'contacted': 5, 'hostel': 5, 'hv': 5, 'amt': 5, 'respond': 5, 'ticket': 5, 'dollars': 5, 'acc': 5, 'woman': 5, 'flat': 5, 'sec': 5, 'conditions': 5, 'fighting': 5, 'some1': 5, 'spl': 5, 'stylish': 5, '83355': 5, 'returns': 5, 'quote': 5, 'english': 5, 'btw': 5, '2mrw': 5, 'smiles': 5, 'jazz': 5, 'yogasana': 5, '1x150p': 5, 'stopped': 5, 'somethin': 5, 'results': 5, 'euro2004': 5, 'drinks': 5, '80062': 5, 'thursday': 5, 'listening': 5, 'cartoon': 5, 'drug': 5, 'fetch': 5, 'belly': 5, 'lonely': 5, 'law': 5, 'gap': 5, 'timing': 5, 'running': 5, 'mad': 5, 'twice': 5, 'opportunity': 5, 'gals': 5, 'city': 5, 'tis': 5, 'living': 5, 'polyphonic': 5, 'xxxx': 5, 'comuk': 5, 'ages': 5, 'sura': 5, 'playing': 5, 'matter': 5, 'sn': 5, 'hai': 5, 'records': 5, 'cds': 5, 'birds': 5, 'travel': 5, 'lead': 5, 'unsold': 5, 'derek': 5, 'greet': 5, 'white': 5, 'cheaper': 5, 'ym': 5, 'pissed': 5, 'ma': 5, 'wear': 5, 'expensive': 5, 'photos': 5, 'site': 5, 'boring': 5, 'ad': 5, 'salary': 5, 'noline': 5, 'dload': 5, 'videochat': 5, 'videophones': 5, 'rentl': 5, 'java': 5, 'dropped': 5, 'yun': 5, 'jesus': 5, 'gm': 5, '3rd': 5, 'bitch': 5, 'revealed': 5, 'xchat': 5, 'hands': 5, 'receipt': 5, 'interesting': 5, 'uni': 5, 'italian': 5, 'adult': 5, 'oz': 5, 'horrible': 5, 'nw': 5, 'jordan': 5, 'choice': 5, 'mite': 5, 'chinese': 5, 'hun': 5, 'cbe': 5, 'broke': 5, 'original': 5, 'pple': 5, 'arrested': 5, 'linerental': 5, 'vote': 5, 'tells': 5, 'totally': 5, 'rem': 5, 'exams': 5, 'everybody': 5, 'optout': 5, 'google': 5, 'vomit': 5, 'centre': 5, 'airport': 5, 'costs': 5, 'buzz': 5, 'eerie': 5, 'waking': 5, 'ran': 5, '60p': 5, 'boys': 5, 'bin': 5, 'social': 5, 'buns': 5, 'created': 5, 'beer': 5, 'season': 5, 'nvm': 5, 'moms': 5, 'obviously': 5, 'flag': 5, 'eng': 5, 'inclusive': 5, 'looked': 5, 'expecting': 5, 'latr': 5, 'minuts': 5, 'unable': 5, 'remind': 5, 'whether': 5, 'fantasy': 5, 'brilliant': 5, 'police': 5, 'ru': 5, 'cars': 5, 'plenty': 5, 'amount': 5, 'advice': 5, 'behind': 5, 'amazing': 5, 'issues': 5, 'ignore': 5, 'thurs': 5, 'wouldn': 5, 'lik': 5, 'asks': 5, '300': 5, '3510i': 5, '400': 5, 'mths': 5, 'common': 5, 'oni': 4, '87121': 4, 'tkts': 4, 'lives': 4, 'tb': 4, 'oru': 4, 'six': 4, '87575': 4, 'membership': 4, 'str': 4, 'sooner': 4, 'turn': 4, 'child': 4, 'letter': 4, 'inches': 4, 'weak': 4, 'seemed': 4, 'url': 4, 'series': 4, 'iq': 4, 'wah': 4, 'machan': 4, 'becoz': 4, '9pm': 4, 'fml': 4, 'appointment': 4, 'hols': 4, 'legal': 4, 'nyc': 4, 'considering': 4, 'jokes': 4, 'research': 4, 'tt': 4, 'needed': 4, '786': 4, 'unredeemed': 4, 'hasn': 4, 'yetunde': 4, 'ansr': 4, 'tyrone': 4, 'largest': 4, 'befor': 4, 'activities': 4, 'biggest': 4, 'netcollex': 4, 'deleted': 4, 'interview': 4, '434': 4, 'escape': 4, 'bloody': 4, 'anyways': 4, '4742': 4, '145': 4, '0808': 4, '11pm': 4, 'radio': 4, 'unique': 4, 'settled': 4, 'shoot': 4, 'files': 4, 'career': 4, 'cross': 4, 'recd': 4, 'closer': 4, 'argue': 4, 'theory': 4, 'com1win150ppmx3age16': 4, 'affection': 4, 'manda': 4, 'kettoda': 4, 'bcums': 4, 'expect': 4, 'mmm': 4, 'bay': 4, 'hmv': 4, 'passed': 4, 'throw': 4, 'cam': 4, 'accidentally': 4, 'cry': 4, 'def': 4, 'meal': 4, 'dates': 4, 'hanging': 4, 'belovd': 4, 'enemy': 4, 'smart': 4, 'afraid': 4, '08002986906': 4, 'kisses': 4, 'waitin': 4, '83600': 4, '1000s': 4, 'practice': 4, 'wtf': 4, 'further': 4, 'sometime': 4, 'cream': 4, 'tree': 4, 'esplanade': 4, 'fifteen': 4, '3mins': 4, 'wc1n3xx': 4, 'journey': 4, 'jen': 4, 'gorgeous': 4, 'purpose': 4, 'tenants': 4, 'refused': 4, 'ure': 4, 'intelligent': 4, 'result': 4, 'reasons': 4, 'receiving': 4, 'cw25wx': 4, 'tcs': 4, 'charges': 4, 'dry': 4, 'center': 4, 'village': 4, 'bringing': 4, 'jada': 4, 'matured': 4, 'kusruthi': 4, 'prabha': 4, 'mtmsgrcvd18': 4, 'bday': 4, 'rude': 4, 'mas': 4, 'passionate': 4, 'confidence': 4, 'losing': 4, 'three': 4, 'milk': 4, 'essential': 4, 'lab': 4, 'quit': 4, '08715705022': 4, '24': 4, 'grand': 4, '542': 4, 'pie': 4, 'answers': 4, 'often': 4, 'uncles': 4, 'leona': 4, 'bud': 4, 'taken': 4, 'church': 4, 'temple': 4, 'gentle': 4, 'hella': 4, 'bet': 4, 'envelope': 4, 'prepare': 4, 'seem': 4, 'explain': 4, 'purchase': 4, 'weird': 4, 'drivin': 4, 'students': 4, 'height': 4, 'max': 4, 'assume': 4, '81151': 4, '4t': 4, 'faster': 4, 'spoken': 4, 'mt': 4, 'skilgme': 4, '88039': 4, 'meetin': 4, 'apparently': 4, 'smokes': 4, 'sing': 4, 'perfect': 4, '08718727870': 4, 'enjoyed': 4, 'dictionary': 4, 'm263uz': 4, 'appt': 4, '3d': 4, 'ain': 4, 'ache': 4, '3qxj9': 4, '08702840625': 4, '9ae': 4, 'profit': 4, 'cust': 4, 'ppm': 4, 'ibiza': 4, 'meanwhile': 4, 'suite': 4, 'version': 4, 'careful': 4, 'spk': 4, 'saved': 4, 'played': 4, 'wanting': 4, 'pig': 4, 'attend': 4, 'term': 4, 'fever': 4, 'places': 4, 'w1': 4, 'carefully': 4, 'gravity': 4, 'bowl': 4, 'decision': 4, 'sore': 4, 'regret': 4, 'throat': 4, 'lecture': 4, 'raise': 4, 'fool': 4, 'june': 4, 'technical': 4, 'bathing': 4, 'vijay': 4, 'dem': 4, 'fight': 4, 'subscriber': 4, 'aiyar': 4, 'wearing': 4, 'shame': 4, 'credited': 4, 'understanding': 4, 'delivered': 4, 'arms': 4, 'easier': 4, 'txtin': 4, '4info': 4, '08712405020': 4, 'songs': 4, 'exact': 4, '2moro': 4, '80488': 4, 'favour': 4, '3gbp': 4, 'idiot': 4, 'february': 4, 'rush': 4, 'blackberry': 4, 'moji': 4, 'fill': 4, 'gently': 4, '4get': 4, 'msgrcvdhg': 4, 'aiya': 4, 'pod': 4, '7th': 4, '6th': 4, '5th': 4, 'wonders': 4, 'personality': 4, 'purity': 4, 'aint': 4, 'sha': 4, 'total': 4, 'along': 4, 'file': 4, 'shortly': 4, 'ron': 4, '7250i': 4, 'w1jhl': 4, 'preferably': 4, 'idk': 4, 'whom': 4, 'laughing': 4, 'title': 4, 'brought': 4, 'surprised': 4, 'comedy': 4, 'moby': 4, 'action': 4, 'sight': 4, 'remain': 4, 'received': 4, 'ordered': 4, 'rd': 4, 'queen': 4, 'fren': 4, 'connect': 4, 'hook': 4, '05': 4, 'schedule': 4, 'selling': 4, 'settings': 4, 'alert': 4, 'atlanta': 4, 'gaps': 4, 'fills': 4, 'takin': 4, 'answering': 4, 'jess': 4, 'dirty': 4, 'package': 4, '08001950382': 4, 'upto': 4, 'hate': 4, 'skype': 4, 'nearly': 4, 'masters': 4, 'cook': 4, 'cleaning': 4, 'cat': 4, 'hip': 4, '87239': 4, 'freefone': 4, 'lie': 4, 'infernal': 4, 'giv': 4, '84199': 4, 'w111wx': 4, 'yer': 4, 'box39822': 4, 'subs': 4, 'med': 4, 'kidz': 4, 'ntwk': 4, 'frndship': 4, 'freak': 4, 'ref': 4, 'empty': 4, 'wkend': 4, 'letters': 4, 'football': 4, 'happend': 4, 'sugar': 4, 'thangam': 4, 'roger': 4, 'solve': 4, 'cooking': 4, 'key': 4, 'released': 4, 'deliver': 4, 'spending': 4, 'sept': 4, 'public': 4, 'instituitions': 4, 'govt': 4, 'dare': 4, 'teeth': 4, 'iz': 4, 'handle': 4, 'note': 4, 'celebrate': 4, 'tm': 4, 'abi': 4, 'hill': 4, 'relation': 4, 'fromm': 4, '09061221066': 4, 'wylie': 4, 'basic': 4, 'outta': 4, 'inform': 4, 'texted': 4, '26': 4, 'doc': 4, 'taunton': 4, 'loss': 4, '2005': 3, '21st': 3, 'nurungu': 3, 'vettam': 3, 'melle': 3, 'minnaminunginte': 3, 'spell': 3, 'wales': 3, 'scotland': 3, 'clear': 3, 'caught': 3, 'fear': 3, 'xuhui': 3, 'invite': 3, 'yummy': 3, 'fair': 3, 'runs': 3, 'embarassed': 3, 'realized': 3, 'matrix3': 3, '09061209465': 3, 'starwars3': 3, 'suprman': 3, 'roommates': 3, 'dresser': 3, '1500': 3, 'advise': 3, 'recent': 3, 'valuable': 3, 'plane': 3, 'gentleman': 3, 'dignity': 3, 'shy': 3, 'coins': 3, 'requests': 3, 'sheets': 3, 'sum1': 3, 'lido': 3, 'collected': 3, 'verify': 3, 'mix': 3, 'four': 3, 'boston': 3, 'vava': 3, 'loud': 3, 'k52': 3, 'sentence': 3, 'wa': 3, 'anythin': 3, '45239': 3, 'apologise': 3, 'hardcore': 3, 'dot': 3, 'staff': 3, 'female': 3, 'birla': 3, 'soft': 3, 'floor': 3, 'spanish': 3, 'mall': 3, 'maneesha': 3, 'toll': 3, 'satisfied': 3, 'mummy': 3, 'finishes': 3, 'august': 3, 'suggest': 3, 'successfully': 3, 'register': 3, 'mtmsg18': 3, '087187262701': 3, '89545': 3, '50gbp': 3, 'followed': 3, 'pence': 3, 'loses': 3, 'tomarrow': 3, 'avent': 3, 'touched': 3, 'slippers': 3, 'innings': 3, 'bat': 3, 'dearly': 3, '125gift': 3, 'ranjith': 3, '5min': 3, 'networks': 3, 'mini': 3, 'parked': 3, 'jealous': 3, 'flash': 3, 'sorting': 3, '100percent': 3, 'genuine': 3, 'handed': 3, 'gautham': 3, 'buzy': 3, 'upgrade': 3, '0845': 3, 'tease': 3, 'scary': 3, 'gossip': 3, 'fit': 3, 'newest': 3, 'keys': 3, 'garage': 3, 'bstfrnd': 3, 'lifpartnr': 3, 'jstfrnd': 3, 'dear1': 3, 'best1': 3, 'clos1': 3, 'lvblefrnd': 3, 'cutefrnd': 3, 'swtheart': 3, '3uz': 3, 'm26': 3, 'gona': 3, 'flight': 3, 'women': 3, 'record': 3, 'germany': 3, 'supervisor': 3, 'lifetime': 3, 'bless': 3, 'favourite': 3, 'stranger': 3, 'gudnite': 3, 'slap': 3, 'alcohol': 3, 'remembered': 3, 'insha': 3, 'insurance': 3, 'alive': 3, 'gbp': 3, 'ptbo': 3, 'tests': 3, '6months': 3, '08000938767': 3, '4mths': 3, 'or2stoptxt': 3, 'mobilesdirect': 3, 'shut': 3, 'period': 3, 'business': 3, 'picture': 3, 'quickly': 3, 'nd': 3, '87131': 3, 'chechi': 3, 'sender': 3, 'skip': 3, 'names': 3, 'irritating': 3, 'ful': 3, 'lacs': 3, 'bmw': 3, 'arng': 3, 'shortage': 3, 'urgently': 3, 'source': 3, 'iouri': 3, 'sachin': 3, 'oic': 3, 'transfer': 3, '1956669': 3, 'homeowners': 3, 'previously': 3, '75': 3, 'si': 3, 'july': 3, '0207': 3, 'railway': 3, 'doggy': 3, 'fave': 3, 'roads': 3, 'dave': 3, 'transfered': 3, 'banks': 3, '9ja': 3, '9t': 3, 'wise': 3, 'boye': 3, 'dificult': 3, 'fightng': 3, 'fish': 3, '123': 3, '1450': 3, 'fees': 3, 'sory': 3, 'soryda': 3, 'ldnw15h': 3, 'ibhltd': 3, 'cha': 3, 'mono': 3, 'booking': 3, 'behave': 3, 'elsewhere': 3, '09': 3, 'box95qu': 3, '0871': 3, '08717898035': 3, 'malaria': 3, 'ummmmmaah': 3, 'tirupur': 3, 'bloke': 3, 'cock': 3, 'generally': 3, 'likely': 3, 'callin': 3, 'american': 3, 'dick': 3, 'headache': 3, '80878': 3, 'lines': 3, 'exhausted': 3, 'swimming': 3, '2morow': 3, 'paris': 3, 'nichols': 3, '83222': 3, 'market': 3, 'pop': 3, 'postcode': 3, 'seven': 3, 'tlp': 3, 'thanksgiving': 3, '31': 3, 'en': 3, 'peace': 3, '89555': 3, 'textoperator': 3, 'map': 3, 'building': 3, 'accordingly': 3, 'farm': 3, 'ws': 3, 'stress': 3, 'csbcm4235wc1n3xx': 3, 'upset': 3, 'low': 3, 'shouted': 3, 'shorter': 3, 'subscribed': 3, 'realize': 3, 'gimme': 3, '50perwksub': 3, 'tscs087147403231winawk': 3, 'anywhere': 3, 'diff': 3, 'community': 3, 'subpoly': 3, '81618': 3, 'bein': 3, 'jan': 3, 'pieces': 3, 'bid': 3, 'responding': 3, '2u': 3, 'cm2': 3, '220': 3, '08701417012': 3, 'charity': 3, 'alfie': 3, 'm8s': 3, 'nokias': 3, 'brain': 3, 'hahaha': 3, 'given': 3, 'successful': 3, '2morrow': 3, 'sk3': 3, '8wp': 3, 'seconds': 3, 'xavier': 3, 'stomach': 3, 'returned': 3, 'vip': 3, 'supply': 3, 'nap': 3, 'cuddle': 3, 'shesil': 3, '10k': 3, 'liverpool': 3, 'reminder': 3, 'failed': 3, 'outstanding': 3, 'taylor': 3, 'male': 3, '5p': 3, 'msging': 3, 'diet': 3, '88600': 3, 'moments': 3, '114': 3, '14': 3, 'tcr': 3, 'magical': 3, 'welp': 3, 'valid12hrs': 3, '15': 3, 'chicken': 3, 'potential': 3, 'talent': 3, '09063458130': 3, 'polyph': 3, 'fuckin': 3, 'butt': 3, 'terrible': 3, 'prey': 3, 'fancies': 3, 'foreign': 3, 'stamps': 3, 'speechless': 3, 'roast': 3, 'concentrate': 3, 'chatting': 3, 'walked': 3, 'euro': 3, 'drunk': 3, '84025': 3, 'networking': 3, 'juicy': 3, 'dearer': 3, 'itz': 3, 'alwys': 3, 'evn': 3, 'clock': 3, '09061790121': 3, 'ne': 3, 'ground': 3, 'speed': 3, 'catching': 3, 'falls': 3, 'whos': 3, 'le': 3, 'bigger': 3, 'islands': 3, 'celeb': 3, 'pocketbabe': 3, 'voicemail': 3, '2go': 3, 'walmart': 3, 'score': 3, '87021': 3, 'apps': 3, 'anti': 3, 'rofl': 3, 'ph': 3, 'various': 3, 'textcomp': 3, '84128': 3, 'morn': 3, 'docs': 3, 'havin': 3, 'rang': 3, 'sorted': 3, 'executive': 3, 'jane': 3, 'express': 3, 'fran': 3, 'knackered': 3, 'software': 3, 'jamster': 3, 'among': 3, 'cares': 3, '6hrs': 3, 'chill': 3, 'chillin': 3, 'saucy': 3, 'chain': 3, 'suntec': 3, 'messenger': 3, 'screen': 3, 'upload': 3, 'tom': 3, 'shot': 3, 'wt': 3, 'storming': 3, 'invnted': 3, 'margaret': 3, 'telphone': 3, 'phne': 3, 'girlfrnd': 3, 'grahmbell': 3, 'popped': 3, 'shld': 3, 'beware': 3, 'caring': 3, 'option': 3, 'goodnite': 3, 'painful': 3, 'guilty': 3, 'cardiff': 3, 'addie': 3, 'bright': 3, 'certainly': 3, 'twelve': 3, 'aah': 3, '07xxxxxxxxx': 3, 'hubby': 3, 'minmobsmorelkpobox177hp51fl': 3, 'blake': 3, 'karaoke': 3, 'stars': 3, 'eight': 3, 'ese': 3, 'prospects': 3, 'bishan': 3, 'buff': 3, 'gang': 3, 'tablets': 3, 'finishing': 3, 'doors': 3, 'chasing': 3, 'brothas': 3, 'force': 3, 'blame': 3, 'blessings': 3, 'freezing': 3, 'winning': 3, '6pm': 3, 'titles': 3, 'feelin': 3, 'switch': 3, 'monthly': 3, 'ideas': 3, 'maintain': 3, 'sh': 3, 'cramps': 3, 'nan': 3, '81303': 3, 'dislikes': 3, 'likes': 3, 'album': 3, '121': 3, 'standing': 3, 'james': 3, '29': 3, 'chosen': 3, 'di': 3, 'cruise': 3, 'follow': 3, 'stuck': 3, 'regarding': 3, 'adore': 3, 'arcade': 3, 'arun': 3, 'philosophy': 3, 'eye': 3, 'husband': 3, 'norm': 3, 'toa': 3, 'payoh': 3, 'fathima': 3, 'mmmm': 3, '18yrs': 3, 'abta': 3, '80182': 3, '08452810073': 3, 'table': 3, 'ikea': 3, 'cn': 3, 'kadeem': 3, 'wud': 3, 'carry': 3, 'avatar': 3, 'stops': 3, 'constantly': 3, 'lousy': 3, 'ic': 3, 'sweetest': 3, 'honeybee': 3, 'laughed': 3, 'havnt': 3, 'crack': 3, 'boat': 3, 'proof': 3, 'provided': 3, 'yeh': 3, 'members': 3, 'downloads': 3, 'major': 3, 'birth': 3, 'rule': 3, 'natural': 3, 'onwards': 3, '150ppermesssubscription': 3, 'skillgame': 3, 'tscs': 3, '1winaweek': 3, 'eggs': 3, 'boost': 3, 'calicut': 3, 'box97n7qp': 3, 'pink': 3, 'normally': 3, 'rich': 3, 'm8': 3, 'yor': 3, 'jason': 3, 'art': 3, 'feet': 3, 'argh': 3, 'favor': 3, 'tessy': 3, 'shijas': 3, 'aunty': 3, 'china': 3, 'morphine': 3, 'prefer': 3, 'kindly': 3, 'pages': 3, 'pending': 3, 'raji': 3, 'legs': 3, 'distance': 3, 'temp': 3, 'display': 3, 'soup': 3, 'management': 3, 'include': 3, 'regular': 3, 'threats': 3, 'lounge': 3, 'u4': 3, 'cheer': 3, 'cornwall': 3, 'bags': 3, 'iscoming': 3, '80082': 3, 'spook': 3, 'halloween': 3, 'issue': 3, 'sky': 3, 'measure': 3, 'thm': 3, 'instantly': 3, 'drinking': 3, 'wn': 3, 'impossible': 3, 'responce': 3, 'vodka': 3, 'okey': 3, 'neighbour': 3, 'questioned': 3, 'gardener': 3, 'vegetables': 3, 'science': 3, 'madam': 3, 'settle': 3, 'citizen': 3, 'indians': 3, 'sry': 3, '09066612661': 3, 'greetings': 3, 'dai': 3, 'maga': 3, 'medicine': 3, 'violence': 3, 'incident': 3, 'erm': 3, 'instructions': 3, '3lp': 3, 'death': 3, 'hon': 3, 'reality': 3, 'usc': 3, 'booty': 3, 'lil': 3, 'remains': 3, 'bros': 3, 'bro': 3, 'response': 3, 'shirts': 3, 'petrol': 3, 'uks': 3, '2stoptxt': 3, 'luxury': 3, 'ben': 3, 'middle': 3, 'dark': 3, 'enuff': 3, 'strike': 3, 'moved': 3, 'porn': 3, 'dress': 3, 'collecting': 3, 'flaked': 3, 'gary': 3, 'history': 3, 'bell': 3, 'understood': 3, 'bottom': 3, '33': 3, 'books': 3, 'prove': 3, 'blow': 3, 'knowing': 3, 'challenge': 3, 'randomly': 3, 'tape': 3, 'films': 3, 'lick': 3, 'auto': 3, 'praying': 3, 'hug': 3, 'deliveredtomorrow': 3, 'smoking': 3, 'in2': 3, 'billed': 3, 'ths': 3, 'callback': 3, 'wedding': 3, 'accident': 3, 'wisdom': 3, 'cann': 3, 'symbol': 3, 'prolly': 3, 'confirmed': 3, 'dubsack': 3, 'macho': 3, 'audition': 3, 'fell': 3, 'senthil': 3, 'forevr': 3, 'eaten': 3, 'nat': 3, 'possession': 3, 'concert': 3, 'born': 3, 'affairs': 3, 'california': 3, 'university': 3, 'value': 3, 'mnth': 3, 'tog': 3, 'haiz': 3, 'previous': 3, 'parking': 3, 'wallpaper': 3, 'step': 3, 'buffet': 2, 'fa': 2, '08452810075over18': 2, 'hor': 2, 'rcv': 2, 'kl341': 2, 'receivea': 2, '09061701461': 2, '08002986030': 2, 'tsandcs': 2, 'csh11': 2, '6days': 2, 'chances': 2, '4403ldnw1a7rw18': 2, 'jackpot': 2, 'dbuk': 2, 'lccltd': 2, '81010': 2, 'blessing': 2, 'goals': 2, '4txt': 2, 'slice': 2, 'convincing': 2, 'frying': 2, 'sarcastic': 2, '8am': 2, 'mmmmmm': 2, 'burns': 2, 'hospitals': 2, 'gram': 2, 'eighth': 2, 'detroit': 2, 'hockey': 2, 'odi': 2, 'killing': 2, 'burger': 2, '09066364589': 2, 'dedicated': 2, 'dedicate': 2, 'eurodisinc': 2, 'shracomorsglsuplt': 2, 'entry41': 2, '3aj': 2, 'trav': 2, 'ls1': 2, 'aco': 2, 'morefrmmob': 2, 'divorce': 2, 'earn': 2, 'jacket': 2, 'nitros': 2, 'ela': 2, 'pours': 2, '169': 2, '6031': 2, '92h': 2, 'usher': 2, 'britney': 2, '450ppw': 2, '5249': 2, 'mk17': 2, '85069': 2, 'telugu': 2, 'loans': 2, 'animation': 2, 'location': 2, 'noun': 2, 'gent': 2, '09064012160': 2, 'puttin': 2, 'goodo': 2, 'potato': 2, 'tortilla': 2, '08719180248': 2, '07742676969': 2, 'sum': 2, 'algarve': 2, '69888': 2, '31p': 2, 'msn': 2, 'pouch': 2, 'hearts': 2, 'somtimes': 2, 'occupy': 2, '08700621170150p': 2, 'randy': 2, 'flowing': 2, 'plaza': 2, 'everywhere': 2, 'windows': 2, 'mouth': 2, '0871277810810': 2, 'module': 2, 'avoid': 2, 'beloved': 2, 'clark': 2, 'form': 2, 'utter': 2, 'completed': 2, 'stays': 2, 'wishin': 2, 'hamster': 2, 'keralacircle': 2, 'inr': 2, 'refilled': 2, 'kr': 2, 'prepaid': 2, 'ericsson': 2, 'bruv': 2, 'rewarding': 2, 'heading': 2, 'installing': 2, 'repair': 2, 'star': 2, 'teacher': 2, 'teaches': 2, 'upstairs': 2, 'printed': 2, '09058094597': 2, '447801259231': 2, 'signing': 2, 'shining': 2, 'although': 2, 'commercial': 2, 'drpd': 2, 'deepak': 2, 'deeraj': 2, '2wks': 2, 'lag': 2, 'shipping': 2, 'headin': 2, 'necessarily': 2, 'jolt': 2, 'suzy': 2, 'mk45': 2, '2wt': 2, 'chart': 2, 'gf': 2, 'tool': 2, 'jenny': 2, '021': 2, '3680': 2, 'grave': 2, 'taxi': 2, 'crash': 2, 'shocking': 2, 'actor': 2, 'hide': 2, 'thread': 2, '82468': 2, 'funky': 2, 'anot': 2, 'lo': 2, 'tahan': 2, 'buses': 2, 'bristol': 2, 'apo': 2, '861': 2, '85': 2, 'prepayment': 2, '0844': 2, 'paperwork': 2, 'violated': 2, 'privacy': 2, 'caroline': 2, 'cleared': 2, 'misbehaved': 2, 'tissco': 2, 'tayseer': 2, 'audrey': 2, 'status': 2, 'breathe': 2, 'update_now': 2, 'cuddling': 2, 'agree': 2, 'recognise': 2, 'hes': 2, 'ovulation': 2, 'n9dx': 2, 'licks': 2, '30ish': 2, 'grace': 2, 'inshah': 2, 'sharing': 2, 'salam': 2, 'field': 2, 'shipped': 2, 'burning': 2, 'loxahatchee': 2, 'wld': 2, 'darlings': 2, 'fav': 2, 'slightly': 2, 'box334sk38ch': 2, 'whatsup': 2, 'goal': 2, '80086': 2, 'txttowin': 2, 'mobno': 2, 'ads': 2, 'name1': 2, 'adam': 2, 'name2': 2, '07123456789': 2, 'txtno': 2, 'siva': 2, 'expression': 2, 'speaking': 2, '3650': 2, '09066382422': 2, 'bcm4284': 2, '300603': 2, 'applebees': 2, 'cricketer': 2, 'bhaji': 2, 'improve': 2, 'oreo': 2, 'truffles': 2, 'amy': 2, 'coping': 2, 'decisions': 2, 'individual': 2, '26th': 2, '153': 2, 'position': 2, 'language': 2, '09061743806': 2, 'box326': 2, 'screamed': 2, 'removed': 2, 'infront': 2, 'broken': 2, 'tension': 2, 'taste': 2, '07781482378': 2, 'trade': 2, '7ish': 2, 'rec': 2, 'b4280703': 2, '08718727868': 2, '09050002311': 2, 'hyde': 2, 'anthony': 2, 'scrounge': 2, 'forgiven': 2, 'slide': 2, 'renewal': 2, 'transport': 2, 'definite': 2, 'nos': 2, 'ebay': 2, 'pickle': 2, 'tacos': 2, '872': 2, '24hrs': 2, 'pg': 2, 'channel': 2, '08718738001': 2, 'web': 2, '2stop': 2, 'ability': 2, 'develop': 2, 'recovery': 2, 'cali': 2, 'cutting': 2, 'reminding': 2, 'owns': 2, 'faggy': 2, 'demand': 2, 'fo': 2, 'loose': 2, 'perhaps': 2, 'mei': 2, 'geeeee': 2, 'ey': 2, 'oooh': 2, 'call09050000327': 2, 'claims': 2, 'dancing': 2, 'snake': 2, 'bite': 2, 'hardly': 2, 'promo': 2, 'ag': 2, '08712402050': 2, '10ppm': 2, '0825': 2, 'tsunamis': 2, 'soiree': 2, '22': 2, 'ques': 2, 'suits': 2, 'reaction': 2, 'shock': 2, 'grow': 2, 'useful': 2, 'officially': 2, '89693': 2, 'textbuddy': 2, 'gaytextbuddy': 2, '09064019014': 2, '4882': 2, 'hundred': 2, 'expressoffer': 2, 'sweetheart': 2, 'biola': 2, 'effects': 2, 'wee': 2, 'trains': 2, 'ham': 2, 'jolly': 2, '40533': 2, 'sw7': 2, '3ss': 2, 'rstm': 2, 'panic': 2, 'dealer': 2, 'impatient': 2, 'river': 2, 'premium': 2, 'lays': 2, 'posts': 2, 'yelling': 2, 'hex': 2, 'cochin': 2, '4d': 2, 'poop': 2, 'gpu': 2, 'hurried': 2, 'aeroplane': 2, 'calld': 2, 'professors': 2, 'wer': 2, 'aeronautics': 2, 'datz': 2, 'dorm': 2, 'mobilesvary': 2, '050703': 2, 'callcost': 2, '1250': 2, '09071512433': 2, 'cookies': 2, 'correction': 2, 'admit': 2, 'ba': 2, 'spring': 2, 'mtmsg': 2, 'ctxt': 2, 'nokia6650': 2, 'attached': 2, '930': 2, 'helpline': 2, '08706091795': 2, 'gist': 2, 'thousands': 2, '40': 2, 'premier': 2, 'lip': 2, 'confused': 2, 'spare': 2, 'faith': 2, 'acting': 2, 'schools': 2, 'inch': 2, 'begging': 2, '0578': 2, 'opening': 2, 'pole': 2, 'thot': 2, 'petey': 2, 'nic': 2, '8077': 2, 'cashto': 2, 'getstop': 2, '88222': 2, 'php': 2, '08000407165': 2, 'imp': 2, 'bec': 2, 'nervous': 2, 'hint': 2, 'borrow': 2, 'dobby': 2, 'galileo': 2, 'enjoyin': 2, 'loveme': 2, 'cappuccino': 2, 'mojibiola': 2, '07821230901': 2, '09065174042': 2, 'hol': 2, 'kz': 2, 'aburo': 2, 'ultimatum': 2, 'countin': 2, 'skyped': 2, '08002888812': 2, 'inconsiderate': 2, 'hence': 2, 'recession': 2, 'nag': 2, 'soo': 2, '09066350750': 2, 'warning': 2, 'shoes': 2, 'lovejen': 2, 'discreet': 2, 'worlds': 2, 'named': 2, 'genius': 2, 'connections': 2, 'lotta': 2, 'lately': 2, 'virgin': 2, 'mystery': 2, 'approx': 2, 'smsco': 2, 'peaceful': 2, 'consider': 2, 'walls': 2, '41685': 2, '07': 2, 'fixedline': 2, '5k': 2, '09064011000': 2, 'cr01327bt': 2, 'castor': 2, '09058094565': 2, '08': 2, 'stopsms': 2, '09065171142': 2, 'downloaded': 2, 'ear': 2, 'oil': 2, 'usb': 2, 'mac': 2, 'gibbs': 2, 'unbelievable': 2, 'superb': 2, 'several': 2, 'worst': 2, 'charles': 2, 'stores': 2, 'peak': 2, '08709222922': 2, '8p': 2, 'sweets': 2, 'chip': 2, 'addicted': 2, 'yck': 2, 'ashley': 2, 'lux': 2, 'jeans': 2, 'tons': 2, 'scores': 2, 'application': 2, 'ms': 2, 'filthy': 2, 'simpler': 2, '09050001808': 2, 'm95': 2, 'necklace': 2, 'rice': 2, 'racing': 2, 'closes': 2, 'crap': 2, 'borin': 2, 'chocolate': 2, 'reckon': 2, 'tech': 2, '65': 2, 'sd': 2, 'blessed': 2, 'quiet': 2, 'aunts': 2, 'helen': 2, 'fan': 2, 'lovers': 2, 'drove': 2, 'exe': 2, 'pen': 2, 'anniversary': 2, 'datebox1282essexcm61xn': 2, 'secretly': 2, 'pattern': 2, 'plm': 2, 'sheffield': 2, 'zoe': 2, 'setting': 2, 'filling': 2, 'sufficient': 2, 'thx': 2, 'gnt': 2, 'rightly': 2, 'viva': 2, 'edison': 2, 'ls15hb': 2, 'educational': 2, 'flirting': 2, 'kickoff': 2, 'sells': 2, 'thesis': 2, 'sends': 2, 'deciding': 2, 'herself': 2, 'compare': 2, 'eastenders': 2, 'tulip': 2, 'wkent': 2, 'violet': 2, '150p16': 2, 'lily': 2, 'prepared': 2, 'm6': 2, '09058091854': 2, 'box385': 2, '6wu': 2, '09050003091': 2, 'c52': 2, 'oi': 2, 'craziest': 2, 'curry': 2, 'thoughts': 2, 'singing': 2, 'breath': 2, 'planet': 2, '28days': 2, '2yr': 2, 'm221bp': 2, 'box177': 2, '09061221061': 2, '99': 2, 'warranty': 2, 'tomorro': 2, 'fret': 2, 'wind': 2, 'depressed': 2, 'math': 2, 'dhoni': 2, 'rocks': 2, 'durban': 2, '08000776320': 2, 'survey': 2, 'difficulties': 2, 'sar': 2, 'tank': 2, 'silently': 2, 'drms': 2, 'itcould': 2, 'wrc': 2, 'rally': 2, 'lucozade': 2, '61200': 2, 'packs': 2, 'toot': 2, 'annoying': 2, 'makin': 2, 'popcorn': 2, 'beneficiary': 2, 'neft': 2, 'subs16': 2, '1win150ppmx3': 2, 'appreciated': 2, 'apart': 2, 'creepy': 2, '08719181513': 2, 'nok': 2, 'invest': 2, 'delay': 2, '1hr': 2, 'purse': 2, 'europe': 2, 'flip': 2, 'accounts': 2, 'jd': 2, 'weirdest': 2, 'l8tr': 2, 'minmoremobsemspobox45po139wa': 2, 'tee': 2, 'control': 2, 'dough': 2, 'irritates': 2, 'fails': 2, 'jerry': 2, 'drinkin': 2, '5pm': 2, 'birthdate': 2, 'nydc': 2, 'ola': 2, 'items': 2, 'garbage': 2, 'logos': 2, 'gold': 2, 'lionp': 2, 'lionm': 2, 'lions': 2, 'jokin': 2, 'colours': 2, 'whenevr': 2, 'remembr': 2, 'harry': 2, 'potter': 2, 'phoenix': 2, 'readers': 2, 'canada': 2, 'goodnoon': 2, 'patty': 2, 'interest': 2, 'george': 2, '89080': 2, 'free2day': 2, '0870241182716': 2, 'theres': 2, 'tmrw': 2, 'soul': 2, 'ned': 2, 'main': 2, 'hurting': 2, 'sweetie': 2, '4a': 2, 'whn': 2, 'dance': 2, 'bar': 2, '08718730666': 2, 'bears': 2, 'juan': 2, 'lf56': 2, 'tlk': 2, 'front': 2, 'ideal': 2, 'arm': 2, 'tirunelvali': 2, 'effect': 2, 'bk': 2, 'kidding': 2, 'stretch': 2, 'urn': 2, 'sinco': 2, 'disclose': 2, 'frauds': 2, 'payee': 2, 'icicibank': 2, 'kaiez': 2, 'babies': 2, 'practicing': 2, 'pale': 2, 'beneath': 2, 'silver': 2, 'silence': 2, 'revision': 2, 'exeter': 2, 'whose': 2, 'condition': 2, 'arsenal': 2, 'missin': 2, 'tues': 2, 'restaurant': 2, 'textpod': 2, 'desperate': 2, 'monkeys': 2, 'practical': 2, 'mails': 2, 'claire': 2, 'costing': 2, 'ke': 2, 'program': 2, '09066362231': 2, 'ny': 2, 'lotr': 2, 'modules': 2, 'musthu': 2, 'testing': 2, 'nit': 2, 'format': 2, 'sarcasm': 2, 'forum': 2, 'aunt': 2, 'unfortunately': 2, 'wihtuot': 2, 'exmpel': 2, 'jsut': 2, 'tihs': 2, 'waht': 2, 'yuo': 2, 'splleing': 2, 'evrey': 2, 'wrnog': 2, 'raed': 2, 'sitll': 2, 'mitsake': 2, 'rael': 2, 'gving': 2, 'ayn': 2, 'konw': 2, 'ow': 2, 'finance': 2, 'joining': 2, 'filled': 2, 'jia': 2, 'sux': 2, 'kegger': 2, 'adventure': 2, 'pack': 2, 'wifi': 2, 'rumour': 2, '7250': 2, 'boyfriend': 2, 'driver': 2, 'kicks': 2, 'falling': 2, 'smeone': 2, 'fire': 2, 'propose': 2, 'gods': 2, 'gifted': 2, 'lovingly': 2, 'tomeandsaid': 2, 'itwhichturnedinto': 2, 'dippeditinadew': 2, 'ringtoneking': 2, 'batch': 2, 'flaky': 2, 'sooooo': 2, 'tooo': 2, '09058094599': 2, 'confuses': 2, 'wating': 2, 'sw73ss': 2, 'british': 2, 'hotels': 2, 'adoring': 2, 'ghost': 2, 'dracula': 2, 'addamsfa': 2, 'munsters': 2, 'exorcist': 2, 'twilight': 2, 'cared': 2, 'constant': 2, 'allow': 2, 'hlp': 2, '2rcv': 2, '82242': 2, 'msg150p': 2, '08712317606': 2, 'fly': 2, 'event': 2, 'movietrivia': 2, '08712405022': 2, '80608': 2, 'partnership': 2, 'mostly': 2, 'mornin': 2, 'jas': 2, 'poker': 2, 'messy': 2, 'slip': 2, 'moves': 2, 'traffic': 2, 'nus': 2, 'wkg': 2, 'keeps': 2, 'gotten': 2, 'promises': 2, 'unknown': 2, 'vu': 2, 'bcm1896wc1n3xx': 2, '09094646899': 2, 'pre': 2, '2007': 2, 'indeed': 2, 'rents': 2, '48': 2, 'alaipayuthe': 2, 'maangalyam': 2, 'easter': 2, 'telephone': 2, '08081560665': 2, 'bahamas': 2, '07786200117': 2, 'callfreefone': 2, 'up4': 2, 'calm': 2, 'habit': 2, 'contacts': 2, 'forgets': 2, 'mandan': 2, 'ibh': 2, '07734396839': 2, 'nokia6600': 2, 'invaders': 2, 'orig': 2, 'console': 2, 'recharge': 2, 'transfr': 2, '82050': 2, 'prizes': 2, 'foley': 2, 'fake': 2, 'desparate': 2, '3100': 2, 'combine': 2, 'sian': 2, 'g696ga': 2, 'joanna': 2, 'replacement': 2, 'telly': 2, 'tooth': 2, '12mths': 2, 'mth': 2, 'wipro': 2, 'laundry': 2, 'underwear': 2, 'delete': 2, 'waheed': 2, 'pushes': 2, 'beyond': 2, 'avoiding': 2, '0776xxxxxxx': 2, '326': 2, 'uh': 2, 'heads': 2, 'vday': 2, 'build': 2, 'snowman': 2, 'fights': 2, 'prescription': 2, 'electricity': 2, 'fujitsu': 2, 'scold': 2, 'se': 2, 'prompts': 2, '09066358152': 2, 'disturbing': 2, 'flies': 2, 'woken': 2, 'aka': 2, 'delhi': 2, 'held': 2, 'fringe': 2, 'distract': 2, 'tones2you': 2, '61610': 2, '08712400602450p': 2, 'mel': 2, 'responsibility': 2, '08006344447': 2, 'kid': 2, 'affair': 2, 'parco': 2, 'nb': 2, 'hallaq': 2, 'bck': 2, 'color': 2, 'lyk': 2, 'gender': 2, 'sleepwell': 2, 'mca': 2, 'vomiting': 2, 'rub': 2, 'clever': 2, '113': 2, 'bray': 2, 'wicklow': 2, 'stamped': 2, 'eire': 2, 'ryan': 2, 'idew': 2, 'manage': 2, 'xam': 2, 'diamonds': 2, 'shitload': 2, 'mcat': 2, '27': 2, 'beg': 2, 'sacrifice': 2, 'stayin': 2, 'satisfy': 2, 'cld': 2, 'miles': 2, 'killed': 2, 'smashed': 2, 'ps': 2, 'tok': 2, 'specific': 2, 'figures': 2, 'cousin': 2, 'excuses': 2, 'neck': 2, 'continue': 2, 'holy': 2, 'billion': 2, 'classes': 2, 'turning': 2, 'youre': 2, 'belive': 2, 'slots': 2, 'discussed': 2, 'prem': 2, '2morro': 2, 'spoiled': 2, 'complaint': 2, 'sales': 2, 'lk': 2, 'lov': 2, 'comfort': 2, '300p': 2, '8552': 2, '2end': 2, '01223585334': 2, '2c': 2, 'shagged': 2, '88066': 2, 'bedrm': 2, '700': 2, 'waited': 2, 'huge': 2, 'mids': 2, 'upd8': 2, 'oranges': 2, 'annie': 2, 'messaging': 2, 'retrieve': 2, 'mailbox': 2, '09056242159': 2, '21870000': 2, 'hrishi': 2, 'nothin': 2, 'poem': 2, 'duchess': 2, '008704050406': 2, 'dan': 2, 'aww': 2, 'staring': 2, 'cm': 2, 'unnecessarily': 2, '08701417012150p': 2, 'weigh': 2, 'scoring': 2, 'active': 2, '250k': 2, '88088': 2, 'gamestar': 2, 'expired': 2, 'opinions': 2, 'lv': 2, 'lived': 2, 'propsd': 2, 'happily': 2, '2gthr': 2, 'gv': 2, 'speeding': 2, 'thy': 2, 'lttrs': 2, 'aproach': 2, 'threw': 2, 'evrydy': 2, 'truck': 2, 'dt': 2, 'paragon': 2, 'arent': 2, 'bluff': 2, 'sary': 2, 'piece': 2, 'scotch': 2, 'yarasu': 2, 'shampain': 2, 'brandy': 2, 'vaazhthukkal': 2, 'gin': 2, 'kudi': 2, 'dhina': 2, 'rum': 2, 'wiskey': 2, 'kg': 2, 'dumb': 2, 'dressed': 2, 'kills': 2, 'kay': 2, 'nasty': 2, 'wasted': 2, 'christ': 2, 'tears': 2, 'push': 2, 'answered': 2, 'rgds': 2, '8pm': 2, 'wrote': 2, 'rights': 2, 'jobs': 2, 'lane': 2, 'donno': 2, 'properly': 2, '630': 2, 'lock': 2, 'furniture': 2, 'shoving': 2, 'papers': 2, 'strange': 2, 'acl03530150pm': 2, 'indyarocks': 2, 'resume': 2, 'bids': 2, 'whr': 2, 'yunny': 2, '83383': 2, 'mmmmm': 2, 'relatives': 2, 'benefits': 2, 'environment': 2, 'terrific': 2, 'dr': 2, 'superior': 2, 'vid': 2, 'ruin': 2, 'conform': 2, 'department': 2, 'bc': 2, 'toshiba': 2, 'wrk': 2, 'innocent': 2, 'mental': 2, 'hoped': 2, 'bills': 2, '2marrow': 2, 'treated': 2, 'wks': 2, 'fab': 2, 'tiwary': 2, 'battle': 2, 'bang': 2, 'pap': 2, 'arts': 2, 'pandy': 2, 'edu': 2, 'secretary': 2, 'dollar': 2, 'pull': 2, 'amongst': 2, '69696': 2, 'nalla': 2, 'pouts': 2, 'stomps': 2, 'northampton': 2, 'abj': 2, 'serving': 2, 'smith': 2, 'nagar': 2, 'anna': 2, 'sports': 2, 'evr': 2, 'hugs': 2, 'neither': 2, 'snogs': 2, 'west': 2, 'growing': 2, 'fastest': 2, 'chase': 2, 'steam': 2, 'reg': 2, 'canary': 2, 'sleepy': 2, 'mag': 2, 'diwali': 2, 'tick': 2, 'onion': 2, 'thgt': 2, 'lower': 2, 'exhaust': 2, 'pee': 2, 'contents': 2, 'success': 2, 'division': 2, 'creep': 2, 'lies': 2, 'property': 2, '7876150ppm': 2, '09058099801': 2, 'b4190604': 2, 'bbd': 2, 'pimples': 2, 'yellow': 2, 'frog': 2, '88888': 2, 'doubt': 2, 'japanese': 2, 'proverb': 2, 'freedom': 2, 'seat': 2, 'twenty': 2, 'painting': 2, 'nowadays': 2, 'talks': 2, 'probs': 2, 'swatch': 2, 'ganesh': 2, 'trips': 2, 'helloooo': 2, 'welcomes': 2, '54': 2, '2geva': 2, 'wuld': 2, 'solved': 2, 'ing': 2, 'sake': 2, 'bruce': 2, 'teaching': 2, 'chest': 2, 'covers': 2, 'hang': 2, 'reboot': 2, 'pt2': 2, 'phoned': 2, 'improved': 2, 'hm': 2, 'salon': 2, 'evenings': 2, 'raj': 2, 'payment': 2, 'clearing': 2, 'shore': 2, 'range': 2, 'changes': 2, 'topic': 2, 'admin': 2, 'visionsms': 2, 'andros': 2, 'meets': 2, 'penis': 2, 'foot': 2, 'sigh': 2, 'eveb': 2, 'window': 2, 'removal': 2, '08708034412': 2, 'cancelled': 2, 'neway': 2, 'xxxxx': 2, 'count': 2, 'otside': 2, 'size': 2, '08712101358': 2, 'tight': 2, 'av': 2, 'everyday': 2, 'curious': 2, 'postcard': 2, 'bread': 2, 'mahal': 2, 'luvs': 2, 'ding': 2, 'allowed': 2, 'shared': 2, 'watever': 2, 'necessary': 2, 'messaged': 2, 'deus': 2, 'broad': 2, 'canal': 2, 'spile': 2, 'tap': 2, 'engin': 2, 'edge': 2, 'east': 2, 'howard': 2, 'cooked': 2, 'cheat': 2, 'block': 2, 'ruining': 2, 'easily': 2, 'selfish': 2, 'custom': 2, 'sac': 2, 'jiayin': 2, 'pobox45w2tg150p': 2, 'forgotten': 2, 'reverse': 2, 'cheating': 2, 'mathematics': 2, '2waxsto': 2, 'minimum': 2, 'elaine': 2, 'drunken': 2, 'mess': 2, 'crisis': 2, '____': 2, 'ias': 2, 'mb': 2, '600': 2, 'desires': 2, '1030': 2, 'careers': 2, '447797706009': 2, 'bloomberg': 2, 'priscilla': 2, 'kent': 2, 'vale': 2, '83049': 2, 'unkempt': 2, 'westlife': 2, 'unbreakable': 2, 'untamed': 2, 'wan2': 2, 'prince': 2, 'cdgt': 2, 'granite': 2, 'nasdaq': 2, 'explosive': 2, 'base': 2, 'placement': 2, 'sumthin': 2, 'lion': 2, 'devouring': 2, 'airtel': 2, 'processed': 2, '69669': 2, 'jaya': 2, 'forums': 2, 'incredible': 2, '18p': 2, 'o2fwd': 2, 'ship': 2, 'maturity': 2, 'kavalan': 2, 'causing': 2, 'blank': 2, 'tonights': 2, 'xin': 2, 'lib': 2, 'difference': 2, 'despite': 2, 'swoop': 2, 'langport': 2, 'mistakes': 2, 'lou': 2, 'pool': 2, '09065989182': 2, 'x49': 2, 'ibn': 2, 'verified': 2, 'confirmd': 2, 'terrorist': 2, 'cnn': 2, 'disconnect': 2, 'hppnss': 2, 'goodfriend': 2, 'sorrow': 2, 'stayed': 2, 'stone': 2, 'help08718728876': 2, 'increments': 2, 'mila': 2, '69866': 2, '30pp': 2, 'blonde': 2, '5free': 2, 'mtalk': 2, 'age23': 2, 'atlast': 2, 'desert': 2, 'funk': 2, 'tones2u': 2, 'funeral': 2, 'vivek': 2, 'tnc': 2, 'brah': 2, 'passwords': 2, 'sensitive': 2, 'protect': 2, 'sib': 2, 'ipad': 2, 'bird': 2, 'cheese': 2, 'widelive': 2, 'index': 2, 'tms': 2, 'wml': 2, 'hsbc': 2, 'asp': 2, '09061702893': 2, 'eek': 2, 'melt': 2, '09061743386': 2, '674': 2, 'eta': 2, '0871750': 2, '77': 2, 'landlines': 2, 'housewives': 2, 'dial': 2, '09066364311': 2, 'literally': 2, 'kothi': 2, 'sem': 2, 'student': 2, 'actual': 2, 'dealing': 2, 'reasonable': 2, 'kappa': 2, 'piss': 2, 'receipts': 2, 'guessing': 2, 'royal': 2, 'sticky': 2, 'indicate': 2, 'repeat': 2, 'calculation': 2, 'clothes': 2, 'lush': 2, '2find': 2, 'courage': 2, 'defeat': 2, 'greatest': 2, 'bear': 2, 'fucked': 2, 'beauty': 2, 'natalja': 2, '440': 2, 'nat27081980': 2, 'moving': 2, 'jogging': 2, 'shelf': 2, 'captain': 2, 'mokka': 2, 'polyh': 2, '09061744553': 2, 'bone': 2, 'steve': 2, 'epsilon': 2, 'mesages': 2, 'lst': 2, 'massive': 2, 'absolutly': 2, 'forms': 2, '373': 2, 'w1j': 2, '6hl': 2, 'polo': 2, 'coast': 2, 'suppose': 2, '02073162414': 2, 'secs': 2, 'explicit': 2, 'clearly': 2, 'gain': 2, 'realise': 2, 'mnths': 2, 'subscribe6gbp': 2, '86888': 2, '3hrs': 2, 'txtstop': 2, 'managed': 2, 'capital': 2, 'acted': 2, '09066380611': 2, 'loyal': 2, 'customers': 2, 'print': 2, 'dokey': 2, 'error': 2, 'sleepin': 2, 'minor': 2, 'woulda': 2, 'santa': 2, 'miserable': 2, 'shoppin': 2, '08718726270': 2, 'celebration': 2, 'warner': 2, 'select': 2, 'sarasota': 2, '13': 2, 'cherish': 2, 'slp': 2, 'muah': 2, '4eva': 2, 'garden': 2, 'notxt': 2, 'seeds': 2, 'bulbs': 2, 'scotsman': 2, 'go2': 2, 'replace': 2, 'reduce': 2, 'limiting': 2, 'gastroenteritis': 2, 'illness': 2, '09061213237': 2, 'm227xy': 2, '177': 2, 'respectful': 2, 'pride': 2, 'bottle': 2, 'amused': 2, 'mega': 2, 'island': 2, 'amore': 1, 'jurong': 1, 'chgs': 1, 'patent': 1, 'aids': 1, 'cried': 1, 'breather': 1, 'granted': 1, 'fulfil': 1, 'xxxmobilemovieclub': 1, 'qjkgighjjgcbl': 1, 'gota': 1, 'poboxox36504w45wq': 1, 'macedonia': 1, 'u1': 1, 'ffffffffff': 1, 'forced': 1, 'packing': 1, 'ahhh': 1, 'vaguely': 1, 'actin': 1, 'badly': 1, 'apologetic': 1, 'fallen': 1, 'spoilt': 1, 'housework': 1, 'cuppa': 1, 'fainting': 1, 'timings': 1, 'watts': 1, 'steed': 1, 'arabian': 1, 'rodger': 1, '07732584351': 1, 'endowed': 1, 'hep': 1, 'immunisation': 1, 'sucker': 1, 'suckers': 1, 'stubborn': 1, 'thinked': 1, 'smarter': 1, 'crashing': 1, 'accomodations': 1, 'offered': 1, 'embarassing': 1, 'cave': 1, 'wings': 1, 'incorrect': 1, 'jersey': 1, 'devils': 1, 'sptv': 1, 'sherawat': 1, 'mallika': 1, 'gauti': 1, 'sehwag': 1, 'seekers': 1, 'barbie': 1, 'ken': 1, 'performed': 1, 'peoples': 1, 'operate': 1, 'multis': 1, 'factory': 1, 'casualty': 1, 'stuff42moro': 1, 'includes': 1, 'hairdressers': 1, 'beforehand': 1, 'ams': 1, '4the': 1, 'signin': 1, 'memorable': 1, 'server': 1, 'minecraft': 1, 'ip': 1, 'grumpy': 1, 'lying': 1, 'plural': 1, 'formal': 1, 'openin': 1, '0871277810910p': 1, 'ratio': 1, '09064019788': 1, 'box42wr29c': 1, 'malarky': 1, 'apples': 1, 'pairs': 1, '4041': 1, '7548': 1, 'sao': 1, 'predict': 1, 'involve': 1, 'imposed': 1, 'lucyxx': 1, 'tmorrow': 1, 'accomodate': 1, 'gravel': 1, 'hotmail': 1, 'svc': 1, '69988': 1, 'nver': 1, 'ummma': 1, 'sindu': 1, 'nevering': 1, 'typical': 1, 'chores': 1, 'mist': 1, 'dirt': 1, 'exist': 1, 'hail': 1, 'aaooooright': 1, '07046744435': 1, 'annoncement': 1, 'envy': 1, 'excited': 1, '32': 1, 'bootydelious': 1, 'bangb': 1, 'bangbabes': 1, 'cultures': 1, 's89': 1, '09061701939': 1, 'missunderstding': 1, 'bridge': 1, 'lager': 1, 'axis': 1, 'surname': 1, 'clue': 1, 'begins': 1, 'hopes': 1, 'lifted': 1, 'approaches': 1, 'finding': 1, 'handsome': 1, 'areyouunique': 1, '30th': 1, 'league': 1, 'stool': 1, 'ors': 1, '1pm': 1, 'babyjontet': 1, 'enc': 1, 'ga': 1, 'alter': 1, 'dogg': 1, 'dats': 1, 'refund': 1, 'prediction': 1, 'os': 1, 'ubandu': 1, 'disk': 1, 'scenery': 1, 'aries': 1, 'flyng': 1, 'horo': 1, 'mudyadhu': 1, 'elama': 1, 'conducts': 1, 'strict': 1, 'gandhipuram': 1, 'rubber': 1, 'thirtyeight': 1, 'hearing': 1, 'pleassssssseeeeee': 1, 'sportsx': 1, 'watches': 1, 'baig': 1, '3days': 1, 'usps': 1, 'nipost': 1, 'bribe': 1, 'ups': 1, 'luton': 1, '0125698789': 1, '69698': 1, 'sometme': 1, 'club4': 1, 'box1146': 1, 'club4mobiles': 1, '87070': 1, 'evo': 1, 'narcotics': 1, 'objection': 1, 'theater': 1, 'mack': 1, 'rob': 1, 'celebrations': 1, 'gdeve': 1, 'ahold': 1, 'cruisin': 1, 'raksha': 1, 'varunnathu': 1, 'edukkukayee': 1, 'ollu': 1, 'resend': 1, '28thfeb': 1, 'gurl': 1, 'appropriate': 1, 'diesel': 1, 'fridge': 1, 'womdarfull': 1, 'rodds1': 1, 'icmb3cktz8r7': 1, 'aberdeen': 1, 'img': 1, 'kingdom': 1, 'united': 1, 'blind': 1, 'remb': 1, 'jos': 1, 'bookshelf': 1, '85222': 1, 'gbp1': 1, '84': 1, 'winnersclub': 1, 'mylife': 1, 'l8': 1, 'gon': 1, 'guild': 1, 'evaporated': 1, 'stealing': 1, 'employer': 1, 'daaaaa': 1, 'dined': 1, 'wined': 1, 'hiding': 1, 'huiming': 1, 'prestige': 1, 'xxuk': 1, 'sextextuk': 1, 'shag': 1, '69876': 1, 'jeremiah': 1, 'iphone': 1, 'apeshit': 1, 'safely': 1, 'onam': 1, 'sirji': 1, 'tata': 1, 'aig': 1, '08708800282': 1, 'unemployed': 1, 'andrews': 1, 'db': 1, 'dawns': 1, 'refreshed': 1, 'f4q': 1, 'regalportfolio': 1, 'rp176781': 1, '08717205546': 1, 'uniform': 1, 'spoil': 1, '09057039994': 1, 't91': 1, 'lindsay': 1, 'bars': 1, 'heron': 1, 'payasam': 1, 'rinu': 1, 'prabu': 1, 'verifying': 1, 'becaus': 1, 'taught': 1, 'followin': 1, 'repairs': 1, 'wallet': 1, '945': 1, 'owl': 1, 'kickboxing': 1, 'lap': 1, 'performance': 1, 'calculated': 1, 'visitor': 1, 'wahleykkum': 1, 'administrator': 1, '2814032': 1, '150pw': 1, '3x': 1, 'stoners': 1, 'disastrous': 1, 'busetop': 1, 'iron': 1, 'blah': 1, 'okies': 1, 'wendy': 1, '09064012103': 1, 'pobox12n146tf150p': 1, '09111032124': 1, '09058094455': 1, 'attractive': 1, 'rowdy': 1, 'sentiment': 1, 'attitude': 1, 'urination': 1, 'hillsborough': 1, 'shoul': 1, 'hasnt': 1, 'monkeespeople': 1, 'werethe': 1, 'jobyet': 1, 'howu': 1, 'monkeyaround': 1, 'howdy': 1, 'foundurself': 1, 'sausage': 1, 'exercise': 1, 'blimey': 1, 'concentration': 1, 'hanks': 1, 'lotsly': 1, 'detail': 1, 'optimistic': 1, 'practicum': 1, 'consistently': 1, 'links': 1, 'ears': 1, 'wavering': 1, 'heal': 1, '9153': 1, 'upgrdcentre': 1, 'oral': 1, 'slippery': 1, 'bike': 1, 'okmail': 1, 'differ': 1, 'enters': 1, '69888nyt': 1, 'machi': 1, 'mcr': 1, 'falconerf': 1, 'thuglyfe': 1, 'jaykwon': 1, 'glory': 1, 'ralphs': 1, 'faded': 1, 'reunion': 1, 'accenture': 1, 'jackson': 1, 'reache': 1, 'nuerologist': 1, 'lolnice': 1, 'westshore': 1, 'significance': 1, 'ammo': 1, 'ak': 1, 'toxic': 1, 'poly3': 1, 'jamz': 1, 'boltblue': 1, 'topped': 1, 'tgxxrz': 1, 'bubbletext': 1, 'problematic': 1, 'abnormally': 1, 'adults': 1, 'unconscious': 1, '9755': 1, 'teletext': 1, 'recieve': 1, 'faggot': 1, '07815296484': 1, '41782': 1, 'bani': 1, 'leads': 1, 'buttons': 1, 'max6': 1, 'csc': 1, 'applausestore': 1, 'monthlysubscription': 1, 'famous': 1, 'temper': 1, 'unconditionally': 1, 'bash': 1, 'oclock': 1, 'cooped': 1, 'weddin': 1, 'invitation': 1, 'alibi': 1, 'paces': 1, 'surrounded': 1, 'cuck': 1, 'sink': 1, 'cage': 1, 'deficient': 1, 'acknowledgement': 1, 'tactless': 1, 'astoundingly': 1, 'oath': 1, 'magic': 1, 'pan': 1, 'silly': 1, 'mutations': 1, 'uv': 1, 'causes': 1, 'sunscreen': 1, 'thesedays': 1, 'sugardad': 1, 'bao': 1, 'brownie': 1, 'ninish': 1, 'freek': 1, 'icky': 1, 'ridden': 1, 'missy': 1, 'goggles': 1, 'arguing': 1, '09050005321': 1, 'unfortuntly': 1, 'arngd': 1, 'frnt': 1, 'walkin': 1, 'sayin': 1, 'bites': 1, 'textand': 1, '08002988890': 1, 'tendencies': 1, 'jjc': 1, 'gotany': 1, 'meive': 1, 'yi': 1, 'srsly': 1, '07753741225': 1, '08715203677': 1, '42478': 1, 'prix': 1, 'nitz': 1, 'stands': 1, 'occur': 1, 'rajnikant': 1, 'blastin': 1, 'ocean': 1, 'speciale': 1, 'roses': 1, '07880867867': 1, 'zouk': 1, 'clubsaisai': 1, '07946746291': 1, 'xclusive': 1, 'banter': 1, 'bridgwater': 1, 'dependents': 1, 'cer': 1, 'thanx4': 1, 'beauties': 1, 'hundreds': 1, 'aunties': 1, 'handsomes': 1, 'friendships': 1, 'dismay': 1, 'concerned': 1, 'tootsie': 1, 'seventeen': 1, 'ml': 1, 'fetching': 1, 'restock': 1, 'brighten': 1, 'braved': 1, 'allo': 1, 'triumphed': 1, 'uncomfortable': 1, '08715203694': 1, 'rough': 1, 'sonetimes': 1, 'wesleys': 1, 'cloud': 1, 'wikipedia': 1, '08718711108': 1, '89034': 1, '88800': 1, 'repent': 1, 'sutra': 1, 'kama': 1, 'positions': 1, 'nange': 1, 'bakra': 1, 'kalstiya': 1, 'lakhs': 1, 'sun0819': 1, '08452810071': 1, 'ditto': 1, 'wetherspoons': 1, 'piggy': 1, 'freaky': 1, 'scrappy': 1, 'sdryb8i': 1, '1da': 1, 'lapdancer': 1, '150ppmsg': 1, 'sue': 1, 'g2': 1, 'tomorw': 1, 'imprtant': 1, 'crying': 1, 'bfore': 1, 'tmorow': 1, 'cherthala': 1, 'engaged': 1, '08712404000': 1, '448712404000': 1, '1680': 1, '1405': 1, '1843': 1, 'entrepreneurs': 1, 'corporation': 1, 'fluids': 1, 'dehydration': 1, 'prevent': 1, 'trek': 1, 'harri': 1, 'deck': 1, 'cnupdates': 1, 'gage': 1, 'alerts': 1, 'newsletter': 1, 'shitstorm': 1, 'attributed': 1, '08714712388': 1, '449071512431': 1, 'specs': 1, 'sth': 1, 'px3748': 1, '08714712394': 1, 'mindset': 1, 'macha': 1, 'wondar': 1, 'flim': 1, 'jelly': 1, 'scrumptious': 1, 'dao': 1, 'half8th': 1, 'visiting': 1, 'jide': 1, 'alertfrom': 1, 'drvgsto': 1, 'stewartsize': 1, 'jeri': 1, 'prescripiton': 1, '2kbsubject': 1, 'steak': 1, 'neglect': 1, 'prayers': 1, 'wahay': 1, 'hadn': 1, 'clocks': 1, 'realised': 1, 'gaze': 1, '82324': 1, 'tattoos': 1, 'caveboy': 1, 'vibrate': 1, '79': 1, '08704439680ts': 1, 'hungover': 1, 'grandmas': 1, 'closingdate04': 1, 'm39m51': 1, 'claimcode': 1, 'mobypobox734ls27yf': 1, '09066368327': 1, '50pmmorefrommobile2bremoved': 1, 'unclaimed': 1, 'gua': 1, 'faber': 1, 'dramatic': 1, 'hunting': 1, 'drunkard': 1, 'weaseling': 1, 'idc': 1, 'trash': 1, 'punish': 1, 'beerage': 1, 'randomlly': 1, 'fixes': 1, 'spelling': 1, '100p': 1, '087018728737': 1, 'tune': 1, 'toppoly': 1, 'fondly': 1, 'dogbreath': 1, 'sounding': 1, 'weighed': 1, 'woohoo': 1, 'uncountable': 1, '14thmarch': 1, 'availa': 1, '9996': 1, 'canlove': 1, 'whereare': 1, 'thekingshead': 1, 'friendsare': 1, 'rg21': 1, '4jx': 1, 'dled': 1, 'smokin': 1, 'boooo': 1, 'costumes': 1, 'yowifes': 1, 'notifications': 1, 'outbid': 1, 'plyr': 1, 'simonwatson5120': 1, 'shinco': 1, 'smsrewards': 1, 'youi': 1, 'yourjob': 1, 'soonlots': 1, 'llspeak': 1, 'starshine': 1, 'sips': 1, 'yourinclusive': 1, 'smsservices': 1, 'bits': 1, 'turned': 1, 'burial': 1, 'rvx': 1, 'rv': 1, 'comprehensive': 1, 'prashanthettan': 1, 'doug': 1, 'realizes': 1, 'guitar': 1, 'samantha': 1, 'impress': 1, 'trauma': 1, 'swear': 1, 'inner': 1, 'tigress': 1, 'overdose': 1, 'urfeeling': 1, 'bettersn': 1, 'probthat': 1, '83110': 1, 'ana': 1, 'rto': 1, 'sathy': 1, 'spoons': 1, 'corvettes': 1, '50pm': 1, '09061104283': 1, 'bunkers': 1, '07808': 1, 'xxxxxx': 1, '08719899217': 1, 'posh': 1, 'dob': 1, 'chaps': 1, 'prods': 1, 'trial': 1, 'champneys': 1, '0721072': 1, 'hole': 1, 'philosophical': 1, 'shakespeare': 1, 'atleast': 1, 'mymoby': 1, 'doit': 1, 'curfew': 1, 'getsleep': 1, 'gibe': 1, 'studdying': 1, 'woul': 1, 'massages': 1, 'yoyyooo': 1, 'permissions': 1, 'hussey': 1, 'mike': 1, 'faglord': 1, 'ctter': 1, 'cttargg': 1, 'ie': 1, 'cttergg': 1, 'ctagg': 1, 'cutter': 1, 'ctargg': 1, 'nutter': 1, 'thus': 1, 'grateful': 1, 'happier': 1, 'experiment': 1, 'agents': 1, 'invoices': 1, 'smell': 1, 'tobacco': 1, 'assumed': 1, 'racal': 1, 'dizzee': 1, 'bookmark': 1, 'stereophonics': 1, 'marley': 1, 'strokes': 1, 'libertines': 1, 'lastest': 1, 'nookii': 1, 'grinule': 1, 'oreos': 1, 'fudge': 1, 'zaher': 1, 'dieting': 1, 'nauseous': 1, 'hollalater': 1, 'avalarr': 1, 'rounds': 1, 'blogging': 1, 'magicalsongs': 1, 'blogspot': 1, 'slices': 1, 'kvb': 1, 'w1t1jy': 1, 'box403': 1, 'ppt150x3': 1, '1million': 1, 'alternative': 1, 'owo': 1, 'fro': 1, 'ore': 1, 'samus': 1, 'shoulders': 1, '09063440451': 1, 'ppm150': 1, 'matthew': 1, 'box334': 1, 'vomitin': 1, '528': 1, '1yf': 1, 'hp20': 1, '09061749602': 1, 'writhing': 1, 'bleh': 1, 'stuffed': 1, 'pockets': 1, 'paypal': 1, 'voila': 1, 'theyre': 1, 'folks': 1, 'sorta': 1, 'blown': 1, 'secondary': 1, 'applying': 1, 'ogunrinde': 1, 'sophas': 1, 'lodging': 1, 'chk': 1, 'dict': 1, 'shb': 1, 'stories': 1, 'retired': 1, 'natwest': 1, 'chad': 1, 'gymnastics': 1, 'christians': 1, 'token': 1, 'liking': 1, 'aptitude': 1, 'horse': 1, 'wrongly': 1, 'boggy': 1, 'biatch': 1, 'weakness': 1, 'hesitate': 1, 'notebook': 1, 'carpark': 1, 'eightish': 1, '5wkg': 1, 'cres': 1, 'ere': 1, 'ubi': 1, '67441233': 1, 'irene': 1, '61': 1, 'bus8': 1, '382': 1, '6ph': 1, '66': 1, '7am': 1, '5ish': 1, 'relaxing': 1, 'stripes': 1, 'skirt': 1, 'escalator': 1, 'beth': 1, 'charlie': 1, 'syllabus': 1, 'panasonic': 1, 'bluetoothhdset': 1, 'doubletxt': 1, 'doublemins': 1, '30pm': 1, 'kolathupalayam': 1, 'unjalur': 1, 'poyyarikatur': 1, 'erode': 1, 'apt': 1, 'hero': 1, 'meat': 1, 'supreme': 1, 'cudnt': 1, 'ctla': 1, 'ishtamayoo': 1, 'bakrid': 1, 'ente': 1, 'images': 1, 'fond': 1, 'finds': 1, 'cougar': 1, 'coaxing': 1, 'souveniers': 1, 'glorious': 1, '09065394514': 1, 'scratches': 1, 'nanny': 1, 'hardest': 1, 'shitin': 1, 'lekdog': 1, 'defo': 1, 'millions': 1, 'blankets': 1, 'atten': 1, '09058097218': 1, 'analysis': 1, 'data': 1, 'belligerent': 1, 'rudi': 1, 'les': 1, 'snoring': 1, 'ink': 1, '515': 1, 'throwing': 1, 'finalise': 1, 'flirtparty': 1, 'replys150': 1, 'dentist': 1, 'shes': 1, 'oyea': 1, 'nurses': 1, 'lul': 1, 'obese': 1, 'ami': 1, 'parchi': 1, 'kicchu': 1, 'korte': 1, 'tul': 1, 'korche': 1, 'iccha': 1, 'kaaj': 1, 'copies': 1, 'sculpture': 1, 'surya': 1, 'pokkiri': 1, 'sorrows': 1, 'praises': 1, 'sambar': 1, 'makiing': 1, 'attraction': 1, 'proove': 1, 'ndship': 1, '4few': 1, 'conected': 1, 'needle': 1, 'spatula': 1, 'outrageous': 1, 'complexities': 1, 'freely': 1, 'taxes': 1, 'ryder': 1, 'presleys': 1, 'elvis': 1, 'postal': 1, 'strips': 1, 'gifts': 1, 'cliff': 1, 'wrking': 1, 'sittin': 1, 'drops': 1, 'hen': 1, 'smoked': 1, 'teju': 1, 'hourish': 1, 'amla': 1, 'convenience': 1, 'evaluation': 1, '09050000301': 1, '449050000301': 1, '80155': 1, 'chat80155': 1, 'rcd': 1, 'speedchat': 1, 'swap': 1, 'chatter': 1, 'cheyyamo': 1, '80160': 1, 'txt43': 1, 'brothers': 1, 'throws': 1, 'hmv1': 1, 'errors': 1, 'piah': 1, 'tau': 1, '1stchoice': 1, '08707808226': 1, 'shade': 1, 'copied': 1, 'notified': 1, 'marketing': 1, '08450542832': 1, '84122': 1, 'theirs': 1, 'sexual': 1, 'virgins': 1, '69911': 1, '4fil': 1, 'kaitlyn': 1, 'sitter': 1, 'peeps': 1, 'danger': 1, 'comment': 1, 'veggie': 1, 'neighbors': 1, 'computerless': 1, 'balloon': 1, 'melody': 1, 'macs': 1, 'hme': 1, 'velachery': 1, 'flippin': 1, 'breaking': 1, 'cstore': 1, 'hangin': 1, 'lodge': 1, 'worrying': 1, 'quizzes': 1, '087016248': 1, '08719181503': 1, 'thin': 1, 'arguments': 1, 'fed': 1, 'himso': 1, 'semi': 1, 'exp': 1, '30apr': 1, 'maaaan': 1, 'guessin': 1, 'wuldnt': 1, 'personally': 1, 'ilol': 1, 'lunchtime': 1, 'organise': 1, '5years': 1, 'passable': 1, 'phd': 1, 'prakesh': 1, 'products': 1, 'betta': 1, 'aging': 1, 'global': 1, '08700435505150p': 1, 'accommodation': 1, 'phb1': 1, 'submitting': 1, 'snatch': 1, 'dancce': 1, 'basq': 1, 'pthis': 1, 'senrd': 1, 'ihave': 1, '0quit': 1, 'edrunk': 1, 'xxxxxxx': 1, 'iff': 1, 'ros': 1, 'drivby': 1, 'drum': 1, 'dnot': 1, '2nhite': 1, 'westonzoyland': 1, 'relieved': 1, 'greatness': 1, 'goin2bed': 1, 'only1more': 1, 'mc': 1, 'ifink': 1, 'everythin': 1, 'ava': 1, 'every1': 1, 'melnite': 1, 'oli': 1, 'goodtime': 1, 'l8rs': 1, '08712402779': 1, 'shun': 1, 'exhibition': 1, 'glass': 1, 'bian': 1, 'nino': 1, 'himself': 1, 'el': 1, 'downstem': 1, '08718730555': 1, 'wahala': 1, 'insects': 1, 'listening2the': 1, 'evil': 1, 'plumbing': 1, 'leafcutter': 1, 'inperialmusic': 1, 'molested': 1, 'acid': 1, 'remixed': 1, 'didntgive': 1, 'jenxxx': 1, 'thepub': 1, 'bellearlier': 1, 'bedbut': 1, 'uwana': 1, '09096102316': 1, 'cheery': 1, 'weirdo': 1, 'profiles': 1, 'stalk': 1, 'pax': 1, 'deposit': 1, '95': 1, 'jap': 1, 'disappeared': 1, 'certificate': 1, 'publish': 1, 'wheellock': 1, 'destination': 1, 'fifty': 1, 'happenin': 1, 'settling': 1, 'ipads': 1, 'worthless': 1, 'novelty': 1, 'cocksuckers': 1, 'janx': 1, 'dads': 1, 'developer': 1, 'designation': 1, 'musicnews': 1, 'videosounds': 1, '09701213186': 1, 'videosound': 1, 'spirit': 1, 'shattered': 1, 'girlie': 1, 'darker': 1, 'styling': 1, 'listn': 1, 'gray': 1, 'watevr': 1, 'paragraphs': 1, 'minus': 1, 'coveragd': 1, 'vasai': 1, 'retard': 1, 'bathroom': 1, 'sang': 1, 'uptown': 1, '80': 1, 'icic': 1, 'syria': 1, 'gauge': 1, 'completing': 1, 'ax': 1, 'unfolds': 1, 'emergency': 1, 'surgical': 1, 'korean': 1, 'fredericksburg': 1, 'pases': 1, 'que': 1, 'buen': 1, 'tiempo': 1, 'compass': 1, 'way2sms': 1, 'gnun': 1, 'youuuuu': 1, 'misss': 1, 'baaaaabe': 1, 'convince': 1, 'witot': 1, 'buyer': 1, 'undrstndng': 1, 'suffer': 1, 'becz': 1, 'avoids': 1, 'steamboat': 1, 'forgive': 1, 'tp': 1, '6ish': 1, 'bbq': 1, 'panicks': 1, 'everyso': 1, 'types': 1, 'nick': 1, 'auntie': 1, 'huai': 1, 'path': 1, 'paths': 1, 'appear': 1, 'thirunelvali': 1, 'reserve': 1, 'tackle': 1, 'tonght': 1, 'ironing': 1, 'pile': 1, 'chinky': 1, 'ploughing': 1, 'wi': 1, 'nz': 1, 'aust': 1, 'recharged': 1, 'papa': 1, 'detailed': 1, 'losers': 1, 'beta': 1, 'noncomittal': 1, 'snickering': 1, 'chords': 1, 'win150ppmx3age16': 1, 'boyf': 1, 'interviw': 1, 'entire': 1, 'determine': 1, 'spreadsheet': 1, 'trebles': 1, 'dartboard': 1, 'doubles': 1, 'coat': 1, 'recognises': 1, 'wisheds': 1, 'duo': 1, 'intrepid': 1, 'breeze': 1, 'fresh': 1, 'twittering': 1, 'chinchillas': 1, 'ducking': 1, 'function': 1, 'headstart': 1, 'rummer': 1, 'flying': 1, 'charts': 1, 'bbc': 1, 'optin': 1, 'thanks2': 1, 'rajini': 1, 'summers': 1, 'matched': 1, 'help08714742804': 1, 'spys': 1, '09099725823': 1, 'offering': 1, 'meow': 1, 'edhae': 1, 'bilo': 1, 'innu': 1, 'astne': 1, 'vargu': 1, 'lyfu': 1, 'halla': 1, 'yalru': 1, 'lyf': 1, 'mundhe': 1, 'ali': 1, 'ovr': 1, 'prone': 1, 'usa': 1, 'msgrcvd18': 1, '07801543489': 1, 'latests': 1, 'llc': 1, 'permission': 1, '09099726395': 1, 'meetins': 1, 'cumin': 1, 'lucy': 1, 'tablet': 1, 'dose': 1, 'incomm': 1, 'maps': 1, 'concentrating': 1, 'tiring': 1, 'browsin': 1, 'compulsory': 1, 'investigate': 1, 'vitamin': 1, 'crucial': 1, '2channel': 1, 'psychic': 1, 'jsco': 1, 'skills': 1, 'leadership': 1, 'host': 1, 'systems': 1, 'linux': 1, 'based': 1, 'idps': 1, 'converter': 1, 'sayy': 1, 'leanne': 1, 'disc': 1, 'glasgow': 1, 'champ': 1, 'lovin': 1, 'browse': 1, 'artists': 1, 'install': 1, 'speling': 1, 'corect': 1, '4719': 1, '523': 1, 'employee': 1, 'cts': 1, 'nike': 1, 'sooo': 1, 'shouting': 1, 'dang': 1, 'earliest': 1, 'nordstrom': 1, 'conference': 1, 'degree': 1, 'bleak': 1, 'shant': 1, 'nearer': 1, 'raiden': 1, 'totes': 1, 'cardin': 1, 'pierre': 1, 'establish': 1, 'rhythm': 1, 'truro': 1, 'ext': 1, 'cloth': 1, 'sunroof': 1, 'blanked': 1, 'image': 1, 'kalainar': 1, 'thenampet': 1, 'freaked': 1, 'reacting': 1, 'nosy': 1, 'imposter': 1, 'destiny': 1, 'satanic': 1, 'pudunga': 1, 'chef': 1, 'exterminator': 1, 'pest': 1, 'aaniye': 1, 'sympathetic': 1, 'venaam': 1, 'psychologist': 1, 'athletic': 1, 'determined': 1, 'companion': 1, 'stylist': 1, 'healer': 1, 'courageous': 1, 'organizer': 1, 'dependable': 1, 'listener': 1, 'psychiatrist': 1, 'chez': 1, 'jules': 1, 'nig': 1, 'hhahhaahahah': 1, 'leonardo': 1, 'dime': 1, 'strain': 1, '2years': 1, 'withdraw': 1, 'anyhow': 1, 'millers': 1, 'rawring': 1, 'xoxo': 1, 'spark': 1, 'flame': 1, 'crushes': 1, 'somewhr': 1, 'honeymoon': 1, 'outfit': 1, '08719899230': 1, 'cheque': 1, 'olympics': 1, 'leo': 1, 'haul': 1, 'want2come': 1, 'wizzle': 1, 'wildlife': 1, 'that2worzels': 1, 'cya': 1, 'shanghai': 1, '645': 1, 'redeemable': 1, 'rt': 1, '08701237397': 1, 'pro': 1, 'thnx': 1, 'anjie': 1, 'sef': 1, 'fring': 1, 'nte': 1, '526': 1, 'bx': 1, '02072069400': 1, 'talents': 1, 'animal': 1, 'warming': 1, 'shiny': 1, 'fooled': 1, 'french': 1, 'responsible': 1, 'companies': 1, 'guarantee': 1, 'suppliers': 1, '0a': 1, 'lnly': 1, 'keen': 1, 'dammit': 1, 'wright': 1, 'wrecked': 1, 'somewhat': 1, 'laden': 1, 'goodevening': 1, 'spontaneously': 1, 'rgent': 1, 'busty': 1, 'daytime': 1, '09099726429': 1, 'janinexx': 1, 'spageddies': 1, 'fourth': 1, 'dimension': 1, 'phasing': 1, 'compromised': 1, 'meaningful': 1, '09050001295': 1, 'a21': 1, '391784': 1, 'mobsi': 1, 'dub': 1, 'je': 1, 'toughest': 1, 'unspoken': 1, 'squatting': 1, 'digits': 1, '0089': 1, '09063442151': 1, 'sonathaya': 1, 'soladha': 1, 'raping': 1, 'dudes': 1, 'weightloss': 1, 'embarrassed': 1, 'mushy': 1, 'stash': 1, 'priya': 1, 'kilos': 1, 'accidant': 1, 'tookplace': 1, 'ghodbandar': 1, 'slovely': 1, 'sc': 1, 'wad': 1, 'specialise': 1, 'desparately': 1, 'mi': 1, 'stereo': 1, 'classmates': 1, 'fires': 1, 'vipclub4u': 1, 'trackmarque': 1, 'missionary': 1, 'entertaining': 1, 'hugh': 1, 'stick': 1, 'laurie': 1, 'praps': 1, 'jon': 1, 'dinero': 1, 'spain': 1, '000pes': 1, 'complaining': 1, 'mandy': 1, '09041940223': 1, 'transferred': 1, 'fm': 1, 'hotmix': 1, 'sullivan': 1, 'finn': 1, 'thew': 1, 'theacusations': 1, 'iwana': 1, 'wotu': 1, 'haventcn': 1, 'downon': 1, 'itxt': 1, 'nething': 1, 'dine': 1, '09111030116': 1, 'conacted': 1, 'pobox12n146tf15': 1, 'inspection': 1, 'nursery': 1, 'becomes': 1, 'panren': 1, 'paru': 1, 'trainners': 1, 'carryin': 1, 'bac': 1, 'chuckin': 1, 'dhanush': 1, 'needing': 1, 'habba': 1, 'dileep': 1, 'venugopal': 1, 'muchand': 1, 'mentioned': 1, 'remembrs': 1, 'everytime': 1, 'edition': 1, 'algorithms': 1, 'textbook': 1, '3230': 1, 'cro1327': 1, '09064018838': 1, 'iwas': 1, 'urmom': 1, 'careabout': 1, 'itried2tell': 1, 'marine': 1, 'intend': 1, 'learned': 1, 'traveling': 1, 'honest': 1, 'afghanistan': 1, 'stable': 1, 'iraq': 1, '50award': 1, '1225': 1, 'pai': 1, 'seh': 1, 'parts': 1, 'walsall': 1, 'terry': 1, 'tue': 1, 'ccna': 1, 'shrek': 1, 'fellow': 1, 'dying': 1, 'lifting': 1, 'teresa': 1, 'aid': 1, 'ld': 1, 'bam': 1, 'dec': 1, 'usmle': 1, 'squishy': 1, 'mwahs': 1, 'hottest': 1, 'prominent': 1, 'cheek': 1, 'september': 1, 'bcm': 1, 'neo69': 1, 'backdoor': 1, '8027': 1, 'hack': 1, 'subscribe': 1, 'fraction': 1, 'dps': 1, '09050280520': 1, 'comingdown': 1, 'murali': 1, 'sts': 1, 'kissing': 1, 'elliot': 1, 'mia': 1, 'engalnd': 1, 'd3wv': 1, '100txt': 1, '2price': 1, 'matric': 1, '650': 1, '850': 1, '08718726970': 1, 'payments': 1, 'fedex': 1, 'reception': 1, 'consensus': 1, 'entertain': 1, 'pillows': 1, 'strewn': 1, 'bras': 1, 'tag': 1, 'exposes': 1, 'weaknesses': 1, 'knee': 1, 'pulls': 1, 'wicked': 1, 'supports': 1, 'srt': 1, 'ps3': 1, 'jontin': 1, 'banned': 1, 'biro': 1, '09058094594': 1, 'unconsciously': 1, 'unhappy': 1, 'shell': 1, 'jog': 1, '09061743811': 1, 'lark': 1, 'sic': 1, '0870753331018': 1, '7mp': 1, '09090900040': 1, 'extreme': 1, 'wild': 1, 'fones': 1, 'stop2stop': 1, 'lim': 1, 'parachute': 1, 'placed': 1, 'lambda': 1, 'snowball': 1, 'angels': 1, 'ello': 1, 'duffer': 1, 'ofice': 1, 'pharmacy': 1, 'grr': 1, 'cnl': 1, '08715500022': 1, 'rpl': 1, 'nor': 1, 'fffff': 1, 'lifebook': 1, 'zhong': 1, 'qing': 1, 'act': 1, 'hypertension': 1, 'annoyin': 1, '08702490080': 1, 'vpod': 1, 'nigro': 1, 'scratching': 1, 'anyplaces': 1, 'priority': 1, 'ecstasy': 1, 'minded': 1, '09090204448': 1, 'ls278bb': 1, 'minapn': 1, 'hittng': 1, 'reflex': 1, 'egbon': 1, 'adewale': 1, 'mary': 1, 'deduct': 1, 'wrks': 1, 'asshole': 1, 'monkey': 1, 'grab': 1, 'sliding': 1, '09065394973': 1, 'payback': 1, 'tescos': 1, 'bowa': 1, 'feathery': 1, 'infra': 1, 'gep': 1, 'fifa': 1, '2006': 1, 'shhhhh': 1, 'arul': 1, 'related': 1, 'amk': 1, '09061743810': 1, 'length': 1, 'corrct': 1, 'antha': 1, 'dane': 1, 'basket': 1, 'rupaul': 1, 'curtsey': 1, 'practising': 1, '4my': 1, 'havebeen': 1, '2i': 1, 'feelingood': 1, 'ordinator': 1, 'preschoolco': 1, 'rise': 1, 'havbeen': 1, 'payed2day': 1, 'memory': 1, 'converted': 1, 'soil': 1, 'african': 1, 'outreach': 1, 'roles': 1, '8lb': 1, 'brilliantly': 1, '7oz': 1, 'forwarding': 1, 'visitors': 1, 'intention': 1, 'bend': 1, 'rules': 1, 'thia': 1, 'inlude': 1, 'previews': 1, 'madurai': 1, 'marrge': 1, 'dha': 1, 'ambrith': 1, 'kitty': 1, 'shaved': 1, 'tactful': 1, 'pert': 1, 'crammed': 1, 'satsgettin': 1, '47per': 1, 'apologize': 1, 'pei': 1, 'subtoitles': 1, 'jot': 1, 'cereals': 1, 'gari': 1, 'bold2': 1, '1er': 1, 'm60': 1, 'cast': 1, 'aom': 1, '09094100151': 1, 'gbp5': 1, 'box61': 1, 'thkin': 1, 'resubbing': 1, 'shadow': 1, 'breadstick': 1, 'saeed': 1, '09066362220': 1, 'purple': 1, 'brown': 1, 'yelow': 1, 'arranging': 1, 'eldest': 1, 'drugdealer': 1, 'wither': 1, '23f': 1, '23g': 1, 'wondarfull': 1, 'txt250': 1, 'web2mobile': 1, 'txtx': 1, 'box139': 1, 'la32wu': 1, 'onbus': 1, 'donyt': 1, 'latelyxxx': 1, '85233': 1, 'endof': 1, 'justthought': 1, '2hook': 1, 'uwant': 1, 'offdam': 1, 'nevamind': 1, 'sayhey': 1, 'stressed': 1, 'provider': 1, 'soooo': 1, 'tming': 1, 'cutest': 1, 'dice': 1, '08700469649': 1, 'box420': 1, 'mathe': 1, 'samachara': 1, 'howda': 1, 'autocorrect': 1, 'audrie': 1, 'readiness': 1, 'simulate': 1, 'lara': 1, 'supplies': 1, 'attach': 1, 'guesses': 1, '087123002209am': 1, 'nickey': 1, 'nobbing': 1, 'platt': 1, 'washob': 1, 'sterling': 1, 'spotty': 1, 'province': 1, 'hall': 1, 'hesitation': 1, 'ponnungale': 1, 'intha': 1, 'ipaditan': 1, 'rejected': 1, 'noisy': 1, 'needa': 1, 'reset': 1, 'troubleshooting': 1, 'manual': 1, 'b4utele': 1, 'marsms': 1, 'b4u': 1, '08717168528': 1, 'stifled': 1, 'creativity': 1, 'strongly': 1, 'requirements': 1, '2getha': 1, 'qlynnbv': 1, 'help08700621170150p': 1, 'buffy': 1, 'nosh': 1, 'lololo': 1, 'waaaat': 1, 'occupied': 1, 'documents': 1, 'stapati': 1, 'submitted': 1, 'hills': 1, 'cutie': 1, 'honesty': 1, 'beggar': 1, 'shakara': 1, 'specialisation': 1, 'labor': 1, 'dent': 1, 'crickiting': 1, 'isv': 1, 'urgoin': 1, 'tome': 1, 'reallyneed': 1, '2docd': 1, 'dontignore': 1, 'mycalls': 1, 'imin': 1, 'outl8r': 1, 'dontmatter': 1, 'thecd': 1, 'dontplease': 1, 'yavnt': 1, 'ibuprofens': 1, 'popping': 1, 'sip': 1, 'grown': 1, 'chinatown': 1, 'claypot': 1, 'beehoon': 1, 'fishhead': 1, 'yam': 1, 'porridge': 1, 'jaklin': 1, 'cliffs': 1, 'nearby': 1, 'bundle': 1, 'mf': 1, 'deals': 1, '49': 1, 'avble': 1, '4got': 1, 'weds': 1, 'moseley': 1, 'ooh': 1, 'thankyou': 1, 'ternal': 1, 'ntimate': 1, 'namous': 1, 'aluable': 1, 'atural': 1, 'oble': 1, 'ffectionate': 1, 'oveable': 1, 'ruthful': 1, 'textin': 1, 'burn': 1, 'amigos': 1, 'progress': 1, 'weren': 1, 'tryin': 1, 'collages': 1, 'arty': 1, '2hrs': 1, 'waliking': 1, 'cartons': 1, 'shelves': 1, '08714712379': 1, 'mirror': 1, '09065069120': 1, 'k718': 1, 'keris': 1, 'smidgin': 1, 'jod': 1, 'intentions': 1, 'accordin': 1, 'knocking': 1, 'como': 1, 'abel': 1, 'listened2the': 1, 'air1': 1, 'braindance': 1, 'plaid': 1, 'ofstuff': 1, 'hilarious': 1, 'hav2hear': 1, 'aphex': 1, 'nelson': 1, 'unmits': 1, 'newspapers': 1, 'yummmm': 1, 'puzzeles': 1, 'scammers': 1, '4goten': 1, '09099726481': 1, 'passion': 1, 'dena': 1, '09065069154': 1, 'r836': 1, 'shifad': 1, 'raised': 1, 'doctors': 1, 'reminds': 1, 'tolerat': 1, 'bcs': 1, 'subscrition': 1, 'splashmobile': 1, 'dust': 1, '88877': 1, '3pound': 1, 'watchin': 1, 'meaningless': 1, 'jones': 1, 'brdget': 1, 'inever': 1, 'hype': 1, 'studio': 1, 'velly': 1, 'marking': 1, '2stoptx': 1, '08718738034': 1, 'va': 1, 'hanger': 1, 'arrow': 1, 'blanket': 1, '08718726971': 1, 'tddnewsletter': 1, 'dozens': 1, 'thedailydraw': 1, 'emc1': 1, 'prizeswith': 1, 'significant': 1, 'waqt': 1, 'ko': 1, 'pehle': 1, 'wo': 1, 'naseeb': 1, 'jeetey': 1, 'nahi': 1, 'kisi': 1, 'kuch': 1, 'jo': 1, 'zyada': 1, 'milta': 1, 'hum': 1, 'zindgi': 1, 'sochte': 1, 'stalking': 1, 'reminded': 1, 'elaya': 1, 'varaya': 1, '09066368753': 1, '97n7qp': 1, 'anand': 1, 'expected': 1, 'beach': 1, 'workand': 1, 'jez': 1, 'whilltake': 1, 'todo': 1, 'zogtorius': 1, 'financial': 1, 'alian': 1, 'or2optout': 1, 'hv9d': 1, 'century': 1, 'frwd': 1, 'posible': 1, 'affectionate': 1, 'sorts': 1, 'restrictions': 1, 'buddys': 1, '08712402902': 1, 'owned': 1, 'possessive': 1, 'clarification': 1, 'coimbatore': 1, 'stream': 1, '0871212025016': 1, 'monos': 1, 'monoc': 1, 'polyc': 1, 'categories': 1, 'transcribing': 1, 'ethnicity': 1, 'census': 1, 'cakes': 1, 'draws': 1, 'asusual': 1, 'franyxxxxx': 1, 'goodmate': 1, 'cheered': 1, 'batt': 1, 'pobox1': 1, 'becausethey': 1, 'w14rg': 1, '09058098002': 1, 'gained': 1, 'limits': 1, 'pressure': 1, 'doke': 1, 'laying': 1, 'neshanth': 1, 'byatch': 1, 'whassup': 1, 'cl': 1, 'slo': 1, '4msgs': 1, 'filthyguys': 1, 'chiong': 1, 'reltnship': 1, 'wipe': 1, 'dialogue': 1, 'dryer': 1, 'comb': 1, 'pose': 1, 'fps': 1, 'computational': 1, 'disturbance': 1, 'premarica': 1, 'dlf': 1, 'gotto': 1, '220cm2': 1, 'err': 1, 'bloo': 1, 'hitter': 1, 'offline': 1, 'anjola': 1, 'asjesus': 1, 'ki': 1, 'imf': 1, 'corrupt': 1, 'deposited': 1, 'projects': 1, 'pura': 1, 'karo': 1, 'crore': 1, 'padhe': 1, 'suply': 1, 'lac': 1, 'blocked': 1, 'itna': 1, 'directors': 1, 'politicians': 1, 'swiss': 1, 'taxless': 1, 'torrents': 1, 'slowing': 1, 'particularly': 1, 'commit': 1, '83370': 1, 'trivia': 1, 'rightio': 1, 'brum': 1, 'scorable': 1, 'paranoid': 1, 'sheet': 1, 'brin': 1, 'bsnl': 1, 'complain': 1, 'offc': 1, 'bettr': 1, 'payed': 1, 'suganya': 1, 'dessert': 1, 'abeg': 1, 'sponsors': 1, 'onum': 1, 'imagination': 1, 'poet': 1, 'rr': 1, 'famamus': 1, 'locks': 1, 'jenne': 1, 'easiest': 1, 'barcelona': 1, 'sppok': 1, 'complementary': 1, '2px': 1, 'wa14': 1, 'pansy': 1, 'jungle': 1, 'kanji': 1, 'srs': 1, 'drizzling': 1, 'appointments': 1, 'excused': 1, 'reppurcussions': 1, 'necessity': 1, 'drama': 1, 'struggling': 1, 'ego': 1, 'cosign': 1, '09061701444': 1, 'hvae': 1, 'requires': 1, 'suman': 1, 'telephonic': 1, 'hcl': 1, 'freshers': 1, 'reliant': 1, 'fwiw': 1, 'afford': 1, 'arrival': 1, 'sq825': 1, 'citylink': 1, 'props': 1, 'statements': 1, 'pleasant': 1, 'pobox114': 1, '14tcr': 1, '6230': 1, 'splendid': 1, 'bognor': 1, 'ktv': 1, 'misplaced': 1, 'computers': 1, 'permanent': 1, 'registration': 1, 'begun': 1, 'residency': 1, 'risks': 1, 'predicting': 1, 'accumulation': 1, 'programs': 1, 'grief': 1, 'belongs': 1, 'shoranur': 1, 'prior': 1, 'fuelled': 1, 'fated': 1, 'concern': 1, 'txt82228': 1, 'text82228': 1, 'promptly': 1, 'honestly': 1, 'burnt': 1, 'quizclub': 1, '80122300p': 1, 'rwm': 1, '08704050406': 1, 'snap': 1, 'connected': 1, 'gmw': 1, 'someplace': 1, 'goods': 1, 'pressies': 1, 'ultimately': 1, 'achieve': 1, 'motive': 1, 'tui': 1, 'tor': 1, 'korli': 1, 'dock': 1, 'newscaster': 1, 'rolled': 1, 'flute': 1, 'wheel': 1, 'dabbles': 1, 'picsfree1': 1, 'keyword': 1, 'the4th': 1, 'october': 1, '83435': 1, 'safety': 1, 'aspects': 1, 'elaborating': 1, '85555': 1, 'tarot': 1, 'ours': 1, 'horniest': 1, 'cysts': 1, 'flow': 1, 'shrink': 1, 'ovarian': 1, 'developed': 1, 'grams': 1, 'upping': 1, 'timin': 1, 'apes': 1, 'ibm': 1, 'hp': 1, 'gosh': 1, 'spose': 1, 'rimac': 1, 'dosomething': 1, 'arestaurant': 1, 'squid': 1, 'dabooks': 1, 'eachother': 1, 'luckily': 1, 'starring': 1, 'restocked': 1, 'stoptxtstop': 1, 'knock': 1, 'tkls': 1, 'challenging': 1, 'smoothly': 1, 'breakfast': 1, 'hamper': 1, 'cc100p': 1, 'above': 1, '0870737910216yrs': 1, 'unni': 1, 'dramastorm': 1, 'particular': 1, 'lacking': 1, 'forfeit': 1, 'coupla': 1, 'digi': 1, '077xxx': 1, '09066362206': 1, 'sundayish': 1, 'prasad': 1, 'rcb': 1, 'kochi': 1, 'smear': 1, 'checkup': 1, 'gobi': 1, '4w': 1, 'technologies': 1, 'olowoyey': 1, 'argentina': 1, 'taxt': 1, 'lool': 1, 'tie': 1, 'massage': 1, 'pos': 1, 'shaking': 1, 'scarcasim': 1, 'naal': 1, 'eruku': 1, 'w4': 1, '5wq': 1, 'sensible': 1, 'impressively': 1, 'obedient': 1, 'ft': 1, 'combination': 1, 'needy': 1, 'playng': 1, 'jorge': 1, 'mcfly': 1, 'sara': 1, 'ab': 1, 'yupz': 1, 'ericson': 1, 'luks': 1, 'modl': 1, 'der': 1, 'frosty': 1, 'cheesy': 1, 'witin': 1, '0870141701216': 1, '120p': 1, 'fans': 1, '09050000555': 1, 'ba128nnfwfly150ppm': 1, '10th': 1, 'themed': 1, 'nudist': 1, 'pump': 1, 'signal': 1, 'unusual': 1, 'palm': 1, 'handing': 1, 'printing': 1, '83021': 1, 'stated': 1, 'perpetual': 1, 'dd': 1, 'flung': 1, 'pract': 1, 'brains': 1, 'justbeen': 1, 'overa': 1, 'mush': 1, 'tunde': 1, 'missions': 1, '20m12aq': 1, 'eh74rr': 1, 'avo': 1, 'cuddled': 1, 'crashed': 1, 'chachi': 1, 'pl': 1, 'tiz': 1, 'kanagu': 1, 'prices': 1, 'ringing': 1, 'houseful': 1, 'pulling': 1, 'brats': 1, 'derp': 1, 'abusers': 1, 'lipo': 1, 'netflix': 1, 'clash': 1, 'arr': 1, 'oscar': 1, 'rebtel': 1, 'firefox': 1, 'bcmsfwc1n3xx': 1, '69969': 1, 'impressed': 1, 'funs': 1, 'footy': 1, 'coca': 1, 'stadium': 1, 'cola': 1, 'large': 1, 'teenager': 1, 'replacing': 1, 'paracetamol': 1, 'mittelschmertz': 1, 'arrived': 1, 'references': 1, 'cthen': 1, 'conclusion': 1, 'instant': 1, '08715203028': 1, '9th': 1, 'rugby': 1, 'courtroom': 1, 'twiggs': 1, 'affidavit': 1, 'showers': 1, 'possessiveness': 1, 'golden': 1, 'poured': 1, 'lasting': 1, 'mobs': 1, 'ymca': 1, 'crazyin': 1, 'sleepingwith': 1, 'finest': 1, 'breathe1': 1, 'pobox365o4w45wq': 1, 'wtc': 1, 'weiyi': 1, '505060': 1, 'flowers': 1, 'interflora': 1, 'paining': 1, 'outgoing': 1, 'romcapspam': 1, 'presence': 1, 'mee': 1, 'maggi': 1, '08712103738': 1, 'cough': 1, 'pooja': 1, 'sweatter': 1, 'ambitious': 1, 'miiiiiiissssssssss': 1, 'tunji': 1, 'frndz': 1, '6missed': 1, 'misscall': 1, 'mad1': 1, 'mad2': 1, 'tall': 1, 'avenge': 1, 'robs': 1, 'gudni8': 1, 'choices': 1, 'toss': 1, 'coin': 1, 'dancin': 1, 'nora': 1, 'explicitly': 1, 'gayle': 1, 'crucify': 1, 'butting': 1, 'vs': 1, 'cedar': 1, 'reserved': 1, 'durham': 1, '69855': 1, 'sf': 1, 'stopbcm': 1, 'wall': 1, 'groovying': 1, 'printer': 1, 'groovy': 1, 'acnt': 1, 'harish': 1, 'transfred': 1, 'shaping': 1, 'showrooms': 1, 'attending': 1, 'doinat': 1, 'callon': 1, 'yhl': 1, 'pdate_now': 1, 'configure': 1, 'anal': 1, 'pears': 1, 'such': 1, 'oooooh': 1, '09058094454': 1, 'expiry': 1, 'resubmit': 1, 'mint': 1, 'humans': 1, 'studyn': 1, 'everyboy': 1, 'xxxxxxxx': 1, 'answr': 1, '1thing': 1, 'liquor': 1, 'loko': 1, '730': 1, 'lined': 1, 'laughs': 1, 'fireplace': 1, 'icon': 1, '08712400200': 1, 'weasels': 1, 'fifth': 1, 'woozles': 1, '08718723815': 1, 'machines': 1, 'fucks': 1, 'ignorant': 1, 'mys': 1, 'downs': 1, 'fletcher': 1, '08714714011': 1, 'bowls': 1, 'cozy': 1, 'shake': 1, 'buzzzz': 1, 'vibrator': 1, 'pros': 1, 'jet': 1, 'nuclear': 1, 'cons': 1, 'trends': 1, 'brief': 1, 'iter': 1, 'description': 1, 'fusion': 1, 'shitinnit': 1, 'ikno': 1, 'nowhere': 1, 'doesdiscount': 1, 'jabo': 1, 'slower': 1, 'maniac': 1, 'manege': 1, 'swalpa': 1, 'hogidhe': 1, 'chinnu': 1, 'sapna': 1, 'agidhane': 1, 'footbl': 1, 'crckt': 1, 'swell': 1, 'bollox': 1, 'tim': 1, 'tol': 1, 'ingredients': 1, 'pocy': 1, 'non': 1, '4qf2': 1, 'senor': 1, 'person2die': 1, 'possibly': 1, 'nvq': 1, 'giggle': 1, 'professional': 1, 'tiger': 1, 'woods': 1, 'grinder': 1, 'buyers': 1, 'figuring': 1, 'entirely': 1, 'disconnected': 1, 'onluy': 1, 'offcampus': 1, 'matters': 1, 'riley': 1, 'ew': 1, 'wesley': 1, 'lingo': 1, '400mins': 1, 'j5q': 1, 'chrgd': 1, '69200': 1, '2exit': 1, 'independence': 1, 'afternoons': 1, 'ugadi': 1, 'sankranti': 1, 'dasara': 1, 'rememberi': 1, 'teachers': 1, 'republic': 1, 'fools': 1, 'festival': 1, 'childrens': 1, 'approaching': 1, 'shivratri': 1, 'mornings': 1, 'joys': 1, 'greeting': 1, 'somewheresomeone': 1, 'daywith': 1, 'tosend': 1, 'lifeis': 1, 'selflessness': 1, 'initiate': 1, 'tallent': 1, 'wasting': 1, 'portal': 1, 't4get2text': 1, 'lennon': 1, 'bothering': 1, 'crab': 1, 'fox': 1, 'waves': 1, 'footprints': 1, 'frndsship': 1, 'dwn': 1, 'summon': 1, 'slaaaaave': 1, 'appendix': 1, 'slob': 1, 'smiled': 1, 'webpage': 1, 'yeesh': 1, 'gotbabes': 1, 'subscriptions': 1, 'hunks': 1, 'unsubscribed': 1, 'participate': 1, 'gopalettan': 1, 'stopcost': 1, '08712400603': 1, 'abroad': 1, 'xxsp': 1, 'mat': 1, 'agent': 1, 'goodies': 1, 'ay': 1, 'steal': 1, 'isaiah': 1, 'expert': 1, 'thinl': 1, 'importantly': 1, 'tightly': 1, 'fals': 1, 'pretsovru': 1, 'nav': 1, 'wnevr': 1, 'vth': 1, 'pretsorginta': 1, 'yen': 1, 'fal': 1, 'alwa': 1, 'madodu': 1, 'nammanna': 1, 'stdtxtrate': 1, 'soundtrack': 1, 'lord': 1, 'rings': 1, 'pc1323': 1, 'phyhcmk': 1, 'sg': 1, 'emigrated': 1, 'hopeful': 1, 'olol': 1, 'victors': 1, 'winterstone': 1, 'stagwood': 1, 'jp': 1, 'mofo': 1, 'maraikara': 1, 'pathaya': 1, 'enketa': 1, 'priest': 1, 'reserves': 1, 'intrude': 1, 'walkabout': 1, 'cashed': 1, 'announced': 1, 'blog': 1, '28th': 1, 'neville': 1, 'footie': 1, 'phil': 1, 'abbey': 1, 'returning': 1, 'punj': 1, 'str8': 1, 'classic': 1, '200p': 1, 'sacked': 1, 'mmsto': 1, '35p': 1, 'lookatme': 1, 'clip': 1, '32323': 1, 'twat': 1, 'punch': 1, 'barred': 1, 'decking': 1, 'dungerees': 1, 'mentionned': 1, 'vat': 1, 'grl': 1, 'madstini': 1, 'eerulli': 1, 'agalla': 1, 'kodstini': 1, 'hogli': 1, 'kodthini': 1, 'mutai': 1, 'hogolo': 1, 'messed': 1, 'illspeak': 1, 'thasa': 1, 'shudvetold': 1, 'urgran': 1, 'u2moro': 1, 'updat': 1, 'okden': 1, 'likeyour': 1, 'mecause': 1, 'werebored': 1, 'countinlots': 1, 'uin': 1, 'tex': 1, 'gr8fun': 1, 'tagged': 1, 'hdd': 1, 'casing': 1, 'opened': 1, 'describe': 1, '140ppm': 1, '08718725756': 1, '09053750005': 1, '310303': 1, 'asus': 1, 'reformat': 1, 'plumbers': 1, 'wrench': 1, 'bcum': 1, 'appeal': 1, 'thriller': 1, 'director': 1, 'shove': 1, 'um': 1, 'elephant': 1, 'cr': 1, 'pookie': 1, 'nri': 1, 'x2': 1, 'deserve': 1, 'neighbor': 1, 'toothpaste': 1, 'diddy': 1, 'poking': 1, 'coccooning': 1, 'mus': 1, 'talkin': 1, 'newquay': 1, '1im': 1, 'windy': 1, 'y87': 1, '09066358361': 1, 'tirunelvai': 1, 'dusk': 1, 'puzzles': 1, '09065989180': 1, 'x29': 1, 'phews': 1, 'stairs': 1, 'earning': 1, 'recycling': 1, 'toledo': 1, 'reservations': 1, 'tai': 1, 'feng': 1, 'swimsuit': 1, 'frndshp': 1, 'luvd': 1, 'squeeeeeze': 1, 'hurricanes': 1, 'disasters': 1, 'erupt': 1, 'sway': 1, 'aroundn': 1, 'arise': 1, 'volcanoes': 1, 'lighters': 1, 'lasagna': 1, 'woould': 1, 'chickened': 1, '08718726978': 1, '7732584351': 1, '44': 1, 'raviyog': 1, 'bhayandar': 1, 'peripherals': 1, 'sunoco': 1, 'musical': 1, 'leftovers': 1, 'plate': 1, 'starving': 1, 'fatty': 1, 'badrith': 1, 'owe': 1, 'checkin': 1, 'swann': 1, 'armenia': 1, '09058097189': 1, '1205': 1, '330': 1, '1120': 1, 'justify': 1, 'hunt': 1, 'hava': 1, '1131': 1, '5226': 1, 'adrian': 1, 'thnq': 1, 'rct': 1, 'vatian': 1, 'babysitting': 1, 'everyones': 1, 'buttheres': 1, 'ofsi': 1, 'aboutas': 1, 'gonnamissu': 1, 'yaxx': 1, 'breakin': 1, 'merememberin': 1, 'asthere': 1, 'neglet': 1, 'ramaduth': 1, 'siguviri': 1, 'mahaveer': 1, 'nalli': 1, 'ee': 1, 'problum': 1, 'dodda': 1, 'pavanaputra': 1, 'ondu': 1, 'keluviri': 1, 'maruti': 1, 'maretare': 1, 'poortiyagi': 1, 'bajarangabali': 1, 'sankatmochan': 1, 'hanumanji': 1, 'olage': 1, 'hanuman': 1, 'kalisidare': 1, 'odalebeku': 1, 'janarige': 1, 'idu': 1, 'inde': 1, 'ivatte': 1, 'matra': 1, 'ijust': 1, 'talked': 1, 'opps': 1, 'dl': 1, 'gei': 1, 'tron': 1, 'workage': 1, 'spiffing': 1, 'craving': 1, 'babysit': 1, 'supose': 1, 'embassy': 1, 'spaces': 1, 'lightly': 1, 'checkboxes': 1, 'batsman': 1, 'yetty': 1, '09050000928': 1, 'yifeng': 1, 'emailed': 1, 'slurp': 1, '3miles': 1, 'doll': 1, 'barolla': 1, 'brainless': 1, 'sariyag': 1, 'vehicle': 1, 'madoke': 1, '07090201529': 1, 'postponed': 1, 'stocked': 1, 'tiime': 1, 'afternon': 1, 'interviews': 1, 'resizing': 1, '09066364349': 1, 'box434sk38wp150ppm18': 1, 'opposed': 1, '08081263000': 1, '83332': 1, 'shortcode': 1, 'refunded': 1, 'somerset': 1, 'nigpun': 1, 'overtime': 1, 'dismissial': 1, 'screwd': 1, '08712402972': 1, 'bull': 1, 'floating': 1, '09058095201': 1, 'heehee': 1, 'percentages': 1, 'arithmetic': 1, 'chillaxin': 1, 'iknow': 1, 'peril': 1, 'das': 1, 'studentfinancial': 1, 'wellda': 1, 'monster': 1, 'obey': 1, 'uhhhhrmm': 1, 'deltomorrow': 1, '09066368470': 1, '24m': 1, 'subscriptn3gbp': 1, '68866': 1, '08448714184': 1, 'landlineonly': 1, 'smartcall': 1, 'orno': 1, 'minmobsmore': 1, 'fink': 1, 'lkpobox177hp51fl': 1, 'carlie': 1, 'promised': 1, '09099726553': 1, 'youwanna': 1, 'youphone': 1, 'athome': 1, 'jack': 1, 'hypotheticalhuagauahahuagahyuhagga': 1, 'pretend': 1, 'helpful': 1, 'brainy': 1, 'reflection': 1, 'occasion': 1, 'traditions': 1, 'values': 1, 'affections': 1, 'celebrated': 1, 'katexxx': 1, 'myparents': 1, 'cantdo': 1, 'anythingtomorrow': 1, 'outfor': 1, 'aretaking': 1, 'level': 1, 'gate': 1, '89105': 1, 'lingerie': 1, 'weddingfriend': 1, 'petticoatdreams': 1, 'bridal': 1, 'inst': 1, 'overheating': 1, 'board': 1, 'reslove': 1, 'western': 1, 'oblisingately': 1, 'bambling': 1, 'notixiquating': 1, 'champlaxigating': 1, 'laxinorficated': 1, 'opted': 1, 'masteriastering': 1, 'wotz': 1, 'atrocious': 1, 'entropication': 1, 'amplikater': 1, 'fidalfication': 1, 'junna': 1, 'knickers': 1, 'nikiyu4': 1, '01223585236': 1, 'divert': 1, 'a30': 1, 'wadebridge': 1, 'orc': 1, 'vill': 1, 'seeking': 1, 'wherre': 1, 'phone750': 1, 'resolution': 1, 'frank': 1, 'logoff': 1, 'parkin': 1, 'asa': 1, '09050000878': 1, 'charming': 1, 'served': 1, 'mention': 1, 'arnt': 1, 'xxxxxxxxxxxxxx': 1, 'dorothy': 1, 'kiefer': 1, 'allalo': 1, 'mone': 1, 'alle': 1, 'eppolum': 1, 'fundamentals': 1, 'whoever': 1, 'dooms': 1, '5digital': 1, '3optical': 1, '1mega': 1, 'pixels': 1, 'js': 1, 'noi': 1, 'captaining': 1, 'burgundy': 1, 'amrita': 1, 'bpo': 1, 'profile': 1, 'persevered': 1, 'nighters': 1, 'regretted': 1, 'spouse': 1, 'pmt': 1, 'shldxxxx': 1, '4give': 1, 'scenario': 1, 'nytho': 1, 'frmcloud': 1, 'spun': 1, '2mwen': 1, 'fonin': 1, 'tx': 1, 'wrld': 1, '09071517866': 1, '150ppmpobox10183bhamb64xe': 1, 'pounded': 1, 'broadband': 1, 'installation': 1, 'tensed': 1, 'coughing': 1, 'warned': 1, 'sprint': 1, 'gower': 1, 'morrow': 1, '450p': 1, 'filth': 1, '9yt': 1, 'stop2': 1, 'e14': 1, '08701752560': 1, 'saristar': 1, 'chik': 1, '420': 1, '9061100010': 1, 'mobcudb': 1, '1st4terms': 1, 'wire3': 1, 'sabarish': 1, '09050000460': 1, 'j89': 1, 'box245c2150pm': 1, 'flea': 1, 'inpersonation': 1, 'banneduk': 1, 'maximum': 1, 'highest': 1, '71': 1, 'hari': 1, 'wifes': 1, 'mumtaz': 1, 'facts': 1, 'taj': 1, 'arises': 1, 'known': 1, 'shahjahan': 1, 'lesser': 1, '69101': 1, 'rtf': 1, 'sphosting': 1, 'webadres': 1, 'geting': 1, 'passport': 1, 'independently': 1, 'showed': 1, 'multiply': 1, 'twins': 1, '02085076972': 1, 'strt': 1, 'ltdhelpdesk': 1, 'pesky': 1, 'cyclists': 1, 'uneventful': 1, 'equally': 1, 'nattil': 1, 'adi': 1, 'entey': 1, 'kittum': 1, 'hitman': 1, 'hire': 1, '2309': 1, '09066660100': 1, 'outages': 1, 'conserve': 1, 'cps': 1, 'voted': 1, 'epi': 1, 'bare': 1, 'bhaskar': 1, 'gong': 1, 'kaypoh': 1, 'outdoors': 1, 'basketball': 1, 'interfued': 1, 'listed': 1, 'apology': 1, 'harlem': 1, 'forth': 1, 'hustle': 1, 'fats': 1, 'workout': 1, 'zac': 1, 'hui': 1, 'versus': 1, 'underdtand': 1, 'locaxx': 1, 'muchxxlove': 1, '07090298926': 1, '9307622': 1, 'winds': 1, 'skateboarding': 1, 'thrown': 1, 'bandages': 1, 'html': 1, '1146': 1, 'mfl': 1, 'gbp4': 1, 'hectic': 1, 'dogs': 1, 'doggin': 1, 'wamma': 1, 'virtual': 1, 'apnt': 1, 'pants': 1, 'lanka': 1, 'go2sri': 1, 'gudnyt': 1, 'relationship': 1, 'wherevr': 1, 'merely': 1, 'smacks': 1, 'plum': 1, 'alot': 1, '50s': 1, 'formatting': 1, 'attracts': 1, '8714714': 1, 'promotion': 1, 'vegas': 1, 'lancaster': 1, 'soc': 1, 'advising': 1, 'bsn': 1, 'lobby': 1, 'showered': 1, 'ything': 1, 'lubly': 1, 'vewy': 1, '087147123779am': 1, 'catches': 1, 'domain': 1, 'specify': 1, 'nusstu': 1, 'bari': 1, 'hudgi': 1, 'yorge': 1, 'ertini': 1, 'pataistha': 1, 'hoops': 1, 'hasbro': 1, 'jump': 1, 'ummifying': 1, 'associate': 1, 'uterus': 1, 'rip': 1, 'jacuzzi': 1, 'txtstar': 1, 'uve': 1, '2nights': 1, 'wildest': 1, 'aldrine': 1, 'rtm': 1, 'unhappiness': 1, 'sources': 1, 'events': 1, 'functions': 1, 'irritated': 1, '4wrd': 1, 'colleg': 1, 'necesity': 1, 'wthout': 1, 'witout': 1, 'espe': 1, 'wth': 1, 'takecare': 1, 'univ': 1, 'rajas': 1, 'burrito': 1, 'stitch': 1, 'trouser': 1, '146tf150p': 1, 'cheetos': 1, 'synced': 1, 'shangela': 1, 'passes': 1, '08704439680': 1, 'poo': 1, 'uup': 1, 'gloucesterroad': 1, 'ouch': 1, 'fruit': 1, 'forgiveness': 1, 'glo': 1, '09058095107': 1, 's3xy': 1, 'wlcome': 1, 'timi': 1, 'strtd': 1, 'sack': 1, '1stone': 1, 'throwin': 1, 'fishrman': 1, 'stones': 1, '08717895698': 1, 'mobstorequiz10ppm': 1, 'physics': 1, 'delicious': 1, 'praveesh': 1, 'beers': 1, 'salad': 1, 'whore': 1, 'scallies': 1, 'twinks': 1, 'skins': 1, '08712466669': 1, 'jocks': 1, 'flood': 1, 'beads': 1, 'section': 1, 'nitro': 1, 'wishlist': 1, 'sold': 1, 'creative': 1, 'reffering': 1, 'getiing': 1, 'weirdy': 1, 'brownies': 1, '12hours': 1, 'k61': 1, '09061701851': 1, 'restrict': 1, '74355': 1, 'greece': 1, 'recorded': 1, 'someday': 1, 'grandfather': 1, 'november': 1, '75max': 1, 'blu': 1, '09061104276': 1, 'yuou': 1, 'spot': 1, 'lotto': 1, 'bunch': 1, 'purchases': 1, 'authorise': 1, '45pm': 1, 'goss': 1, 'gimmi': 1, 'ystrday': 1, 'chile': 1, 'subletting': 1, 'steering': 1, 'ammae': 1, 'sleeps': 1, 'required': 1, 'rounder': 1, 'batchlor': 1, 'lambu': 1, 'ji': 1, 'zoom': 1, '08717890890': 1, 'stopcs': 1, '62220cncl': 1, '0430': 1, 'true18': 1, '37819': 1, '1b6a5ecef91ff9': 1, 'jul': 1, 'chg': 1, 'cst': 1, 'xafter': 1, 'pure': 1, 'hearted': 1, 'enemies': 1, 'smiley': 1, 'yaxxx': 1, 'gail': 1, 'theoretically': 1, 'hooked': 1, 'formally': 1, 'multimedia': 1, 'housing': 1, 'accounting': 1, 'agency': 1, 'delayed': 1, 'renting': 1, 'vague': 1, 'presents': 1, 'nicky': 1, 'gumby': 1, 'wave': 1, '44345': 1, 'alto18': 1, 'sized': 1, 'springs': 1, 'tarpon': 1, 'steps': 1, 'cab': 1, 'limited': 1, 'hf8': 1, '08719181259': 1, 'radiator': 1, 'tongued': 1, 'proper': 1, 'shorts': 1, 'qi': 1, 'suddenly': 1, 'flurries': 1, 'webeburnin': 1, 'babygoodbye': 1, 'golddigger': 1, 'dontcha': 1, 'pushbutton': 1, 'real1': 1, 'perform': 1, 'cards': 1, 'rebooting': 1, 'nigh': 1, 'cable': 1, 'outage': 1, 'nooooooo': 1, 'sos': 1, 'playin': 1, 'guoyang': 1, 'rahul': 1, 'dengra': 1, 'fieldof': 1, 'selfindependence': 1, 'contention': 1, 'antelope': 1, 'toplay': 1, 'gnarls': 1, 'barkleys': 1, 'borderline': 1, '545': 1, 'nightnight': 1, 'possibility': 1, 'grooved': 1, 'mising': 1, '6669': 1, 'unsecured': 1, '195': 1, 'secured': 1, 'fakeye': 1, 'eckankar': 1, 'lanre': 1, '3000': 1, 'heater': 1, 'degrees': 1, 'dodgey': 1, 'asssssholeeee': 1, 'seing': 1, 'dreamz': 1, 'blokes': 1, 'rebel': 1, 'buddy': 1, 'ceri': 1, '84484': 1, 'nationwide': 1, 'newport': 1, 'juliana': 1, 'nachos': 1, 'dizzamn': 1, 'suitemates': 1, 'nimbomsons': 1, 'continent': 1, '087104711148': 1, 'emerging': 1, 'fiend': 1, 'impede': 1, 'hesitant': 1, '400thousad': 1, '60': 1, 'essay': 1, 'nose': 1, 'tram': 1, 'vic': 1, 'coherently': 1, 'echo': 1, 'triple': 1, 'cusoon': 1, 'afew': 1, 'honi': 1, 'onlyfound': 1, 'gran': 1, 'bx526': 1, 'southern': 1, 'rayan': 1, 'macleran': 1, 'balls': 1, 'olave': 1, 'mandara': 1, 'trishul': 1, 'woo': 1, 'hoo': 1, 'panties': 1, 'thout': 1, 'pints': 1, 'flatter': 1, 'carlin': 1, 'ciao': 1, 'starve': 1, 'impression': 1, 'darkness': 1, 'motivate': 1, 'wknd': 1, 'heltini': 1, 'trusting': 1, 'uttered': 1, 'yalrigu': 1, 'iyo': 1, 'noice': 1, 'esaplanade': 1, '139': 1, 'accessible': 1, '08709501522': 1, 'la3': 1, '2wu': 1, 'occurs': 1, 'enna': 1, 'kalaachutaarama': 1, 'prof': 1, 'coco': 1, 'sporadically': 1, 'pobox75ldns7': 1, '09064017305': 1, '38': 1, 'persolvo': 1, 'tbs': 1, 'manchester': 1, 'kath': 1, 'burden': 1, 'noworriesloans': 1, '08717111821': 1, 'harder': 1, 'nbme': 1, 'sickness': 1, 'villa': 1, 'sathya': 1, 'gam': 1, 'smash': 1, 'religiously': 1, 'tips': 1, 'heroes': 1, '08715203649': 1, '07973788240': 1, 'penny': 1, 'muhommad': 1, 'fiting': 1, 'load': 1, 'mj': 1, 'unconvinced': 1, 'willpower': 1, 'elaborate': 1, 'absence': 1, 'answerin': 1, 'evey': 1, 'prin': 1, '08714342399': 1, 'gigolo': 1, 'spam': 1, 'mens': 1, '50rcvd': 1, 'gsoh': 1, 'oncall': 1, 'mjzgroup': 1, 'ashwini': 1, '08707500020': 1, '09061790125': 1, 'ukp': 1, 'skinny': 1, 'casting': 1, 'thet': 1, 'elections': 1, '116': 1, 'serena': 1, 'amrca': 1, 'hlday': 1, 'camp': 1, 'prescribed': 1, 'meatballs': 1, 'approve': 1, 'panalam': 1, 'spjanuary': 1, 'fortune': 1, 'allday': 1, 'perf': 1, 'outsider': 1, '98321561': 1, 'familiar': 1, 'depression': 1, 'infact': 1, 'band': 1, 'simpsons': 1, 'kip': 1, 'shite': 1, 'hont': 1, 'upgrading': 1, 'amanda': 1, 'renewing': 1, 'subject': 1, 'regard': 1, 'nannys': 1, 'perspective': 1, 'puts': 1, 'conveying': 1, 'debating': 1, 'wtlp': 1, 'jb': 1, 'florida': 1, 'hidden': 1, 'teams': 1, 'swhrt': 1, 'po19': 1, '2ez': 1, '47': 1, '0906346330': 1, 'general': 1, 'jetton': 1, 'cmon': 1, 'replies': 1, 'lunsford': 1, 'enjoying': 1, '0796xxxxxx': 1, 'prizeawaiting': 1, 'gravy': 1, 'meals': 1, 'kfc': 1, '07008009200': 1, 'attended': 1, 'mw': 1, 'tuth': 1, 'eviction': 1, 'michael': 1, 'spiral': 1, 'riddance': 1, 'suffers': 1, 'cricket': 1, 'edward': 1, 'closeby': 1, 'raglan': 1, 'skye': 1, 'bookedthe': 1, 'hut': 1, 'drastic': 1, '3750': 1, 'garments': 1, 'sez': 1, 'evry1': 1, 'arab': 1, 'eshxxxxxxxxxxx': 1, 'lay': 1, 'bimbo': 1, 'ugo': 1, '3lions': 1, 'portege': 1, 'm100': 1, 'semiobscure': 1, 'gprs': 1, 'loosu': 1, 'careless': 1, 'freaking': 1, 'myspace': 1, 'logged': 1, 'method': 1, 'blur': 1, 'jewelry': 1, 'deluxe': 1, 'bbdeluxe': 1, 'features': 1, 'breaker': 1, 'graphics': 1, 'fumbling': 1, 'weekdays': 1, 'nails': 1, 'asia': 1, 'tobed': 1, '430': 1, 'stil': 1, 'asthma': 1, 'attack': 1, 'ball': 1, 'spin': 1, 'million': 1, 'haiyoh': 1, 'saves': 1, 'prsn': 1, 'sunlight': 1, 'relocate': 1, 'audiitions': 1, 'pocked': 1, 'motivating': 1, 'brison': 1, 'caps': 1, 'bullshit': 1, 'spelled': 1, 'motherfucker': 1, 'ig11': 1, '1013': 1, 'kit': 1, 'oja': 1, 'strip': 1, '08712402578': 1, 'thesmszone': 1, 'masked': 1, 'anonymous': 1, 'abuse': 1, 'parish': 1, 'magazine': 1, 'woodland': 1, 'avenue': 1, 'billy': 1, 'awww': 1, 'useless': 1, 'loo': 1, 'ed': 1, 'glands': 1, 'swollen': 1, 'bcaz': 1, 'truble': 1, 'evone': 1, 'hates': 1, 'stu': 1, 'view': 1, 'gays': 1, 'dual': 1, 'hostile': 1, 'breezy': 1, 'haircut': 1, '1tulsi': 1, 'leaf': 1, 'diseases': 1, 'litres': 1, 'watr': 1, 'problms': 1, '1apple': 1, '1lemon': 1, 'snd': 1, '1cup': 1, 'lavender': 1, 'manky': 1, 'inmind': 1, 'travelling': 1, 'scouse': 1, 'recreation': 1, 'judgemental': 1, 'fridays': 1, 'waheeda': 1, 'notes': 1, 'bot': 1, 'eventually': 1, 'hits': 1, 'tolerance': 1, '0789xxxxxxx': 1, 'hellogorgeous': 1, 'nitw': 1, 'texd': 1, 'jaz': 1, '4ward': 1, 'hopeu': 1, '09058091870': 1, 'exorcism': 1, 'emily': 1, 'prayrs': 1, 'othrwise': 1, 'evry': 1, 'emotion': 1, 'dsn': 1, 'sandiago': 1, 'parantella': 1, 'ujhhhhhhh': 1, 'hugging': 1, 'mango': 1, 'sweater': 1, 'involved': 1, 'bob': 1, 'barry': 1, '83738': 1, 'landmark': 1, 'consent': 1, 'clubzed': 1, 'tonexs': 1, 'renewed': 1, 'billing': 1, 'mathews': 1, 'anderson': 1, 'edwards': 1, 'tait': 1, 'promoting': 1, 'haunt': 1, 'crowd': 1, '8000930705': 1, 'snowboarding': 1, 'christmassy': 1, 'recpt': 1, 'baaaaaaaabe': 1, 'ignoring': 1, 'zealand': 1, 'education': 1, 'academic': 1, 'completes': 1, 'sagamu': 1, 'vital': 1, 'lautech': 1, 'shola': 1, 'qet': 1, 'browser': 1, 'surf': 1, 'subscribers': 1, 'conversations': 1, 'overemphasise': 1, 'senses': 1, 'convinced': 1, 'adp': 1, 'headset': 1, 'internal': 1, 'extract': 1, 'immed': 1, 'bevies': 1, 'waz': 1, 'fancied': 1, 'spoon': 1, 'comfey': 1, 'watchng': 1, 'othrs': 1, 'skint': 1, 'least5times': 1, 'quitting': 1, 'wudn': 1, 'frequently': 1, 'cupboard': 1, 'route': 1, '2mro': 1, 'floppy': 1, 'snappy': 1, 'risk': 1, 'grasp': 1, 'flavour': 1, 'laready': 1, 'denying': 1, 'dom': 1, 'ffffuuuuuuu': 1, 'julianaland': 1, 'oblivious': 1, 'dehydrated': 1, 'dogwood': 1, 'tiny': 1, 'mapquest': 1, 'archive': 1, '08719839835': 1, 'mgs': 1, '89123': 1, 'behalf': 1, 'lengths': 1, 'stunning': 1, 'visa': 1, 'gucci': 1, 'talkbut': 1, 'culdnt': 1, 'wenwecan': 1, 'sozi': 1, 'wannatell': 1, 'smsing': 1, 'efficient': 1, '15pm': 1, 'thandiyachu': 1, 'erutupalam': 1, 'invention': 1, 'lyrics': 1, 'somone': 1, 'nevr': 1, 'undrstnd': 1, 'definitly': 1, 'unrecognized': 1, 'valuing': 1, 'toking': 1, 'ger': 1, 'syd': 1, 'dhorte': 1, 'lage': 1, 'kintu': 1, 'opponenter': 1, 'khelate': 1, 'spares': 1, 'looovvve': 1, 'fried': 1, 'warwick': 1, 'tmw': 1, 'canceled': 1, 'havn': 1, 'tops': 1, 'grandma': 1, 'parade': 1, 'norcorp': 1, 'proze': 1, 'posting': 1, '7cfca1a': 1, 'grumble': 1, 'algebra': 1, 'linear': 1, 'decorating': 1, '946': 1, 'wining': 1, 'roomate': 1, 'graduated': 1, 'adjustable': 1, 'allows': 1, 'cooperative': 1, 'nottingham': 1, '40mph': 1, '63miles': 1, 'thanku': 1, 'guessed': 1, '50ea': 1, 'la1': 1, 'strings': 1, '7ws': 1, '89938': 1, 'otbox': 1, '731': 1, 'beside': 1, 'walks': 1, 'brisk': 1, 'dirtiest': 1, 'sexiest': 1, '89070': 1, 'tellmiss': 1, 'contribute': 1, 'greatly': 1, 'duvet': 1, 'smells': 1, 'urgh': 1, 'predictive': 1, 'coach': 1, 'w8in': 1, '4utxt': 1, '24th': 1, 'pist': 1, 'beverage': 1, 'surrender': 1, 'symptoms': 1, 'rdy': 1, 'backwards': 1, 'abstract': 1, 'avin': 1, 'africa': 1, 'chit': 1, '4217': 1, 'logon': 1, '6zf': 1, 'w1a': 1, '118p': 1, '8883': 1, 'cu': 1, 'quiteamuzing': 1, '4brekkie': 1, 'scool': 1, 'lrg': 1, 'psxtra': 1, 'satthen': 1, 'probpop': 1, 'portions': 1, '1000call': 1, '09071512432': 1, '300603t': 1, 'callcost150ppmmobilesvary': 1, 'rows': 1, 'njan': 1, 'fixd': 1, 'sudn': 1, 'engagement': 1, 'vilikkam': 1, 'maths': 1, 'chapter': 1, 'chop': 1, 'noooooooo': 1, '08718727870150ppm': 1, 'firsg': 1, 'split': 1, 'wasnt': 1, 'applyed': 1, 'heat': 1, 'sumfing': 1, 'ithink': 1, 'amnow': 1, 'bedreal': 1, 'layin': 1, 'tonsolitusaswell': 1, 'hopeso': 1, 'lotsof': 1, 'hiphop': 1, 'oxygen': 1, 'resort': 1, 'roller': 1, 'australia': 1, 'recorder': 1, 'canname': 1, 'mquiz': 1, 'showr': 1, 'upon': 1, 'ceiling': 1, 'ennal': 1, 'prakasam': 1, 'bcz': 1, 'prakasamanu': 1, 'presnts': 1, 'sneham': 1, 'mns': 1, 'jeevithathile': 1, 'neekunna': 1, 'irulinae': 1, 'mis': 1, 'blowing': 1, '7634': 1, '7684': 1, 'firmware': 1, 'vijaykanth': 1, 'anythiing': 1, 'clubmoby': 1, '08717509990': 1, 'ripped': 1, 'keypad': 1, 'btwn': 1, 'expects': 1, 'decades': 1, 'goverment': 1, 'spice': 1, 'ettans': 1, 'prasanth': 1, '08718738002': 1, '48922': 1, 'fizz': 1, 'contains': 1, 'appy': 1, 'genus': 1, 'robinson': 1, 'outs': 1, 'soz': 1, 'mums': 1, 'imat': 1, 'freinds': 1, 'sometext': 1, '9280114': 1, '07099833605': 1, 'chloe': 1, '130': 1, 'wewa': 1, 'iriver': 1, '255': 1, '128': 1, 'bw': 1, 'surly': 1, '9758': 1, '07808726822': 1, 'snuggles': 1, 'contented': 1, 'whispers': 1, 'mmmmmmm': 1, 'healthy': 1, '2bold': 1, 'barrel': 1, 'scraped': 1, 'misfits': 1, 'clearer': 1, 'sections': 1, 'peach': 1, 'tasts': 1, 'termsapply': 1, 'golf': 1, 'rayman': 1, 'activ8': 1, 'shindig': 1, 'phonebook': 1, 'ashes': 1, 'rocking': 1, 'shijutta': 1, 'offense': 1, 'dvg': 1, 'vinobanagar': 1, 'ovulate': 1, '3wks': 1, 'realising': 1, 'woah': 1, 'orh': 1, 'n8': 1, 'secrets': 1, 'hides': 1, 'akon': 1, 'axel': 1, 'eyed': 1, 'cashbin': 1, 'canteen': 1, 'stressfull': 1, 'adds': 1, 'continued': 1, 'president': 1, '180': 1, '140': 1, 'pleasured': 1, 'providing': 1, 'assistance': 1, 'whens': 1, '1172': 1, 'memories': 1, 'lonlines': 1, 'built': 1, 'lotz': 1, 'gailxx': 1, 'complacent': 1, 'denis': 1, 'mina': 1, 'miwa': 1, '09066649731from': 1, 'opposite': 1, 'heavily': 1, 'swayze': 1, 'patrick': 1, 'dolls': 1, '09077818151': 1, '30s': 1, 'santacalling': 1, 'calls1': 1, '50ppm': 1, 'quarter': 1, 'fired': 1, 'limping': 1, 'aa': 1, '08719180219': 1, '078498': 1, 'oga': 1, 'punishment': 1, 'poorly': 1, 'brb': 1, 'kill': 1, 'predicte': 1, 'situations': 1, 'loosing': 1, 'capacity': 1, 'smaller': 1, 'fgkslpo': 1, 'videos': 1, 'netun': 1, 'shsex': 1, 'fgkslpopw': 1, '0871277810710p': 1, 'defer': 1, 'admission': 1, 'checkmate': 1, 'maat': 1, 'shah': 1, 'chess': 1, 'persian': 1, 'phrase': 1, 'rats': 1, 'themes': 1, 'photoshop': 1, 'manageable': 1, '08715203652': 1, '42810': 1, 'increase': 1, 'carolina': 1, 'north': 1, 'texas': 1, 'gre': 1, 'bomb': 1, 'breathing': 1, 'powerful': 1, 'weapon': 1, 'lovly': 1, 'customercare': 1, 'msgrcvd': 1, 'clas': 1, 'lit': 1, 'couch': 1, 'loooooool': 1, 'swashbuckling': 1, 'terror': 1, 'cruel': 1, 'decent': 1, 'joker': 1, 'dip': 1, 'gek1510': 1, 'nuther': 1, 'lyricalladie': 1, 'hmmross': 1, '910': 1, 'differences': 1, 'happiest': 1, 'characters': 1, 'lists': 1, 'infections': 1, 'antibiotic': 1, 'gynae': 1, 'abdomen': 1, '6times': 1, 'exposed': 1, 'device': 1, 'beatings': 1, 'chastity': 1, 'uses': 1, 'wrenching': 1, 'gut': 1, 'tallahassee': 1, 'ou': 1, 'taka': 1, 'nr31': 1, '450pw': 1, 'pobox202': 1, '7zs': 1, 'ritten': 1, 'fold': 1, 'colin': 1, 'kiosk': 1, 'swat': 1, 'mre': 1, 'farrell': 1, '83118': 1, 'solihull': 1, 'terminated': 1, 'nhs': 1, '2b': 1, 'inconvenience': 1, 'dentists': 1, 'margin': 1, 'bergkamp': 1, 'yards': 1, 'henry': 1, '78': 1, 'parent': 1, 'unintentional': 1, 'snot': 1, 'nonetheless': 1, 'knees': 1, 'toaday': 1, 'grazed': 1, 'splat': 1, 'hooch': 1, 'deny': 1, 'hearin': 1, 'yah': 1, 'torture': 1, 'hopeing': 1, 'sisters': 1, 'sexychat': 1, 'lips': 1, 'congratulation': 1, 'court': 1, 'frontierville': 1, 'chapel': 1, 'mountain': 1, 'deer': 1, 'varma': 1, 'mailed': 1, 'secure': 1, 'parties': 1, 'farting': 1, 'ortxt': 1, 'dialling': 1, '402': 1, 'advisors': 1, 'trained': 1, 'stuffing': 1, 'woot': 1, 'ahhhh': 1, 'dining': 1, 'vouch4me': 1, 'etlp': 1, 'experiencehttp': 1, 'kaila': 1, '09058094507': 1, 'tsunami': 1, 'disaster': 1, 'donate': 1, 'unicef': 1, 'asian': 1, 'fund': 1, '864233': 1, 'cme': 1, 'hos': 1, 'collapsed': 1, 'cumming': 1, 'jade': 1, 'paul': 1, 'barmed': 1, 'thinkthis': 1, 'dangerous': 1, '762': 1, 'goldviking': 1, 'rushing': 1, 'coulda': 1, 'phony': 1, 'okday': 1, 'petexxx': 1, 'fromwrk': 1, 'adrink': 1, 'bthere': 1, 'buz': 1, 'outsomewhere': 1, 'wedlunch': 1, '2watershd': 1, 'hmph': 1, 'baller': 1, 'punto': 1, 'travelled': 1, 'ayo': 1, '125': 1, 'freeentry': 1, 'xt': 1, 'toyota': 1, 'olayiwola': 1, 'landing': 1, 'camry': 1, 'mileage': 1, 'clover': 1, 'amma': 1, 'achan': 1, 'rencontre': 1, 'mountains': 1, '08714712412': 1, 'puppy': 1, 'noise': 1, 'meg': 1, '08715203685': 1, '4xx26': 1, 'crossing': 1, '09094646631': 1, 'deepest': 1, 'darkest': 1, 'inconvenient': 1, 'adsense': 1, 'approved': 1, 'dudette': 1, 'perumbavoor': 1, 'stage': 1, 'clarify': 1, 'preponed': 1, 'natalie2k9': 1, '165': 1, 'natalie': 1, 'younger': 1, '08701213186': 1, 'liver': 1, 'opener': 1, 'guides': 1, 'watched': 1, 'loneliness': 1, 'skyving': 1, 'onwords': 1, 'mtnl': 1, 'mumbai': 1, 'mustprovide': 1, '62735': 1, '83039': 1, 'accommodationvouchers': 1, '450': 1, '15541': 1, 'rajitha': 1, 'ranju': 1, 'styles': 1, '1winawk': 1, 'tscs08714740323': 1, '50perweeksub': 1, '09066361921': 1, 'disagreeable': 1, 'afterwards': 1, 'vivekanand': 1, 'uawake': 1, 'deviousbitch': 1, 'aletter': 1, 'thatmum': 1, '4thnov': 1, 'gotmarried': 1, 'fuckinnice': 1, 'ourbacks': 1, 'justfound': 1, 'feellikw': 1, 'election': 1, 'rearrange': 1, 'eleven': 1, 'dormitory': 1, 'hitler': 1, 'starer': 1, 'recount': 1, 'astronomer': 1, 'worms': 1, 'suffering': 1, 'dysentry': 1, 'virgil': 1, 'andre': 1, 'gokila': 1, 'uncut': 1, 'dino': 1, 'diamond': 1, 'shanil': 1, 'exchanged': 1, 'kotees': 1, 'zebra': 1, 'sugababes': 1, 'panther': 1, 'badass': 1, 'hoody': 1, 'resent': 1, 'queries': 1, 'customersqueries': 1, 'netvision': 1, 'haughaighgtujhyguj': 1, 'andres': 1, 'hassling': 1, 'londn': 1, 'fassyole': 1, 'blacko': 1, 'responsibilities': 1, '08715205273': 1, 'vco': 1, 'humanities': 1, 'reassurance': 1, 'albi': 1, 'mahfuuz': 1, 'beeen': 1, 'tohar': 1, 'aslamalaikkum': 1, 'mufti': 1, 'muht': 1, '078': 1, 'tocall': 1, 'enufcredeit': 1, 'ileave': 1, 'treats': 1, 'okors': 1, 'ibored': 1, 'adding': 1, 'savings': 1, 'zeros': 1, 'goigng': 1, 'perfume': 1, 'sday': 1, 'grocers': 1, 'pubs': 1, 'bennys': 1, 'frankie': 1, 'owed': 1, 'diapers': 1, 'changing': 1, 'unlike': 1, 'turkeys': 1, 'patients': 1, 'princes': 1, 'helens': 1, 'unintentionally': 1, 'wenever': 1, 'stability': 1, 'vibrant': 1, 'colourful': 1, 'tranquility': 1, 'failing': 1, 'failure': 1, 'bawling': 1, 'velusamy': 1, 'facilities': 1, 'karnan': 1, 'bluray': 1, 'salt': 1, 'wounds': 1, 'logging': 1, 'geoenvironmental': 1, 'implications': 1, 'fuuuuck': 1, 'salmon': 1, 'uploaded': 1, 'wrkin': 1, 'ree': 1, 'compensation': 1, 'awkward': 1, 'splash': 1, 'musta': 1, 'leg': 1, 'overdid': 1, 'telediscount': 1, 'foned': 1, 'chuck': 1, 'port': 1, 'stuffs': 1, 'juswoke': 1, 'docks': 1, 'spinout': 1, 'boatin': 1, '08715203656': 1, '42049': 1, 'uworld': 1, 'assessment': 1, 'qbank': 1, 'someonone': 1, '09064015307': 1, 'tke': 1, 'temales': 1, 'finishd': 1, 'dull': 1, 'studies': 1, 'anyones': 1, 'craigslist': 1, 'treadmill': 1, 'absolutely': 1, 'swan': 1, 'hehe': 1, 'shexy': 1, 'sall': 1, 'lamp': 1, 'foward': 1, '09061790126': 1, 'misundrstud': 1, '2u2': 1, 'genes': 1, 'com1win150ppmx3age16subscription': 1, 'resuming': 1, 'reapply': 1, 'treatin': 1, 'treacle': 1, 'mumhas': 1, 'beendropping': 1, 'theplace': 1, 'adress': 1, 'favorite': 1, 'rumbling': 1, 'sashimi': 1, 'oyster': 1, 'marandratha': 1, 'correctly': 1, 'alaikkum': 1, 'heaven': 1, 'pisces': 1, 'aquarius': 1, '2yrs': 1, 'wicket': 1, 'steyn': 1, 'sterm': 1, 'resolved': 1, 'wheat': 1, 'chex': 1, 'hannaford': 1, 'jam': 1, 'grownup': 1, 'costume': 1, 'jerk': 1, 'stink': 1, 'subsequent': 1, 'follows': 1, 'openings': 1, 'upcharge': 1, 'guai': 1, 'astrology': 1, 'slacking': 1, 'mentor': 1, 'percent': 1, 'erotic': 1, 'ecstacy': 1, '09095350301': 1, 'dept': 1, '08717507382': 1, 'coincidence': 1, 'sane': 1, 'helping': 1, 'pause': 1, '151': 1, 'leading': 1, '8800': 1, 'psp': 1, 'gr8prizes': 1, 'spacebucks': 1, '083': 1, '6089': 1, 'squeezed': 1, 'maintaining': 1, 'dreading': 1, 'thou': 1, 'suggestion': 1, 'forgt': 1, 'lands': 1, 'helps': 1, 'ajith': 1, 'yoville': 1, 'ooooooh': 1, 'asda': 1, 'counts': 1, 'officer': 1, 'carly': 1, 'bffs': 1, '\u3028ud': 1, 'seperated': 1, 'franxx': 1, 'brolly': 1, 'syrup': 1, '5mls': 1, 'feed': 1, 'prometazine': 1, 'singapore': 1, 'shu': 1, 'victoria': 1, 'pocay': 1, '2morrowxxxx': 1, 'wocay': 1, 'ramen': 1, 'broth': 1, 'fowler': 1, 'tats': 1, 'flew': 1, '09058094583': 1, 'attention': 1, 'tix': 1, 'fne': 1, 'youdoing': 1, 'foregate': 1, 'shrub': 1, 'worc': 1, 'get4an18th': 1, '32000': 1, 'efreefone': 1, 'legitimat': 1, 'pendent': 1, 'toilet': 1, 'cops': 1, 'stolen': 1, 'navigate': 1, 'require': 1, 'choosing': 1, 'hu': 1, 'guidance': 1, 'chick': 1, 'boobs': 1, 'revealing': 1, 'org': 1, '2025050': 1, '0121': 1, 'shortbreaks': 1, 'sparkling': 1, 'breaks': 1, '45': 1, 'gyno': 1, 'belong': 1, 'gamb': 1, 'treasure': 1, '820554ad0a1705572711': 1, '09050000332': 1, 'negative': 1, 'positive': 1, 'hmmmm': 1, 'command': 1, 'stressful': 1, 'holby': 1, '09064017295': 1, 'li': 1, 'lecturer': 1, 'repeating': 1, 'motor': 1, 'yeovil': 1, 'rhode': 1, 'bong': 1, 'ofcourse': 1, '08448350055': 1, 'planettalkinstant': 1, '2p': 1, 'spider': 1, 'marvel': 1, 'ultimate': 1, '8ball': 1, '83338': 1, 'tamilnadu': 1, 'tip': 1, '07808247860': 1, '40411': 1, '08719899229': 1, 'identification': 1, 'boundaries': 1, 'limit': 1, 'endless': 1, 'reassuring': 1, 'young': 1, 'referin': 1, 'saibaba': 1, 'colany': 1, 'declare': 1, 'chic': 1, '49557': 1, 'disappointment': 1, 'irritation': 1, 'tantrum': 1, 'compliments': 1, 'adventuring': 1, 'chief': 1, 'gsex': 1, 'wc1n': 1, '2667': 1, '3xx': 1, 'l8er': 1, 'bailiff': 1, 'inclu': 1, '3mobile': 1, 'servs': 1, 'chatlines': 1, 'mouse': 1, 'desk': 1, 'childporn': 1, 'jumpers': 1, 'belt': 1, 'cribbs': 1, 'hat': 1, 'spiritual': 1, 'barring': 1, 'influx': 1, 'sudden': 1, 'kane': 1, 'shud': 1, 'pshew': 1, '4years': 1, 'units': 1, 'accent': 1, 'dental': 1, 'nmde': 1, 'dump': 1, 'heap': 1, 'lowes': 1, 'salesman': 1, '087187272008': 1, 'now1': 1, 'suggestions': 1, 'pity': 1, 'bitching': 1}), 'lowercase': True, 'n': 5574, 'ngram_range': (1, 1), 'normalize': True, 'on': None, 'preprocessor': None, 'processing_steps': [<function strip_accents_unicode at 0x7ff10730dc10>, <method 'lower' of 'str' objects>, <built-in method findall of re.Pattern object at 0x7ff107682440>], 'strip_accents': True, 'tokenizer': <built-in method findall of re.Pattern object at 0x7ff107682440>} RandomUnderSampler(BernoulliNB) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': True, '_rng': RandomState(MT19937) at 0x7FF107657940, 'classifier': BernoulliNB ( alpha=0 true_threshold=0. ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } Now let's try to use logistic regression to classify messages. We will use different tips to make my model perform better. As in the previous example, we rebalance the classes of our dataset. The logistics regression will be fed from a TF-IDF. from river import linear_model from river import optim from river import preprocessing X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | preprocessing . Normalizer () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( optimizer = optim . SGD ( .9 ), loss = optim . losses . Log () ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.946039 The confusion matrix: cm False True False 4655 172 True 54 693 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] TFIDF {'dfs': Counter({'to': 1687, 'you': 1591, 'the': 1035, 'in': 810, 'and': 795, 'is': 752, 'me': 690, 'for': 624, 'it': 614, 'my': 613, 'your': 587, 'call': 551, 'of': 550, 'have': 531, 'that': 512, 'on': 488, 'now': 481, 'are': 445, 'so': 433, 'can': 425, 'but': 422, 'not': 418, 'or': 398, 'at': 378, 'get': 368, 'just': 365, 'do': 359, 'be': 355, 'will': 354, 'with': 350, 'if': 350, 'we': 350, 'no': 343, 'this': 319, 'ur': 310, 'up': 297, 'how': 293, 'ok': 280, 'what': 279, 'from': 273, 'when': 272, 'go': 264, 'out': 262, 'all': 261, 'll': 258, 'know': 247, 'gt': 242, 'lt': 242, 'good': 233, 'then': 232, 'got': 229, 'free': 229, 'like': 229, 'come': 217, 'there': 214, 'am': 214, 'day': 212, 'only': 211, 'its': 210, 'time': 210, 'was': 196, 'send': 195, 'want': 188, 'love': 178, 'text': 174, 'he': 171, 'going': 167, 'txt': 165, 'home': 165, 'one': 163, 'by': 162, 'need': 162, 'today': 159, 'see': 158, 'as': 156, 'still': 155, 'about': 151, 'sorry': 151, 'don': 148, 'back': 147, 'lor': 145, 'our': 143, 'take': 140, 'dont': 139, 'stop': 138, 'reply': 135, 'da': 134, 'tell': 134, 'new': 133, 'later': 132, 'please': 131, 'any': 131, 'think': 129, 'mobile': 128, 'been': 126, 'phone': 124, 'here': 122, 'hi': 120, 'some': 119, 'did': 119, 'well': 119, 'she': 117, 'they': 116, 're': 114, 'where': 114, 'hope': 112, 'hey': 111, 'dear': 111, 'week': 110, 'oh': 110, 'pls': 110, 'has': 109, 'much': 109, 'night': 109, 'great': 108, 'claim': 108, 'an': 108, 'too': 107, 'msg': 106, 'more': 103, 'yes': 103, 'wat': 102, 'had': 101, 'him': 101, 'www': 100, 'her': 100, 'make': 99, 'work': 98, 'give': 98, 'way': 97, 've': 95, 'won': 95, 'who': 95, 'message': 93, 'number': 92, 'tomorrow': 90, 'after': 90, 'say': 89, 'already': 89, 'should': 89, 'doing': 88, 'right': 86, 'yeah': 86, 'happy': 86, 'prize': 84, 'why': 84, 'really': 83, 'ask': 83, 'find': 81, 'meet': 80, 'said': 80, 'cash': 78, 'im': 78, 'last': 78, 'let': 77, 'morning': 76, 'babe': 76, 'them': 76, 'lol': 74, 'thanks': 74, 'miss': 73, 'cos': 73, 'care': 73, 'anything': 72, '150p': 71, 'amp': 71, 'uk': 71, 'pick': 71, 'also': 71, 'win': 70, 'very': 70, 'urgent': 69, 'com': 69, 'would': 69, 'sure': 68, 'something': 68, 'contact': 68, 'over': 67, 'life': 67, 'sent': 67, 'again': 66, 'keep': 66, 'wait': 65, 'every': 65, 'his': 64, 'cant': 64, 'first': 62, 'us': 62, 'buy': 62, 'gud': 62, 'before': 62, 'thing': 62, 'even': 61, 'min': 61, 'soon': 60, 'next': 60, 'place': 60, 'off': 60, '50': 59, 'nice': 59, 'which': 59, 'customer': 58, 'tonight': 58, 'always': 58, 'service': 58, 'around': 57, 'per': 57, 'were': 57, 'late': 57, 'someone': 57, 'gonna': 56, 'feel': 56, 'could': 56, 'money': 56, 'help': 55, 'down': 55, 'sms': 55, 'sleep': 55, 'leave': 55, 'co': 54, '16': 54, 'other': 54, 'many': 54, 'wan': 54, 'nokia': 53, 'went': 53, 'told': 53, 'friends': 52, 'try': 52, 'waiting': 52, 'dun': 51, '18': 51, 'chat': 51, 'friend': 51, 'may': 50, 'fine': 50, 'guaranteed': 50, 'ya': 50, 'coming': 50, 'getting': 49, 'done': 49, 'special': 49, 'yet': 49, 'people': 49, 'haha': 49, 'use': 48, 'year': 48, 'same': 48, 'mins': 48, 'wish': 48, 'didn': 47, 'things': 47, 'holiday': 47, 'thk': 47, 'name': 46, 'man': 46, 'best': 46, 'thought': 46, 'talk': 45, 'bit': 45, 'hello': 44, 'draw': 44, 'few': 44, '500': 44, 'person': 44, 'cs': 44, 'stuff': 43, 'yup': 43, 'trying': 43, 'meeting': 43, 'thats': 43, 'job': 43, 'line': 43, 'heart': 43, 'being': 42, 'class': 42, 'never': 42, 'cool': 42, 'long': 42, 'better': 42, 'ill': 42, 'having': 42, 'days': 42, 'tone': 42, 'cost': 41, '100': 41, 'car': 41, 'live': 41, '1000': 41, 'house': 41, 'ready': 41, 'mind': 41, 'finish': 40, 'enjoy': 40, 'lunch': 39, 'half': 39, 'play': 39, 'check': 39, '10': 39, 'real': 39, 'lot': 39, 'dat': 39, 'chance': 39, 'god': 39, 'word': 38, 'awarded': 38, 'wanna': 38, 'box': 38, 'nothing': 38, 'guess': 38, 'sir': 38, 'lar': 37, 'latest': 37, 'end': 37, 'another': 37, 'liao': 37, 'guys': 37, 'than': 37, 'dinner': 36, 'month': 36, 'sweet': 36, 'ah': 36, 'shows': 36, 'big': 36, 'into': 36, 'shit': 36, '1st': 36, 'world': 35, 'xxx': 35, 'eat': 35, 'po': 35, 'account': 35, 'bt': 35, 'might': 35, 'problem': 35, 'quite': 35, 'receive': 34, 'camera': 34, 'watching': 34, 'smile': 34, '150ppm': 34, 'landline': 34, 'because': 34, 'start': 34, 'speak': 33, 'wont': 33, 'room': 33, 'yo': 33, 'wk': 33, 'aight': 33, 'luv': 33, 'tv': 33, 'offer': 33, 'called': 33, 'two': 33, 'probably': 33, 'rate': 32, 'apply': 32, 'remember': 32, 'left': 32, 'weekend': 32, 'once': 32, 'forgot': 32, 'jus': 32, 'watch': 32, 'plan': 32, 'actually': 32, 'bad': 32, 'princess': 32, 'early': 31, 'code': 31, 'does': 31, 'look': 31, 'maybe': 31, 'hear': 31, 'between': 31, 'easy': 31, 'reach': 31, 'thanx': 31, 'video': 31, 'shopping': 31, 'shall': 31, 'dunno': 31, 'minutes': 31, 'office': 31, 'fun': 30, '2nd': 30, 'part': 30, 'anyway': 30, 'didnt': 30, 'hour': 30, 'baby': 30, 'ever': 30, 'sat': 30, 'network': 29, 'selected': 29, 'enough': 29, 'thank': 29, 'bus': 29, 'ringtone': 29, 'pa': 29, 'looking': 29, 'bed': 29, 'birthday': 29, 'girl': 29, 'little': 29, 'working': 29, 'leh': 29, 'made': 29, 'orange': 29, 'put': 29, 'dad': 29, 'town': 29, 'pay': 28, 'calls': 28, 'afternoon': 28, 'those': 28, 'evening': 28, 'collect': 28, 'everything': 28, 'asked': 28, 'true': 28, 'texts': 28, 'den': 28, 'while': 28, 'kiss': 28, 'until': 27, 'though': 27, '000': 27, 'since': 27, 'pain': 27, 'dis': 27, 'came': 27, 'okay': 27, 'must': 27, 'join': 27, 'tmr': 27, 'details': 27, 'fuck': 27, 'wif': 26, 'wanted': 26, 'most': 26, 'means': 26, 'says': 26, 'mail': 26, 'able': 26, 'important': 26, 'wake': 26, 'collection': 26, 'goes': 25, 'times': 25, 'mob': 25, 'haven': 25, '5000': 25, 'show': 25, 'price': 25, 'school': 25, '2000': 25, 'sexy': 25, 'til': 25, 'guy': 25, 'away': 25, 'valid': 24, 'alright': 24, 'messages': 24, 'missed': 24, 'saw': 24, 'yesterday': 24, 'wen': 24, 'havent': 24, 'abt': 24, 'else': 24, 'juz': 24, 'years': 24, 'hav': 24, 'weekly': 24, 'wot': 24, 'bring': 24, 'attempt': 24, 'yours': 23, 'run': 23, 'making': 23, 'worry': 23, 'haf': 23, 'coz': 23, 'id': 23, 'oso': 23, '10p': 23, 'music': 23, 'stay': 23, 'bored': 23, 'these': 23, 'wife': 23, 'gift': 23, 'plus': 23, 'lei': 23, 'question': 22, 'colour': 22, 'net': 22, 'words': 22, 'national': 22, 'tried': 22, 'yourself': 22, 'address': 22, 'food': 22, 'top': 22, 'without': 22, 'boy': 22, 'decimal': 22, 'shop': 22, 'nite': 22, 'hot': 22, 'book': 22, 'friendship': 22, 'dude': 22, 'change': 22, 'feeling': 22, 'either': 22, '800': 22, 'online': 22, 'family': 22, 'de': 22, 'entry': 21, 'hours': 21, 'http': 21, 'ard': 21, 'till': 21, 'delivery': 21, 'hair': 21, 'bonus': 21, 'test': 21, 'driving': 21, 'busy': 21, 'todays': 21, 'answer': 21, 'nt': 21, 'xmas': 21, 'both': 21, 'vouchers': 21, 'full': 21, 'plz': 21, 'tones': 21, 'calling': 21, 'tot': 21, 'sae': 21, 'together': 21, 'wants': 21, 'goin': 21, 'sad': 21, 'brother': 20, 'set': 20, 'date': 20, 'trip': 20, 'comes': 20, 'movie': 20, 'mean': 20, 'old': 20, 'points': 20, 'award': 20, 'leaving': 20, 'order': 20, 'believe': 20, 'story': 20, 'sleeping': 20, 'noe': 20, 'happen': 20, 'face': 20, 'wid': 20, 'ring': 20, 'huh': 20, 'sch': 20, 'game': 20, 'makes': 20, 'await': 20, 'pounds': 19, 'news': 19, 'aft': 19, 'doesn': 19, 'tomo': 19, 'congrats': 19, 'took': 19, 'double': 19, 'finished': 19, 'started': 19, 'private': 19, 'gr8': 19, 'minute': 19, 'awesome': 19, 'wil': 19, '750': 19, '86688': 19, 'hurt': 19, 'row': 19, 'pm': 19, 'head': 19, 'eve': 19, 'beautiful': 19, 'thinking': 19, 'mum': 19, 'saying': 19, 'rite': 19, 'available': 18, 'final': 18, 'update': 18, 'tho': 18, 'xx': 18, 'close': 18, 'cause': 18, 'services': 18, 'taking': 18, 'missing': 18, 'touch': 18, 'walk': 18, 'unsubscribe': 18, 'charge': 18, 'lets': 18, 'post': 18, 'drink': 18, '250': 18, 'land': 18, 'gd': 18, '150': 18, 'mine': 18, 'pics': 18, 'pub': 18, 'email': 18, 'drive': 18, 'drop': 18, 'dreams': 18, '11': 17, 'lesson': 17, 'second': 17, 'lucky': 17, 'search': 17, '12hrs': 17, 'statement': 17, 'expires': 17, 'msgs': 17, 'open': 17, 'whats': 17, 'lots': 17, 'everyone': 17, 'carlos': 17, 'worth': 17, 'sis': 17, 'sounds': 17, 'company': 17, 'choose': 17, 'club': 17, 'okie': 17, 'card': 17, 'sister': 17, 'chikku': 17, 'poly': 17, 'dating': 17, 'opt': 17, 'neva': 17, 'anyone': 17, 'loving': 17, 'alone': 17, 'treat': 16, 'winner': 16, 'info': 16, 'pobox': 16, 'saturday': 16, 'decided': 16, 'forget': 16, '08000930705': 16, 'girls': 16, 'smiling': 16, 'prob': 16, 'gone': 16, 'happened': 16, 'identifier': 16, 'type': 16, 'ni8': 16, 'ltd': 16, 'hard': 16, 'each': 16, 'boytoy': 16, 'found': 16, 'college': 16, 'break': 16, 'anytime': 16, 'far': 16, 'games': 16, 'mobileupd8': 16, 'bout': 16, 'kind': 16, 'visit': 16, 'fast': 16, 'voucher': 16, 'sun': 16, '8007': 16, 'hows': 16, 'wonderful': 15, 'smth': 15, 'mom': 15, 'camcorder': 15, 'used': 15, 'hit': 15, 'operator': 15, 'friday': 15, 'quiz': 15, 'player': 15, 'parents': 15, 'frnd': 15, 'finally': 15, 'darlin': 15, 'goodmorning': 15, 'oredi': 15, 'secret': 15, 'congratulations': 15, 'hold': 15, 'takes': 15, 'read': 15, 'suite342': 15, '2lands': 15, '08000839402': 15, 'fucking': 15, 'nope': 15, 'outside': 15, 'pretty': 15, 'sea': 15, 'whatever': 15, 'weeks': 15, 'lovely': 15, 'mates': 15, 'wrong': 15, 'party': 15, 'nyt': 15, 'pic': 15, 'crazy': 14, 'wkly': 14, 'freemsg': 14, 'credit': 14, 'seeing': 14, 'whole': 14, 'frnds': 14, 'isn': 14, 'hmm': 14, 'mu': 14, 'their': 14, 'content': 14, 'fancy': 14, 'log': 14, 'course': 14, 'mrng': 14, 'tc': 14, 'thinks': 14, 'case': 14, 'tel': 14, 'meant': 14, 'fr': 14, 'angry': 14, 'light': 14, 'jay': 14, 'project': 14, 'reason': 14, 'ten': 14, 'welcome': 14, 'cum': 14, 'b4': 14, 'mate': 14, 'least': 14, 'earlier': 14, 'chennai': 14, '30': 14, 'point': 13, 'press': 13, 'valued': 13, 'hungry': 13, 'almost': 13, 'hee': 13, '0800': 13, 'felt': 13, 'invited': 13, '03': 13, 'caller': 13, 'numbers': 13, 'yr': 13, 'tired': 13, 'wit': 13, 'needs': 13, 'hmmm': 13, 'mr': 13, 'smoke': 13, 'balance': 13, 'march': 13, 'side': 13, '87066': 13, 'dnt': 13, 'unlimited': 13, 'fone': 13, 'stupid': 13, 'bslvyl': 13, 'lost': 13, 'reading': 13, 'txts': 13, 'ago': 13, 'currently': 13, 'motorola': 13, 'talking': 13, 'couple': 13, 'phones': 13, 'ass': 13, 'park': 13, 'frm': 13, 'fri': 13, 'offers': 13, 'within': 13, '2003': 13, 'un': 13, 'listen': 13, 'yar': 13, 'knw': 13, 'sex': 13, 'mayb': 13, 'understand': 13, 'knew': 13, 'gas': 13, 'comp': 12, '12': 12, 'mobiles': 12, '20': 12, 'eh': 12, 'confirm': 12, 'telling': 12, 'wow': 12, 'correct': 12, 'pass': 12, 'etc': 12, 'complimentary': 12, 'gotta': 12, 'loads': 12, 'computer': 12, 'mah': 12, 'askd': 12, 'uncle': 12, 'sending': 12, 'direct': 12, 'age': 12, 'hand': 12, 'bank': 12, 'bcoz': 12, 'laptop': 12, 'questions': 12, 'swing': 12, 'ge': 12, 'ends': 12, 'die': 12, '200': 12, 'via': 12, 'met': 12, 'call2optout': 12, 'seen': 12, 'rental': 12, 'india': 12, 'doin': 12, 'lose': 12, 'ipod': 12, '04': 12, 'redeemed': 12, 'through': 12, 'gym': 12, 'happiness': 12, 'snow': 12, 'area': 12, 'sound': 12, 'picking': 12, 'ugh': 12, 'extra': 12, 'heard': 12, 'support': 12, 'surprise': 12, 'information': 12, 'grins': 12, 'luck': 12, 'enter': 12, 'auction': 12, 'difficult': 12, 'wasn': 12, 'std': 11, 'usf': 11, 'sunday': 11, 'eg': 11, 'comin': 11, 'charged': 11, 'abiola': 11, 'crave': 11, 'gets': 11, 'ac': 11, 'move': 11, 'checking': 11, 'cut': 11, 'rply': 11, 'download': 11, 'shower': 11, 'entered': 11, 'match': 11, '350': 11, 'txting': 11, 'lovable': 11, 'wine': 11, 'safe': 11, 'orchard': 11, 'kate': 11, 'rs': 11, 'semester': 11, 'wana': 11, 'somebody': 11, 'rest': 11, 'christmas': 11, 'pete': 11, 'plans': 11, 'small': 11, 'ex': 11, 'w1j6hl': 11, 'hg': 11, 'discount': 11, 'slow': 11, 'yep': 11, 'th': 11, 'supposed': 11, 'asking': 11, 'remove': 11, 'monday': 11, 'simple': 11, 'noon': 11, 'darren': 11, 'ans': 11, 'store': 11, 'wonder': 11, 'sort': 11, 'asap': 11, 'na': 11, 'nobody': 11, 'nah': 10, '900': 10, 'months': 10, 'link': 10, 'ha': 10, 'worried': 10, 'myself': 10, 'knows': 10, 'oops': 10, 'hospital': 10, 'red': 10, 'reached': 10, 'forever': 10, 'song': 10, 'save': 10, 'tickets': 10, 'il': 10, 'representative': 10, 'gave': 10, 'rates': 10, 'del': 10, 'sony': 10, 'pray': 10, 'dream': 10, 'spend': 10, 'muz': 10, 'bath': 10, 'bathe': 10, 'study': 10, 'exam': 10, 'street': 10, 'reveal': 10, 'admirer': 10, 'deep': 10, 'own': 10, 'leaves': 10, 'blue': 10, 'usual': 10, 'somewhere': 10, 'normal': 10, 'merry': 10, 'immediately': 10, 'custcare': 10, 'weed': 10, 'rakhesh': 10, 'moment': 10, 'st': 10, 'woke': 10, 'mm': 10, 'voice': 10, 'ldn': 10, 'booked': 10, 'different': 10, 'terms': 10, 'water': 10, 'sub': 10, '00': 10, 'across': 10, 'warm': 10, 'cheap': 10, 'clean': 10, 'em': 10, 'ts': 10, 'drugs': 10, 'laugh': 10, 'fantastic': 10, 'glad': 10, 'wishing': 10, 'getzed': 10, 'whenever': 10, 'otherwise': 10, 'ntt': 10, 'truth': 10, 'gn': 10, 'convey': 10, 'film': 10, '2nite': 10, 'write': 10, 'fact': 10, 'loved': 10, 'slowly': 10, 'cup': 9, 'copy': 9, 'reward': 9, 'england': 9, 'seriously': 9, 'sick': 9, 'catch': 9, 'decide': 9, 'ice': 9, 'situation': 9, 'short': 9, 'rain': 9, 'coffee': 9, 'men': 9, 'boss': 9, 'specially': 9, 'ending': 9, 'sunshine': 9, 'lazy': 9, 'completely': 9, 'staying': 9, 'doesnt': 9, 'especially': 9, 'studying': 9, 'trust': 9, 'using': 9, 'deal': 9, 'itself': 9, 'dead': 9, 'mrt': 9, 'bill': 9, 'lessons': 9, 'goodnight': 9, 'cd': 9, 'ldew': 9, 'lover': 9, 'disturb': 9, 'credits': 9, 'worries': 9, 'tonite': 9, 'unless': 9, '4u': 9, '2day': 9, '11mths': 9, 'valentines': 9, 'urself': 9, 'bluetooth': 9, 'rock': 9, 'starts': 9, 'kinda': 9, 'loan': 9, 'meh': 9, 'near': 9, 'rent': 9, 'silent': 9, 'less': 9, 'children': 9, 'hoping': 9, 'age16': 9, 'self': 9, 'train': 9, 'forwarded': 9, 'starting': 9, 'paper': 9, 'seems': 9, 'sell': 9, 'eyes': 9, 'possible': 9, 'ones': 9, 'gettin': 9, 'poor': 9, 'tampa': 9, 'user': 9, 'mo': 9, 'against': 9, 'hiya': 9, 'doctor': 9, 'mon': 9, 'john': 9, 'mode': 9, 'others': 9, 'wondering': 9, 'ringtones': 9, 'bb': 9, 'tht': 9, '20p': 9, 'moral': 9, 'excellent': 9, 'father': 9, 'thinkin': 9, 'sitting': 9, 'sofa': 9, 'request': 8, 'entitled': 8, 'anymore': 8, 'promise': 8, 'wap': 8, 'pizza': 8, 'mark': 8, 'cheers': 8, 'quick': 8, 'replying': 8, 'nigeria': 8, 'cinema': 8, 'ip4': 8, '5we': 8, 'stand': 8, 'spent': 8, 'loves': 8, 'hurts': 8, 'trouble': 8, 'planning': 8, 'ave': 8, 'wishes': 8, 'weekends': 8, 'apartment': 8, 'inc': 8, 'paying': 8, '2004': 8, 'buying': 8, 'bak': 8, 'sp': 8, 'dvd': 8, 'dogging': 8, 'swt': 8, 'joy': 8, 'goto': 8, 'freephone': 8, 'joined': 8, 'however': 8, 'ive': 8, 'slept': 8, 'sign': 8, 'kick': 8, 'lemme': 8, 'rose': 8, 'cake': 8, 'fixed': 8, 'rcvd': 8, 'interested': 8, 'round': 8, 'figure': 8, 'reference': 8, 'mistake': 8, 'facebook': 8, 'al': 8, 'yahoo': 8, 'aha': 8, '3030': 8, 'funny': 8, 'giving': 8, 'din': 8, 'thru': 8, 'style': 8, 'opinion': 8, '02': 8, 'savamob': 8, 'member': 8, 'fingers': 8, '50p': 8, 'blood': 8, 'daddy': 8, 'door': 8, 'kids': 8, 'pound': 8, 'ar': 8, 'alex': 8, 'longer': 8, '25p': 8, 'pc': 8, 'xy': 8, 'bedroom': 8, 'king': 8, 'idea': 8, 'add': 8, 'library': 8, 'slave': 8, 'omg': 8, 'no1': 8, 'polys': 8, 'moon': 8, 'training': 8, 'gay': 8, 'sale': 8, '08712460324': 8, 'registered': 8, 'miracle': 8, 'during': 8, 'movies': 8, 'digital': 8, 'black': 8, 'awaiting': 8, 'cancel': 8, 'cute': 8, 'energy': 8, 'complete': 8, 'honey': 8, 'picked': 8, 'vl': 8, 'frens': 8, 'reaching': 8, '0870': 8, 'cover': 8, '06': 8, 'south': 8, 'inside': 8, 'hw': 8, 'wednesday': 8, 'pix': 8, 'mood': 8, 'la': 7, 'bugis': 7, 'cine': 7, 'naughty': 7, 'sucks': 7, 'tea': 7, 'eating': 7, 'learn': 7, 'ahead': 7, 'kept': 7, 'liked': 7, 'bx420': 7, 'joke': 7, 'wun': 7, 'following': 7, 'ta': 7, 'pleasure': 7, '10am': 7, 'password': 7, 'cuz': 7, 'page': 7, 'umma': 7, 'weight': 7, 'bother': 7, 'country': 7, '82277': 7, 'yijue': 7, 'lect': 7, 'persons': 7, 'sometimes': 7, 'become': 7, '62468': 7, 'internet': 7, 'waste': 7, 'hell': 7, 'experience': 7, 'towards': 7, 'bucks': 7, 'past': 7, 'biz': 7, 'appreciate': 7, 'road': 7, 'battery': 7, '25': 7, 'kallis': 7, 'cal': 7, 'showing': 7, 'naked': 7, 'horny': 7, 'quality': 7, 'definitely': 7, 'sense': 7, 'sim': 7, 'loyalty': 7, 'high': 7, 'advance': 7, 'power': 7, 'return': 7, 'access': 7, '08718720201': 7, 'wiv': 7, 'fault': 7, 'maximize': 7, 'cold': 7, 'forward': 7, 'happening': 7, 'lift': 7, 'tough': 7, 'tenerife': 7, 'notice': 7, '8th': 7, 'depends': 7, 'realy': 7, 'mp3': 7, '85023': 7, 'unsub': 7, 'single': 7, 'fat': 7, 'married': 7, 'rather': 7, 'hotel': 7, 'omw': 7, 'hurry': 7, 'workin': 7, 'gee': 7, 'izzit': 7, 'spree': 7, 'present': 7, 'valentine': 7, 'future': 7, 'shuhui': 7, 'weather': 7, 'login': 7, 'tuesday': 7, 'ho': 7, 'awake': 7, 'bold': 7, 'looks': 7, 'dey': 7, 'sit': 7, '7pm': 7, 'holla': 7, 'summer': 7, 'damn': 7, 'space': 7, '36504': 7, 'bag': 7, 'model': 7, 'mother': 7, 'yrs': 7, 'mid': 7, 'midnight': 7, 'january': 7, 'iam': 7, 'photo': 7, 'sk38xh': 7, 'recently': 7, 'feels': 7, 'heavy': 7, 'nxt': 7, '3g': 7, 'o2': 7, 'onto': 7, 'station': 7, 'tuition': 7, 'strong': 7, 'cell': 7, 'dog': 7, 'alrite': 7, 'shd': 7, 'croydon': 7, 'cr9': 7, '5wb': 7, '1327': 7, 'meaning': 7, 'players': 7, 'share': 7, 'lmao': 7, 'except': 7, 'arrive': 7, 'instead': 7, 'holding': 7, 'list': 7, 'thnk': 7, 'excuse': 7, 'costa': 7, 'sol': 7, 'including': 7, 'vikky': 7, 'colleagues': 7, 'tear': 7, 'worse': 7, 'murderer': 7, 'maid': 7, 'murdered': 7, 'happens': 7, 'feb': 7, 'planned': 7, 'joking': 6, 'hl': 6, 'click': 6, 'team': 6, 'texting': 6, 'tyler': 6, 'usually': 6, 'fyi': 6, '150pm': 6, 'review': 6, 'pleased': 6, 'kano': 6, 'simply': 6, 'changed': 6, 'eatin': 6, 'flights': 6, 'directly': 6, 'informed': 6, 'app': 6, 'standard': 6, '08712300220': 6, 'shouldn': 6, 'replied': 6, 'local': 6, 'qatar': 6, 'arrange': 6, 'inviting': 6, 'turns': 6, 'spoke': 6, 'personal': 6, 'nights': 6, 'system': 6, 'partner': 6, 'died': 6, 'website': 6, 'tncs': 6, 'childish': 6, 'handset': 6, 'dint': 6, 'sunny': 6, 'ended': 6, 'anybody': 6, 'imagine': 6, 'babes': 6, 'sport': 6, 'accept': 6, 'kb': 6, 'yoga': 6, 'track': 6, 'ish': 6, 'cc': 6, 'posted': 6, 'air': 6, 'willing': 6, 'body': 6, 'relax': 6, 'pilates': 6, 'putting': 6, 'fullonsms': 6, 'competition': 6, 'aathi': 6, 'wnt': 6, 'vry': 6, 'vary': 6, 'askin': 6, 'group': 6, 'ttyl': 6, 'isnt': 6, 'gives': 6, 'moan': 6, 'fb': 6, 'activate': 6, 'character': 6, 'jst': 6, 'tat': 6, '40gb': 6, 'pin': 6, 'campus': 6, 'lady': 6, 'l8r': 6, 'aiyo': 6, 'barely': 6, 'scream': 6, 'marriage': 6, 'announcement': 6, 'indian': 6, 'ladies': 6, '28': 6, 'imma': 6, 'daily': 6, 'paid': 6, 'vodafone': 6, 'matches': 6, 'holder': 6, 'evng': 6, 'earth': 6, 'under': 6, 'torch': 6, 'aftr': 6, 'exactly': 6, 'yay': 6, 'txtauction': 6, 'yest': 6, 'closed': 6, 'wats': 6, 'couldn': 6, 'pobox84': 6, 'norm150p': 6, 'w45wq': 6, 'hunny': 6, 'boo': 6, 'teasing': 6, 'zed': 6, 'green': 6, 'surely': 6, 'five': 6, 'wed': 6, 'fall': 6, 'sup': 6, 'murder': 6, 'due': 6, 'teach': 6, 'ate': 6, 'wherever': 6, 'medical': 6, 'brand': 6, 'contract': 6, 'kerala': 6, 'asleep': 6, 'loverboy': 6, 'serious': 6, 'april': 6, 'flower': 6, 'process': 6, 'works': 6, 'regards': 6, 'sipix': 6, 'aiyah': 6, 'urawinner': 6, 'gal': 6, 'howz': 6, 'raining': 6, 'thts': 6, 'tour': 6, 'super': 6, 'marry': 6, 'problems': 6, 'fantasies': 6, '08707509020': 6, 'walking': 6, 'cafe': 6, 'bought': 6, '4th': 6, 'nature': 6, 'keeping': 6, 'screaming': 6, '86021': 6, 'london': 6, 'lookin': 6, 'exciting': 6, 'toclaim': 6, 'max10mins': 6, 'pobox334': 6, '09050090044': 6, 'stockport': 6, 'theatre': 6, 'ahmad': 6, 'official': 6, 'armand': 6, 'nimya': 6, 'sed': 6, 'role': 6, 'checked': 6, 'added': 6, 'pussy': 6, 'budget': 6, 'random': 6, 'er': 6, 'hr': 6, 'hrs': 6, 'cancer': 6, 'tariffs': 6, 'meds': 6, 'darling': 5, 'callers': 5, 'callertune': 5, 'searching': 5, 'wet': 5, '87077': 5, 'stock': 5, 'egg': 5, 'subscription': 5, 'roommate': 5, 'hopefully': 5, 'ride': 5, 'respect': 5, 'urgnt': 5, '530': 5, 'truly': 5, 'scared': 5, 'cabin': 5, 'voda': 5, 'quoting': 5, 'ec2a': 5, 'laid': 5, 'locations': 5, 'rooms': 5, 'begin': 5, 'shirt': 5, 'menu': 5, 'hop': 5, 'discuss': 5, 'bye': 5, '9am': 5, 'transaction': 5, 'cannot': 5, 'straight': 5, 'connection': 5, 'sen': 5, 'atm': 5, 'romantic': 5, '2optout': 5, 'flirt': 5, 'sam': 5, 'argument': 5, 'wins': 5, 'fix': 5, 'singles': 5, 'rays': 5, 'bf': 5, '21': 5, 'themob': 5, 'selection': 5, 'aren': 5, 'pongal': 5, 'december': 5, 'ppl': 5, 'cud': 5, 'report': 5, 'surfing': 5, 'num': 5, 'basically': 5, 'allah': 5, 'sonyericsson': 5, 'geeee': 5, 'sighs': 5, 'brings': 5, 'guide': 5, 'intro': 5, 'current': 5, 'pictures': 5, 'none': 5, 'yan': 5, 'jiu': 5, 'logo': 5, 'pobox36504w45wq': 5, 'contacted': 5, 'hostel': 5, 'hv': 5, 'amt': 5, 'respond': 5, 'ticket': 5, 'dollars': 5, 'acc': 5, 'woman': 5, 'flat': 5, 'sec': 5, 'conditions': 5, 'fighting': 5, 'some1': 5, 'spl': 5, 'stylish': 5, '83355': 5, 'returns': 5, 'quote': 5, 'english': 5, 'btw': 5, '2mrw': 5, 'smiles': 5, 'jazz': 5, 'yogasana': 5, '1x150p': 5, 'stopped': 5, 'somethin': 5, 'results': 5, 'euro2004': 5, 'drinks': 5, '80062': 5, 'thursday': 5, 'listening': 5, 'cartoon': 5, 'drug': 5, 'fetch': 5, 'belly': 5, 'lonely': 5, 'law': 5, 'gap': 5, 'timing': 5, 'running': 5, 'mad': 5, 'twice': 5, 'opportunity': 5, 'gals': 5, 'city': 5, 'tis': 5, 'living': 5, 'polyphonic': 5, 'xxxx': 5, 'comuk': 5, 'ages': 5, 'sura': 5, 'playing': 5, 'matter': 5, 'sn': 5, 'hai': 5, 'records': 5, 'cds': 5, 'birds': 5, 'travel': 5, 'lead': 5, 'unsold': 5, 'derek': 5, 'greet': 5, 'white': 5, 'cheaper': 5, 'ym': 5, 'pissed': 5, 'ma': 5, 'wear': 5, 'expensive': 5, 'photos': 5, 'site': 5, 'boring': 5, 'ad': 5, 'salary': 5, 'noline': 5, 'dload': 5, 'videochat': 5, 'videophones': 5, 'rentl': 5, 'java': 5, 'dropped': 5, 'yun': 5, 'jesus': 5, 'gm': 5, '3rd': 5, 'bitch': 5, 'revealed': 5, 'xchat': 5, 'hands': 5, 'receipt': 5, 'interesting': 5, 'uni': 5, 'italian': 5, 'adult': 5, 'oz': 5, 'horrible': 5, 'nw': 5, 'jordan': 5, 'choice': 5, 'mite': 5, 'chinese': 5, 'hun': 5, 'cbe': 5, 'broke': 5, 'original': 5, 'pple': 5, 'arrested': 5, 'linerental': 5, 'vote': 5, 'tells': 5, 'totally': 5, 'rem': 5, 'exams': 5, 'everybody': 5, 'optout': 5, 'google': 5, 'vomit': 5, 'centre': 5, 'airport': 5, 'costs': 5, 'buzz': 5, 'eerie': 5, 'waking': 5, 'ran': 5, '60p': 5, 'boys': 5, 'bin': 5, 'social': 5, 'buns': 5, 'created': 5, 'beer': 5, 'season': 5, 'nvm': 5, 'moms': 5, 'obviously': 5, 'flag': 5, 'eng': 5, 'inclusive': 5, 'looked': 5, 'expecting': 5, 'latr': 5, 'minuts': 5, 'unable': 5, 'remind': 5, 'whether': 5, 'fantasy': 5, 'brilliant': 5, 'police': 5, 'ru': 5, 'cars': 5, 'plenty': 5, 'amount': 5, 'advice': 5, 'behind': 5, 'amazing': 5, 'issues': 5, 'ignore': 5, 'thurs': 5, 'wouldn': 5, 'lik': 5, 'asks': 5, '300': 5, '3510i': 5, '400': 5, 'mths': 5, 'common': 5, 'oni': 4, '87121': 4, 'tkts': 4, 'lives': 4, 'tb': 4, 'oru': 4, 'six': 4, '87575': 4, 'membership': 4, 'str': 4, 'sooner': 4, 'turn': 4, 'child': 4, 'letter': 4, 'inches': 4, 'weak': 4, 'seemed': 4, 'url': 4, 'series': 4, 'iq': 4, 'wah': 4, 'machan': 4, 'becoz': 4, '9pm': 4, 'fml': 4, 'appointment': 4, 'hols': 4, 'legal': 4, 'nyc': 4, 'considering': 4, 'jokes': 4, 'research': 4, 'tt': 4, 'needed': 4, '786': 4, 'unredeemed': 4, 'hasn': 4, 'yetunde': 4, 'ansr': 4, 'tyrone': 4, 'largest': 4, 'befor': 4, 'activities': 4, 'biggest': 4, 'netcollex': 4, 'deleted': 4, 'interview': 4, '434': 4, 'escape': 4, 'bloody': 4, 'anyways': 4, '4742': 4, '145': 4, '0808': 4, '11pm': 4, 'radio': 4, 'unique': 4, 'settled': 4, 'shoot': 4, 'files': 4, 'career': 4, 'cross': 4, 'recd': 4, 'closer': 4, 'argue': 4, 'theory': 4, 'com1win150ppmx3age16': 4, 'affection': 4, 'manda': 4, 'kettoda': 4, 'bcums': 4, 'expect': 4, 'mmm': 4, 'bay': 4, 'hmv': 4, 'passed': 4, 'throw': 4, 'cam': 4, 'accidentally': 4, 'cry': 4, 'def': 4, 'meal': 4, 'dates': 4, 'hanging': 4, 'belovd': 4, 'enemy': 4, 'smart': 4, 'afraid': 4, '08002986906': 4, 'kisses': 4, 'waitin': 4, '83600': 4, '1000s': 4, 'practice': 4, 'wtf': 4, 'further': 4, 'sometime': 4, 'cream': 4, 'tree': 4, 'esplanade': 4, 'fifteen': 4, '3mins': 4, 'wc1n3xx': 4, 'journey': 4, 'jen': 4, 'gorgeous': 4, 'purpose': 4, 'tenants': 4, 'refused': 4, 'ure': 4, 'intelligent': 4, 'result': 4, 'reasons': 4, 'receiving': 4, 'cw25wx': 4, 'tcs': 4, 'charges': 4, 'dry': 4, 'center': 4, 'village': 4, 'bringing': 4, 'jada': 4, 'matured': 4, 'kusruthi': 4, 'prabha': 4, 'mtmsgrcvd18': 4, 'bday': 4, 'rude': 4, 'mas': 4, 'passionate': 4, 'confidence': 4, 'losing': 4, 'three': 4, 'milk': 4, 'essential': 4, 'lab': 4, 'quit': 4, '08715705022': 4, '24': 4, 'grand': 4, '542': 4, 'pie': 4, 'answers': 4, 'often': 4, 'uncles': 4, 'leona': 4, 'bud': 4, 'taken': 4, 'church': 4, 'temple': 4, 'gentle': 4, 'hella': 4, 'bet': 4, 'envelope': 4, 'prepare': 4, 'seem': 4, 'explain': 4, 'purchase': 4, 'weird': 4, 'drivin': 4, 'students': 4, 'height': 4, 'max': 4, 'assume': 4, '81151': 4, '4t': 4, 'faster': 4, 'spoken': 4, 'mt': 4, 'skilgme': 4, '88039': 4, 'meetin': 4, 'apparently': 4, 'smokes': 4, 'sing': 4, 'perfect': 4, '08718727870': 4, 'enjoyed': 4, 'dictionary': 4, 'm263uz': 4, 'appt': 4, '3d': 4, 'ain': 4, 'ache': 4, '3qxj9': 4, '08702840625': 4, '9ae': 4, 'profit': 4, 'cust': 4, 'ppm': 4, 'ibiza': 4, 'meanwhile': 4, 'suite': 4, 'version': 4, 'careful': 4, 'spk': 4, 'saved': 4, 'played': 4, 'wanting': 4, 'pig': 4, 'attend': 4, 'term': 4, 'fever': 4, 'places': 4, 'w1': 4, 'carefully': 4, 'gravity': 4, 'bowl': 4, 'decision': 4, 'sore': 4, 'regret': 4, 'throat': 4, 'lecture': 4, 'raise': 4, 'fool': 4, 'june': 4, 'technical': 4, 'bathing': 4, 'vijay': 4, 'dem': 4, 'fight': 4, 'subscriber': 4, 'aiyar': 4, 'wearing': 4, 'shame': 4, 'credited': 4, 'understanding': 4, 'delivered': 4, 'arms': 4, 'easier': 4, 'txtin': 4, '4info': 4, '08712405020': 4, 'songs': 4, 'exact': 4, '2moro': 4, '80488': 4, 'favour': 4, '3gbp': 4, 'idiot': 4, 'february': 4, 'rush': 4, 'blackberry': 4, 'moji': 4, 'fill': 4, 'gently': 4, '4get': 4, 'msgrcvdhg': 4, 'aiya': 4, 'pod': 4, '7th': 4, '6th': 4, '5th': 4, 'wonders': 4, 'personality': 4, 'purity': 4, 'aint': 4, 'sha': 4, 'total': 4, 'along': 4, 'file': 4, 'shortly': 4, 'ron': 4, '7250i': 4, 'w1jhl': 4, 'preferably': 4, 'idk': 4, 'whom': 4, 'laughing': 4, 'title': 4, 'brought': 4, 'surprised': 4, 'comedy': 4, 'moby': 4, 'action': 4, 'sight': 4, 'remain': 4, 'received': 4, 'ordered': 4, 'rd': 4, 'queen': 4, 'fren': 4, 'connect': 4, 'hook': 4, '05': 4, 'schedule': 4, 'selling': 4, 'settings': 4, 'alert': 4, 'atlanta': 4, 'gaps': 4, 'fills': 4, 'takin': 4, 'answering': 4, 'jess': 4, 'dirty': 4, 'package': 4, '08001950382': 4, 'upto': 4, 'hate': 4, 'skype': 4, 'nearly': 4, 'masters': 4, 'cook': 4, 'cleaning': 4, 'cat': 4, 'hip': 4, '87239': 4, 'freefone': 4, 'lie': 4, 'infernal': 4, 'giv': 4, '84199': 4, 'w111wx': 4, 'yer': 4, 'box39822': 4, 'subs': 4, 'med': 4, 'kidz': 4, 'ntwk': 4, 'frndship': 4, 'freak': 4, 'ref': 4, 'empty': 4, 'wkend': 4, 'letters': 4, 'football': 4, 'happend': 4, 'sugar': 4, 'thangam': 4, 'roger': 4, 'solve': 4, 'cooking': 4, 'key': 4, 'released': 4, 'deliver': 4, 'spending': 4, 'sept': 4, 'public': 4, 'instituitions': 4, 'govt': 4, 'dare': 4, 'teeth': 4, 'iz': 4, 'handle': 4, 'note': 4, 'celebrate': 4, 'tm': 4, 'abi': 4, 'hill': 4, 'relation': 4, 'fromm': 4, '09061221066': 4, 'wylie': 4, 'basic': 4, 'outta': 4, 'inform': 4, 'texted': 4, '26': 4, 'doc': 4, 'taunton': 4, 'loss': 4, '2005': 3, '21st': 3, 'nurungu': 3, 'vettam': 3, 'melle': 3, 'minnaminunginte': 3, 'spell': 3, 'wales': 3, 'scotland': 3, 'clear': 3, 'caught': 3, 'fear': 3, 'xuhui': 3, 'invite': 3, 'yummy': 3, 'fair': 3, 'runs': 3, 'embarassed': 3, 'realized': 3, 'matrix3': 3, '09061209465': 3, 'starwars3': 3, 'suprman': 3, 'roommates': 3, 'dresser': 3, '1500': 3, 'advise': 3, 'recent': 3, 'valuable': 3, 'plane': 3, 'gentleman': 3, 'dignity': 3, 'shy': 3, 'coins': 3, 'requests': 3, 'sheets': 3, 'sum1': 3, 'lido': 3, 'collected': 3, 'verify': 3, 'mix': 3, 'four': 3, 'boston': 3, 'vava': 3, 'loud': 3, 'k52': 3, 'sentence': 3, 'wa': 3, 'anythin': 3, '45239': 3, 'apologise': 3, 'hardcore': 3, 'dot': 3, 'staff': 3, 'female': 3, 'birla': 3, 'soft': 3, 'floor': 3, 'spanish': 3, 'mall': 3, 'maneesha': 3, 'toll': 3, 'satisfied': 3, 'mummy': 3, 'finishes': 3, 'august': 3, 'suggest': 3, 'successfully': 3, 'register': 3, 'mtmsg18': 3, '087187262701': 3, '89545': 3, '50gbp': 3, 'followed': 3, 'pence': 3, 'loses': 3, 'tomarrow': 3, 'avent': 3, 'touched': 3, 'slippers': 3, 'innings': 3, 'bat': 3, 'dearly': 3, '125gift': 3, 'ranjith': 3, '5min': 3, 'networks': 3, 'mini': 3, 'parked': 3, 'jealous': 3, 'flash': 3, 'sorting': 3, '100percent': 3, 'genuine': 3, 'handed': 3, 'gautham': 3, 'buzy': 3, 'upgrade': 3, '0845': 3, 'tease': 3, 'scary': 3, 'gossip': 3, 'fit': 3, 'newest': 3, 'keys': 3, 'garage': 3, 'bstfrnd': 3, 'lifpartnr': 3, 'jstfrnd': 3, 'dear1': 3, 'best1': 3, 'clos1': 3, 'lvblefrnd': 3, 'cutefrnd': 3, 'swtheart': 3, '3uz': 3, 'm26': 3, 'gona': 3, 'flight': 3, 'women': 3, 'record': 3, 'germany': 3, 'supervisor': 3, 'lifetime': 3, 'bless': 3, 'favourite': 3, 'stranger': 3, 'gudnite': 3, 'slap': 3, 'alcohol': 3, 'remembered': 3, 'insha': 3, 'insurance': 3, 'alive': 3, 'gbp': 3, 'ptbo': 3, 'tests': 3, '6months': 3, '08000938767': 3, '4mths': 3, 'or2stoptxt': 3, 'mobilesdirect': 3, 'shut': 3, 'period': 3, 'business': 3, 'picture': 3, 'quickly': 3, 'nd': 3, '87131': 3, 'chechi': 3, 'sender': 3, 'skip': 3, 'names': 3, 'irritating': 3, 'ful': 3, 'lacs': 3, 'bmw': 3, 'arng': 3, 'shortage': 3, 'urgently': 3, 'source': 3, 'iouri': 3, 'sachin': 3, 'oic': 3, 'transfer': 3, '1956669': 3, 'homeowners': 3, 'previously': 3, '75': 3, 'si': 3, 'july': 3, '0207': 3, 'railway': 3, 'doggy': 3, 'fave': 3, 'roads': 3, 'dave': 3, 'transfered': 3, 'banks': 3, '9ja': 3, '9t': 3, 'wise': 3, 'boye': 3, 'dificult': 3, 'fightng': 3, 'fish': 3, '123': 3, '1450': 3, 'fees': 3, 'sory': 3, 'soryda': 3, 'ldnw15h': 3, 'ibhltd': 3, 'cha': 3, 'mono': 3, 'booking': 3, 'behave': 3, 'elsewhere': 3, '09': 3, 'box95qu': 3, '0871': 3, '08717898035': 3, 'malaria': 3, 'ummmmmaah': 3, 'tirupur': 3, 'bloke': 3, 'cock': 3, 'generally': 3, 'likely': 3, 'callin': 3, 'american': 3, 'dick': 3, 'headache': 3, '80878': 3, 'lines': 3, 'exhausted': 3, 'swimming': 3, '2morow': 3, 'paris': 3, 'nichols': 3, '83222': 3, 'market': 3, 'pop': 3, 'postcode': 3, 'seven': 3, 'tlp': 3, 'thanksgiving': 3, '31': 3, 'en': 3, 'peace': 3, '89555': 3, 'textoperator': 3, 'map': 3, 'building': 3, 'accordingly': 3, 'farm': 3, 'ws': 3, 'stress': 3, 'csbcm4235wc1n3xx': 3, 'upset': 3, 'low': 3, 'shouted': 3, 'shorter': 3, 'subscribed': 3, 'realize': 3, 'gimme': 3, '50perwksub': 3, 'tscs087147403231winawk': 3, 'anywhere': 3, 'diff': 3, 'community': 3, 'subpoly': 3, '81618': 3, 'bein': 3, 'jan': 3, 'pieces': 3, 'bid': 3, 'responding': 3, '2u': 3, 'cm2': 3, '220': 3, '08701417012': 3, 'charity': 3, 'alfie': 3, 'm8s': 3, 'nokias': 3, 'brain': 3, 'hahaha': 3, 'given': 3, 'successful': 3, '2morrow': 3, 'sk3': 3, '8wp': 3, 'seconds': 3, 'xavier': 3, 'stomach': 3, 'returned': 3, 'vip': 3, 'supply': 3, 'nap': 3, 'cuddle': 3, 'shesil': 3, '10k': 3, 'liverpool': 3, 'reminder': 3, 'failed': 3, 'outstanding': 3, 'taylor': 3, 'male': 3, '5p': 3, 'msging': 3, 'diet': 3, '88600': 3, 'moments': 3, '114': 3, '14': 3, 'tcr': 3, 'magical': 3, 'welp': 3, 'valid12hrs': 3, '15': 3, 'chicken': 3, 'potential': 3, 'talent': 3, '09063458130': 3, 'polyph': 3, 'fuckin': 3, 'butt': 3, 'terrible': 3, 'prey': 3, 'fancies': 3, 'foreign': 3, 'stamps': 3, 'speechless': 3, 'roast': 3, 'concentrate': 3, 'chatting': 3, 'walked': 3, 'euro': 3, 'drunk': 3, '84025': 3, 'networking': 3, 'juicy': 3, 'dearer': 3, 'itz': 3, 'alwys': 3, 'evn': 3, 'clock': 3, '09061790121': 3, 'ne': 3, 'ground': 3, 'speed': 3, 'catching': 3, 'falls': 3, 'whos': 3, 'le': 3, 'bigger': 3, 'islands': 3, 'celeb': 3, 'pocketbabe': 3, 'voicemail': 3, '2go': 3, 'walmart': 3, 'score': 3, '87021': 3, 'apps': 3, 'anti': 3, 'rofl': 3, 'ph': 3, 'various': 3, 'textcomp': 3, '84128': 3, 'morn': 3, 'docs': 3, 'havin': 3, 'rang': 3, 'sorted': 3, 'executive': 3, 'jane': 3, 'express': 3, 'fran': 3, 'knackered': 3, 'software': 3, 'jamster': 3, 'among': 3, 'cares': 3, '6hrs': 3, 'chill': 3, 'chillin': 3, 'saucy': 3, 'chain': 3, 'suntec': 3, 'messenger': 3, 'screen': 3, 'upload': 3, 'tom': 3, 'shot': 3, 'wt': 3, 'storming': 3, 'invnted': 3, 'margaret': 3, 'telphone': 3, 'phne': 3, 'girlfrnd': 3, 'grahmbell': 3, 'popped': 3, 'shld': 3, 'beware': 3, 'caring': 3, 'option': 3, 'goodnite': 3, 'painful': 3, 'guilty': 3, 'cardiff': 3, 'addie': 3, 'bright': 3, 'certainly': 3, 'twelve': 3, 'aah': 3, '07xxxxxxxxx': 3, 'hubby': 3, 'minmobsmorelkpobox177hp51fl': 3, 'blake': 3, 'karaoke': 3, 'stars': 3, 'eight': 3, 'ese': 3, 'prospects': 3, 'bishan': 3, 'buff': 3, 'gang': 3, 'tablets': 3, 'finishing': 3, 'doors': 3, 'chasing': 3, 'brothas': 3, 'force': 3, 'blame': 3, 'blessings': 3, 'freezing': 3, 'winning': 3, '6pm': 3, 'titles': 3, 'feelin': 3, 'switch': 3, 'monthly': 3, 'ideas': 3, 'maintain': 3, 'sh': 3, 'cramps': 3, 'nan': 3, '81303': 3, 'dislikes': 3, 'likes': 3, 'album': 3, '121': 3, 'standing': 3, 'james': 3, '29': 3, 'chosen': 3, 'di': 3, 'cruise': 3, 'follow': 3, 'stuck': 3, 'regarding': 3, 'adore': 3, 'arcade': 3, 'arun': 3, 'philosophy': 3, 'eye': 3, 'husband': 3, 'norm': 3, 'toa': 3, 'payoh': 3, 'fathima': 3, 'mmmm': 3, '18yrs': 3, 'abta': 3, '80182': 3, '08452810073': 3, 'table': 3, 'ikea': 3, 'cn': 3, 'kadeem': 3, 'wud': 3, 'carry': 3, 'avatar': 3, 'stops': 3, 'constantly': 3, 'lousy': 3, 'ic': 3, 'sweetest': 3, 'honeybee': 3, 'laughed': 3, 'havnt': 3, 'crack': 3, 'boat': 3, 'proof': 3, 'provided': 3, 'yeh': 3, 'members': 3, 'downloads': 3, 'major': 3, 'birth': 3, 'rule': 3, 'natural': 3, 'onwards': 3, '150ppermesssubscription': 3, 'skillgame': 3, 'tscs': 3, '1winaweek': 3, 'eggs': 3, 'boost': 3, 'calicut': 3, 'box97n7qp': 3, 'pink': 3, 'normally': 3, 'rich': 3, 'm8': 3, 'yor': 3, 'jason': 3, 'art': 3, 'feet': 3, 'argh': 3, 'favor': 3, 'tessy': 3, 'shijas': 3, 'aunty': 3, 'china': 3, 'morphine': 3, 'prefer': 3, 'kindly': 3, 'pages': 3, 'pending': 3, 'raji': 3, 'legs': 3, 'distance': 3, 'temp': 3, 'display': 3, 'soup': 3, 'management': 3, 'include': 3, 'regular': 3, 'threats': 3, 'lounge': 3, 'u4': 3, 'cheer': 3, 'cornwall': 3, 'bags': 3, 'iscoming': 3, '80082': 3, 'spook': 3, 'halloween': 3, 'issue': 3, 'sky': 3, 'measure': 3, 'thm': 3, 'instantly': 3, 'drinking': 3, 'wn': 3, 'impossible': 3, 'responce': 3, 'vodka': 3, 'okey': 3, 'neighbour': 3, 'questioned': 3, 'gardener': 3, 'vegetables': 3, 'science': 3, 'madam': 3, 'settle': 3, 'citizen': 3, 'indians': 3, 'sry': 3, '09066612661': 3, 'greetings': 3, 'dai': 3, 'maga': 3, 'medicine': 3, 'violence': 3, 'incident': 3, 'erm': 3, 'instructions': 3, '3lp': 3, 'death': 3, 'hon': 3, 'reality': 3, 'usc': 3, 'booty': 3, 'lil': 3, 'remains': 3, 'bros': 3, 'bro': 3, 'response': 3, 'shirts': 3, 'petrol': 3, 'uks': 3, '2stoptxt': 3, 'luxury': 3, 'ben': 3, 'middle': 3, 'dark': 3, 'enuff': 3, 'strike': 3, 'moved': 3, 'porn': 3, 'dress': 3, 'collecting': 3, 'flaked': 3, 'gary': 3, 'history': 3, 'bell': 3, 'understood': 3, 'bottom': 3, '33': 3, 'books': 3, 'prove': 3, 'blow': 3, 'knowing': 3, 'challenge': 3, 'randomly': 3, 'tape': 3, 'films': 3, 'lick': 3, 'auto': 3, 'praying': 3, 'hug': 3, 'deliveredtomorrow': 3, 'smoking': 3, 'in2': 3, 'billed': 3, 'ths': 3, 'callback': 3, 'wedding': 3, 'accident': 3, 'wisdom': 3, 'cann': 3, 'symbol': 3, 'prolly': 3, 'confirmed': 3, 'dubsack': 3, 'macho': 3, 'audition': 3, 'fell': 3, 'senthil': 3, 'forevr': 3, 'eaten': 3, 'nat': 3, 'possession': 3, 'concert': 3, 'born': 3, 'affairs': 3, 'california': 3, 'university': 3, 'value': 3, 'mnth': 3, 'tog': 3, 'haiz': 3, 'previous': 3, 'parking': 3, 'wallpaper': 3, 'step': 3, 'buffet': 2, 'fa': 2, '08452810075over18': 2, 'hor': 2, 'rcv': 2, 'kl341': 2, 'receivea': 2, '09061701461': 2, '08002986030': 2, 'tsandcs': 2, 'csh11': 2, '6days': 2, 'chances': 2, '4403ldnw1a7rw18': 2, 'jackpot': 2, 'dbuk': 2, 'lccltd': 2, '81010': 2, 'blessing': 2, 'goals': 2, '4txt': 2, 'slice': 2, 'convincing': 2, 'frying': 2, 'sarcastic': 2, '8am': 2, 'mmmmmm': 2, 'burns': 2, 'hospitals': 2, 'gram': 2, 'eighth': 2, 'detroit': 2, 'hockey': 2, 'odi': 2, 'killing': 2, 'burger': 2, '09066364589': 2, 'dedicated': 2, 'dedicate': 2, 'eurodisinc': 2, 'shracomorsglsuplt': 2, 'entry41': 2, '3aj': 2, 'trav': 2, 'ls1': 2, 'aco': 2, 'morefrmmob': 2, 'divorce': 2, 'earn': 2, 'jacket': 2, 'nitros': 2, 'ela': 2, 'pours': 2, '169': 2, '6031': 2, '92h': 2, 'usher': 2, 'britney': 2, '450ppw': 2, '5249': 2, 'mk17': 2, '85069': 2, 'telugu': 2, 'loans': 2, 'animation': 2, 'location': 2, 'noun': 2, 'gent': 2, '09064012160': 2, 'puttin': 2, 'goodo': 2, 'potato': 2, 'tortilla': 2, '08719180248': 2, '07742676969': 2, 'sum': 2, 'algarve': 2, '69888': 2, '31p': 2, 'msn': 2, 'pouch': 2, 'hearts': 2, 'somtimes': 2, 'occupy': 2, '08700621170150p': 2, 'randy': 2, 'flowing': 2, 'plaza': 2, 'everywhere': 2, 'windows': 2, 'mouth': 2, '0871277810810': 2, 'module': 2, 'avoid': 2, 'beloved': 2, 'clark': 2, 'form': 2, 'utter': 2, 'completed': 2, 'stays': 2, 'wishin': 2, 'hamster': 2, 'keralacircle': 2, 'inr': 2, 'refilled': 2, 'kr': 2, 'prepaid': 2, 'ericsson': 2, 'bruv': 2, 'rewarding': 2, 'heading': 2, 'installing': 2, 'repair': 2, 'star': 2, 'teacher': 2, 'teaches': 2, 'upstairs': 2, 'printed': 2, '09058094597': 2, '447801259231': 2, 'signing': 2, 'shining': 2, 'although': 2, 'commercial': 2, 'drpd': 2, 'deepak': 2, 'deeraj': 2, '2wks': 2, 'lag': 2, 'shipping': 2, 'headin': 2, 'necessarily': 2, 'jolt': 2, 'suzy': 2, 'mk45': 2, '2wt': 2, 'chart': 2, 'gf': 2, 'tool': 2, 'jenny': 2, '021': 2, '3680': 2, 'grave': 2, 'taxi': 2, 'crash': 2, 'shocking': 2, 'actor': 2, 'hide': 2, 'thread': 2, '82468': 2, 'funky': 2, 'anot': 2, 'lo': 2, 'tahan': 2, 'buses': 2, 'bristol': 2, 'apo': 2, '861': 2, '85': 2, 'prepayment': 2, '0844': 2, 'paperwork': 2, 'violated': 2, 'privacy': 2, 'caroline': 2, 'cleared': 2, 'misbehaved': 2, 'tissco': 2, 'tayseer': 2, 'audrey': 2, 'status': 2, 'breathe': 2, 'update_now': 2, 'cuddling': 2, 'agree': 2, 'recognise': 2, 'hes': 2, 'ovulation': 2, 'n9dx': 2, 'licks': 2, '30ish': 2, 'grace': 2, 'inshah': 2, 'sharing': 2, 'salam': 2, 'field': 2, 'shipped': 2, 'burning': 2, 'loxahatchee': 2, 'wld': 2, 'darlings': 2, 'fav': 2, 'slightly': 2, 'box334sk38ch': 2, 'whatsup': 2, 'goal': 2, '80086': 2, 'txttowin': 2, 'mobno': 2, 'ads': 2, 'name1': 2, 'adam': 2, 'name2': 2, '07123456789': 2, 'txtno': 2, 'siva': 2, 'expression': 2, 'speaking': 2, '3650': 2, '09066382422': 2, 'bcm4284': 2, '300603': 2, 'applebees': 2, 'cricketer': 2, 'bhaji': 2, 'improve': 2, 'oreo': 2, 'truffles': 2, 'amy': 2, 'coping': 2, 'decisions': 2, 'individual': 2, '26th': 2, '153': 2, 'position': 2, 'language': 2, '09061743806': 2, 'box326': 2, 'screamed': 2, 'removed': 2, 'infront': 2, 'broken': 2, 'tension': 2, 'taste': 2, '07781482378': 2, 'trade': 2, '7ish': 2, 'rec': 2, 'b4280703': 2, '08718727868': 2, '09050002311': 2, 'hyde': 2, 'anthony': 2, 'scrounge': 2, 'forgiven': 2, 'slide': 2, 'renewal': 2, 'transport': 2, 'definite': 2, 'nos': 2, 'ebay': 2, 'pickle': 2, 'tacos': 2, '872': 2, '24hrs': 2, 'pg': 2, 'channel': 2, '08718738001': 2, 'web': 2, '2stop': 2, 'ability': 2, 'develop': 2, 'recovery': 2, 'cali': 2, 'cutting': 2, 'reminding': 2, 'owns': 2, 'faggy': 2, 'demand': 2, 'fo': 2, 'loose': 2, 'perhaps': 2, 'mei': 2, 'geeeee': 2, 'ey': 2, 'oooh': 2, 'call09050000327': 2, 'claims': 2, 'dancing': 2, 'snake': 2, 'bite': 2, 'hardly': 2, 'promo': 2, 'ag': 2, '08712402050': 2, '10ppm': 2, '0825': 2, 'tsunamis': 2, 'soiree': 2, '22': 2, 'ques': 2, 'suits': 2, 'reaction': 2, 'shock': 2, 'grow': 2, 'useful': 2, 'officially': 2, '89693': 2, 'textbuddy': 2, 'gaytextbuddy': 2, '09064019014': 2, '4882': 2, 'hundred': 2, 'expressoffer': 2, 'sweetheart': 2, 'biola': 2, 'effects': 2, 'wee': 2, 'trains': 2, 'ham': 2, 'jolly': 2, '40533': 2, 'sw7': 2, '3ss': 2, 'rstm': 2, 'panic': 2, 'dealer': 2, 'impatient': 2, 'river': 2, 'premium': 2, 'lays': 2, 'posts': 2, 'yelling': 2, 'hex': 2, 'cochin': 2, '4d': 2, 'poop': 2, 'gpu': 2, 'hurried': 2, 'aeroplane': 2, 'calld': 2, 'professors': 2, 'wer': 2, 'aeronautics': 2, 'datz': 2, 'dorm': 2, 'mobilesvary': 2, '050703': 2, 'callcost': 2, '1250': 2, '09071512433': 2, 'cookies': 2, 'correction': 2, 'admit': 2, 'ba': 2, 'spring': 2, 'mtmsg': 2, 'ctxt': 2, 'nokia6650': 2, 'attached': 2, '930': 2, 'helpline': 2, '08706091795': 2, 'gist': 2, 'thousands': 2, '40': 2, 'premier': 2, 'lip': 2, 'confused': 2, 'spare': 2, 'faith': 2, 'acting': 2, 'schools': 2, 'inch': 2, 'begging': 2, '0578': 2, 'opening': 2, 'pole': 2, 'thot': 2, 'petey': 2, 'nic': 2, '8077': 2, 'cashto': 2, 'getstop': 2, '88222': 2, 'php': 2, '08000407165': 2, 'imp': 2, 'bec': 2, 'nervous': 2, 'hint': 2, 'borrow': 2, 'dobby': 2, 'galileo': 2, 'enjoyin': 2, 'loveme': 2, 'cappuccino': 2, 'mojibiola': 2, '07821230901': 2, '09065174042': 2, 'hol': 2, 'kz': 2, 'aburo': 2, 'ultimatum': 2, 'countin': 2, 'skyped': 2, '08002888812': 2, 'inconsiderate': 2, 'hence': 2, 'recession': 2, 'nag': 2, 'soo': 2, '09066350750': 2, 'warning': 2, 'shoes': 2, 'lovejen': 2, 'discreet': 2, 'worlds': 2, 'named': 2, 'genius': 2, 'connections': 2, 'lotta': 2, 'lately': 2, 'virgin': 2, 'mystery': 2, 'approx': 2, 'smsco': 2, 'peaceful': 2, 'consider': 2, 'walls': 2, '41685': 2, '07': 2, 'fixedline': 2, '5k': 2, '09064011000': 2, 'cr01327bt': 2, 'castor': 2, '09058094565': 2, '08': 2, 'stopsms': 2, '09065171142': 2, 'downloaded': 2, 'ear': 2, 'oil': 2, 'usb': 2, 'mac': 2, 'gibbs': 2, 'unbelievable': 2, 'superb': 2, 'several': 2, 'worst': 2, 'charles': 2, 'stores': 2, 'peak': 2, '08709222922': 2, '8p': 2, 'sweets': 2, 'chip': 2, 'addicted': 2, 'yck': 2, 'ashley': 2, 'lux': 2, 'jeans': 2, 'tons': 2, 'scores': 2, 'application': 2, 'ms': 2, 'filthy': 2, 'simpler': 2, '09050001808': 2, 'm95': 2, 'necklace': 2, 'rice': 2, 'racing': 2, 'closes': 2, 'crap': 2, 'borin': 2, 'chocolate': 2, 'reckon': 2, 'tech': 2, '65': 2, 'sd': 2, 'blessed': 2, 'quiet': 2, 'aunts': 2, 'helen': 2, 'fan': 2, 'lovers': 2, 'drove': 2, 'exe': 2, 'pen': 2, 'anniversary': 2, 'datebox1282essexcm61xn': 2, 'secretly': 2, 'pattern': 2, 'plm': 2, 'sheffield': 2, 'zoe': 2, 'setting': 2, 'filling': 2, 'sufficient': 2, 'thx': 2, 'gnt': 2, 'rightly': 2, 'viva': 2, 'edison': 2, 'ls15hb': 2, 'educational': 2, 'flirting': 2, 'kickoff': 2, 'sells': 2, 'thesis': 2, 'sends': 2, 'deciding': 2, 'herself': 2, 'compare': 2, 'eastenders': 2, 'tulip': 2, 'wkent': 2, 'violet': 2, '150p16': 2, 'lily': 2, 'prepared': 2, 'm6': 2, '09058091854': 2, 'box385': 2, '6wu': 2, '09050003091': 2, 'c52': 2, 'oi': 2, 'craziest': 2, 'curry': 2, 'thoughts': 2, 'singing': 2, 'breath': 2, 'planet': 2, '28days': 2, '2yr': 2, 'm221bp': 2, 'box177': 2, '09061221061': 2, '99': 2, 'warranty': 2, 'tomorro': 2, 'fret': 2, 'wind': 2, 'depressed': 2, 'math': 2, 'dhoni': 2, 'rocks': 2, 'durban': 2, '08000776320': 2, 'survey': 2, 'difficulties': 2, 'sar': 2, 'tank': 2, 'silently': 2, 'drms': 2, 'itcould': 2, 'wrc': 2, 'rally': 2, 'lucozade': 2, '61200': 2, 'packs': 2, 'toot': 2, 'annoying': 2, 'makin': 2, 'popcorn': 2, 'beneficiary': 2, 'neft': 2, 'subs16': 2, '1win150ppmx3': 2, 'appreciated': 2, 'apart': 2, 'creepy': 2, '08719181513': 2, 'nok': 2, 'invest': 2, 'delay': 2, '1hr': 2, 'purse': 2, 'europe': 2, 'flip': 2, 'accounts': 2, 'jd': 2, 'weirdest': 2, 'l8tr': 2, 'minmoremobsemspobox45po139wa': 2, 'tee': 2, 'control': 2, 'dough': 2, 'irritates': 2, 'fails': 2, 'jerry': 2, 'drinkin': 2, '5pm': 2, 'birthdate': 2, 'nydc': 2, 'ola': 2, 'items': 2, 'garbage': 2, 'logos': 2, 'gold': 2, 'lionp': 2, 'lionm': 2, 'lions': 2, 'jokin': 2, 'colours': 2, 'whenevr': 2, 'remembr': 2, 'harry': 2, 'potter': 2, 'phoenix': 2, 'readers': 2, 'canada': 2, 'goodnoon': 2, 'patty': 2, 'interest': 2, 'george': 2, '89080': 2, 'free2day': 2, '0870241182716': 2, 'theres': 2, 'tmrw': 2, 'soul': 2, 'ned': 2, 'main': 2, 'hurting': 2, 'sweetie': 2, '4a': 2, 'whn': 2, 'dance': 2, 'bar': 2, '08718730666': 2, 'bears': 2, 'juan': 2, 'lf56': 2, 'tlk': 2, 'front': 2, 'ideal': 2, 'arm': 2, 'tirunelvali': 2, 'effect': 2, 'bk': 2, 'kidding': 2, 'stretch': 2, 'urn': 2, 'sinco': 2, 'disclose': 2, 'frauds': 2, 'payee': 2, 'icicibank': 2, 'kaiez': 2, 'babies': 2, 'practicing': 2, 'pale': 2, 'beneath': 2, 'silver': 2, 'silence': 2, 'revision': 2, 'exeter': 2, 'whose': 2, 'condition': 2, 'arsenal': 2, 'missin': 2, 'tues': 2, 'restaurant': 2, 'textpod': 2, 'desperate': 2, 'monkeys': 2, 'practical': 2, 'mails': 2, 'claire': 2, 'costing': 2, 'ke': 2, 'program': 2, '09066362231': 2, 'ny': 2, 'lotr': 2, 'modules': 2, 'musthu': 2, 'testing': 2, 'nit': 2, 'format': 2, 'sarcasm': 2, 'forum': 2, 'aunt': 2, 'unfortunately': 2, 'wihtuot': 2, 'exmpel': 2, 'jsut': 2, 'tihs': 2, 'waht': 2, 'yuo': 2, 'splleing': 2, 'evrey': 2, 'wrnog': 2, 'raed': 2, 'sitll': 2, 'mitsake': 2, 'rael': 2, 'gving': 2, 'ayn': 2, 'konw': 2, 'ow': 2, 'finance': 2, 'joining': 2, 'filled': 2, 'jia': 2, 'sux': 2, 'kegger': 2, 'adventure': 2, 'pack': 2, 'wifi': 2, 'rumour': 2, '7250': 2, 'boyfriend': 2, 'driver': 2, 'kicks': 2, 'falling': 2, 'smeone': 2, 'fire': 2, 'propose': 2, 'gods': 2, 'gifted': 2, 'lovingly': 2, 'tomeandsaid': 2, 'itwhichturnedinto': 2, 'dippeditinadew': 2, 'ringtoneking': 2, 'batch': 2, 'flaky': 2, 'sooooo': 2, 'tooo': 2, '09058094599': 2, 'confuses': 2, 'wating': 2, 'sw73ss': 2, 'british': 2, 'hotels': 2, 'adoring': 2, 'ghost': 2, 'dracula': 2, 'addamsfa': 2, 'munsters': 2, 'exorcist': 2, 'twilight': 2, 'cared': 2, 'constant': 2, 'allow': 2, 'hlp': 2, '2rcv': 2, '82242': 2, 'msg150p': 2, '08712317606': 2, 'fly': 2, 'event': 2, 'movietrivia': 2, '08712405022': 2, '80608': 2, 'partnership': 2, 'mostly': 2, 'mornin': 2, 'jas': 2, 'poker': 2, 'messy': 2, 'slip': 2, 'moves': 2, 'traffic': 2, 'nus': 2, 'wkg': 2, 'keeps': 2, 'gotten': 2, 'promises': 2, 'unknown': 2, 'vu': 2, 'bcm1896wc1n3xx': 2, '09094646899': 2, 'pre': 2, '2007': 2, 'indeed': 2, 'rents': 2, '48': 2, 'alaipayuthe': 2, 'maangalyam': 2, 'easter': 2, 'telephone': 2, '08081560665': 2, 'bahamas': 2, '07786200117': 2, 'callfreefone': 2, 'up4': 2, 'calm': 2, 'habit': 2, 'contacts': 2, 'forgets': 2, 'mandan': 2, 'ibh': 2, '07734396839': 2, 'nokia6600': 2, 'invaders': 2, 'orig': 2, 'console': 2, 'recharge': 2, 'transfr': 2, '82050': 2, 'prizes': 2, 'foley': 2, 'fake': 2, 'desparate': 2, '3100': 2, 'combine': 2, 'sian': 2, 'g696ga': 2, 'joanna': 2, 'replacement': 2, 'telly': 2, 'tooth': 2, '12mths': 2, 'mth': 2, 'wipro': 2, 'laundry': 2, 'underwear': 2, 'delete': 2, 'waheed': 2, 'pushes': 2, 'beyond': 2, 'avoiding': 2, '0776xxxxxxx': 2, '326': 2, 'uh': 2, 'heads': 2, 'vday': 2, 'build': 2, 'snowman': 2, 'fights': 2, 'prescription': 2, 'electricity': 2, 'fujitsu': 2, 'scold': 2, 'se': 2, 'prompts': 2, '09066358152': 2, 'disturbing': 2, 'flies': 2, 'woken': 2, 'aka': 2, 'delhi': 2, 'held': 2, 'fringe': 2, 'distract': 2, 'tones2you': 2, '61610': 2, '08712400602450p': 2, 'mel': 2, 'responsibility': 2, '08006344447': 2, 'kid': 2, 'affair': 2, 'parco': 2, 'nb': 2, 'hallaq': 2, 'bck': 2, 'color': 2, 'lyk': 2, 'gender': 2, 'sleepwell': 2, 'mca': 2, 'vomiting': 2, 'rub': 2, 'clever': 2, '113': 2, 'bray': 2, 'wicklow': 2, 'stamped': 2, 'eire': 2, 'ryan': 2, 'idew': 2, 'manage': 2, 'xam': 2, 'diamonds': 2, 'shitload': 2, 'mcat': 2, '27': 2, 'beg': 2, 'sacrifice': 2, 'stayin': 2, 'satisfy': 2, 'cld': 2, 'miles': 2, 'killed': 2, 'smashed': 2, 'ps': 2, 'tok': 2, 'specific': 2, 'figures': 2, 'cousin': 2, 'excuses': 2, 'neck': 2, 'continue': 2, 'holy': 2, 'billion': 2, 'classes': 2, 'turning': 2, 'youre': 2, 'belive': 2, 'slots': 2, 'discussed': 2, 'prem': 2, '2morro': 2, 'spoiled': 2, 'complaint': 2, 'sales': 2, 'lk': 2, 'lov': 2, 'comfort': 2, '300p': 2, '8552': 2, '2end': 2, '01223585334': 2, '2c': 2, 'shagged': 2, '88066': 2, 'bedrm': 2, '700': 2, 'waited': 2, 'huge': 2, 'mids': 2, 'upd8': 2, 'oranges': 2, 'annie': 2, 'messaging': 2, 'retrieve': 2, 'mailbox': 2, '09056242159': 2, '21870000': 2, 'hrishi': 2, 'nothin': 2, 'poem': 2, 'duchess': 2, '008704050406': 2, 'dan': 2, 'aww': 2, 'staring': 2, 'cm': 2, 'unnecessarily': 2, '08701417012150p': 2, 'weigh': 2, 'scoring': 2, 'active': 2, '250k': 2, '88088': 2, 'gamestar': 2, 'expired': 2, 'opinions': 2, 'lv': 2, 'lived': 2, 'propsd': 2, 'happily': 2, '2gthr': 2, 'gv': 2, 'speeding': 2, 'thy': 2, 'lttrs': 2, 'aproach': 2, 'threw': 2, 'evrydy': 2, 'truck': 2, 'dt': 2, 'paragon': 2, 'arent': 2, 'bluff': 2, 'sary': 2, 'piece': 2, 'scotch': 2, 'yarasu': 2, 'shampain': 2, 'brandy': 2, 'vaazhthukkal': 2, 'gin': 2, 'kudi': 2, 'dhina': 2, 'rum': 2, 'wiskey': 2, 'kg': 2, 'dumb': 2, 'dressed': 2, 'kills': 2, 'kay': 2, 'nasty': 2, 'wasted': 2, 'christ': 2, 'tears': 2, 'push': 2, 'answered': 2, 'rgds': 2, '8pm': 2, 'wrote': 2, 'rights': 2, 'jobs': 2, 'lane': 2, 'donno': 2, 'properly': 2, '630': 2, 'lock': 2, 'furniture': 2, 'shoving': 2, 'papers': 2, 'strange': 2, 'acl03530150pm': 2, 'indyarocks': 2, 'resume': 2, 'bids': 2, 'whr': 2, 'yunny': 2, '83383': 2, 'mmmmm': 2, 'relatives': 2, 'benefits': 2, 'environment': 2, 'terrific': 2, 'dr': 2, 'superior': 2, 'vid': 2, 'ruin': 2, 'conform': 2, 'department': 2, 'bc': 2, 'toshiba': 2, 'wrk': 2, 'innocent': 2, 'mental': 2, 'hoped': 2, 'bills': 2, '2marrow': 2, 'treated': 2, 'wks': 2, 'fab': 2, 'tiwary': 2, 'battle': 2, 'bang': 2, 'pap': 2, 'arts': 2, 'pandy': 2, 'edu': 2, 'secretary': 2, 'dollar': 2, 'pull': 2, 'amongst': 2, '69696': 2, 'nalla': 2, 'pouts': 2, 'stomps': 2, 'northampton': 2, 'abj': 2, 'serving': 2, 'smith': 2, 'nagar': 2, 'anna': 2, 'sports': 2, 'evr': 2, 'hugs': 2, 'neither': 2, 'snogs': 2, 'west': 2, 'growing': 2, 'fastest': 2, 'chase': 2, 'steam': 2, 'reg': 2, 'canary': 2, 'sleepy': 2, 'mag': 2, 'diwali': 2, 'tick': 2, 'onion': 2, 'thgt': 2, 'lower': 2, 'exhaust': 2, 'pee': 2, 'contents': 2, 'success': 2, 'division': 2, 'creep': 2, 'lies': 2, 'property': 2, '7876150ppm': 2, '09058099801': 2, 'b4190604': 2, 'bbd': 2, 'pimples': 2, 'yellow': 2, 'frog': 2, '88888': 2, 'doubt': 2, 'japanese': 2, 'proverb': 2, 'freedom': 2, 'seat': 2, 'twenty': 2, 'painting': 2, 'nowadays': 2, 'talks': 2, 'probs': 2, 'swatch': 2, 'ganesh': 2, 'trips': 2, 'helloooo': 2, 'welcomes': 2, '54': 2, '2geva': 2, 'wuld': 2, 'solved': 2, 'ing': 2, 'sake': 2, 'bruce': 2, 'teaching': 2, 'chest': 2, 'covers': 2, 'hang': 2, 'reboot': 2, 'pt2': 2, 'phoned': 2, 'improved': 2, 'hm': 2, 'salon': 2, 'evenings': 2, 'raj': 2, 'payment': 2, 'clearing': 2, 'shore': 2, 'range': 2, 'changes': 2, 'topic': 2, 'admin': 2, 'visionsms': 2, 'andros': 2, 'meets': 2, 'penis': 2, 'foot': 2, 'sigh': 2, 'eveb': 2, 'window': 2, 'removal': 2, '08708034412': 2, 'cancelled': 2, 'neway': 2, 'xxxxx': 2, 'count': 2, 'otside': 2, 'size': 2, '08712101358': 2, 'tight': 2, 'av': 2, 'everyday': 2, 'curious': 2, 'postcard': 2, 'bread': 2, 'mahal': 2, 'luvs': 2, 'ding': 2, 'allowed': 2, 'shared': 2, 'watever': 2, 'necessary': 2, 'messaged': 2, 'deus': 2, 'broad': 2, 'canal': 2, 'spile': 2, 'tap': 2, 'engin': 2, 'edge': 2, 'east': 2, 'howard': 2, 'cooked': 2, 'cheat': 2, 'block': 2, 'ruining': 2, 'easily': 2, 'selfish': 2, 'custom': 2, 'sac': 2, 'jiayin': 2, 'pobox45w2tg150p': 2, 'forgotten': 2, 'reverse': 2, 'cheating': 2, 'mathematics': 2, '2waxsto': 2, 'minimum': 2, 'elaine': 2, 'drunken': 2, 'mess': 2, 'crisis': 2, '____': 2, 'ias': 2, 'mb': 2, '600': 2, 'desires': 2, '1030': 2, 'careers': 2, '447797706009': 2, 'bloomberg': 2, 'priscilla': 2, 'kent': 2, 'vale': 2, '83049': 2, 'unkempt': 2, 'westlife': 2, 'unbreakable': 2, 'untamed': 2, 'wan2': 2, 'prince': 2, 'cdgt': 2, 'granite': 2, 'nasdaq': 2, 'explosive': 2, 'base': 2, 'placement': 2, 'sumthin': 2, 'lion': 2, 'devouring': 2, 'airtel': 2, 'processed': 2, '69669': 2, 'jaya': 2, 'forums': 2, 'incredible': 2, '18p': 2, 'o2fwd': 2, 'ship': 2, 'maturity': 2, 'kavalan': 2, 'causing': 2, 'blank': 2, 'tonights': 2, 'xin': 2, 'lib': 2, 'difference': 2, 'despite': 2, 'swoop': 2, 'langport': 2, 'mistakes': 2, 'lou': 2, 'pool': 2, '09065989182': 2, 'x49': 2, 'ibn': 2, 'verified': 2, 'confirmd': 2, 'terrorist': 2, 'cnn': 2, 'disconnect': 2, 'hppnss': 2, 'goodfriend': 2, 'sorrow': 2, 'stayed': 2, 'stone': 2, 'help08718728876': 2, 'increments': 2, 'mila': 2, '69866': 2, '30pp': 2, 'blonde': 2, '5free': 2, 'mtalk': 2, 'age23': 2, 'atlast': 2, 'desert': 2, 'funk': 2, 'tones2u': 2, 'funeral': 2, 'vivek': 2, 'tnc': 2, 'brah': 2, 'passwords': 2, 'sensitive': 2, 'protect': 2, 'sib': 2, 'ipad': 2, 'bird': 2, 'cheese': 2, 'widelive': 2, 'index': 2, 'tms': 2, 'wml': 2, 'hsbc': 2, 'asp': 2, '09061702893': 2, 'eek': 2, 'melt': 2, '09061743386': 2, '674': 2, 'eta': 2, '0871750': 2, '77': 2, 'landlines': 2, 'housewives': 2, 'dial': 2, '09066364311': 2, 'literally': 2, 'kothi': 2, 'sem': 2, 'student': 2, 'actual': 2, 'dealing': 2, 'reasonable': 2, 'kappa': 2, 'piss': 2, 'receipts': 2, 'guessing': 2, 'royal': 2, 'sticky': 2, 'indicate': 2, 'repeat': 2, 'calculation': 2, 'clothes': 2, 'lush': 2, '2find': 2, 'courage': 2, 'defeat': 2, 'greatest': 2, 'bear': 2, 'fucked': 2, 'beauty': 2, 'natalja': 2, '440': 2, 'nat27081980': 2, 'moving': 2, 'jogging': 2, 'shelf': 2, 'captain': 2, 'mokka': 2, 'polyh': 2, '09061744553': 2, 'bone': 2, 'steve': 2, 'epsilon': 2, 'mesages': 2, 'lst': 2, 'massive': 2, 'absolutly': 2, 'forms': 2, '373': 2, 'w1j': 2, '6hl': 2, 'polo': 2, 'coast': 2, 'suppose': 2, '02073162414': 2, 'secs': 2, 'explicit': 2, 'clearly': 2, 'gain': 2, 'realise': 2, 'mnths': 2, 'subscribe6gbp': 2, '86888': 2, '3hrs': 2, 'txtstop': 2, 'managed': 2, 'capital': 2, 'acted': 2, '09066380611': 2, 'loyal': 2, 'customers': 2, 'print': 2, 'dokey': 2, 'error': 2, 'sleepin': 2, 'minor': 2, 'woulda': 2, 'santa': 2, 'miserable': 2, 'shoppin': 2, '08718726270': 2, 'celebration': 2, 'warner': 2, 'select': 2, 'sarasota': 2, '13': 2, 'cherish': 2, 'slp': 2, 'muah': 2, '4eva': 2, 'garden': 2, 'notxt': 2, 'seeds': 2, 'bulbs': 2, 'scotsman': 2, 'go2': 2, 'replace': 2, 'reduce': 2, 'limiting': 2, 'gastroenteritis': 2, 'illness': 2, '09061213237': 2, 'm227xy': 2, '177': 2, 'respectful': 2, 'pride': 2, 'bottle': 2, 'amused': 2, 'mega': 2, 'island': 2, 'amore': 1, 'jurong': 1, 'chgs': 1, 'patent': 1, 'aids': 1, 'cried': 1, 'breather': 1, 'granted': 1, 'fulfil': 1, 'xxxmobilemovieclub': 1, 'qjkgighjjgcbl': 1, 'gota': 1, 'poboxox36504w45wq': 1, 'macedonia': 1, 'u1': 1, 'ffffffffff': 1, 'forced': 1, 'packing': 1, 'ahhh': 1, 'vaguely': 1, 'actin': 1, 'badly': 1, 'apologetic': 1, 'fallen': 1, 'spoilt': 1, 'housework': 1, 'cuppa': 1, 'fainting': 1, 'timings': 1, 'watts': 1, 'steed': 1, 'arabian': 1, 'rodger': 1, '07732584351': 1, 'endowed': 1, 'hep': 1, 'immunisation': 1, 'sucker': 1, 'suckers': 1, 'stubborn': 1, 'thinked': 1, 'smarter': 1, 'crashing': 1, 'accomodations': 1, 'offered': 1, 'embarassing': 1, 'cave': 1, 'wings': 1, 'incorrect': 1, 'jersey': 1, 'devils': 1, 'sptv': 1, 'sherawat': 1, 'mallika': 1, 'gauti': 1, 'sehwag': 1, 'seekers': 1, 'barbie': 1, 'ken': 1, 'performed': 1, 'peoples': 1, 'operate': 1, 'multis': 1, 'factory': 1, 'casualty': 1, 'stuff42moro': 1, 'includes': 1, 'hairdressers': 1, 'beforehand': 1, 'ams': 1, '4the': 1, 'signin': 1, 'memorable': 1, 'server': 1, 'minecraft': 1, 'ip': 1, 'grumpy': 1, 'lying': 1, 'plural': 1, 'formal': 1, 'openin': 1, '0871277810910p': 1, 'ratio': 1, '09064019788': 1, 'box42wr29c': 1, 'malarky': 1, 'apples': 1, 'pairs': 1, '4041': 1, '7548': 1, 'sao': 1, 'predict': 1, 'involve': 1, 'imposed': 1, 'lucyxx': 1, 'tmorrow': 1, 'accomodate': 1, 'gravel': 1, 'hotmail': 1, 'svc': 1, '69988': 1, 'nver': 1, 'ummma': 1, 'sindu': 1, 'nevering': 1, 'typical': 1, 'chores': 1, 'mist': 1, 'dirt': 1, 'exist': 1, 'hail': 1, 'aaooooright': 1, '07046744435': 1, 'annoncement': 1, 'envy': 1, 'excited': 1, '32': 1, 'bootydelious': 1, 'bangb': 1, 'bangbabes': 1, 'cultures': 1, 's89': 1, '09061701939': 1, 'missunderstding': 1, 'bridge': 1, 'lager': 1, 'axis': 1, 'surname': 1, 'clue': 1, 'begins': 1, 'hopes': 1, 'lifted': 1, 'approaches': 1, 'finding': 1, 'handsome': 1, 'areyouunique': 1, '30th': 1, 'league': 1, 'stool': 1, 'ors': 1, '1pm': 1, 'babyjontet': 1, 'enc': 1, 'ga': 1, 'alter': 1, 'dogg': 1, 'dats': 1, 'refund': 1, 'prediction': 1, 'os': 1, 'ubandu': 1, 'disk': 1, 'scenery': 1, 'aries': 1, 'flyng': 1, 'horo': 1, 'mudyadhu': 1, 'elama': 1, 'conducts': 1, 'strict': 1, 'gandhipuram': 1, 'rubber': 1, 'thirtyeight': 1, 'hearing': 1, 'pleassssssseeeeee': 1, 'sportsx': 1, 'watches': 1, 'baig': 1, '3days': 1, 'usps': 1, 'nipost': 1, 'bribe': 1, 'ups': 1, 'luton': 1, '0125698789': 1, '69698': 1, 'sometme': 1, 'club4': 1, 'box1146': 1, 'club4mobiles': 1, '87070': 1, 'evo': 1, 'narcotics': 1, 'objection': 1, 'theater': 1, 'mack': 1, 'rob': 1, 'celebrations': 1, 'gdeve': 1, 'ahold': 1, 'cruisin': 1, 'raksha': 1, 'varunnathu': 1, 'edukkukayee': 1, 'ollu': 1, 'resend': 1, '28thfeb': 1, 'gurl': 1, 'appropriate': 1, 'diesel': 1, 'fridge': 1, 'womdarfull': 1, 'rodds1': 1, 'icmb3cktz8r7': 1, 'aberdeen': 1, 'img': 1, 'kingdom': 1, 'united': 1, 'blind': 1, 'remb': 1, 'jos': 1, 'bookshelf': 1, '85222': 1, 'gbp1': 1, '84': 1, 'winnersclub': 1, 'mylife': 1, 'l8': 1, 'gon': 1, 'guild': 1, 'evaporated': 1, 'stealing': 1, 'employer': 1, 'daaaaa': 1, 'dined': 1, 'wined': 1, 'hiding': 1, 'huiming': 1, 'prestige': 1, 'xxuk': 1, 'sextextuk': 1, 'shag': 1, '69876': 1, 'jeremiah': 1, 'iphone': 1, 'apeshit': 1, 'safely': 1, 'onam': 1, 'sirji': 1, 'tata': 1, 'aig': 1, '08708800282': 1, 'unemployed': 1, 'andrews': 1, 'db': 1, 'dawns': 1, 'refreshed': 1, 'f4q': 1, 'regalportfolio': 1, 'rp176781': 1, '08717205546': 1, 'uniform': 1, 'spoil': 1, '09057039994': 1, 't91': 1, 'lindsay': 1, 'bars': 1, 'heron': 1, 'payasam': 1, 'rinu': 1, 'prabu': 1, 'verifying': 1, 'becaus': 1, 'taught': 1, 'followin': 1, 'repairs': 1, 'wallet': 1, '945': 1, 'owl': 1, 'kickboxing': 1, 'lap': 1, 'performance': 1, 'calculated': 1, 'visitor': 1, 'wahleykkum': 1, 'administrator': 1, '2814032': 1, '150pw': 1, '3x': 1, 'stoners': 1, 'disastrous': 1, 'busetop': 1, 'iron': 1, 'blah': 1, 'okies': 1, 'wendy': 1, '09064012103': 1, 'pobox12n146tf150p': 1, '09111032124': 1, '09058094455': 1, 'attractive': 1, 'rowdy': 1, 'sentiment': 1, 'attitude': 1, 'urination': 1, 'hillsborough': 1, 'shoul': 1, 'hasnt': 1, 'monkeespeople': 1, 'werethe': 1, 'jobyet': 1, 'howu': 1, 'monkeyaround': 1, 'howdy': 1, 'foundurself': 1, 'sausage': 1, 'exercise': 1, 'blimey': 1, 'concentration': 1, 'hanks': 1, 'lotsly': 1, 'detail': 1, 'optimistic': 1, 'practicum': 1, 'consistently': 1, 'links': 1, 'ears': 1, 'wavering': 1, 'heal': 1, '9153': 1, 'upgrdcentre': 1, 'oral': 1, 'slippery': 1, 'bike': 1, 'okmail': 1, 'differ': 1, 'enters': 1, '69888nyt': 1, 'machi': 1, 'mcr': 1, 'falconerf': 1, 'thuglyfe': 1, 'jaykwon': 1, 'glory': 1, 'ralphs': 1, 'faded': 1, 'reunion': 1, 'accenture': 1, 'jackson': 1, 'reache': 1, 'nuerologist': 1, 'lolnice': 1, 'westshore': 1, 'significance': 1, 'ammo': 1, 'ak': 1, 'toxic': 1, 'poly3': 1, 'jamz': 1, 'boltblue': 1, 'topped': 1, 'tgxxrz': 1, 'bubbletext': 1, 'problematic': 1, 'abnormally': 1, 'adults': 1, 'unconscious': 1, '9755': 1, 'teletext': 1, 'recieve': 1, 'faggot': 1, '07815296484': 1, '41782': 1, 'bani': 1, 'leads': 1, 'buttons': 1, 'max6': 1, 'csc': 1, 'applausestore': 1, 'monthlysubscription': 1, 'famous': 1, 'temper': 1, 'unconditionally': 1, 'bash': 1, 'oclock': 1, 'cooped': 1, 'weddin': 1, 'invitation': 1, 'alibi': 1, 'paces': 1, 'surrounded': 1, 'cuck': 1, 'sink': 1, 'cage': 1, 'deficient': 1, 'acknowledgement': 1, 'tactless': 1, 'astoundingly': 1, 'oath': 1, 'magic': 1, 'pan': 1, 'silly': 1, 'mutations': 1, 'uv': 1, 'causes': 1, 'sunscreen': 1, 'thesedays': 1, 'sugardad': 1, 'bao': 1, 'brownie': 1, 'ninish': 1, 'freek': 1, 'icky': 1, 'ridden': 1, 'missy': 1, 'goggles': 1, 'arguing': 1, '09050005321': 1, 'unfortuntly': 1, 'arngd': 1, 'frnt': 1, 'walkin': 1, 'sayin': 1, 'bites': 1, 'textand': 1, '08002988890': 1, 'tendencies': 1, 'jjc': 1, 'gotany': 1, 'meive': 1, 'yi': 1, 'srsly': 1, '07753741225': 1, '08715203677': 1, '42478': 1, 'prix': 1, 'nitz': 1, 'stands': 1, 'occur': 1, 'rajnikant': 1, 'blastin': 1, 'ocean': 1, 'speciale': 1, 'roses': 1, '07880867867': 1, 'zouk': 1, 'clubsaisai': 1, '07946746291': 1, 'xclusive': 1, 'banter': 1, 'bridgwater': 1, 'dependents': 1, 'cer': 1, 'thanx4': 1, 'beauties': 1, 'hundreds': 1, 'aunties': 1, 'handsomes': 1, 'friendships': 1, 'dismay': 1, 'concerned': 1, 'tootsie': 1, 'seventeen': 1, 'ml': 1, 'fetching': 1, 'restock': 1, 'brighten': 1, 'braved': 1, 'allo': 1, 'triumphed': 1, 'uncomfortable': 1, '08715203694': 1, 'rough': 1, 'sonetimes': 1, 'wesleys': 1, 'cloud': 1, 'wikipedia': 1, '08718711108': 1, '89034': 1, '88800': 1, 'repent': 1, 'sutra': 1, 'kama': 1, 'positions': 1, 'nange': 1, 'bakra': 1, 'kalstiya': 1, 'lakhs': 1, 'sun0819': 1, '08452810071': 1, 'ditto': 1, 'wetherspoons': 1, 'piggy': 1, 'freaky': 1, 'scrappy': 1, 'sdryb8i': 1, '1da': 1, 'lapdancer': 1, '150ppmsg': 1, 'sue': 1, 'g2': 1, 'tomorw': 1, 'imprtant': 1, 'crying': 1, 'bfore': 1, 'tmorow': 1, 'cherthala': 1, 'engaged': 1, '08712404000': 1, '448712404000': 1, '1680': 1, '1405': 1, '1843': 1, 'entrepreneurs': 1, 'corporation': 1, 'fluids': 1, 'dehydration': 1, 'prevent': 1, 'trek': 1, 'harri': 1, 'deck': 1, 'cnupdates': 1, 'gage': 1, 'alerts': 1, 'newsletter': 1, 'shitstorm': 1, 'attributed': 1, '08714712388': 1, '449071512431': 1, 'specs': 1, 'sth': 1, 'px3748': 1, '08714712394': 1, 'mindset': 1, 'macha': 1, 'wondar': 1, 'flim': 1, 'jelly': 1, 'scrumptious': 1, 'dao': 1, 'half8th': 1, 'visiting': 1, 'jide': 1, 'alertfrom': 1, 'drvgsto': 1, 'stewartsize': 1, 'jeri': 1, 'prescripiton': 1, '2kbsubject': 1, 'steak': 1, 'neglect': 1, 'prayers': 1, 'wahay': 1, 'hadn': 1, 'clocks': 1, 'realised': 1, 'gaze': 1, '82324': 1, 'tattoos': 1, 'caveboy': 1, 'vibrate': 1, '79': 1, '08704439680ts': 1, 'hungover': 1, 'grandmas': 1, 'closingdate04': 1, 'm39m51': 1, 'claimcode': 1, 'mobypobox734ls27yf': 1, '09066368327': 1, '50pmmorefrommobile2bremoved': 1, 'unclaimed': 1, 'gua': 1, 'faber': 1, 'dramatic': 1, 'hunting': 1, 'drunkard': 1, 'weaseling': 1, 'idc': 1, 'trash': 1, 'punish': 1, 'beerage': 1, 'randomlly': 1, 'fixes': 1, 'spelling': 1, '100p': 1, '087018728737': 1, 'tune': 1, 'toppoly': 1, 'fondly': 1, 'dogbreath': 1, 'sounding': 1, 'weighed': 1, 'woohoo': 1, 'uncountable': 1, '14thmarch': 1, 'availa': 1, '9996': 1, 'canlove': 1, 'whereare': 1, 'thekingshead': 1, 'friendsare': 1, 'rg21': 1, '4jx': 1, 'dled': 1, 'smokin': 1, 'boooo': 1, 'costumes': 1, 'yowifes': 1, 'notifications': 1, 'outbid': 1, 'plyr': 1, 'simonwatson5120': 1, 'shinco': 1, 'smsrewards': 1, 'youi': 1, 'yourjob': 1, 'soonlots': 1, 'llspeak': 1, 'starshine': 1, 'sips': 1, 'yourinclusive': 1, 'smsservices': 1, 'bits': 1, 'turned': 1, 'burial': 1, 'rvx': 1, 'rv': 1, 'comprehensive': 1, 'prashanthettan': 1, 'doug': 1, 'realizes': 1, 'guitar': 1, 'samantha': 1, 'impress': 1, 'trauma': 1, 'swear': 1, 'inner': 1, 'tigress': 1, 'overdose': 1, 'urfeeling': 1, 'bettersn': 1, 'probthat': 1, '83110': 1, 'ana': 1, 'rto': 1, 'sathy': 1, 'spoons': 1, 'corvettes': 1, '50pm': 1, '09061104283': 1, 'bunkers': 1, '07808': 1, 'xxxxxx': 1, '08719899217': 1, 'posh': 1, 'dob': 1, 'chaps': 1, 'prods': 1, 'trial': 1, 'champneys': 1, '0721072': 1, 'hole': 1, 'philosophical': 1, 'shakespeare': 1, 'atleast': 1, 'mymoby': 1, 'doit': 1, 'curfew': 1, 'getsleep': 1, 'gibe': 1, 'studdying': 1, 'woul': 1, 'massages': 1, 'yoyyooo': 1, 'permissions': 1, 'hussey': 1, 'mike': 1, 'faglord': 1, 'ctter': 1, 'cttargg': 1, 'ie': 1, 'cttergg': 1, 'ctagg': 1, 'cutter': 1, 'ctargg': 1, 'nutter': 1, 'thus': 1, 'grateful': 1, 'happier': 1, 'experiment': 1, 'agents': 1, 'invoices': 1, 'smell': 1, 'tobacco': 1, 'assumed': 1, 'racal': 1, 'dizzee': 1, 'bookmark': 1, 'stereophonics': 1, 'marley': 1, 'strokes': 1, 'libertines': 1, 'lastest': 1, 'nookii': 1, 'grinule': 1, 'oreos': 1, 'fudge': 1, 'zaher': 1, 'dieting': 1, 'nauseous': 1, 'hollalater': 1, 'avalarr': 1, 'rounds': 1, 'blogging': 1, 'magicalsongs': 1, 'blogspot': 1, 'slices': 1, 'kvb': 1, 'w1t1jy': 1, 'box403': 1, 'ppt150x3': 1, '1million': 1, 'alternative': 1, 'owo': 1, 'fro': 1, 'ore': 1, 'samus': 1, 'shoulders': 1, '09063440451': 1, 'ppm150': 1, 'matthew': 1, 'box334': 1, 'vomitin': 1, '528': 1, '1yf': 1, 'hp20': 1, '09061749602': 1, 'writhing': 1, 'bleh': 1, 'stuffed': 1, 'pockets': 1, 'paypal': 1, 'voila': 1, 'theyre': 1, 'folks': 1, 'sorta': 1, 'blown': 1, 'secondary': 1, 'applying': 1, 'ogunrinde': 1, 'sophas': 1, 'lodging': 1, 'chk': 1, 'dict': 1, 'shb': 1, 'stories': 1, 'retired': 1, 'natwest': 1, 'chad': 1, 'gymnastics': 1, 'christians': 1, 'token': 1, 'liking': 1, 'aptitude': 1, 'horse': 1, 'wrongly': 1, 'boggy': 1, 'biatch': 1, 'weakness': 1, 'hesitate': 1, 'notebook': 1, 'carpark': 1, 'eightish': 1, '5wkg': 1, 'cres': 1, 'ere': 1, 'ubi': 1, '67441233': 1, 'irene': 1, '61': 1, 'bus8': 1, '382': 1, '6ph': 1, '66': 1, '7am': 1, '5ish': 1, 'relaxing': 1, 'stripes': 1, 'skirt': 1, 'escalator': 1, 'beth': 1, 'charlie': 1, 'syllabus': 1, 'panasonic': 1, 'bluetoothhdset': 1, 'doubletxt': 1, 'doublemins': 1, '30pm': 1, 'kolathupalayam': 1, 'unjalur': 1, 'poyyarikatur': 1, 'erode': 1, 'apt': 1, 'hero': 1, 'meat': 1, 'supreme': 1, 'cudnt': 1, 'ctla': 1, 'ishtamayoo': 1, 'bakrid': 1, 'ente': 1, 'images': 1, 'fond': 1, 'finds': 1, 'cougar': 1, 'coaxing': 1, 'souveniers': 1, 'glorious': 1, '09065394514': 1, 'scratches': 1, 'nanny': 1, 'hardest': 1, 'shitin': 1, 'lekdog': 1, 'defo': 1, 'millions': 1, 'blankets': 1, 'atten': 1, '09058097218': 1, 'analysis': 1, 'data': 1, 'belligerent': 1, 'rudi': 1, 'les': 1, 'snoring': 1, 'ink': 1, '515': 1, 'throwing': 1, 'finalise': 1, 'flirtparty': 1, 'replys150': 1, 'dentist': 1, 'shes': 1, 'oyea': 1, 'nurses': 1, 'lul': 1, 'obese': 1, 'ami': 1, 'parchi': 1, 'kicchu': 1, 'korte': 1, 'tul': 1, 'korche': 1, 'iccha': 1, 'kaaj': 1, 'copies': 1, 'sculpture': 1, 'surya': 1, 'pokkiri': 1, 'sorrows': 1, 'praises': 1, 'sambar': 1, 'makiing': 1, 'attraction': 1, 'proove': 1, 'ndship': 1, '4few': 1, 'conected': 1, 'needle': 1, 'spatula': 1, 'outrageous': 1, 'complexities': 1, 'freely': 1, 'taxes': 1, 'ryder': 1, 'presleys': 1, 'elvis': 1, 'postal': 1, 'strips': 1, 'gifts': 1, 'cliff': 1, 'wrking': 1, 'sittin': 1, 'drops': 1, 'hen': 1, 'smoked': 1, 'teju': 1, 'hourish': 1, 'amla': 1, 'convenience': 1, 'evaluation': 1, '09050000301': 1, '449050000301': 1, '80155': 1, 'chat80155': 1, 'rcd': 1, 'speedchat': 1, 'swap': 1, 'chatter': 1, 'cheyyamo': 1, '80160': 1, 'txt43': 1, 'brothers': 1, 'throws': 1, 'hmv1': 1, 'errors': 1, 'piah': 1, 'tau': 1, '1stchoice': 1, '08707808226': 1, 'shade': 1, 'copied': 1, 'notified': 1, 'marketing': 1, '08450542832': 1, '84122': 1, 'theirs': 1, 'sexual': 1, 'virgins': 1, '69911': 1, '4fil': 1, 'kaitlyn': 1, 'sitter': 1, 'peeps': 1, 'danger': 1, 'comment': 1, 'veggie': 1, 'neighbors': 1, 'computerless': 1, 'balloon': 1, 'melody': 1, 'macs': 1, 'hme': 1, 'velachery': 1, 'flippin': 1, 'breaking': 1, 'cstore': 1, 'hangin': 1, 'lodge': 1, 'worrying': 1, 'quizzes': 1, '087016248': 1, '08719181503': 1, 'thin': 1, 'arguments': 1, 'fed': 1, 'himso': 1, 'semi': 1, 'exp': 1, '30apr': 1, 'maaaan': 1, 'guessin': 1, 'wuldnt': 1, 'personally': 1, 'ilol': 1, 'lunchtime': 1, 'organise': 1, '5years': 1, 'passable': 1, 'phd': 1, 'prakesh': 1, 'products': 1, 'betta': 1, 'aging': 1, 'global': 1, '08700435505150p': 1, 'accommodation': 1, 'phb1': 1, 'submitting': 1, 'snatch': 1, 'dancce': 1, 'basq': 1, 'pthis': 1, 'senrd': 1, 'ihave': 1, '0quit': 1, 'edrunk': 1, 'xxxxxxx': 1, 'iff': 1, 'ros': 1, 'drivby': 1, 'drum': 1, 'dnot': 1, '2nhite': 1, 'westonzoyland': 1, 'relieved': 1, 'greatness': 1, 'goin2bed': 1, 'only1more': 1, 'mc': 1, 'ifink': 1, 'everythin': 1, 'ava': 1, 'every1': 1, 'melnite': 1, 'oli': 1, 'goodtime': 1, 'l8rs': 1, '08712402779': 1, 'shun': 1, 'exhibition': 1, 'glass': 1, 'bian': 1, 'nino': 1, 'himself': 1, 'el': 1, 'downstem': 1, '08718730555': 1, 'wahala': 1, 'insects': 1, 'listening2the': 1, 'evil': 1, 'plumbing': 1, 'leafcutter': 1, 'inperialmusic': 1, 'molested': 1, 'acid': 1, 'remixed': 1, 'didntgive': 1, 'jenxxx': 1, 'thepub': 1, 'bellearlier': 1, 'bedbut': 1, 'uwana': 1, '09096102316': 1, 'cheery': 1, 'weirdo': 1, 'profiles': 1, 'stalk': 1, 'pax': 1, 'deposit': 1, '95': 1, 'jap': 1, 'disappeared': 1, 'certificate': 1, 'publish': 1, 'wheellock': 1, 'destination': 1, 'fifty': 1, 'happenin': 1, 'settling': 1, 'ipads': 1, 'worthless': 1, 'novelty': 1, 'cocksuckers': 1, 'janx': 1, 'dads': 1, 'developer': 1, 'designation': 1, 'musicnews': 1, 'videosounds': 1, '09701213186': 1, 'videosound': 1, 'spirit': 1, 'shattered': 1, 'girlie': 1, 'darker': 1, 'styling': 1, 'listn': 1, 'gray': 1, 'watevr': 1, 'paragraphs': 1, 'minus': 1, 'coveragd': 1, 'vasai': 1, 'retard': 1, 'bathroom': 1, 'sang': 1, 'uptown': 1, '80': 1, 'icic': 1, 'syria': 1, 'gauge': 1, 'completing': 1, 'ax': 1, 'unfolds': 1, 'emergency': 1, 'surgical': 1, 'korean': 1, 'fredericksburg': 1, 'pases': 1, 'que': 1, 'buen': 1, 'tiempo': 1, 'compass': 1, 'way2sms': 1, 'gnun': 1, 'youuuuu': 1, 'misss': 1, 'baaaaabe': 1, 'convince': 1, 'witot': 1, 'buyer': 1, 'undrstndng': 1, 'suffer': 1, 'becz': 1, 'avoids': 1, 'steamboat': 1, 'forgive': 1, 'tp': 1, '6ish': 1, 'bbq': 1, 'panicks': 1, 'everyso': 1, 'types': 1, 'nick': 1, 'auntie': 1, 'huai': 1, 'path': 1, 'paths': 1, 'appear': 1, 'thirunelvali': 1, 'reserve': 1, 'tackle': 1, 'tonght': 1, 'ironing': 1, 'pile': 1, 'chinky': 1, 'ploughing': 1, 'wi': 1, 'nz': 1, 'aust': 1, 'recharged': 1, 'papa': 1, 'detailed': 1, 'losers': 1, 'beta': 1, 'noncomittal': 1, 'snickering': 1, 'chords': 1, 'win150ppmx3age16': 1, 'boyf': 1, 'interviw': 1, 'entire': 1, 'determine': 1, 'spreadsheet': 1, 'trebles': 1, 'dartboard': 1, 'doubles': 1, 'coat': 1, 'recognises': 1, 'wisheds': 1, 'duo': 1, 'intrepid': 1, 'breeze': 1, 'fresh': 1, 'twittering': 1, 'chinchillas': 1, 'ducking': 1, 'function': 1, 'headstart': 1, 'rummer': 1, 'flying': 1, 'charts': 1, 'bbc': 1, 'optin': 1, 'thanks2': 1, 'rajini': 1, 'summers': 1, 'matched': 1, 'help08714742804': 1, 'spys': 1, '09099725823': 1, 'offering': 1, 'meow': 1, 'edhae': 1, 'bilo': 1, 'innu': 1, 'astne': 1, 'vargu': 1, 'lyfu': 1, 'halla': 1, 'yalru': 1, 'lyf': 1, 'mundhe': 1, 'ali': 1, 'ovr': 1, 'prone': 1, 'usa': 1, 'msgrcvd18': 1, '07801543489': 1, 'latests': 1, 'llc': 1, 'permission': 1, '09099726395': 1, 'meetins': 1, 'cumin': 1, 'lucy': 1, 'tablet': 1, 'dose': 1, 'incomm': 1, 'maps': 1, 'concentrating': 1, 'tiring': 1, 'browsin': 1, 'compulsory': 1, 'investigate': 1, 'vitamin': 1, 'crucial': 1, '2channel': 1, 'psychic': 1, 'jsco': 1, 'skills': 1, 'leadership': 1, 'host': 1, 'systems': 1, 'linux': 1, 'based': 1, 'idps': 1, 'converter': 1, 'sayy': 1, 'leanne': 1, 'disc': 1, 'glasgow': 1, 'champ': 1, 'lovin': 1, 'browse': 1, 'artists': 1, 'install': 1, 'speling': 1, 'corect': 1, '4719': 1, '523': 1, 'employee': 1, 'cts': 1, 'nike': 1, 'sooo': 1, 'shouting': 1, 'dang': 1, 'earliest': 1, 'nordstrom': 1, 'conference': 1, 'degree': 1, 'bleak': 1, 'shant': 1, 'nearer': 1, 'raiden': 1, 'totes': 1, 'cardin': 1, 'pierre': 1, 'establish': 1, 'rhythm': 1, 'truro': 1, 'ext': 1, 'cloth': 1, 'sunroof': 1, 'blanked': 1, 'image': 1, 'kalainar': 1, 'thenampet': 1, 'freaked': 1, 'reacting': 1, 'nosy': 1, 'imposter': 1, 'destiny': 1, 'satanic': 1, 'pudunga': 1, 'chef': 1, 'exterminator': 1, 'pest': 1, 'aaniye': 1, 'sympathetic': 1, 'venaam': 1, 'psychologist': 1, 'athletic': 1, 'determined': 1, 'companion': 1, 'stylist': 1, 'healer': 1, 'courageous': 1, 'organizer': 1, 'dependable': 1, 'listener': 1, 'psychiatrist': 1, 'chez': 1, 'jules': 1, 'nig': 1, 'hhahhaahahah': 1, 'leonardo': 1, 'dime': 1, 'strain': 1, '2years': 1, 'withdraw': 1, 'anyhow': 1, 'millers': 1, 'rawring': 1, 'xoxo': 1, 'spark': 1, 'flame': 1, 'crushes': 1, 'somewhr': 1, 'honeymoon': 1, 'outfit': 1, '08719899230': 1, 'cheque': 1, 'olympics': 1, 'leo': 1, 'haul': 1, 'want2come': 1, 'wizzle': 1, 'wildlife': 1, 'that2worzels': 1, 'cya': 1, 'shanghai': 1, '645': 1, 'redeemable': 1, 'rt': 1, '08701237397': 1, 'pro': 1, 'thnx': 1, 'anjie': 1, 'sef': 1, 'fring': 1, 'nte': 1, '526': 1, 'bx': 1, '02072069400': 1, 'talents': 1, 'animal': 1, 'warming': 1, 'shiny': 1, 'fooled': 1, 'french': 1, 'responsible': 1, 'companies': 1, 'guarantee': 1, 'suppliers': 1, '0a': 1, 'lnly': 1, 'keen': 1, 'dammit': 1, 'wright': 1, 'wrecked': 1, 'somewhat': 1, 'laden': 1, 'goodevening': 1, 'spontaneously': 1, 'rgent': 1, 'busty': 1, 'daytime': 1, '09099726429': 1, 'janinexx': 1, 'spageddies': 1, 'fourth': 1, 'dimension': 1, 'phasing': 1, 'compromised': 1, 'meaningful': 1, '09050001295': 1, 'a21': 1, '391784': 1, 'mobsi': 1, 'dub': 1, 'je': 1, 'toughest': 1, 'unspoken': 1, 'squatting': 1, 'digits': 1, '0089': 1, '09063442151': 1, 'sonathaya': 1, 'soladha': 1, 'raping': 1, 'dudes': 1, 'weightloss': 1, 'embarrassed': 1, 'mushy': 1, 'stash': 1, 'priya': 1, 'kilos': 1, 'accidant': 1, 'tookplace': 1, 'ghodbandar': 1, 'slovely': 1, 'sc': 1, 'wad': 1, 'specialise': 1, 'desparately': 1, 'mi': 1, 'stereo': 1, 'classmates': 1, 'fires': 1, 'vipclub4u': 1, 'trackmarque': 1, 'missionary': 1, 'entertaining': 1, 'hugh': 1, 'stick': 1, 'laurie': 1, 'praps': 1, 'jon': 1, 'dinero': 1, 'spain': 1, '000pes': 1, 'complaining': 1, 'mandy': 1, '09041940223': 1, 'transferred': 1, 'fm': 1, 'hotmix': 1, 'sullivan': 1, 'finn': 1, 'thew': 1, 'theacusations': 1, 'iwana': 1, 'wotu': 1, 'haventcn': 1, 'downon': 1, 'itxt': 1, 'nething': 1, 'dine': 1, '09111030116': 1, 'conacted': 1, 'pobox12n146tf15': 1, 'inspection': 1, 'nursery': 1, 'becomes': 1, 'panren': 1, 'paru': 1, 'trainners': 1, 'carryin': 1, 'bac': 1, 'chuckin': 1, 'dhanush': 1, 'needing': 1, 'habba': 1, 'dileep': 1, 'venugopal': 1, 'muchand': 1, 'mentioned': 1, 'remembrs': 1, 'everytime': 1, 'edition': 1, 'algorithms': 1, 'textbook': 1, '3230': 1, 'cro1327': 1, '09064018838': 1, 'iwas': 1, 'urmom': 1, 'careabout': 1, 'itried2tell': 1, 'marine': 1, 'intend': 1, 'learned': 1, 'traveling': 1, 'honest': 1, 'afghanistan': 1, 'stable': 1, 'iraq': 1, '50award': 1, '1225': 1, 'pai': 1, 'seh': 1, 'parts': 1, 'walsall': 1, 'terry': 1, 'tue': 1, 'ccna': 1, 'shrek': 1, 'fellow': 1, 'dying': 1, 'lifting': 1, 'teresa': 1, 'aid': 1, 'ld': 1, 'bam': 1, 'dec': 1, 'usmle': 1, 'squishy': 1, 'mwahs': 1, 'hottest': 1, 'prominent': 1, 'cheek': 1, 'september': 1, 'bcm': 1, 'neo69': 1, 'backdoor': 1, '8027': 1, 'hack': 1, 'subscribe': 1, 'fraction': 1, 'dps': 1, '09050280520': 1, 'comingdown': 1, 'murali': 1, 'sts': 1, 'kissing': 1, 'elliot': 1, 'mia': 1, 'engalnd': 1, 'd3wv': 1, '100txt': 1, '2price': 1, 'matric': 1, '650': 1, '850': 1, '08718726970': 1, 'payments': 1, 'fedex': 1, 'reception': 1, 'consensus': 1, 'entertain': 1, 'pillows': 1, 'strewn': 1, 'bras': 1, 'tag': 1, 'exposes': 1, 'weaknesses': 1, 'knee': 1, 'pulls': 1, 'wicked': 1, 'supports': 1, 'srt': 1, 'ps3': 1, 'jontin': 1, 'banned': 1, 'biro': 1, '09058094594': 1, 'unconsciously': 1, 'unhappy': 1, 'shell': 1, 'jog': 1, '09061743811': 1, 'lark': 1, 'sic': 1, '0870753331018': 1, '7mp': 1, '09090900040': 1, 'extreme': 1, 'wild': 1, 'fones': 1, 'stop2stop': 1, 'lim': 1, 'parachute': 1, 'placed': 1, 'lambda': 1, 'snowball': 1, 'angels': 1, 'ello': 1, 'duffer': 1, 'ofice': 1, 'pharmacy': 1, 'grr': 1, 'cnl': 1, '08715500022': 1, 'rpl': 1, 'nor': 1, 'fffff': 1, 'lifebook': 1, 'zhong': 1, 'qing': 1, 'act': 1, 'hypertension': 1, 'annoyin': 1, '08702490080': 1, 'vpod': 1, 'nigro': 1, 'scratching': 1, 'anyplaces': 1, 'priority': 1, 'ecstasy': 1, 'minded': 1, '09090204448': 1, 'ls278bb': 1, 'minapn': 1, 'hittng': 1, 'reflex': 1, 'egbon': 1, 'adewale': 1, 'mary': 1, 'deduct': 1, 'wrks': 1, 'asshole': 1, 'monkey': 1, 'grab': 1, 'sliding': 1, '09065394973': 1, 'payback': 1, 'tescos': 1, 'bowa': 1, 'feathery': 1, 'infra': 1, 'gep': 1, 'fifa': 1, '2006': 1, 'shhhhh': 1, 'arul': 1, 'related': 1, 'amk': 1, '09061743810': 1, 'length': 1, 'corrct': 1, 'antha': 1, 'dane': 1, 'basket': 1, 'rupaul': 1, 'curtsey': 1, 'practising': 1, '4my': 1, 'havebeen': 1, '2i': 1, 'feelingood': 1, 'ordinator': 1, 'preschoolco': 1, 'rise': 1, 'havbeen': 1, 'payed2day': 1, 'memory': 1, 'converted': 1, 'soil': 1, 'african': 1, 'outreach': 1, 'roles': 1, '8lb': 1, 'brilliantly': 1, '7oz': 1, 'forwarding': 1, 'visitors': 1, 'intention': 1, 'bend': 1, 'rules': 1, 'thia': 1, 'inlude': 1, 'previews': 1, 'madurai': 1, 'marrge': 1, 'dha': 1, 'ambrith': 1, 'kitty': 1, 'shaved': 1, 'tactful': 1, 'pert': 1, 'crammed': 1, 'satsgettin': 1, '47per': 1, 'apologize': 1, 'pei': 1, 'subtoitles': 1, 'jot': 1, 'cereals': 1, 'gari': 1, 'bold2': 1, '1er': 1, 'm60': 1, 'cast': 1, 'aom': 1, '09094100151': 1, 'gbp5': 1, 'box61': 1, 'thkin': 1, 'resubbing': 1, 'shadow': 1, 'breadstick': 1, 'saeed': 1, '09066362220': 1, 'purple': 1, 'brown': 1, 'yelow': 1, 'arranging': 1, 'eldest': 1, 'drugdealer': 1, 'wither': 1, '23f': 1, '23g': 1, 'wondarfull': 1, 'txt250': 1, 'web2mobile': 1, 'txtx': 1, 'box139': 1, 'la32wu': 1, 'onbus': 1, 'donyt': 1, 'latelyxxx': 1, '85233': 1, 'endof': 1, 'justthought': 1, '2hook': 1, 'uwant': 1, 'offdam': 1, 'nevamind': 1, 'sayhey': 1, 'stressed': 1, 'provider': 1, 'soooo': 1, 'tming': 1, 'cutest': 1, 'dice': 1, '08700469649': 1, 'box420': 1, 'mathe': 1, 'samachara': 1, 'howda': 1, 'autocorrect': 1, 'audrie': 1, 'readiness': 1, 'simulate': 1, 'lara': 1, 'supplies': 1, 'attach': 1, 'guesses': 1, '087123002209am': 1, 'nickey': 1, 'nobbing': 1, 'platt': 1, 'washob': 1, 'sterling': 1, 'spotty': 1, 'province': 1, 'hall': 1, 'hesitation': 1, 'ponnungale': 1, 'intha': 1, 'ipaditan': 1, 'rejected': 1, 'noisy': 1, 'needa': 1, 'reset': 1, 'troubleshooting': 1, 'manual': 1, 'b4utele': 1, 'marsms': 1, 'b4u': 1, '08717168528': 1, 'stifled': 1, 'creativity': 1, 'strongly': 1, 'requirements': 1, '2getha': 1, 'qlynnbv': 1, 'help08700621170150p': 1, 'buffy': 1, 'nosh': 1, 'lololo': 1, 'waaaat': 1, 'occupied': 1, 'documents': 1, 'stapati': 1, 'submitted': 1, 'hills': 1, 'cutie': 1, 'honesty': 1, 'beggar': 1, 'shakara': 1, 'specialisation': 1, 'labor': 1, 'dent': 1, 'crickiting': 1, 'isv': 1, 'urgoin': 1, 'tome': 1, 'reallyneed': 1, '2docd': 1, 'dontignore': 1, 'mycalls': 1, 'imin': 1, 'outl8r': 1, 'dontmatter': 1, 'thecd': 1, 'dontplease': 1, 'yavnt': 1, 'ibuprofens': 1, 'popping': 1, 'sip': 1, 'grown': 1, 'chinatown': 1, 'claypot': 1, 'beehoon': 1, 'fishhead': 1, 'yam': 1, 'porridge': 1, 'jaklin': 1, 'cliffs': 1, 'nearby': 1, 'bundle': 1, 'mf': 1, 'deals': 1, '49': 1, 'avble': 1, '4got': 1, 'weds': 1, 'moseley': 1, 'ooh': 1, 'thankyou': 1, 'ternal': 1, 'ntimate': 1, 'namous': 1, 'aluable': 1, 'atural': 1, 'oble': 1, 'ffectionate': 1, 'oveable': 1, 'ruthful': 1, 'textin': 1, 'burn': 1, 'amigos': 1, 'progress': 1, 'weren': 1, 'tryin': 1, 'collages': 1, 'arty': 1, '2hrs': 1, 'waliking': 1, 'cartons': 1, 'shelves': 1, '08714712379': 1, 'mirror': 1, '09065069120': 1, 'k718': 1, 'keris': 1, 'smidgin': 1, 'jod': 1, 'intentions': 1, 'accordin': 1, 'knocking': 1, 'como': 1, 'abel': 1, 'listened2the': 1, 'air1': 1, 'braindance': 1, 'plaid': 1, 'ofstuff': 1, 'hilarious': 1, 'hav2hear': 1, 'aphex': 1, 'nelson': 1, 'unmits': 1, 'newspapers': 1, 'yummmm': 1, 'puzzeles': 1, 'scammers': 1, '4goten': 1, '09099726481': 1, 'passion': 1, 'dena': 1, '09065069154': 1, 'r836': 1, 'shifad': 1, 'raised': 1, 'doctors': 1, 'reminds': 1, 'tolerat': 1, 'bcs': 1, 'subscrition': 1, 'splashmobile': 1, 'dust': 1, '88877': 1, '3pound': 1, 'watchin': 1, 'meaningless': 1, 'jones': 1, 'brdget': 1, 'inever': 1, 'hype': 1, 'studio': 1, 'velly': 1, 'marking': 1, '2stoptx': 1, '08718738034': 1, 'va': 1, 'hanger': 1, 'arrow': 1, 'blanket': 1, '08718726971': 1, 'tddnewsletter': 1, 'dozens': 1, 'thedailydraw': 1, 'emc1': 1, 'prizeswith': 1, 'significant': 1, 'waqt': 1, 'ko': 1, 'pehle': 1, 'wo': 1, 'naseeb': 1, 'jeetey': 1, 'nahi': 1, 'kisi': 1, 'kuch': 1, 'jo': 1, 'zyada': 1, 'milta': 1, 'hum': 1, 'zindgi': 1, 'sochte': 1, 'stalking': 1, 'reminded': 1, 'elaya': 1, 'varaya': 1, '09066368753': 1, '97n7qp': 1, 'anand': 1, 'expected': 1, 'beach': 1, 'workand': 1, 'jez': 1, 'whilltake': 1, 'todo': 1, 'zogtorius': 1, 'financial': 1, 'alian': 1, 'or2optout': 1, 'hv9d': 1, 'century': 1, 'frwd': 1, 'posible': 1, 'affectionate': 1, 'sorts': 1, 'restrictions': 1, 'buddys': 1, '08712402902': 1, 'owned': 1, 'possessive': 1, 'clarification': 1, 'coimbatore': 1, 'stream': 1, '0871212025016': 1, 'monos': 1, 'monoc': 1, 'polyc': 1, 'categories': 1, 'transcribing': 1, 'ethnicity': 1, 'census': 1, 'cakes': 1, 'draws': 1, 'asusual': 1, 'franyxxxxx': 1, 'goodmate': 1, 'cheered': 1, 'batt': 1, 'pobox1': 1, 'becausethey': 1, 'w14rg': 1, '09058098002': 1, 'gained': 1, 'limits': 1, 'pressure': 1, 'doke': 1, 'laying': 1, 'neshanth': 1, 'byatch': 1, 'whassup': 1, 'cl': 1, 'slo': 1, '4msgs': 1, 'filthyguys': 1, 'chiong': 1, 'reltnship': 1, 'wipe': 1, 'dialogue': 1, 'dryer': 1, 'comb': 1, 'pose': 1, 'fps': 1, 'computational': 1, 'disturbance': 1, 'premarica': 1, 'dlf': 1, 'gotto': 1, '220cm2': 1, 'err': 1, 'bloo': 1, 'hitter': 1, 'offline': 1, 'anjola': 1, 'asjesus': 1, 'ki': 1, 'imf': 1, 'corrupt': 1, 'deposited': 1, 'projects': 1, 'pura': 1, 'karo': 1, 'crore': 1, 'padhe': 1, 'suply': 1, 'lac': 1, 'blocked': 1, 'itna': 1, 'directors': 1, 'politicians': 1, 'swiss': 1, 'taxless': 1, 'torrents': 1, 'slowing': 1, 'particularly': 1, 'commit': 1, '83370': 1, 'trivia': 1, 'rightio': 1, 'brum': 1, 'scorable': 1, 'paranoid': 1, 'sheet': 1, 'brin': 1, 'bsnl': 1, 'complain': 1, 'offc': 1, 'bettr': 1, 'payed': 1, 'suganya': 1, 'dessert': 1, 'abeg': 1, 'sponsors': 1, 'onum': 1, 'imagination': 1, 'poet': 1, 'rr': 1, 'famamus': 1, 'locks': 1, 'jenne': 1, 'easiest': 1, 'barcelona': 1, 'sppok': 1, 'complementary': 1, '2px': 1, 'wa14': 1, 'pansy': 1, 'jungle': 1, 'kanji': 1, 'srs': 1, 'drizzling': 1, 'appointments': 1, 'excused': 1, 'reppurcussions': 1, 'necessity': 1, 'drama': 1, 'struggling': 1, 'ego': 1, 'cosign': 1, '09061701444': 1, 'hvae': 1, 'requires': 1, 'suman': 1, 'telephonic': 1, 'hcl': 1, 'freshers': 1, 'reliant': 1, 'fwiw': 1, 'afford': 1, 'arrival': 1, 'sq825': 1, 'citylink': 1, 'props': 1, 'statements': 1, 'pleasant': 1, 'pobox114': 1, '14tcr': 1, '6230': 1, 'splendid': 1, 'bognor': 1, 'ktv': 1, 'misplaced': 1, 'computers': 1, 'permanent': 1, 'registration': 1, 'begun': 1, 'residency': 1, 'risks': 1, 'predicting': 1, 'accumulation': 1, 'programs': 1, 'grief': 1, 'belongs': 1, 'shoranur': 1, 'prior': 1, 'fuelled': 1, 'fated': 1, 'concern': 1, 'txt82228': 1, 'text82228': 1, 'promptly': 1, 'honestly': 1, 'burnt': 1, 'quizclub': 1, '80122300p': 1, 'rwm': 1, '08704050406': 1, 'snap': 1, 'connected': 1, 'gmw': 1, 'someplace': 1, 'goods': 1, 'pressies': 1, 'ultimately': 1, 'achieve': 1, 'motive': 1, 'tui': 1, 'tor': 1, 'korli': 1, 'dock': 1, 'newscaster': 1, 'rolled': 1, 'flute': 1, 'wheel': 1, 'dabbles': 1, 'picsfree1': 1, 'keyword': 1, 'the4th': 1, 'october': 1, '83435': 1, 'safety': 1, 'aspects': 1, 'elaborating': 1, '85555': 1, 'tarot': 1, 'ours': 1, 'horniest': 1, 'cysts': 1, 'flow': 1, 'shrink': 1, 'ovarian': 1, 'developed': 1, 'grams': 1, 'upping': 1, 'timin': 1, 'apes': 1, 'ibm': 1, 'hp': 1, 'gosh': 1, 'spose': 1, 'rimac': 1, 'dosomething': 1, 'arestaurant': 1, 'squid': 1, 'dabooks': 1, 'eachother': 1, 'luckily': 1, 'starring': 1, 'restocked': 1, 'stoptxtstop': 1, 'knock': 1, 'tkls': 1, 'challenging': 1, 'smoothly': 1, 'breakfast': 1, 'hamper': 1, 'cc100p': 1, 'above': 1, '0870737910216yrs': 1, 'unni': 1, 'dramastorm': 1, 'particular': 1, 'lacking': 1, 'forfeit': 1, 'coupla': 1, 'digi': 1, '077xxx': 1, '09066362206': 1, 'sundayish': 1, 'prasad': 1, 'rcb': 1, 'kochi': 1, 'smear': 1, 'checkup': 1, 'gobi': 1, '4w': 1, 'technologies': 1, 'olowoyey': 1, 'argentina': 1, 'taxt': 1, 'lool': 1, 'tie': 1, 'massage': 1, 'pos': 1, 'shaking': 1, 'scarcasim': 1, 'naal': 1, 'eruku': 1, 'w4': 1, '5wq': 1, 'sensible': 1, 'impressively': 1, 'obedient': 1, 'ft': 1, 'combination': 1, 'needy': 1, 'playng': 1, 'jorge': 1, 'mcfly': 1, 'sara': 1, 'ab': 1, 'yupz': 1, 'ericson': 1, 'luks': 1, 'modl': 1, 'der': 1, 'frosty': 1, 'cheesy': 1, 'witin': 1, '0870141701216': 1, '120p': 1, 'fans': 1, '09050000555': 1, 'ba128nnfwfly150ppm': 1, '10th': 1, 'themed': 1, 'nudist': 1, 'pump': 1, 'signal': 1, 'unusual': 1, 'palm': 1, 'handing': 1, 'printing': 1, '83021': 1, 'stated': 1, 'perpetual': 1, 'dd': 1, 'flung': 1, 'pract': 1, 'brains': 1, 'justbeen': 1, 'overa': 1, 'mush': 1, 'tunde': 1, 'missions': 1, '20m12aq': 1, 'eh74rr': 1, 'avo': 1, 'cuddled': 1, 'crashed': 1, 'chachi': 1, 'pl': 1, 'tiz': 1, 'kanagu': 1, 'prices': 1, 'ringing': 1, 'houseful': 1, 'pulling': 1, 'brats': 1, 'derp': 1, 'abusers': 1, 'lipo': 1, 'netflix': 1, 'clash': 1, 'arr': 1, 'oscar': 1, 'rebtel': 1, 'firefox': 1, 'bcmsfwc1n3xx': 1, '69969': 1, 'impressed': 1, 'funs': 1, 'footy': 1, 'coca': 1, 'stadium': 1, 'cola': 1, 'large': 1, 'teenager': 1, 'replacing': 1, 'paracetamol': 1, 'mittelschmertz': 1, 'arrived': 1, 'references': 1, 'cthen': 1, 'conclusion': 1, 'instant': 1, '08715203028': 1, '9th': 1, 'rugby': 1, 'courtroom': 1, 'twiggs': 1, 'affidavit': 1, 'showers': 1, 'possessiveness': 1, 'golden': 1, 'poured': 1, 'lasting': 1, 'mobs': 1, 'ymca': 1, 'crazyin': 1, 'sleepingwith': 1, 'finest': 1, 'breathe1': 1, 'pobox365o4w45wq': 1, 'wtc': 1, 'weiyi': 1, '505060': 1, 'flowers': 1, 'interflora': 1, 'paining': 1, 'outgoing': 1, 'romcapspam': 1, 'presence': 1, 'mee': 1, 'maggi': 1, '08712103738': 1, 'cough': 1, 'pooja': 1, 'sweatter': 1, 'ambitious': 1, 'miiiiiiissssssssss': 1, 'tunji': 1, 'frndz': 1, '6missed': 1, 'misscall': 1, 'mad1': 1, 'mad2': 1, 'tall': 1, 'avenge': 1, 'robs': 1, 'gudni8': 1, 'choices': 1, 'toss': 1, 'coin': 1, 'dancin': 1, 'nora': 1, 'explicitly': 1, 'gayle': 1, 'crucify': 1, 'butting': 1, 'vs': 1, 'cedar': 1, 'reserved': 1, 'durham': 1, '69855': 1, 'sf': 1, 'stopbcm': 1, 'wall': 1, 'groovying': 1, 'printer': 1, 'groovy': 1, 'acnt': 1, 'harish': 1, 'transfred': 1, 'shaping': 1, 'showrooms': 1, 'attending': 1, 'doinat': 1, 'callon': 1, 'yhl': 1, 'pdate_now': 1, 'configure': 1, 'anal': 1, 'pears': 1, 'such': 1, 'oooooh': 1, '09058094454': 1, 'expiry': 1, 'resubmit': 1, 'mint': 1, 'humans': 1, 'studyn': 1, 'everyboy': 1, 'xxxxxxxx': 1, 'answr': 1, '1thing': 1, 'liquor': 1, 'loko': 1, '730': 1, 'lined': 1, 'laughs': 1, 'fireplace': 1, 'icon': 1, '08712400200': 1, 'weasels': 1, 'fifth': 1, 'woozles': 1, '08718723815': 1, 'machines': 1, 'fucks': 1, 'ignorant': 1, 'mys': 1, 'downs': 1, 'fletcher': 1, '08714714011': 1, 'bowls': 1, 'cozy': 1, 'shake': 1, 'buzzzz': 1, 'vibrator': 1, 'pros': 1, 'jet': 1, 'nuclear': 1, 'cons': 1, 'trends': 1, 'brief': 1, 'iter': 1, 'description': 1, 'fusion': 1, 'shitinnit': 1, 'ikno': 1, 'nowhere': 1, 'doesdiscount': 1, 'jabo': 1, 'slower': 1, 'maniac': 1, 'manege': 1, 'swalpa': 1, 'hogidhe': 1, 'chinnu': 1, 'sapna': 1, 'agidhane': 1, 'footbl': 1, 'crckt': 1, 'swell': 1, 'bollox': 1, 'tim': 1, 'tol': 1, 'ingredients': 1, 'pocy': 1, 'non': 1, '4qf2': 1, 'senor': 1, 'person2die': 1, 'possibly': 1, 'nvq': 1, 'giggle': 1, 'professional': 1, 'tiger': 1, 'woods': 1, 'grinder': 1, 'buyers': 1, 'figuring': 1, 'entirely': 1, 'disconnected': 1, 'onluy': 1, 'offcampus': 1, 'matters': 1, 'riley': 1, 'ew': 1, 'wesley': 1, 'lingo': 1, '400mins': 1, 'j5q': 1, 'chrgd': 1, '69200': 1, '2exit': 1, 'independence': 1, 'afternoons': 1, 'ugadi': 1, 'sankranti': 1, 'dasara': 1, 'rememberi': 1, 'teachers': 1, 'republic': 1, 'fools': 1, 'festival': 1, 'childrens': 1, 'approaching': 1, 'shivratri': 1, 'mornings': 1, 'joys': 1, 'greeting': 1, 'somewheresomeone': 1, 'daywith': 1, 'tosend': 1, 'lifeis': 1, 'selflessness': 1, 'initiate': 1, 'tallent': 1, 'wasting': 1, 'portal': 1, 't4get2text': 1, 'lennon': 1, 'bothering': 1, 'crab': 1, 'fox': 1, 'waves': 1, 'footprints': 1, 'frndsship': 1, 'dwn': 1, 'summon': 1, 'slaaaaave': 1, 'appendix': 1, 'slob': 1, 'smiled': 1, 'webpage': 1, 'yeesh': 1, 'gotbabes': 1, 'subscriptions': 1, 'hunks': 1, 'unsubscribed': 1, 'participate': 1, 'gopalettan': 1, 'stopcost': 1, '08712400603': 1, 'abroad': 1, 'xxsp': 1, 'mat': 1, 'agent': 1, 'goodies': 1, 'ay': 1, 'steal': 1, 'isaiah': 1, 'expert': 1, 'thinl': 1, 'importantly': 1, 'tightly': 1, 'fals': 1, 'pretsovru': 1, 'nav': 1, 'wnevr': 1, 'vth': 1, 'pretsorginta': 1, 'yen': 1, 'fal': 1, 'alwa': 1, 'madodu': 1, 'nammanna': 1, 'stdtxtrate': 1, 'soundtrack': 1, 'lord': 1, 'rings': 1, 'pc1323': 1, 'phyhcmk': 1, 'sg': 1, 'emigrated': 1, 'hopeful': 1, 'olol': 1, 'victors': 1, 'winterstone': 1, 'stagwood': 1, 'jp': 1, 'mofo': 1, 'maraikara': 1, 'pathaya': 1, 'enketa': 1, 'priest': 1, 'reserves': 1, 'intrude': 1, 'walkabout': 1, 'cashed': 1, 'announced': 1, 'blog': 1, '28th': 1, 'neville': 1, 'footie': 1, 'phil': 1, 'abbey': 1, 'returning': 1, 'punj': 1, 'str8': 1, 'classic': 1, '200p': 1, 'sacked': 1, 'mmsto': 1, '35p': 1, 'lookatme': 1, 'clip': 1, '32323': 1, 'twat': 1, 'punch': 1, 'barred': 1, 'decking': 1, 'dungerees': 1, 'mentionned': 1, 'vat': 1, 'grl': 1, 'madstini': 1, 'eerulli': 1, 'agalla': 1, 'kodstini': 1, 'hogli': 1, 'kodthini': 1, 'mutai': 1, 'hogolo': 1, 'messed': 1, 'illspeak': 1, 'thasa': 1, 'shudvetold': 1, 'urgran': 1, 'u2moro': 1, 'updat': 1, 'okden': 1, 'likeyour': 1, 'mecause': 1, 'werebored': 1, 'countinlots': 1, 'uin': 1, 'tex': 1, 'gr8fun': 1, 'tagged': 1, 'hdd': 1, 'casing': 1, 'opened': 1, 'describe': 1, '140ppm': 1, '08718725756': 1, '09053750005': 1, '310303': 1, 'asus': 1, 'reformat': 1, 'plumbers': 1, 'wrench': 1, 'bcum': 1, 'appeal': 1, 'thriller': 1, 'director': 1, 'shove': 1, 'um': 1, 'elephant': 1, 'cr': 1, 'pookie': 1, 'nri': 1, 'x2': 1, 'deserve': 1, 'neighbor': 1, 'toothpaste': 1, 'diddy': 1, 'poking': 1, 'coccooning': 1, 'mus': 1, 'talkin': 1, 'newquay': 1, '1im': 1, 'windy': 1, 'y87': 1, '09066358361': 1, 'tirunelvai': 1, 'dusk': 1, 'puzzles': 1, '09065989180': 1, 'x29': 1, 'phews': 1, 'stairs': 1, 'earning': 1, 'recycling': 1, 'toledo': 1, 'reservations': 1, 'tai': 1, 'feng': 1, 'swimsuit': 1, 'frndshp': 1, 'luvd': 1, 'squeeeeeze': 1, 'hurricanes': 1, 'disasters': 1, 'erupt': 1, 'sway': 1, 'aroundn': 1, 'arise': 1, 'volcanoes': 1, 'lighters': 1, 'lasagna': 1, 'woould': 1, 'chickened': 1, '08718726978': 1, '7732584351': 1, '44': 1, 'raviyog': 1, 'bhayandar': 1, 'peripherals': 1, 'sunoco': 1, 'musical': 1, 'leftovers': 1, 'plate': 1, 'starving': 1, 'fatty': 1, 'badrith': 1, 'owe': 1, 'checkin': 1, 'swann': 1, 'armenia': 1, '09058097189': 1, '1205': 1, '330': 1, '1120': 1, 'justify': 1, 'hunt': 1, 'hava': 1, '1131': 1, '5226': 1, 'adrian': 1, 'thnq': 1, 'rct': 1, 'vatian': 1, 'babysitting': 1, 'everyones': 1, 'buttheres': 1, 'ofsi': 1, 'aboutas': 1, 'gonnamissu': 1, 'yaxx': 1, 'breakin': 1, 'merememberin': 1, 'asthere': 1, 'neglet': 1, 'ramaduth': 1, 'siguviri': 1, 'mahaveer': 1, 'nalli': 1, 'ee': 1, 'problum': 1, 'dodda': 1, 'pavanaputra': 1, 'ondu': 1, 'keluviri': 1, 'maruti': 1, 'maretare': 1, 'poortiyagi': 1, 'bajarangabali': 1, 'sankatmochan': 1, 'hanumanji': 1, 'olage': 1, 'hanuman': 1, 'kalisidare': 1, 'odalebeku': 1, 'janarige': 1, 'idu': 1, 'inde': 1, 'ivatte': 1, 'matra': 1, 'ijust': 1, 'talked': 1, 'opps': 1, 'dl': 1, 'gei': 1, 'tron': 1, 'workage': 1, 'spiffing': 1, 'craving': 1, 'babysit': 1, 'supose': 1, 'embassy': 1, 'spaces': 1, 'lightly': 1, 'checkboxes': 1, 'batsman': 1, 'yetty': 1, '09050000928': 1, 'yifeng': 1, 'emailed': 1, 'slurp': 1, '3miles': 1, 'doll': 1, 'barolla': 1, 'brainless': 1, 'sariyag': 1, 'vehicle': 1, 'madoke': 1, '07090201529': 1, 'postponed': 1, 'stocked': 1, 'tiime': 1, 'afternon': 1, 'interviews': 1, 'resizing': 1, '09066364349': 1, 'box434sk38wp150ppm18': 1, 'opposed': 1, '08081263000': 1, '83332': 1, 'shortcode': 1, 'refunded': 1, 'somerset': 1, 'nigpun': 1, 'overtime': 1, 'dismissial': 1, 'screwd': 1, '08712402972': 1, 'bull': 1, 'floating': 1, '09058095201': 1, 'heehee': 1, 'percentages': 1, 'arithmetic': 1, 'chillaxin': 1, 'iknow': 1, 'peril': 1, 'das': 1, 'studentfinancial': 1, 'wellda': 1, 'monster': 1, 'obey': 1, 'uhhhhrmm': 1, 'deltomorrow': 1, '09066368470': 1, '24m': 1, 'subscriptn3gbp': 1, '68866': 1, '08448714184': 1, 'landlineonly': 1, 'smartcall': 1, 'orno': 1, 'minmobsmore': 1, 'fink': 1, 'lkpobox177hp51fl': 1, 'carlie': 1, 'promised': 1, '09099726553': 1, 'youwanna': 1, 'youphone': 1, 'athome': 1, 'jack': 1, 'hypotheticalhuagauahahuagahyuhagga': 1, 'pretend': 1, 'helpful': 1, 'brainy': 1, 'reflection': 1, 'occasion': 1, 'traditions': 1, 'values': 1, 'affections': 1, 'celebrated': 1, 'katexxx': 1, 'myparents': 1, 'cantdo': 1, 'anythingtomorrow': 1, 'outfor': 1, 'aretaking': 1, 'level': 1, 'gate': 1, '89105': 1, 'lingerie': 1, 'weddingfriend': 1, 'petticoatdreams': 1, 'bridal': 1, 'inst': 1, 'overheating': 1, 'board': 1, 'reslove': 1, 'western': 1, 'oblisingately': 1, 'bambling': 1, 'notixiquating': 1, 'champlaxigating': 1, 'laxinorficated': 1, 'opted': 1, 'masteriastering': 1, 'wotz': 1, 'atrocious': 1, 'entropication': 1, 'amplikater': 1, 'fidalfication': 1, 'junna': 1, 'knickers': 1, 'nikiyu4': 1, '01223585236': 1, 'divert': 1, 'a30': 1, 'wadebridge': 1, 'orc': 1, 'vill': 1, 'seeking': 1, 'wherre': 1, 'phone750': 1, 'resolution': 1, 'frank': 1, 'logoff': 1, 'parkin': 1, 'asa': 1, '09050000878': 1, 'charming': 1, 'served': 1, 'mention': 1, 'arnt': 1, 'xxxxxxxxxxxxxx': 1, 'dorothy': 1, 'kiefer': 1, 'allalo': 1, 'mone': 1, 'alle': 1, 'eppolum': 1, 'fundamentals': 1, 'whoever': 1, 'dooms': 1, '5digital': 1, '3optical': 1, '1mega': 1, 'pixels': 1, 'js': 1, 'noi': 1, 'captaining': 1, 'burgundy': 1, 'amrita': 1, 'bpo': 1, 'profile': 1, 'persevered': 1, 'nighters': 1, 'regretted': 1, 'spouse': 1, 'pmt': 1, 'shldxxxx': 1, '4give': 1, 'scenario': 1, 'nytho': 1, 'frmcloud': 1, 'spun': 1, '2mwen': 1, 'fonin': 1, 'tx': 1, 'wrld': 1, '09071517866': 1, '150ppmpobox10183bhamb64xe': 1, 'pounded': 1, 'broadband': 1, 'installation': 1, 'tensed': 1, 'coughing': 1, 'warned': 1, 'sprint': 1, 'gower': 1, 'morrow': 1, '450p': 1, 'filth': 1, '9yt': 1, 'stop2': 1, 'e14': 1, '08701752560': 1, 'saristar': 1, 'chik': 1, '420': 1, '9061100010': 1, 'mobcudb': 1, '1st4terms': 1, 'wire3': 1, 'sabarish': 1, '09050000460': 1, 'j89': 1, 'box245c2150pm': 1, 'flea': 1, 'inpersonation': 1, 'banneduk': 1, 'maximum': 1, 'highest': 1, '71': 1, 'hari': 1, 'wifes': 1, 'mumtaz': 1, 'facts': 1, 'taj': 1, 'arises': 1, 'known': 1, 'shahjahan': 1, 'lesser': 1, '69101': 1, 'rtf': 1, 'sphosting': 1, 'webadres': 1, 'geting': 1, 'passport': 1, 'independently': 1, 'showed': 1, 'multiply': 1, 'twins': 1, '02085076972': 1, 'strt': 1, 'ltdhelpdesk': 1, 'pesky': 1, 'cyclists': 1, 'uneventful': 1, 'equally': 1, 'nattil': 1, 'adi': 1, 'entey': 1, 'kittum': 1, 'hitman': 1, 'hire': 1, '2309': 1, '09066660100': 1, 'outages': 1, 'conserve': 1, 'cps': 1, 'voted': 1, 'epi': 1, 'bare': 1, 'bhaskar': 1, 'gong': 1, 'kaypoh': 1, 'outdoors': 1, 'basketball': 1, 'interfued': 1, 'listed': 1, 'apology': 1, 'harlem': 1, 'forth': 1, 'hustle': 1, 'fats': 1, 'workout': 1, 'zac': 1, 'hui': 1, 'versus': 1, 'underdtand': 1, 'locaxx': 1, 'muchxxlove': 1, '07090298926': 1, '9307622': 1, 'winds': 1, 'skateboarding': 1, 'thrown': 1, 'bandages': 1, 'html': 1, '1146': 1, 'mfl': 1, 'gbp4': 1, 'hectic': 1, 'dogs': 1, 'doggin': 1, 'wamma': 1, 'virtual': 1, 'apnt': 1, 'pants': 1, 'lanka': 1, 'go2sri': 1, 'gudnyt': 1, 'relationship': 1, 'wherevr': 1, 'merely': 1, 'smacks': 1, 'plum': 1, 'alot': 1, '50s': 1, 'formatting': 1, 'attracts': 1, '8714714': 1, 'promotion': 1, 'vegas': 1, 'lancaster': 1, 'soc': 1, 'advising': 1, 'bsn': 1, 'lobby': 1, 'showered': 1, 'ything': 1, 'lubly': 1, 'vewy': 1, '087147123779am': 1, 'catches': 1, 'domain': 1, 'specify': 1, 'nusstu': 1, 'bari': 1, 'hudgi': 1, 'yorge': 1, 'ertini': 1, 'pataistha': 1, 'hoops': 1, 'hasbro': 1, 'jump': 1, 'ummifying': 1, 'associate': 1, 'uterus': 1, 'rip': 1, 'jacuzzi': 1, 'txtstar': 1, 'uve': 1, '2nights': 1, 'wildest': 1, 'aldrine': 1, 'rtm': 1, 'unhappiness': 1, 'sources': 1, 'events': 1, 'functions': 1, 'irritated': 1, '4wrd': 1, 'colleg': 1, 'necesity': 1, 'wthout': 1, 'witout': 1, 'espe': 1, 'wth': 1, 'takecare': 1, 'univ': 1, 'rajas': 1, 'burrito': 1, 'stitch': 1, 'trouser': 1, '146tf150p': 1, 'cheetos': 1, 'synced': 1, 'shangela': 1, 'passes': 1, '08704439680': 1, 'poo': 1, 'uup': 1, 'gloucesterroad': 1, 'ouch': 1, 'fruit': 1, 'forgiveness': 1, 'glo': 1, '09058095107': 1, 's3xy': 1, 'wlcome': 1, 'timi': 1, 'strtd': 1, 'sack': 1, '1stone': 1, 'throwin': 1, 'fishrman': 1, 'stones': 1, '08717895698': 1, 'mobstorequiz10ppm': 1, 'physics': 1, 'delicious': 1, 'praveesh': 1, 'beers': 1, 'salad': 1, 'whore': 1, 'scallies': 1, 'twinks': 1, 'skins': 1, '08712466669': 1, 'jocks': 1, 'flood': 1, 'beads': 1, 'section': 1, 'nitro': 1, 'wishlist': 1, 'sold': 1, 'creative': 1, 'reffering': 1, 'getiing': 1, 'weirdy': 1, 'brownies': 1, '12hours': 1, 'k61': 1, '09061701851': 1, 'restrict': 1, '74355': 1, 'greece': 1, 'recorded': 1, 'someday': 1, 'grandfather': 1, 'november': 1, '75max': 1, 'blu': 1, '09061104276': 1, 'yuou': 1, 'spot': 1, 'lotto': 1, 'bunch': 1, 'purchases': 1, 'authorise': 1, '45pm': 1, 'goss': 1, 'gimmi': 1, 'ystrday': 1, 'chile': 1, 'subletting': 1, 'steering': 1, 'ammae': 1, 'sleeps': 1, 'required': 1, 'rounder': 1, 'batchlor': 1, 'lambu': 1, 'ji': 1, 'zoom': 1, '08717890890': 1, 'stopcs': 1, '62220cncl': 1, '0430': 1, 'true18': 1, '37819': 1, '1b6a5ecef91ff9': 1, 'jul': 1, 'chg': 1, 'cst': 1, 'xafter': 1, 'pure': 1, 'hearted': 1, 'enemies': 1, 'smiley': 1, 'yaxxx': 1, 'gail': 1, 'theoretically': 1, 'hooked': 1, 'formally': 1, 'multimedia': 1, 'housing': 1, 'accounting': 1, 'agency': 1, 'delayed': 1, 'renting': 1, 'vague': 1, 'presents': 1, 'nicky': 1, 'gumby': 1, 'wave': 1, '44345': 1, 'alto18': 1, 'sized': 1, 'springs': 1, 'tarpon': 1, 'steps': 1, 'cab': 1, 'limited': 1, 'hf8': 1, '08719181259': 1, 'radiator': 1, 'tongued': 1, 'proper': 1, 'shorts': 1, 'qi': 1, 'suddenly': 1, 'flurries': 1, 'webeburnin': 1, 'babygoodbye': 1, 'golddigger': 1, 'dontcha': 1, 'pushbutton': 1, 'real1': 1, 'perform': 1, 'cards': 1, 'rebooting': 1, 'nigh': 1, 'cable': 1, 'outage': 1, 'nooooooo': 1, 'sos': 1, 'playin': 1, 'guoyang': 1, 'rahul': 1, 'dengra': 1, 'fieldof': 1, 'selfindependence': 1, 'contention': 1, 'antelope': 1, 'toplay': 1, 'gnarls': 1, 'barkleys': 1, 'borderline': 1, '545': 1, 'nightnight': 1, 'possibility': 1, 'grooved': 1, 'mising': 1, '6669': 1, 'unsecured': 1, '195': 1, 'secured': 1, 'fakeye': 1, 'eckankar': 1, 'lanre': 1, '3000': 1, 'heater': 1, 'degrees': 1, 'dodgey': 1, 'asssssholeeee': 1, 'seing': 1, 'dreamz': 1, 'blokes': 1, 'rebel': 1, 'buddy': 1, 'ceri': 1, '84484': 1, 'nationwide': 1, 'newport': 1, 'juliana': 1, 'nachos': 1, 'dizzamn': 1, 'suitemates': 1, 'nimbomsons': 1, 'continent': 1, '087104711148': 1, 'emerging': 1, 'fiend': 1, 'impede': 1, 'hesitant': 1, '400thousad': 1, '60': 1, 'essay': 1, 'nose': 1, 'tram': 1, 'vic': 1, 'coherently': 1, 'echo': 1, 'triple': 1, 'cusoon': 1, 'afew': 1, 'honi': 1, 'onlyfound': 1, 'gran': 1, 'bx526': 1, 'southern': 1, 'rayan': 1, 'macleran': 1, 'balls': 1, 'olave': 1, 'mandara': 1, 'trishul': 1, 'woo': 1, 'hoo': 1, 'panties': 1, 'thout': 1, 'pints': 1, 'flatter': 1, 'carlin': 1, 'ciao': 1, 'starve': 1, 'impression': 1, 'darkness': 1, 'motivate': 1, 'wknd': 1, 'heltini': 1, 'trusting': 1, 'uttered': 1, 'yalrigu': 1, 'iyo': 1, 'noice': 1, 'esaplanade': 1, '139': 1, 'accessible': 1, '08709501522': 1, 'la3': 1, '2wu': 1, 'occurs': 1, 'enna': 1, 'kalaachutaarama': 1, 'prof': 1, 'coco': 1, 'sporadically': 1, 'pobox75ldns7': 1, '09064017305': 1, '38': 1, 'persolvo': 1, 'tbs': 1, 'manchester': 1, 'kath': 1, 'burden': 1, 'noworriesloans': 1, '08717111821': 1, 'harder': 1, 'nbme': 1, 'sickness': 1, 'villa': 1, 'sathya': 1, 'gam': 1, 'smash': 1, 'religiously': 1, 'tips': 1, 'heroes': 1, '08715203649': 1, '07973788240': 1, 'penny': 1, 'muhommad': 1, 'fiting': 1, 'load': 1, 'mj': 1, 'unconvinced': 1, 'willpower': 1, 'elaborate': 1, 'absence': 1, 'answerin': 1, 'evey': 1, 'prin': 1, '08714342399': 1, 'gigolo': 1, 'spam': 1, 'mens': 1, '50rcvd': 1, 'gsoh': 1, 'oncall': 1, 'mjzgroup': 1, 'ashwini': 1, '08707500020': 1, '09061790125': 1, 'ukp': 1, 'skinny': 1, 'casting': 1, 'thet': 1, 'elections': 1, '116': 1, 'serena': 1, 'amrca': 1, 'hlday': 1, 'camp': 1, 'prescribed': 1, 'meatballs': 1, 'approve': 1, 'panalam': 1, 'spjanuary': 1, 'fortune': 1, 'allday': 1, 'perf': 1, 'outsider': 1, '98321561': 1, 'familiar': 1, 'depression': 1, 'infact': 1, 'band': 1, 'simpsons': 1, 'kip': 1, 'shite': 1, 'hont': 1, 'upgrading': 1, 'amanda': 1, 'renewing': 1, 'subject': 1, 'regard': 1, 'nannys': 1, 'perspective': 1, 'puts': 1, 'conveying': 1, 'debating': 1, 'wtlp': 1, 'jb': 1, 'florida': 1, 'hidden': 1, 'teams': 1, 'swhrt': 1, 'po19': 1, '2ez': 1, '47': 1, '0906346330': 1, 'general': 1, 'jetton': 1, 'cmon': 1, 'replies': 1, 'lunsford': 1, 'enjoying': 1, '0796xxxxxx': 1, 'prizeawaiting': 1, 'gravy': 1, 'meals': 1, 'kfc': 1, '07008009200': 1, 'attended': 1, 'mw': 1, 'tuth': 1, 'eviction': 1, 'michael': 1, 'spiral': 1, 'riddance': 1, 'suffers': 1, 'cricket': 1, 'edward': 1, 'closeby': 1, 'raglan': 1, 'skye': 1, 'bookedthe': 1, 'hut': 1, 'drastic': 1, '3750': 1, 'garments': 1, 'sez': 1, 'evry1': 1, 'arab': 1, 'eshxxxxxxxxxxx': 1, 'lay': 1, 'bimbo': 1, 'ugo': 1, '3lions': 1, 'portege': 1, 'm100': 1, 'semiobscure': 1, 'gprs': 1, 'loosu': 1, 'careless': 1, 'freaking': 1, 'myspace': 1, 'logged': 1, 'method': 1, 'blur': 1, 'jewelry': 1, 'deluxe': 1, 'bbdeluxe': 1, 'features': 1, 'breaker': 1, 'graphics': 1, 'fumbling': 1, 'weekdays': 1, 'nails': 1, 'asia': 1, 'tobed': 1, '430': 1, 'stil': 1, 'asthma': 1, 'attack': 1, 'ball': 1, 'spin': 1, 'million': 1, 'haiyoh': 1, 'saves': 1, 'prsn': 1, 'sunlight': 1, 'relocate': 1, 'audiitions': 1, 'pocked': 1, 'motivating': 1, 'brison': 1, 'caps': 1, 'bullshit': 1, 'spelled': 1, 'motherfucker': 1, 'ig11': 1, '1013': 1, 'kit': 1, 'oja': 1, 'strip': 1, '08712402578': 1, 'thesmszone': 1, 'masked': 1, 'anonymous': 1, 'abuse': 1, 'parish': 1, 'magazine': 1, 'woodland': 1, 'avenue': 1, 'billy': 1, 'awww': 1, 'useless': 1, 'loo': 1, 'ed': 1, 'glands': 1, 'swollen': 1, 'bcaz': 1, 'truble': 1, 'evone': 1, 'hates': 1, 'stu': 1, 'view': 1, 'gays': 1, 'dual': 1, 'hostile': 1, 'breezy': 1, 'haircut': 1, '1tulsi': 1, 'leaf': 1, 'diseases': 1, 'litres': 1, 'watr': 1, 'problms': 1, '1apple': 1, '1lemon': 1, 'snd': 1, '1cup': 1, 'lavender': 1, 'manky': 1, 'inmind': 1, 'travelling': 1, 'scouse': 1, 'recreation': 1, 'judgemental': 1, 'fridays': 1, 'waheeda': 1, 'notes': 1, 'bot': 1, 'eventually': 1, 'hits': 1, 'tolerance': 1, '0789xxxxxxx': 1, 'hellogorgeous': 1, 'nitw': 1, 'texd': 1, 'jaz': 1, '4ward': 1, 'hopeu': 1, '09058091870': 1, 'exorcism': 1, 'emily': 1, 'prayrs': 1, 'othrwise': 1, 'evry': 1, 'emotion': 1, 'dsn': 1, 'sandiago': 1, 'parantella': 1, 'ujhhhhhhh': 1, 'hugging': 1, 'mango': 1, 'sweater': 1, 'involved': 1, 'bob': 1, 'barry': 1, '83738': 1, 'landmark': 1, 'consent': 1, 'clubzed': 1, 'tonexs': 1, 'renewed': 1, 'billing': 1, 'mathews': 1, 'anderson': 1, 'edwards': 1, 'tait': 1, 'promoting': 1, 'haunt': 1, 'crowd': 1, '8000930705': 1, 'snowboarding': 1, 'christmassy': 1, 'recpt': 1, 'baaaaaaaabe': 1, 'ignoring': 1, 'zealand': 1, 'education': 1, 'academic': 1, 'completes': 1, 'sagamu': 1, 'vital': 1, 'lautech': 1, 'shola': 1, 'qet': 1, 'browser': 1, 'surf': 1, 'subscribers': 1, 'conversations': 1, 'overemphasise': 1, 'senses': 1, 'convinced': 1, 'adp': 1, 'headset': 1, 'internal': 1, 'extract': 1, 'immed': 1, 'bevies': 1, 'waz': 1, 'fancied': 1, 'spoon': 1, 'comfey': 1, 'watchng': 1, 'othrs': 1, 'skint': 1, 'least5times': 1, 'quitting': 1, 'wudn': 1, 'frequently': 1, 'cupboard': 1, 'route': 1, '2mro': 1, 'floppy': 1, 'snappy': 1, 'risk': 1, 'grasp': 1, 'flavour': 1, 'laready': 1, 'denying': 1, 'dom': 1, 'ffffuuuuuuu': 1, 'julianaland': 1, 'oblivious': 1, 'dehydrated': 1, 'dogwood': 1, 'tiny': 1, 'mapquest': 1, 'archive': 1, '08719839835': 1, 'mgs': 1, '89123': 1, 'behalf': 1, 'lengths': 1, 'stunning': 1, 'visa': 1, 'gucci': 1, 'talkbut': 1, 'culdnt': 1, 'wenwecan': 1, 'sozi': 1, 'wannatell': 1, 'smsing': 1, 'efficient': 1, '15pm': 1, 'thandiyachu': 1, 'erutupalam': 1, 'invention': 1, 'lyrics': 1, 'somone': 1, 'nevr': 1, 'undrstnd': 1, 'definitly': 1, 'unrecognized': 1, 'valuing': 1, 'toking': 1, 'ger': 1, 'syd': 1, 'dhorte': 1, 'lage': 1, 'kintu': 1, 'opponenter': 1, 'khelate': 1, 'spares': 1, 'looovvve': 1, 'fried': 1, 'warwick': 1, 'tmw': 1, 'canceled': 1, 'havn': 1, 'tops': 1, 'grandma': 1, 'parade': 1, 'norcorp': 1, 'proze': 1, 'posting': 1, '7cfca1a': 1, 'grumble': 1, 'algebra': 1, 'linear': 1, 'decorating': 1, '946': 1, 'wining': 1, 'roomate': 1, 'graduated': 1, 'adjustable': 1, 'allows': 1, 'cooperative': 1, 'nottingham': 1, '40mph': 1, '63miles': 1, 'thanku': 1, 'guessed': 1, '50ea': 1, 'la1': 1, 'strings': 1, '7ws': 1, '89938': 1, 'otbox': 1, '731': 1, 'beside': 1, 'walks': 1, 'brisk': 1, 'dirtiest': 1, 'sexiest': 1, '89070': 1, 'tellmiss': 1, 'contribute': 1, 'greatly': 1, 'duvet': 1, 'smells': 1, 'urgh': 1, 'predictive': 1, 'coach': 1, 'w8in': 1, '4utxt': 1, '24th': 1, 'pist': 1, 'beverage': 1, 'surrender': 1, 'symptoms': 1, 'rdy': 1, 'backwards': 1, 'abstract': 1, 'avin': 1, 'africa': 1, 'chit': 1, '4217': 1, 'logon': 1, '6zf': 1, 'w1a': 1, '118p': 1, '8883': 1, 'cu': 1, 'quiteamuzing': 1, '4brekkie': 1, 'scool': 1, 'lrg': 1, 'psxtra': 1, 'satthen': 1, 'probpop': 1, 'portions': 1, '1000call': 1, '09071512432': 1, '300603t': 1, 'callcost150ppmmobilesvary': 1, 'rows': 1, 'njan': 1, 'fixd': 1, 'sudn': 1, 'engagement': 1, 'vilikkam': 1, 'maths': 1, 'chapter': 1, 'chop': 1, 'noooooooo': 1, '08718727870150ppm': 1, 'firsg': 1, 'split': 1, 'wasnt': 1, 'applyed': 1, 'heat': 1, 'sumfing': 1, 'ithink': 1, 'amnow': 1, 'bedreal': 1, 'layin': 1, 'tonsolitusaswell': 1, 'hopeso': 1, 'lotsof': 1, 'hiphop': 1, 'oxygen': 1, 'resort': 1, 'roller': 1, 'australia': 1, 'recorder': 1, 'canname': 1, 'mquiz': 1, 'showr': 1, 'upon': 1, 'ceiling': 1, 'ennal': 1, 'prakasam': 1, 'bcz': 1, 'prakasamanu': 1, 'presnts': 1, 'sneham': 1, 'mns': 1, 'jeevithathile': 1, 'neekunna': 1, 'irulinae': 1, 'mis': 1, 'blowing': 1, '7634': 1, '7684': 1, 'firmware': 1, 'vijaykanth': 1, 'anythiing': 1, 'clubmoby': 1, '08717509990': 1, 'ripped': 1, 'keypad': 1, 'btwn': 1, 'expects': 1, 'decades': 1, 'goverment': 1, 'spice': 1, 'ettans': 1, 'prasanth': 1, '08718738002': 1, '48922': 1, 'fizz': 1, 'contains': 1, 'appy': 1, 'genus': 1, 'robinson': 1, 'outs': 1, 'soz': 1, 'mums': 1, 'imat': 1, 'freinds': 1, 'sometext': 1, '9280114': 1, '07099833605': 1, 'chloe': 1, '130': 1, 'wewa': 1, 'iriver': 1, '255': 1, '128': 1, 'bw': 1, 'surly': 1, '9758': 1, '07808726822': 1, 'snuggles': 1, 'contented': 1, 'whispers': 1, 'mmmmmmm': 1, 'healthy': 1, '2bold': 1, 'barrel': 1, 'scraped': 1, 'misfits': 1, 'clearer': 1, 'sections': 1, 'peach': 1, 'tasts': 1, 'termsapply': 1, 'golf': 1, 'rayman': 1, 'activ8': 1, 'shindig': 1, 'phonebook': 1, 'ashes': 1, 'rocking': 1, 'shijutta': 1, 'offense': 1, 'dvg': 1, 'vinobanagar': 1, 'ovulate': 1, '3wks': 1, 'realising': 1, 'woah': 1, 'orh': 1, 'n8': 1, 'secrets': 1, 'hides': 1, 'akon': 1, 'axel': 1, 'eyed': 1, 'cashbin': 1, 'canteen': 1, 'stressfull': 1, 'adds': 1, 'continued': 1, 'president': 1, '180': 1, '140': 1, 'pleasured': 1, 'providing': 1, 'assistance': 1, 'whens': 1, '1172': 1, 'memories': 1, 'lonlines': 1, 'built': 1, 'lotz': 1, 'gailxx': 1, 'complacent': 1, 'denis': 1, 'mina': 1, 'miwa': 1, '09066649731from': 1, 'opposite': 1, 'heavily': 1, 'swayze': 1, 'patrick': 1, 'dolls': 1, '09077818151': 1, '30s': 1, 'santacalling': 1, 'calls1': 1, '50ppm': 1, 'quarter': 1, 'fired': 1, 'limping': 1, 'aa': 1, '08719180219': 1, '078498': 1, 'oga': 1, 'punishment': 1, 'poorly': 1, 'brb': 1, 'kill': 1, 'predicte': 1, 'situations': 1, 'loosing': 1, 'capacity': 1, 'smaller': 1, 'fgkslpo': 1, 'videos': 1, 'netun': 1, 'shsex': 1, 'fgkslpopw': 1, '0871277810710p': 1, 'defer': 1, 'admission': 1, 'checkmate': 1, 'maat': 1, 'shah': 1, 'chess': 1, 'persian': 1, 'phrase': 1, 'rats': 1, 'themes': 1, 'photoshop': 1, 'manageable': 1, '08715203652': 1, '42810': 1, 'increase': 1, 'carolina': 1, 'north': 1, 'texas': 1, 'gre': 1, 'bomb': 1, 'breathing': 1, 'powerful': 1, 'weapon': 1, 'lovly': 1, 'customercare': 1, 'msgrcvd': 1, 'clas': 1, 'lit': 1, 'couch': 1, 'loooooool': 1, 'swashbuckling': 1, 'terror': 1, 'cruel': 1, 'decent': 1, 'joker': 1, 'dip': 1, 'gek1510': 1, 'nuther': 1, 'lyricalladie': 1, 'hmmross': 1, '910': 1, 'differences': 1, 'happiest': 1, 'characters': 1, 'lists': 1, 'infections': 1, 'antibiotic': 1, 'gynae': 1, 'abdomen': 1, '6times': 1, 'exposed': 1, 'device': 1, 'beatings': 1, 'chastity': 1, 'uses': 1, 'wrenching': 1, 'gut': 1, 'tallahassee': 1, 'ou': 1, 'taka': 1, 'nr31': 1, '450pw': 1, 'pobox202': 1, '7zs': 1, 'ritten': 1, 'fold': 1, 'colin': 1, 'kiosk': 1, 'swat': 1, 'mre': 1, 'farrell': 1, '83118': 1, 'solihull': 1, 'terminated': 1, 'nhs': 1, '2b': 1, 'inconvenience': 1, 'dentists': 1, 'margin': 1, 'bergkamp': 1, 'yards': 1, 'henry': 1, '78': 1, 'parent': 1, 'unintentional': 1, 'snot': 1, 'nonetheless': 1, 'knees': 1, 'toaday': 1, 'grazed': 1, 'splat': 1, 'hooch': 1, 'deny': 1, 'hearin': 1, 'yah': 1, 'torture': 1, 'hopeing': 1, 'sisters': 1, 'sexychat': 1, 'lips': 1, 'congratulation': 1, 'court': 1, 'frontierville': 1, 'chapel': 1, 'mountain': 1, 'deer': 1, 'varma': 1, 'mailed': 1, 'secure': 1, 'parties': 1, 'farting': 1, 'ortxt': 1, 'dialling': 1, '402': 1, 'advisors': 1, 'trained': 1, 'stuffing': 1, 'woot': 1, 'ahhhh': 1, 'dining': 1, 'vouch4me': 1, 'etlp': 1, 'experiencehttp': 1, 'kaila': 1, '09058094507': 1, 'tsunami': 1, 'disaster': 1, 'donate': 1, 'unicef': 1, 'asian': 1, 'fund': 1, '864233': 1, 'cme': 1, 'hos': 1, 'collapsed': 1, 'cumming': 1, 'jade': 1, 'paul': 1, 'barmed': 1, 'thinkthis': 1, 'dangerous': 1, '762': 1, 'goldviking': 1, 'rushing': 1, 'coulda': 1, 'phony': 1, 'okday': 1, 'petexxx': 1, 'fromwrk': 1, 'adrink': 1, 'bthere': 1, 'buz': 1, 'outsomewhere': 1, 'wedlunch': 1, '2watershd': 1, 'hmph': 1, 'baller': 1, 'punto': 1, 'travelled': 1, 'ayo': 1, '125': 1, 'freeentry': 1, 'xt': 1, 'toyota': 1, 'olayiwola': 1, 'landing': 1, 'camry': 1, 'mileage': 1, 'clover': 1, 'amma': 1, 'achan': 1, 'rencontre': 1, 'mountains': 1, '08714712412': 1, 'puppy': 1, 'noise': 1, 'meg': 1, '08715203685': 1, '4xx26': 1, 'crossing': 1, '09094646631': 1, 'deepest': 1, 'darkest': 1, 'inconvenient': 1, 'adsense': 1, 'approved': 1, 'dudette': 1, 'perumbavoor': 1, 'stage': 1, 'clarify': 1, 'preponed': 1, 'natalie2k9': 1, '165': 1, 'natalie': 1, 'younger': 1, '08701213186': 1, 'liver': 1, 'opener': 1, 'guides': 1, 'watched': 1, 'loneliness': 1, 'skyving': 1, 'onwords': 1, 'mtnl': 1, 'mumbai': 1, 'mustprovide': 1, '62735': 1, '83039': 1, 'accommodationvouchers': 1, '450': 1, '15541': 1, 'rajitha': 1, 'ranju': 1, 'styles': 1, '1winawk': 1, 'tscs08714740323': 1, '50perweeksub': 1, '09066361921': 1, 'disagreeable': 1, 'afterwards': 1, 'vivekanand': 1, 'uawake': 1, 'deviousbitch': 1, 'aletter': 1, 'thatmum': 1, '4thnov': 1, 'gotmarried': 1, 'fuckinnice': 1, 'ourbacks': 1, 'justfound': 1, 'feellikw': 1, 'election': 1, 'rearrange': 1, 'eleven': 1, 'dormitory': 1, 'hitler': 1, 'starer': 1, 'recount': 1, 'astronomer': 1, 'worms': 1, 'suffering': 1, 'dysentry': 1, 'virgil': 1, 'andre': 1, 'gokila': 1, 'uncut': 1, 'dino': 1, 'diamond': 1, 'shanil': 1, 'exchanged': 1, 'kotees': 1, 'zebra': 1, 'sugababes': 1, 'panther': 1, 'badass': 1, 'hoody': 1, 'resent': 1, 'queries': 1, 'customersqueries': 1, 'netvision': 1, 'haughaighgtujhyguj': 1, 'andres': 1, 'hassling': 1, 'londn': 1, 'fassyole': 1, 'blacko': 1, 'responsibilities': 1, '08715205273': 1, 'vco': 1, 'humanities': 1, 'reassurance': 1, 'albi': 1, 'mahfuuz': 1, 'beeen': 1, 'tohar': 1, 'aslamalaikkum': 1, 'mufti': 1, 'muht': 1, '078': 1, 'tocall': 1, 'enufcredeit': 1, 'ileave': 1, 'treats': 1, 'okors': 1, 'ibored': 1, 'adding': 1, 'savings': 1, 'zeros': 1, 'goigng': 1, 'perfume': 1, 'sday': 1, 'grocers': 1, 'pubs': 1, 'bennys': 1, 'frankie': 1, 'owed': 1, 'diapers': 1, 'changing': 1, 'unlike': 1, 'turkeys': 1, 'patients': 1, 'princes': 1, 'helens': 1, 'unintentionally': 1, 'wenever': 1, 'stability': 1, 'vibrant': 1, 'colourful': 1, 'tranquility': 1, 'failing': 1, 'failure': 1, 'bawling': 1, 'velusamy': 1, 'facilities': 1, 'karnan': 1, 'bluray': 1, 'salt': 1, 'wounds': 1, 'logging': 1, 'geoenvironmental': 1, 'implications': 1, 'fuuuuck': 1, 'salmon': 1, 'uploaded': 1, 'wrkin': 1, 'ree': 1, 'compensation': 1, 'awkward': 1, 'splash': 1, 'musta': 1, 'leg': 1, 'overdid': 1, 'telediscount': 1, 'foned': 1, 'chuck': 1, 'port': 1, 'stuffs': 1, 'juswoke': 1, 'docks': 1, 'spinout': 1, 'boatin': 1, '08715203656': 1, '42049': 1, 'uworld': 1, 'assessment': 1, 'qbank': 1, 'someonone': 1, '09064015307': 1, 'tke': 1, 'temales': 1, 'finishd': 1, 'dull': 1, 'studies': 1, 'anyones': 1, 'craigslist': 1, 'treadmill': 1, 'absolutely': 1, 'swan': 1, 'hehe': 1, 'shexy': 1, 'sall': 1, 'lamp': 1, 'foward': 1, '09061790126': 1, 'misundrstud': 1, '2u2': 1, 'genes': 1, 'com1win150ppmx3age16subscription': 1, 'resuming': 1, 'reapply': 1, 'treatin': 1, 'treacle': 1, 'mumhas': 1, 'beendropping': 1, 'theplace': 1, 'adress': 1, 'favorite': 1, 'rumbling': 1, 'sashimi': 1, 'oyster': 1, 'marandratha': 1, 'correctly': 1, 'alaikkum': 1, 'heaven': 1, 'pisces': 1, 'aquarius': 1, '2yrs': 1, 'wicket': 1, 'steyn': 1, 'sterm': 1, 'resolved': 1, 'wheat': 1, 'chex': 1, 'hannaford': 1, 'jam': 1, 'grownup': 1, 'costume': 1, 'jerk': 1, 'stink': 1, 'subsequent': 1, 'follows': 1, 'openings': 1, 'upcharge': 1, 'guai': 1, 'astrology': 1, 'slacking': 1, 'mentor': 1, 'percent': 1, 'erotic': 1, 'ecstacy': 1, '09095350301': 1, 'dept': 1, '08717507382': 1, 'coincidence': 1, 'sane': 1, 'helping': 1, 'pause': 1, '151': 1, 'leading': 1, '8800': 1, 'psp': 1, 'gr8prizes': 1, 'spacebucks': 1, '083': 1, '6089': 1, 'squeezed': 1, 'maintaining': 1, 'dreading': 1, 'thou': 1, 'suggestion': 1, 'forgt': 1, 'lands': 1, 'helps': 1, 'ajith': 1, 'yoville': 1, 'ooooooh': 1, 'asda': 1, 'counts': 1, 'officer': 1, 'carly': 1, 'bffs': 1, '\u3028ud': 1, 'seperated': 1, 'franxx': 1, 'brolly': 1, 'syrup': 1, '5mls': 1, 'feed': 1, 'prometazine': 1, 'singapore': 1, 'shu': 1, 'victoria': 1, 'pocay': 1, '2morrowxxxx': 1, 'wocay': 1, 'ramen': 1, 'broth': 1, 'fowler': 1, 'tats': 1, 'flew': 1, '09058094583': 1, 'attention': 1, 'tix': 1, 'fne': 1, 'youdoing': 1, 'foregate': 1, 'shrub': 1, 'worc': 1, 'get4an18th': 1, '32000': 1, 'efreefone': 1, 'legitimat': 1, 'pendent': 1, 'toilet': 1, 'cops': 1, 'stolen': 1, 'navigate': 1, 'require': 1, 'choosing': 1, 'hu': 1, 'guidance': 1, 'chick': 1, 'boobs': 1, 'revealing': 1, 'org': 1, '2025050': 1, '0121': 1, 'shortbreaks': 1, 'sparkling': 1, 'breaks': 1, '45': 1, 'gyno': 1, 'belong': 1, 'gamb': 1, 'treasure': 1, '820554ad0a1705572711': 1, '09050000332': 1, 'negative': 1, 'positive': 1, 'hmmmm': 1, 'command': 1, 'stressful': 1, 'holby': 1, '09064017295': 1, 'li': 1, 'lecturer': 1, 'repeating': 1, 'motor': 1, 'yeovil': 1, 'rhode': 1, 'bong': 1, 'ofcourse': 1, '08448350055': 1, 'planettalkinstant': 1, '2p': 1, 'spider': 1, 'marvel': 1, 'ultimate': 1, '8ball': 1, '83338': 1, 'tamilnadu': 1, 'tip': 1, '07808247860': 1, '40411': 1, '08719899229': 1, 'identification': 1, 'boundaries': 1, 'limit': 1, 'endless': 1, 'reassuring': 1, 'young': 1, 'referin': 1, 'saibaba': 1, 'colany': 1, 'declare': 1, 'chic': 1, '49557': 1, 'disappointment': 1, 'irritation': 1, 'tantrum': 1, 'compliments': 1, 'adventuring': 1, 'chief': 1, 'gsex': 1, 'wc1n': 1, '2667': 1, '3xx': 1, 'l8er': 1, 'bailiff': 1, 'inclu': 1, '3mobile': 1, 'servs': 1, 'chatlines': 1, 'mouse': 1, 'desk': 1, 'childporn': 1, 'jumpers': 1, 'belt': 1, 'cribbs': 1, 'hat': 1, 'spiritual': 1, 'barring': 1, 'influx': 1, 'sudden': 1, 'kane': 1, 'shud': 1, 'pshew': 1, '4years': 1, 'units': 1, 'accent': 1, 'dental': 1, 'nmde': 1, 'dump': 1, 'heap': 1, 'lowes': 1, 'salesman': 1, '087187272008': 1, 'now1': 1, 'suggestions': 1, 'pity': 1, 'bitching': 1}), 'lowercase': True, 'n': 5574, 'ngram_range': (1, 1), 'normalize': True, 'on': None, 'preprocessor': None, 'processing_steps': [<function strip_accents_unicode at 0x7ff10730dc10>, <method 'lower' of 'str' objects>, <built-in method findall of re.Pattern object at 0x7ff107682440>], 'strip_accents': True, 'tokenizer': <built-in method findall of re.Pattern object at 0x7ff107682440>} Normalizer {'order': 2} RandomUnderSampler(LogisticRegression) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': True, '_rng': RandomState(MT19937) at 0x7FF107657B40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.9 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } The results of the logistic regression are quite good but still inferior to the naive Bayes model. Let's try to use word embeddings to improve our logistic regression. Word embeddings allow you to represent a word as a vector. Embeddings are developed to build semantically rich vectors. For instance, the vector which represents the word python should be close to the vector which represents the word programming . We will use spaCy to convert our sentence to vectors. spaCy converts a sentence to a vector by calculating the average of the embeddings of the words in the sentence. You can download pre-trained embeddings in many languages. We will use English pre-trained embeddings as our SMS are in English. The command below allows you to download the pre-trained embeddings that spaCy makes available. More informations about spaCy and its installation may be found here here . python -m spacy download en_core_web_sm Here, we create a custom transformer to convert an input sentence to a dict of floats. We will integrate this transformer into our pipeline. import spacy from river.base import Transformer class Embeddings ( Transformer ): \"\"\"My custom transformer, word embedding using spaCy.\"\"\" def __init__ ( self ): self . embeddings = spacy . load ( 'en_core_web_sm' ) def transform_one ( self , x , y = None ): return { dimension : xi for dimension , xi in enumerate ( self . embeddings ( x ) . vector )} Let's train our logistic regression: X_y = datasets . SMSSpam () model = ( extract_body | Embeddings () | preprocessing . Normalizer () | imblearn . RandomOverSampler ( classifier = linear_model . LogisticRegression ( optimizer = optim . SGD ( .5 ), loss = optim . losses . Log () ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.91568 The confusion matrix: cm False True False 4517 310 True 78 669 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] Embeddings {'embeddings': <spacy.lang.en.English object at 0x7ff1085a5940>} Normalizer {'order': 2} RandomOverSampler(LogisticRegression) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': False, '_rng': RandomState(MT19937) at 0x7FF10852DB40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.5 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } The results of the logistic regression using spaCy embeddings are lower than those obtained with TF-IDF values. We could surely improve the results by cleaning up the text. We could also use embeddings more suited to our dataset. However, on this problem, the logistic regression is not better than the Naive Bayes model. No free lunch today.","title":"Sentence classification"},{"location":"examples/sentence_classification/#sentence-classification","text":"In this tutorial we will try to predict whether an SMS is a spam or not. To train our model, we will use the SMSSpam dataset. This dataset is unbalanced, there is only 13.4% spam. Let's look at the data: from river import datasets datasets . SMSSpam () SMS Spam Collection dataset. The data contains 5,574 items and 1 feature (i.e. SMS body). Spam messages represent 13.4% of the dataset. The goal is to predict whether an SMS is a spam or not. Name SMSSpam Task Binary classification Samples 5,574 Features 1 Sparse False Path /Users/max.halford/river_data/SMSSpam/SMSSpamCollection URL https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip Size 466.71 KB Downloaded True from pprint import pprint X_y = datasets . SMSSpam () for x , y in X_y : pprint ( x ) print ( f 'Spam: { y } ' ) break {'body': 'Go until jurong point, crazy.. Available only in bugis n great world ' 'la e buffet... Cine there got amore wat...\\n'} Spam: False Let's start by building a simple model like a Naive Bayes classifier. We will first preprocess the sentences with a TF-IDF transform that our model can consume. Then, we will measure the accuracy of our model with the AUC metric. This is the right metric to use when the classes are not balanced. In addition, the Naive Bayes models can perform very well on unbalanced datasets and can be used for both binary and multi-class classification problems. from river import feature_extraction from river import naive_bayes from river import metrics def extract_body ( x ): \"\"\"Extract the body of the sms.\"\"\" return x [ 'body' ] X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | naive_bayes . BernoulliNB ( alpha = 0 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) if y_pred is not None : metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.929296 The confusion matrix: cm False True False 4809 17 True 103 644 The results are quite good with this first model. Since we are working with an unbalanced dataset, we can use the imblearn module to rebalance the classes of our dataset. For more information about the imblearn module, you can find a dedicated tutorial here . from river import imblearn X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | imblearn . RandomUnderSampler ( classifier = naive_bayes . BernoulliNB ( alpha = 0 ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) if y_pred is not None : metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.951728 The imblearn module improved our results. Not bad! We can visualize the pipeline to understand how the data is processed. The confusion matrix: cm False True False 4624 201 True 41 706 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] TFIDF {'dfs': Counter({'to': 1687, 'you': 1591, 'the': 1035, 'in': 810, 'and': 795, 'is': 752, 'me': 690, 'for': 624, 'it': 614, 'my': 613, 'your': 587, 'call': 551, 'of': 550, 'have': 531, 'that': 512, 'on': 488, 'now': 481, 'are': 445, 'so': 433, 'can': 425, 'but': 422, 'not': 418, 'or': 398, 'at': 378, 'get': 368, 'just': 365, 'do': 359, 'be': 355, 'will': 354, 'with': 350, 'if': 350, 'we': 350, 'no': 343, 'this': 319, 'ur': 310, 'up': 297, 'how': 293, 'ok': 280, 'what': 279, 'from': 273, 'when': 272, 'go': 264, 'out': 262, 'all': 261, 'll': 258, 'know': 247, 'gt': 242, 'lt': 242, 'good': 233, 'then': 232, 'got': 229, 'free': 229, 'like': 229, 'come': 217, 'there': 214, 'am': 214, 'day': 212, 'only': 211, 'its': 210, 'time': 210, 'was': 196, 'send': 195, 'want': 188, 'love': 178, 'text': 174, 'he': 171, 'going': 167, 'txt': 165, 'home': 165, 'one': 163, 'by': 162, 'need': 162, 'today': 159, 'see': 158, 'as': 156, 'still': 155, 'about': 151, 'sorry': 151, 'don': 148, 'back': 147, 'lor': 145, 'our': 143, 'take': 140, 'dont': 139, 'stop': 138, 'reply': 135, 'da': 134, 'tell': 134, 'new': 133, 'later': 132, 'please': 131, 'any': 131, 'think': 129, 'mobile': 128, 'been': 126, 'phone': 124, 'here': 122, 'hi': 120, 'some': 119, 'did': 119, 'well': 119, 'she': 117, 'they': 116, 're': 114, 'where': 114, 'hope': 112, 'hey': 111, 'dear': 111, 'week': 110, 'oh': 110, 'pls': 110, 'has': 109, 'much': 109, 'night': 109, 'great': 108, 'claim': 108, 'an': 108, 'too': 107, 'msg': 106, 'more': 103, 'yes': 103, 'wat': 102, 'had': 101, 'him': 101, 'www': 100, 'her': 100, 'make': 99, 'work': 98, 'give': 98, 'way': 97, 've': 95, 'won': 95, 'who': 95, 'message': 93, 'number': 92, 'tomorrow': 90, 'after': 90, 'say': 89, 'already': 89, 'should': 89, 'doing': 88, 'right': 86, 'yeah': 86, 'happy': 86, 'prize': 84, 'why': 84, 'really': 83, 'ask': 83, 'find': 81, 'meet': 80, 'said': 80, 'cash': 78, 'im': 78, 'last': 78, 'let': 77, 'morning': 76, 'babe': 76, 'them': 76, 'lol': 74, 'thanks': 74, 'miss': 73, 'cos': 73, 'care': 73, 'anything': 72, '150p': 71, 'amp': 71, 'uk': 71, 'pick': 71, 'also': 71, 'win': 70, 'very': 70, 'urgent': 69, 'com': 69, 'would': 69, 'sure': 68, 'something': 68, 'contact': 68, 'over': 67, 'life': 67, 'sent': 67, 'again': 66, 'keep': 66, 'wait': 65, 'every': 65, 'his': 64, 'cant': 64, 'first': 62, 'us': 62, 'buy': 62, 'gud': 62, 'before': 62, 'thing': 62, 'even': 61, 'min': 61, 'soon': 60, 'next': 60, 'place': 60, 'off': 60, '50': 59, 'nice': 59, 'which': 59, 'customer': 58, 'tonight': 58, 'always': 58, 'service': 58, 'around': 57, 'per': 57, 'were': 57, 'late': 57, 'someone': 57, 'gonna': 56, 'feel': 56, 'could': 56, 'money': 56, 'help': 55, 'down': 55, 'sms': 55, 'sleep': 55, 'leave': 55, 'co': 54, '16': 54, 'other': 54, 'many': 54, 'wan': 54, 'nokia': 53, 'went': 53, 'told': 53, 'friends': 52, 'try': 52, 'waiting': 52, 'dun': 51, '18': 51, 'chat': 51, 'friend': 51, 'may': 50, 'fine': 50, 'guaranteed': 50, 'ya': 50, 'coming': 50, 'getting': 49, 'done': 49, 'special': 49, 'yet': 49, 'people': 49, 'haha': 49, 'use': 48, 'year': 48, 'same': 48, 'mins': 48, 'wish': 48, 'didn': 47, 'things': 47, 'holiday': 47, 'thk': 47, 'name': 46, 'man': 46, 'best': 46, 'thought': 46, 'talk': 45, 'bit': 45, 'hello': 44, 'draw': 44, 'few': 44, '500': 44, 'person': 44, 'cs': 44, 'stuff': 43, 'yup': 43, 'trying': 43, 'meeting': 43, 'thats': 43, 'job': 43, 'line': 43, 'heart': 43, 'being': 42, 'class': 42, 'never': 42, 'cool': 42, 'long': 42, 'better': 42, 'ill': 42, 'having': 42, 'days': 42, 'tone': 42, 'cost': 41, '100': 41, 'car': 41, 'live': 41, '1000': 41, 'house': 41, 'ready': 41, 'mind': 41, 'finish': 40, 'enjoy': 40, 'lunch': 39, 'half': 39, 'play': 39, 'check': 39, '10': 39, 'real': 39, 'lot': 39, 'dat': 39, 'chance': 39, 'god': 39, 'word': 38, 'awarded': 38, 'wanna': 38, 'box': 38, 'nothing': 38, 'guess': 38, 'sir': 38, 'lar': 37, 'latest': 37, 'end': 37, 'another': 37, 'liao': 37, 'guys': 37, 'than': 37, 'dinner': 36, 'month': 36, 'sweet': 36, 'ah': 36, 'shows': 36, 'big': 36, 'into': 36, 'shit': 36, '1st': 36, 'world': 35, 'xxx': 35, 'eat': 35, 'po': 35, 'account': 35, 'bt': 35, 'might': 35, 'problem': 35, 'quite': 35, 'receive': 34, 'camera': 34, 'watching': 34, 'smile': 34, '150ppm': 34, 'landline': 34, 'because': 34, 'start': 34, 'speak': 33, 'wont': 33, 'room': 33, 'yo': 33, 'wk': 33, 'aight': 33, 'luv': 33, 'tv': 33, 'offer': 33, 'called': 33, 'two': 33, 'probably': 33, 'rate': 32, 'apply': 32, 'remember': 32, 'left': 32, 'weekend': 32, 'once': 32, 'forgot': 32, 'jus': 32, 'watch': 32, 'plan': 32, 'actually': 32, 'bad': 32, 'princess': 32, 'early': 31, 'code': 31, 'does': 31, 'look': 31, 'maybe': 31, 'hear': 31, 'between': 31, 'easy': 31, 'reach': 31, 'thanx': 31, 'video': 31, 'shopping': 31, 'shall': 31, 'dunno': 31, 'minutes': 31, 'office': 31, 'fun': 30, '2nd': 30, 'part': 30, 'anyway': 30, 'didnt': 30, 'hour': 30, 'baby': 30, 'ever': 30, 'sat': 30, 'network': 29, 'selected': 29, 'enough': 29, 'thank': 29, 'bus': 29, 'ringtone': 29, 'pa': 29, 'looking': 29, 'bed': 29, 'birthday': 29, 'girl': 29, 'little': 29, 'working': 29, 'leh': 29, 'made': 29, 'orange': 29, 'put': 29, 'dad': 29, 'town': 29, 'pay': 28, 'calls': 28, 'afternoon': 28, 'those': 28, 'evening': 28, 'collect': 28, 'everything': 28, 'asked': 28, 'true': 28, 'texts': 28, 'den': 28, 'while': 28, 'kiss': 28, 'until': 27, 'though': 27, '000': 27, 'since': 27, 'pain': 27, 'dis': 27, 'came': 27, 'okay': 27, 'must': 27, 'join': 27, 'tmr': 27, 'details': 27, 'fuck': 27, 'wif': 26, 'wanted': 26, 'most': 26, 'means': 26, 'says': 26, 'mail': 26, 'able': 26, 'important': 26, 'wake': 26, 'collection': 26, 'goes': 25, 'times': 25, 'mob': 25, 'haven': 25, '5000': 25, 'show': 25, 'price': 25, 'school': 25, '2000': 25, 'sexy': 25, 'til': 25, 'guy': 25, 'away': 25, 'valid': 24, 'alright': 24, 'messages': 24, 'missed': 24, 'saw': 24, 'yesterday': 24, 'wen': 24, 'havent': 24, 'abt': 24, 'else': 24, 'juz': 24, 'years': 24, 'hav': 24, 'weekly': 24, 'wot': 24, 'bring': 24, 'attempt': 24, 'yours': 23, 'run': 23, 'making': 23, 'worry': 23, 'haf': 23, 'coz': 23, 'id': 23, 'oso': 23, '10p': 23, 'music': 23, 'stay': 23, 'bored': 23, 'these': 23, 'wife': 23, 'gift': 23, 'plus': 23, 'lei': 23, 'question': 22, 'colour': 22, 'net': 22, 'words': 22, 'national': 22, 'tried': 22, 'yourself': 22, 'address': 22, 'food': 22, 'top': 22, 'without': 22, 'boy': 22, 'decimal': 22, 'shop': 22, 'nite': 22, 'hot': 22, 'book': 22, 'friendship': 22, 'dude': 22, 'change': 22, 'feeling': 22, 'either': 22, '800': 22, 'online': 22, 'family': 22, 'de': 22, 'entry': 21, 'hours': 21, 'http': 21, 'ard': 21, 'till': 21, 'delivery': 21, 'hair': 21, 'bonus': 21, 'test': 21, 'driving': 21, 'busy': 21, 'todays': 21, 'answer': 21, 'nt': 21, 'xmas': 21, 'both': 21, 'vouchers': 21, 'full': 21, 'plz': 21, 'tones': 21, 'calling': 21, 'tot': 21, 'sae': 21, 'together': 21, 'wants': 21, 'goin': 21, 'sad': 21, 'brother': 20, 'set': 20, 'date': 20, 'trip': 20, 'comes': 20, 'movie': 20, 'mean': 20, 'old': 20, 'points': 20, 'award': 20, 'leaving': 20, 'order': 20, 'believe': 20, 'story': 20, 'sleeping': 20, 'noe': 20, 'happen': 20, 'face': 20, 'wid': 20, 'ring': 20, 'huh': 20, 'sch': 20, 'game': 20, 'makes': 20, 'await': 20, 'pounds': 19, 'news': 19, 'aft': 19, 'doesn': 19, 'tomo': 19, 'congrats': 19, 'took': 19, 'double': 19, 'finished': 19, 'started': 19, 'private': 19, 'gr8': 19, 'minute': 19, 'awesome': 19, 'wil': 19, '750': 19, '86688': 19, 'hurt': 19, 'row': 19, 'pm': 19, 'head': 19, 'eve': 19, 'beautiful': 19, 'thinking': 19, 'mum': 19, 'saying': 19, 'rite': 19, 'available': 18, 'final': 18, 'update': 18, 'tho': 18, 'xx': 18, 'close': 18, 'cause': 18, 'services': 18, 'taking': 18, 'missing': 18, 'touch': 18, 'walk': 18, 'unsubscribe': 18, 'charge': 18, 'lets': 18, 'post': 18, 'drink': 18, '250': 18, 'land': 18, 'gd': 18, '150': 18, 'mine': 18, 'pics': 18, 'pub': 18, 'email': 18, 'drive': 18, 'drop': 18, 'dreams': 18, '11': 17, 'lesson': 17, 'second': 17, 'lucky': 17, 'search': 17, '12hrs': 17, 'statement': 17, 'expires': 17, 'msgs': 17, 'open': 17, 'whats': 17, 'lots': 17, 'everyone': 17, 'carlos': 17, 'worth': 17, 'sis': 17, 'sounds': 17, 'company': 17, 'choose': 17, 'club': 17, 'okie': 17, 'card': 17, 'sister': 17, 'chikku': 17, 'poly': 17, 'dating': 17, 'opt': 17, 'neva': 17, 'anyone': 17, 'loving': 17, 'alone': 17, 'treat': 16, 'winner': 16, 'info': 16, 'pobox': 16, 'saturday': 16, 'decided': 16, 'forget': 16, '08000930705': 16, 'girls': 16, 'smiling': 16, 'prob': 16, 'gone': 16, 'happened': 16, 'identifier': 16, 'type': 16, 'ni8': 16, 'ltd': 16, 'hard': 16, 'each': 16, 'boytoy': 16, 'found': 16, 'college': 16, 'break': 16, 'anytime': 16, 'far': 16, 'games': 16, 'mobileupd8': 16, 'bout': 16, 'kind': 16, 'visit': 16, 'fast': 16, 'voucher': 16, 'sun': 16, '8007': 16, 'hows': 16, 'wonderful': 15, 'smth': 15, 'mom': 15, 'camcorder': 15, 'used': 15, 'hit': 15, 'operator': 15, 'friday': 15, 'quiz': 15, 'player': 15, 'parents': 15, 'frnd': 15, 'finally': 15, 'darlin': 15, 'goodmorning': 15, 'oredi': 15, 'secret': 15, 'congratulations': 15, 'hold': 15, 'takes': 15, 'read': 15, 'suite342': 15, '2lands': 15, '08000839402': 15, 'fucking': 15, 'nope': 15, 'outside': 15, 'pretty': 15, 'sea': 15, 'whatever': 15, 'weeks': 15, 'lovely': 15, 'mates': 15, 'wrong': 15, 'party': 15, 'nyt': 15, 'pic': 15, 'crazy': 14, 'wkly': 14, 'freemsg': 14, 'credit': 14, 'seeing': 14, 'whole': 14, 'frnds': 14, 'isn': 14, 'hmm': 14, 'mu': 14, 'their': 14, 'content': 14, 'fancy': 14, 'log': 14, 'course': 14, 'mrng': 14, 'tc': 14, 'thinks': 14, 'case': 14, 'tel': 14, 'meant': 14, 'fr': 14, 'angry': 14, 'light': 14, 'jay': 14, 'project': 14, 'reason': 14, 'ten': 14, 'welcome': 14, 'cum': 14, 'b4': 14, 'mate': 14, 'least': 14, 'earlier': 14, 'chennai': 14, '30': 14, 'point': 13, 'press': 13, 'valued': 13, 'hungry': 13, 'almost': 13, 'hee': 13, '0800': 13, 'felt': 13, 'invited': 13, '03': 13, 'caller': 13, 'numbers': 13, 'yr': 13, 'tired': 13, 'wit': 13, 'needs': 13, 'hmmm': 13, 'mr': 13, 'smoke': 13, 'balance': 13, 'march': 13, 'side': 13, '87066': 13, 'dnt': 13, 'unlimited': 13, 'fone': 13, 'stupid': 13, 'bslvyl': 13, 'lost': 13, 'reading': 13, 'txts': 13, 'ago': 13, 'currently': 13, 'motorola': 13, 'talking': 13, 'couple': 13, 'phones': 13, 'ass': 13, 'park': 13, 'frm': 13, 'fri': 13, 'offers': 13, 'within': 13, '2003': 13, 'un': 13, 'listen': 13, 'yar': 13, 'knw': 13, 'sex': 13, 'mayb': 13, 'understand': 13, 'knew': 13, 'gas': 13, 'comp': 12, '12': 12, 'mobiles': 12, '20': 12, 'eh': 12, 'confirm': 12, 'telling': 12, 'wow': 12, 'correct': 12, 'pass': 12, 'etc': 12, 'complimentary': 12, 'gotta': 12, 'loads': 12, 'computer': 12, 'mah': 12, 'askd': 12, 'uncle': 12, 'sending': 12, 'direct': 12, 'age': 12, 'hand': 12, 'bank': 12, 'bcoz': 12, 'laptop': 12, 'questions': 12, 'swing': 12, 'ge': 12, 'ends': 12, 'die': 12, '200': 12, 'via': 12, 'met': 12, 'call2optout': 12, 'seen': 12, 'rental': 12, 'india': 12, 'doin': 12, 'lose': 12, 'ipod': 12, '04': 12, 'redeemed': 12, 'through': 12, 'gym': 12, 'happiness': 12, 'snow': 12, 'area': 12, 'sound': 12, 'picking': 12, 'ugh': 12, 'extra': 12, 'heard': 12, 'support': 12, 'surprise': 12, 'information': 12, 'grins': 12, 'luck': 12, 'enter': 12, 'auction': 12, 'difficult': 12, 'wasn': 12, 'std': 11, 'usf': 11, 'sunday': 11, 'eg': 11, 'comin': 11, 'charged': 11, 'abiola': 11, 'crave': 11, 'gets': 11, 'ac': 11, 'move': 11, 'checking': 11, 'cut': 11, 'rply': 11, 'download': 11, 'shower': 11, 'entered': 11, 'match': 11, '350': 11, 'txting': 11, 'lovable': 11, 'wine': 11, 'safe': 11, 'orchard': 11, 'kate': 11, 'rs': 11, 'semester': 11, 'wana': 11, 'somebody': 11, 'rest': 11, 'christmas': 11, 'pete': 11, 'plans': 11, 'small': 11, 'ex': 11, 'w1j6hl': 11, 'hg': 11, 'discount': 11, 'slow': 11, 'yep': 11, 'th': 11, 'supposed': 11, 'asking': 11, 'remove': 11, 'monday': 11, 'simple': 11, 'noon': 11, 'darren': 11, 'ans': 11, 'store': 11, 'wonder': 11, 'sort': 11, 'asap': 11, 'na': 11, 'nobody': 11, 'nah': 10, '900': 10, 'months': 10, 'link': 10, 'ha': 10, 'worried': 10, 'myself': 10, 'knows': 10, 'oops': 10, 'hospital': 10, 'red': 10, 'reached': 10, 'forever': 10, 'song': 10, 'save': 10, 'tickets': 10, 'il': 10, 'representative': 10, 'gave': 10, 'rates': 10, 'del': 10, 'sony': 10, 'pray': 10, 'dream': 10, 'spend': 10, 'muz': 10, 'bath': 10, 'bathe': 10, 'study': 10, 'exam': 10, 'street': 10, 'reveal': 10, 'admirer': 10, 'deep': 10, 'own': 10, 'leaves': 10, 'blue': 10, 'usual': 10, 'somewhere': 10, 'normal': 10, 'merry': 10, 'immediately': 10, 'custcare': 10, 'weed': 10, 'rakhesh': 10, 'moment': 10, 'st': 10, 'woke': 10, 'mm': 10, 'voice': 10, 'ldn': 10, 'booked': 10, 'different': 10, 'terms': 10, 'water': 10, 'sub': 10, '00': 10, 'across': 10, 'warm': 10, 'cheap': 10, 'clean': 10, 'em': 10, 'ts': 10, 'drugs': 10, 'laugh': 10, 'fantastic': 10, 'glad': 10, 'wishing': 10, 'getzed': 10, 'whenever': 10, 'otherwise': 10, 'ntt': 10, 'truth': 10, 'gn': 10, 'convey': 10, 'film': 10, '2nite': 10, 'write': 10, 'fact': 10, 'loved': 10, 'slowly': 10, 'cup': 9, 'copy': 9, 'reward': 9, 'england': 9, 'seriously': 9, 'sick': 9, 'catch': 9, 'decide': 9, 'ice': 9, 'situation': 9, 'short': 9, 'rain': 9, 'coffee': 9, 'men': 9, 'boss': 9, 'specially': 9, 'ending': 9, 'sunshine': 9, 'lazy': 9, 'completely': 9, 'staying': 9, 'doesnt': 9, 'especially': 9, 'studying': 9, 'trust': 9, 'using': 9, 'deal': 9, 'itself': 9, 'dead': 9, 'mrt': 9, 'bill': 9, 'lessons': 9, 'goodnight': 9, 'cd': 9, 'ldew': 9, 'lover': 9, 'disturb': 9, 'credits': 9, 'worries': 9, 'tonite': 9, 'unless': 9, '4u': 9, '2day': 9, '11mths': 9, 'valentines': 9, 'urself': 9, 'bluetooth': 9, 'rock': 9, 'starts': 9, 'kinda': 9, 'loan': 9, 'meh': 9, 'near': 9, 'rent': 9, 'silent': 9, 'less': 9, 'children': 9, 'hoping': 9, 'age16': 9, 'self': 9, 'train': 9, 'forwarded': 9, 'starting': 9, 'paper': 9, 'seems': 9, 'sell': 9, 'eyes': 9, 'possible': 9, 'ones': 9, 'gettin': 9, 'poor': 9, 'tampa': 9, 'user': 9, 'mo': 9, 'against': 9, 'hiya': 9, 'doctor': 9, 'mon': 9, 'john': 9, 'mode': 9, 'others': 9, 'wondering': 9, 'ringtones': 9, 'bb': 9, 'tht': 9, '20p': 9, 'moral': 9, 'excellent': 9, 'father': 9, 'thinkin': 9, 'sitting': 9, 'sofa': 9, 'request': 8, 'entitled': 8, 'anymore': 8, 'promise': 8, 'wap': 8, 'pizza': 8, 'mark': 8, 'cheers': 8, 'quick': 8, 'replying': 8, 'nigeria': 8, 'cinema': 8, 'ip4': 8, '5we': 8, 'stand': 8, 'spent': 8, 'loves': 8, 'hurts': 8, 'trouble': 8, 'planning': 8, 'ave': 8, 'wishes': 8, 'weekends': 8, 'apartment': 8, 'inc': 8, 'paying': 8, '2004': 8, 'buying': 8, 'bak': 8, 'sp': 8, 'dvd': 8, 'dogging': 8, 'swt': 8, 'joy': 8, 'goto': 8, 'freephone': 8, 'joined': 8, 'however': 8, 'ive': 8, 'slept': 8, 'sign': 8, 'kick': 8, 'lemme': 8, 'rose': 8, 'cake': 8, 'fixed': 8, 'rcvd': 8, 'interested': 8, 'round': 8, 'figure': 8, 'reference': 8, 'mistake': 8, 'facebook': 8, 'al': 8, 'yahoo': 8, 'aha': 8, '3030': 8, 'funny': 8, 'giving': 8, 'din': 8, 'thru': 8, 'style': 8, 'opinion': 8, '02': 8, 'savamob': 8, 'member': 8, 'fingers': 8, '50p': 8, 'blood': 8, 'daddy': 8, 'door': 8, 'kids': 8, 'pound': 8, 'ar': 8, 'alex': 8, 'longer': 8, '25p': 8, 'pc': 8, 'xy': 8, 'bedroom': 8, 'king': 8, 'idea': 8, 'add': 8, 'library': 8, 'slave': 8, 'omg': 8, 'no1': 8, 'polys': 8, 'moon': 8, 'training': 8, 'gay': 8, 'sale': 8, '08712460324': 8, 'registered': 8, 'miracle': 8, 'during': 8, 'movies': 8, 'digital': 8, 'black': 8, 'awaiting': 8, 'cancel': 8, 'cute': 8, 'energy': 8, 'complete': 8, 'honey': 8, 'picked': 8, 'vl': 8, 'frens': 8, 'reaching': 8, '0870': 8, 'cover': 8, '06': 8, 'south': 8, 'inside': 8, 'hw': 8, 'wednesday': 8, 'pix': 8, 'mood': 8, 'la': 7, 'bugis': 7, 'cine': 7, 'naughty': 7, 'sucks': 7, 'tea': 7, 'eating': 7, 'learn': 7, 'ahead': 7, 'kept': 7, 'liked': 7, 'bx420': 7, 'joke': 7, 'wun': 7, 'following': 7, 'ta': 7, 'pleasure': 7, '10am': 7, 'password': 7, 'cuz': 7, 'page': 7, 'umma': 7, 'weight': 7, 'bother': 7, 'country': 7, '82277': 7, 'yijue': 7, 'lect': 7, 'persons': 7, 'sometimes': 7, 'become': 7, '62468': 7, 'internet': 7, 'waste': 7, 'hell': 7, 'experience': 7, 'towards': 7, 'bucks': 7, 'past': 7, 'biz': 7, 'appreciate': 7, 'road': 7, 'battery': 7, '25': 7, 'kallis': 7, 'cal': 7, 'showing': 7, 'naked': 7, 'horny': 7, 'quality': 7, 'definitely': 7, 'sense': 7, 'sim': 7, 'loyalty': 7, 'high': 7, 'advance': 7, 'power': 7, 'return': 7, 'access': 7, '08718720201': 7, 'wiv': 7, 'fault': 7, 'maximize': 7, 'cold': 7, 'forward': 7, 'happening': 7, 'lift': 7, 'tough': 7, 'tenerife': 7, 'notice': 7, '8th': 7, 'depends': 7, 'realy': 7, 'mp3': 7, '85023': 7, 'unsub': 7, 'single': 7, 'fat': 7, 'married': 7, 'rather': 7, 'hotel': 7, 'omw': 7, 'hurry': 7, 'workin': 7, 'gee': 7, 'izzit': 7, 'spree': 7, 'present': 7, 'valentine': 7, 'future': 7, 'shuhui': 7, 'weather': 7, 'login': 7, 'tuesday': 7, 'ho': 7, 'awake': 7, 'bold': 7, 'looks': 7, 'dey': 7, 'sit': 7, '7pm': 7, 'holla': 7, 'summer': 7, 'damn': 7, 'space': 7, '36504': 7, 'bag': 7, 'model': 7, 'mother': 7, 'yrs': 7, 'mid': 7, 'midnight': 7, 'january': 7, 'iam': 7, 'photo': 7, 'sk38xh': 7, 'recently': 7, 'feels': 7, 'heavy': 7, 'nxt': 7, '3g': 7, 'o2': 7, 'onto': 7, 'station': 7, 'tuition': 7, 'strong': 7, 'cell': 7, 'dog': 7, 'alrite': 7, 'shd': 7, 'croydon': 7, 'cr9': 7, '5wb': 7, '1327': 7, 'meaning': 7, 'players': 7, 'share': 7, 'lmao': 7, 'except': 7, 'arrive': 7, 'instead': 7, 'holding': 7, 'list': 7, 'thnk': 7, 'excuse': 7, 'costa': 7, 'sol': 7, 'including': 7, 'vikky': 7, 'colleagues': 7, 'tear': 7, 'worse': 7, 'murderer': 7, 'maid': 7, 'murdered': 7, 'happens': 7, 'feb': 7, 'planned': 7, 'joking': 6, 'hl': 6, 'click': 6, 'team': 6, 'texting': 6, 'tyler': 6, 'usually': 6, 'fyi': 6, '150pm': 6, 'review': 6, 'pleased': 6, 'kano': 6, 'simply': 6, 'changed': 6, 'eatin': 6, 'flights': 6, 'directly': 6, 'informed': 6, 'app': 6, 'standard': 6, '08712300220': 6, 'shouldn': 6, 'replied': 6, 'local': 6, 'qatar': 6, 'arrange': 6, 'inviting': 6, 'turns': 6, 'spoke': 6, 'personal': 6, 'nights': 6, 'system': 6, 'partner': 6, 'died': 6, 'website': 6, 'tncs': 6, 'childish': 6, 'handset': 6, 'dint': 6, 'sunny': 6, 'ended': 6, 'anybody': 6, 'imagine': 6, 'babes': 6, 'sport': 6, 'accept': 6, 'kb': 6, 'yoga': 6, 'track': 6, 'ish': 6, 'cc': 6, 'posted': 6, 'air': 6, 'willing': 6, 'body': 6, 'relax': 6, 'pilates': 6, 'putting': 6, 'fullonsms': 6, 'competition': 6, 'aathi': 6, 'wnt': 6, 'vry': 6, 'vary': 6, 'askin': 6, 'group': 6, 'ttyl': 6, 'isnt': 6, 'gives': 6, 'moan': 6, 'fb': 6, 'activate': 6, 'character': 6, 'jst': 6, 'tat': 6, '40gb': 6, 'pin': 6, 'campus': 6, 'lady': 6, 'l8r': 6, 'aiyo': 6, 'barely': 6, 'scream': 6, 'marriage': 6, 'announcement': 6, 'indian': 6, 'ladies': 6, '28': 6, 'imma': 6, 'daily': 6, 'paid': 6, 'vodafone': 6, 'matches': 6, 'holder': 6, 'evng': 6, 'earth': 6, 'under': 6, 'torch': 6, 'aftr': 6, 'exactly': 6, 'yay': 6, 'txtauction': 6, 'yest': 6, 'closed': 6, 'wats': 6, 'couldn': 6, 'pobox84': 6, 'norm150p': 6, 'w45wq': 6, 'hunny': 6, 'boo': 6, 'teasing': 6, 'zed': 6, 'green': 6, 'surely': 6, 'five': 6, 'wed': 6, 'fall': 6, 'sup': 6, 'murder': 6, 'due': 6, 'teach': 6, 'ate': 6, 'wherever': 6, 'medical': 6, 'brand': 6, 'contract': 6, 'kerala': 6, 'asleep': 6, 'loverboy': 6, 'serious': 6, 'april': 6, 'flower': 6, 'process': 6, 'works': 6, 'regards': 6, 'sipix': 6, 'aiyah': 6, 'urawinner': 6, 'gal': 6, 'howz': 6, 'raining': 6, 'thts': 6, 'tour': 6, 'super': 6, 'marry': 6, 'problems': 6, 'fantasies': 6, '08707509020': 6, 'walking': 6, 'cafe': 6, 'bought': 6, '4th': 6, 'nature': 6, 'keeping': 6, 'screaming': 6, '86021': 6, 'london': 6, 'lookin': 6, 'exciting': 6, 'toclaim': 6, 'max10mins': 6, 'pobox334': 6, '09050090044': 6, 'stockport': 6, 'theatre': 6, 'ahmad': 6, 'official': 6, 'armand': 6, 'nimya': 6, 'sed': 6, 'role': 6, 'checked': 6, 'added': 6, 'pussy': 6, 'budget': 6, 'random': 6, 'er': 6, 'hr': 6, 'hrs': 6, 'cancer': 6, 'tariffs': 6, 'meds': 6, 'darling': 5, 'callers': 5, 'callertune': 5, 'searching': 5, 'wet': 5, '87077': 5, 'stock': 5, 'egg': 5, 'subscription': 5, 'roommate': 5, 'hopefully': 5, 'ride': 5, 'respect': 5, 'urgnt': 5, '530': 5, 'truly': 5, 'scared': 5, 'cabin': 5, 'voda': 5, 'quoting': 5, 'ec2a': 5, 'laid': 5, 'locations': 5, 'rooms': 5, 'begin': 5, 'shirt': 5, 'menu': 5, 'hop': 5, 'discuss': 5, 'bye': 5, '9am': 5, 'transaction': 5, 'cannot': 5, 'straight': 5, 'connection': 5, 'sen': 5, 'atm': 5, 'romantic': 5, '2optout': 5, 'flirt': 5, 'sam': 5, 'argument': 5, 'wins': 5, 'fix': 5, 'singles': 5, 'rays': 5, 'bf': 5, '21': 5, 'themob': 5, 'selection': 5, 'aren': 5, 'pongal': 5, 'december': 5, 'ppl': 5, 'cud': 5, 'report': 5, 'surfing': 5, 'num': 5, 'basically': 5, 'allah': 5, 'sonyericsson': 5, 'geeee': 5, 'sighs': 5, 'brings': 5, 'guide': 5, 'intro': 5, 'current': 5, 'pictures': 5, 'none': 5, 'yan': 5, 'jiu': 5, 'logo': 5, 'pobox36504w45wq': 5, 'contacted': 5, 'hostel': 5, 'hv': 5, 'amt': 5, 'respond': 5, 'ticket': 5, 'dollars': 5, 'acc': 5, 'woman': 5, 'flat': 5, 'sec': 5, 'conditions': 5, 'fighting': 5, 'some1': 5, 'spl': 5, 'stylish': 5, '83355': 5, 'returns': 5, 'quote': 5, 'english': 5, 'btw': 5, '2mrw': 5, 'smiles': 5, 'jazz': 5, 'yogasana': 5, '1x150p': 5, 'stopped': 5, 'somethin': 5, 'results': 5, 'euro2004': 5, 'drinks': 5, '80062': 5, 'thursday': 5, 'listening': 5, 'cartoon': 5, 'drug': 5, 'fetch': 5, 'belly': 5, 'lonely': 5, 'law': 5, 'gap': 5, 'timing': 5, 'running': 5, 'mad': 5, 'twice': 5, 'opportunity': 5, 'gals': 5, 'city': 5, 'tis': 5, 'living': 5, 'polyphonic': 5, 'xxxx': 5, 'comuk': 5, 'ages': 5, 'sura': 5, 'playing': 5, 'matter': 5, 'sn': 5, 'hai': 5, 'records': 5, 'cds': 5, 'birds': 5, 'travel': 5, 'lead': 5, 'unsold': 5, 'derek': 5, 'greet': 5, 'white': 5, 'cheaper': 5, 'ym': 5, 'pissed': 5, 'ma': 5, 'wear': 5, 'expensive': 5, 'photos': 5, 'site': 5, 'boring': 5, 'ad': 5, 'salary': 5, 'noline': 5, 'dload': 5, 'videochat': 5, 'videophones': 5, 'rentl': 5, 'java': 5, 'dropped': 5, 'yun': 5, 'jesus': 5, 'gm': 5, '3rd': 5, 'bitch': 5, 'revealed': 5, 'xchat': 5, 'hands': 5, 'receipt': 5, 'interesting': 5, 'uni': 5, 'italian': 5, 'adult': 5, 'oz': 5, 'horrible': 5, 'nw': 5, 'jordan': 5, 'choice': 5, 'mite': 5, 'chinese': 5, 'hun': 5, 'cbe': 5, 'broke': 5, 'original': 5, 'pple': 5, 'arrested': 5, 'linerental': 5, 'vote': 5, 'tells': 5, 'totally': 5, 'rem': 5, 'exams': 5, 'everybody': 5, 'optout': 5, 'google': 5, 'vomit': 5, 'centre': 5, 'airport': 5, 'costs': 5, 'buzz': 5, 'eerie': 5, 'waking': 5, 'ran': 5, '60p': 5, 'boys': 5, 'bin': 5, 'social': 5, 'buns': 5, 'created': 5, 'beer': 5, 'season': 5, 'nvm': 5, 'moms': 5, 'obviously': 5, 'flag': 5, 'eng': 5, 'inclusive': 5, 'looked': 5, 'expecting': 5, 'latr': 5, 'minuts': 5, 'unable': 5, 'remind': 5, 'whether': 5, 'fantasy': 5, 'brilliant': 5, 'police': 5, 'ru': 5, 'cars': 5, 'plenty': 5, 'amount': 5, 'advice': 5, 'behind': 5, 'amazing': 5, 'issues': 5, 'ignore': 5, 'thurs': 5, 'wouldn': 5, 'lik': 5, 'asks': 5, '300': 5, '3510i': 5, '400': 5, 'mths': 5, 'common': 5, 'oni': 4, '87121': 4, 'tkts': 4, 'lives': 4, 'tb': 4, 'oru': 4, 'six': 4, '87575': 4, 'membership': 4, 'str': 4, 'sooner': 4, 'turn': 4, 'child': 4, 'letter': 4, 'inches': 4, 'weak': 4, 'seemed': 4, 'url': 4, 'series': 4, 'iq': 4, 'wah': 4, 'machan': 4, 'becoz': 4, '9pm': 4, 'fml': 4, 'appointment': 4, 'hols': 4, 'legal': 4, 'nyc': 4, 'considering': 4, 'jokes': 4, 'research': 4, 'tt': 4, 'needed': 4, '786': 4, 'unredeemed': 4, 'hasn': 4, 'yetunde': 4, 'ansr': 4, 'tyrone': 4, 'largest': 4, 'befor': 4, 'activities': 4, 'biggest': 4, 'netcollex': 4, 'deleted': 4, 'interview': 4, '434': 4, 'escape': 4, 'bloody': 4, 'anyways': 4, '4742': 4, '145': 4, '0808': 4, '11pm': 4, 'radio': 4, 'unique': 4, 'settled': 4, 'shoot': 4, 'files': 4, 'career': 4, 'cross': 4, 'recd': 4, 'closer': 4, 'argue': 4, 'theory': 4, 'com1win150ppmx3age16': 4, 'affection': 4, 'manda': 4, 'kettoda': 4, 'bcums': 4, 'expect': 4, 'mmm': 4, 'bay': 4, 'hmv': 4, 'passed': 4, 'throw': 4, 'cam': 4, 'accidentally': 4, 'cry': 4, 'def': 4, 'meal': 4, 'dates': 4, 'hanging': 4, 'belovd': 4, 'enemy': 4, 'smart': 4, 'afraid': 4, '08002986906': 4, 'kisses': 4, 'waitin': 4, '83600': 4, '1000s': 4, 'practice': 4, 'wtf': 4, 'further': 4, 'sometime': 4, 'cream': 4, 'tree': 4, 'esplanade': 4, 'fifteen': 4, '3mins': 4, 'wc1n3xx': 4, 'journey': 4, 'jen': 4, 'gorgeous': 4, 'purpose': 4, 'tenants': 4, 'refused': 4, 'ure': 4, 'intelligent': 4, 'result': 4, 'reasons': 4, 'receiving': 4, 'cw25wx': 4, 'tcs': 4, 'charges': 4, 'dry': 4, 'center': 4, 'village': 4, 'bringing': 4, 'jada': 4, 'matured': 4, 'kusruthi': 4, 'prabha': 4, 'mtmsgrcvd18': 4, 'bday': 4, 'rude': 4, 'mas': 4, 'passionate': 4, 'confidence': 4, 'losing': 4, 'three': 4, 'milk': 4, 'essential': 4, 'lab': 4, 'quit': 4, '08715705022': 4, '24': 4, 'grand': 4, '542': 4, 'pie': 4, 'answers': 4, 'often': 4, 'uncles': 4, 'leona': 4, 'bud': 4, 'taken': 4, 'church': 4, 'temple': 4, 'gentle': 4, 'hella': 4, 'bet': 4, 'envelope': 4, 'prepare': 4, 'seem': 4, 'explain': 4, 'purchase': 4, 'weird': 4, 'drivin': 4, 'students': 4, 'height': 4, 'max': 4, 'assume': 4, '81151': 4, '4t': 4, 'faster': 4, 'spoken': 4, 'mt': 4, 'skilgme': 4, '88039': 4, 'meetin': 4, 'apparently': 4, 'smokes': 4, 'sing': 4, 'perfect': 4, '08718727870': 4, 'enjoyed': 4, 'dictionary': 4, 'm263uz': 4, 'appt': 4, '3d': 4, 'ain': 4, 'ache': 4, '3qxj9': 4, '08702840625': 4, '9ae': 4, 'profit': 4, 'cust': 4, 'ppm': 4, 'ibiza': 4, 'meanwhile': 4, 'suite': 4, 'version': 4, 'careful': 4, 'spk': 4, 'saved': 4, 'played': 4, 'wanting': 4, 'pig': 4, 'attend': 4, 'term': 4, 'fever': 4, 'places': 4, 'w1': 4, 'carefully': 4, 'gravity': 4, 'bowl': 4, 'decision': 4, 'sore': 4, 'regret': 4, 'throat': 4, 'lecture': 4, 'raise': 4, 'fool': 4, 'june': 4, 'technical': 4, 'bathing': 4, 'vijay': 4, 'dem': 4, 'fight': 4, 'subscriber': 4, 'aiyar': 4, 'wearing': 4, 'shame': 4, 'credited': 4, 'understanding': 4, 'delivered': 4, 'arms': 4, 'easier': 4, 'txtin': 4, '4info': 4, '08712405020': 4, 'songs': 4, 'exact': 4, '2moro': 4, '80488': 4, 'favour': 4, '3gbp': 4, 'idiot': 4, 'february': 4, 'rush': 4, 'blackberry': 4, 'moji': 4, 'fill': 4, 'gently': 4, '4get': 4, 'msgrcvdhg': 4, 'aiya': 4, 'pod': 4, '7th': 4, '6th': 4, '5th': 4, 'wonders': 4, 'personality': 4, 'purity': 4, 'aint': 4, 'sha': 4, 'total': 4, 'along': 4, 'file': 4, 'shortly': 4, 'ron': 4, '7250i': 4, 'w1jhl': 4, 'preferably': 4, 'idk': 4, 'whom': 4, 'laughing': 4, 'title': 4, 'brought': 4, 'surprised': 4, 'comedy': 4, 'moby': 4, 'action': 4, 'sight': 4, 'remain': 4, 'received': 4, 'ordered': 4, 'rd': 4, 'queen': 4, 'fren': 4, 'connect': 4, 'hook': 4, '05': 4, 'schedule': 4, 'selling': 4, 'settings': 4, 'alert': 4, 'atlanta': 4, 'gaps': 4, 'fills': 4, 'takin': 4, 'answering': 4, 'jess': 4, 'dirty': 4, 'package': 4, '08001950382': 4, 'upto': 4, 'hate': 4, 'skype': 4, 'nearly': 4, 'masters': 4, 'cook': 4, 'cleaning': 4, 'cat': 4, 'hip': 4, '87239': 4, 'freefone': 4, 'lie': 4, 'infernal': 4, 'giv': 4, '84199': 4, 'w111wx': 4, 'yer': 4, 'box39822': 4, 'subs': 4, 'med': 4, 'kidz': 4, 'ntwk': 4, 'frndship': 4, 'freak': 4, 'ref': 4, 'empty': 4, 'wkend': 4, 'letters': 4, 'football': 4, 'happend': 4, 'sugar': 4, 'thangam': 4, 'roger': 4, 'solve': 4, 'cooking': 4, 'key': 4, 'released': 4, 'deliver': 4, 'spending': 4, 'sept': 4, 'public': 4, 'instituitions': 4, 'govt': 4, 'dare': 4, 'teeth': 4, 'iz': 4, 'handle': 4, 'note': 4, 'celebrate': 4, 'tm': 4, 'abi': 4, 'hill': 4, 'relation': 4, 'fromm': 4, '09061221066': 4, 'wylie': 4, 'basic': 4, 'outta': 4, 'inform': 4, 'texted': 4, '26': 4, 'doc': 4, 'taunton': 4, 'loss': 4, '2005': 3, '21st': 3, 'nurungu': 3, 'vettam': 3, 'melle': 3, 'minnaminunginte': 3, 'spell': 3, 'wales': 3, 'scotland': 3, 'clear': 3, 'caught': 3, 'fear': 3, 'xuhui': 3, 'invite': 3, 'yummy': 3, 'fair': 3, 'runs': 3, 'embarassed': 3, 'realized': 3, 'matrix3': 3, '09061209465': 3, 'starwars3': 3, 'suprman': 3, 'roommates': 3, 'dresser': 3, '1500': 3, 'advise': 3, 'recent': 3, 'valuable': 3, 'plane': 3, 'gentleman': 3, 'dignity': 3, 'shy': 3, 'coins': 3, 'requests': 3, 'sheets': 3, 'sum1': 3, 'lido': 3, 'collected': 3, 'verify': 3, 'mix': 3, 'four': 3, 'boston': 3, 'vava': 3, 'loud': 3, 'k52': 3, 'sentence': 3, 'wa': 3, 'anythin': 3, '45239': 3, 'apologise': 3, 'hardcore': 3, 'dot': 3, 'staff': 3, 'female': 3, 'birla': 3, 'soft': 3, 'floor': 3, 'spanish': 3, 'mall': 3, 'maneesha': 3, 'toll': 3, 'satisfied': 3, 'mummy': 3, 'finishes': 3, 'august': 3, 'suggest': 3, 'successfully': 3, 'register': 3, 'mtmsg18': 3, '087187262701': 3, '89545': 3, '50gbp': 3, 'followed': 3, 'pence': 3, 'loses': 3, 'tomarrow': 3, 'avent': 3, 'touched': 3, 'slippers': 3, 'innings': 3, 'bat': 3, 'dearly': 3, '125gift': 3, 'ranjith': 3, '5min': 3, 'networks': 3, 'mini': 3, 'parked': 3, 'jealous': 3, 'flash': 3, 'sorting': 3, '100percent': 3, 'genuine': 3, 'handed': 3, 'gautham': 3, 'buzy': 3, 'upgrade': 3, '0845': 3, 'tease': 3, 'scary': 3, 'gossip': 3, 'fit': 3, 'newest': 3, 'keys': 3, 'garage': 3, 'bstfrnd': 3, 'lifpartnr': 3, 'jstfrnd': 3, 'dear1': 3, 'best1': 3, 'clos1': 3, 'lvblefrnd': 3, 'cutefrnd': 3, 'swtheart': 3, '3uz': 3, 'm26': 3, 'gona': 3, 'flight': 3, 'women': 3, 'record': 3, 'germany': 3, 'supervisor': 3, 'lifetime': 3, 'bless': 3, 'favourite': 3, 'stranger': 3, 'gudnite': 3, 'slap': 3, 'alcohol': 3, 'remembered': 3, 'insha': 3, 'insurance': 3, 'alive': 3, 'gbp': 3, 'ptbo': 3, 'tests': 3, '6months': 3, '08000938767': 3, '4mths': 3, 'or2stoptxt': 3, 'mobilesdirect': 3, 'shut': 3, 'period': 3, 'business': 3, 'picture': 3, 'quickly': 3, 'nd': 3, '87131': 3, 'chechi': 3, 'sender': 3, 'skip': 3, 'names': 3, 'irritating': 3, 'ful': 3, 'lacs': 3, 'bmw': 3, 'arng': 3, 'shortage': 3, 'urgently': 3, 'source': 3, 'iouri': 3, 'sachin': 3, 'oic': 3, 'transfer': 3, '1956669': 3, 'homeowners': 3, 'previously': 3, '75': 3, 'si': 3, 'july': 3, '0207': 3, 'railway': 3, 'doggy': 3, 'fave': 3, 'roads': 3, 'dave': 3, 'transfered': 3, 'banks': 3, '9ja': 3, '9t': 3, 'wise': 3, 'boye': 3, 'dificult': 3, 'fightng': 3, 'fish': 3, '123': 3, '1450': 3, 'fees': 3, 'sory': 3, 'soryda': 3, 'ldnw15h': 3, 'ibhltd': 3, 'cha': 3, 'mono': 3, 'booking': 3, 'behave': 3, 'elsewhere': 3, '09': 3, 'box95qu': 3, '0871': 3, '08717898035': 3, 'malaria': 3, 'ummmmmaah': 3, 'tirupur': 3, 'bloke': 3, 'cock': 3, 'generally': 3, 'likely': 3, 'callin': 3, 'american': 3, 'dick': 3, 'headache': 3, '80878': 3, 'lines': 3, 'exhausted': 3, 'swimming': 3, '2morow': 3, 'paris': 3, 'nichols': 3, '83222': 3, 'market': 3, 'pop': 3, 'postcode': 3, 'seven': 3, 'tlp': 3, 'thanksgiving': 3, '31': 3, 'en': 3, 'peace': 3, '89555': 3, 'textoperator': 3, 'map': 3, 'building': 3, 'accordingly': 3, 'farm': 3, 'ws': 3, 'stress': 3, 'csbcm4235wc1n3xx': 3, 'upset': 3, 'low': 3, 'shouted': 3, 'shorter': 3, 'subscribed': 3, 'realize': 3, 'gimme': 3, '50perwksub': 3, 'tscs087147403231winawk': 3, 'anywhere': 3, 'diff': 3, 'community': 3, 'subpoly': 3, '81618': 3, 'bein': 3, 'jan': 3, 'pieces': 3, 'bid': 3, 'responding': 3, '2u': 3, 'cm2': 3, '220': 3, '08701417012': 3, 'charity': 3, 'alfie': 3, 'm8s': 3, 'nokias': 3, 'brain': 3, 'hahaha': 3, 'given': 3, 'successful': 3, '2morrow': 3, 'sk3': 3, '8wp': 3, 'seconds': 3, 'xavier': 3, 'stomach': 3, 'returned': 3, 'vip': 3, 'supply': 3, 'nap': 3, 'cuddle': 3, 'shesil': 3, '10k': 3, 'liverpool': 3, 'reminder': 3, 'failed': 3, 'outstanding': 3, 'taylor': 3, 'male': 3, '5p': 3, 'msging': 3, 'diet': 3, '88600': 3, 'moments': 3, '114': 3, '14': 3, 'tcr': 3, 'magical': 3, 'welp': 3, 'valid12hrs': 3, '15': 3, 'chicken': 3, 'potential': 3, 'talent': 3, '09063458130': 3, 'polyph': 3, 'fuckin': 3, 'butt': 3, 'terrible': 3, 'prey': 3, 'fancies': 3, 'foreign': 3, 'stamps': 3, 'speechless': 3, 'roast': 3, 'concentrate': 3, 'chatting': 3, 'walked': 3, 'euro': 3, 'drunk': 3, '84025': 3, 'networking': 3, 'juicy': 3, 'dearer': 3, 'itz': 3, 'alwys': 3, 'evn': 3, 'clock': 3, '09061790121': 3, 'ne': 3, 'ground': 3, 'speed': 3, 'catching': 3, 'falls': 3, 'whos': 3, 'le': 3, 'bigger': 3, 'islands': 3, 'celeb': 3, 'pocketbabe': 3, 'voicemail': 3, '2go': 3, 'walmart': 3, 'score': 3, '87021': 3, 'apps': 3, 'anti': 3, 'rofl': 3, 'ph': 3, 'various': 3, 'textcomp': 3, '84128': 3, 'morn': 3, 'docs': 3, 'havin': 3, 'rang': 3, 'sorted': 3, 'executive': 3, 'jane': 3, 'express': 3, 'fran': 3, 'knackered': 3, 'software': 3, 'jamster': 3, 'among': 3, 'cares': 3, '6hrs': 3, 'chill': 3, 'chillin': 3, 'saucy': 3, 'chain': 3, 'suntec': 3, 'messenger': 3, 'screen': 3, 'upload': 3, 'tom': 3, 'shot': 3, 'wt': 3, 'storming': 3, 'invnted': 3, 'margaret': 3, 'telphone': 3, 'phne': 3, 'girlfrnd': 3, 'grahmbell': 3, 'popped': 3, 'shld': 3, 'beware': 3, 'caring': 3, 'option': 3, 'goodnite': 3, 'painful': 3, 'guilty': 3, 'cardiff': 3, 'addie': 3, 'bright': 3, 'certainly': 3, 'twelve': 3, 'aah': 3, '07xxxxxxxxx': 3, 'hubby': 3, 'minmobsmorelkpobox177hp51fl': 3, 'blake': 3, 'karaoke': 3, 'stars': 3, 'eight': 3, 'ese': 3, 'prospects': 3, 'bishan': 3, 'buff': 3, 'gang': 3, 'tablets': 3, 'finishing': 3, 'doors': 3, 'chasing': 3, 'brothas': 3, 'force': 3, 'blame': 3, 'blessings': 3, 'freezing': 3, 'winning': 3, '6pm': 3, 'titles': 3, 'feelin': 3, 'switch': 3, 'monthly': 3, 'ideas': 3, 'maintain': 3, 'sh': 3, 'cramps': 3, 'nan': 3, '81303': 3, 'dislikes': 3, 'likes': 3, 'album': 3, '121': 3, 'standing': 3, 'james': 3, '29': 3, 'chosen': 3, 'di': 3, 'cruise': 3, 'follow': 3, 'stuck': 3, 'regarding': 3, 'adore': 3, 'arcade': 3, 'arun': 3, 'philosophy': 3, 'eye': 3, 'husband': 3, 'norm': 3, 'toa': 3, 'payoh': 3, 'fathima': 3, 'mmmm': 3, '18yrs': 3, 'abta': 3, '80182': 3, '08452810073': 3, 'table': 3, 'ikea': 3, 'cn': 3, 'kadeem': 3, 'wud': 3, 'carry': 3, 'avatar': 3, 'stops': 3, 'constantly': 3, 'lousy': 3, 'ic': 3, 'sweetest': 3, 'honeybee': 3, 'laughed': 3, 'havnt': 3, 'crack': 3, 'boat': 3, 'proof': 3, 'provided': 3, 'yeh': 3, 'members': 3, 'downloads': 3, 'major': 3, 'birth': 3, 'rule': 3, 'natural': 3, 'onwards': 3, '150ppermesssubscription': 3, 'skillgame': 3, 'tscs': 3, '1winaweek': 3, 'eggs': 3, 'boost': 3, 'calicut': 3, 'box97n7qp': 3, 'pink': 3, 'normally': 3, 'rich': 3, 'm8': 3, 'yor': 3, 'jason': 3, 'art': 3, 'feet': 3, 'argh': 3, 'favor': 3, 'tessy': 3, 'shijas': 3, 'aunty': 3, 'china': 3, 'morphine': 3, 'prefer': 3, 'kindly': 3, 'pages': 3, 'pending': 3, 'raji': 3, 'legs': 3, 'distance': 3, 'temp': 3, 'display': 3, 'soup': 3, 'management': 3, 'include': 3, 'regular': 3, 'threats': 3, 'lounge': 3, 'u4': 3, 'cheer': 3, 'cornwall': 3, 'bags': 3, 'iscoming': 3, '80082': 3, 'spook': 3, 'halloween': 3, 'issue': 3, 'sky': 3, 'measure': 3, 'thm': 3, 'instantly': 3, 'drinking': 3, 'wn': 3, 'impossible': 3, 'responce': 3, 'vodka': 3, 'okey': 3, 'neighbour': 3, 'questioned': 3, 'gardener': 3, 'vegetables': 3, 'science': 3, 'madam': 3, 'settle': 3, 'citizen': 3, 'indians': 3, 'sry': 3, '09066612661': 3, 'greetings': 3, 'dai': 3, 'maga': 3, 'medicine': 3, 'violence': 3, 'incident': 3, 'erm': 3, 'instructions': 3, '3lp': 3, 'death': 3, 'hon': 3, 'reality': 3, 'usc': 3, 'booty': 3, 'lil': 3, 'remains': 3, 'bros': 3, 'bro': 3, 'response': 3, 'shirts': 3, 'petrol': 3, 'uks': 3, '2stoptxt': 3, 'luxury': 3, 'ben': 3, 'middle': 3, 'dark': 3, 'enuff': 3, 'strike': 3, 'moved': 3, 'porn': 3, 'dress': 3, 'collecting': 3, 'flaked': 3, 'gary': 3, 'history': 3, 'bell': 3, 'understood': 3, 'bottom': 3, '33': 3, 'books': 3, 'prove': 3, 'blow': 3, 'knowing': 3, 'challenge': 3, 'randomly': 3, 'tape': 3, 'films': 3, 'lick': 3, 'auto': 3, 'praying': 3, 'hug': 3, 'deliveredtomorrow': 3, 'smoking': 3, 'in2': 3, 'billed': 3, 'ths': 3, 'callback': 3, 'wedding': 3, 'accident': 3, 'wisdom': 3, 'cann': 3, 'symbol': 3, 'prolly': 3, 'confirmed': 3, 'dubsack': 3, 'macho': 3, 'audition': 3, 'fell': 3, 'senthil': 3, 'forevr': 3, 'eaten': 3, 'nat': 3, 'possession': 3, 'concert': 3, 'born': 3, 'affairs': 3, 'california': 3, 'university': 3, 'value': 3, 'mnth': 3, 'tog': 3, 'haiz': 3, 'previous': 3, 'parking': 3, 'wallpaper': 3, 'step': 3, 'buffet': 2, 'fa': 2, '08452810075over18': 2, 'hor': 2, 'rcv': 2, 'kl341': 2, 'receivea': 2, '09061701461': 2, '08002986030': 2, 'tsandcs': 2, 'csh11': 2, '6days': 2, 'chances': 2, '4403ldnw1a7rw18': 2, 'jackpot': 2, 'dbuk': 2, 'lccltd': 2, '81010': 2, 'blessing': 2, 'goals': 2, '4txt': 2, 'slice': 2, 'convincing': 2, 'frying': 2, 'sarcastic': 2, '8am': 2, 'mmmmmm': 2, 'burns': 2, 'hospitals': 2, 'gram': 2, 'eighth': 2, 'detroit': 2, 'hockey': 2, 'odi': 2, 'killing': 2, 'burger': 2, '09066364589': 2, 'dedicated': 2, 'dedicate': 2, 'eurodisinc': 2, 'shracomorsglsuplt': 2, 'entry41': 2, '3aj': 2, 'trav': 2, 'ls1': 2, 'aco': 2, 'morefrmmob': 2, 'divorce': 2, 'earn': 2, 'jacket': 2, 'nitros': 2, 'ela': 2, 'pours': 2, '169': 2, '6031': 2, '92h': 2, 'usher': 2, 'britney': 2, '450ppw': 2, '5249': 2, 'mk17': 2, '85069': 2, 'telugu': 2, 'loans': 2, 'animation': 2, 'location': 2, 'noun': 2, 'gent': 2, '09064012160': 2, 'puttin': 2, 'goodo': 2, 'potato': 2, 'tortilla': 2, '08719180248': 2, '07742676969': 2, 'sum': 2, 'algarve': 2, '69888': 2, '31p': 2, 'msn': 2, 'pouch': 2, 'hearts': 2, 'somtimes': 2, 'occupy': 2, '08700621170150p': 2, 'randy': 2, 'flowing': 2, 'plaza': 2, 'everywhere': 2, 'windows': 2, 'mouth': 2, '0871277810810': 2, 'module': 2, 'avoid': 2, 'beloved': 2, 'clark': 2, 'form': 2, 'utter': 2, 'completed': 2, 'stays': 2, 'wishin': 2, 'hamster': 2, 'keralacircle': 2, 'inr': 2, 'refilled': 2, 'kr': 2, 'prepaid': 2, 'ericsson': 2, 'bruv': 2, 'rewarding': 2, 'heading': 2, 'installing': 2, 'repair': 2, 'star': 2, 'teacher': 2, 'teaches': 2, 'upstairs': 2, 'printed': 2, '09058094597': 2, '447801259231': 2, 'signing': 2, 'shining': 2, 'although': 2, 'commercial': 2, 'drpd': 2, 'deepak': 2, 'deeraj': 2, '2wks': 2, 'lag': 2, 'shipping': 2, 'headin': 2, 'necessarily': 2, 'jolt': 2, 'suzy': 2, 'mk45': 2, '2wt': 2, 'chart': 2, 'gf': 2, 'tool': 2, 'jenny': 2, '021': 2, '3680': 2, 'grave': 2, 'taxi': 2, 'crash': 2, 'shocking': 2, 'actor': 2, 'hide': 2, 'thread': 2, '82468': 2, 'funky': 2, 'anot': 2, 'lo': 2, 'tahan': 2, 'buses': 2, 'bristol': 2, 'apo': 2, '861': 2, '85': 2, 'prepayment': 2, '0844': 2, 'paperwork': 2, 'violated': 2, 'privacy': 2, 'caroline': 2, 'cleared': 2, 'misbehaved': 2, 'tissco': 2, 'tayseer': 2, 'audrey': 2, 'status': 2, 'breathe': 2, 'update_now': 2, 'cuddling': 2, 'agree': 2, 'recognise': 2, 'hes': 2, 'ovulation': 2, 'n9dx': 2, 'licks': 2, '30ish': 2, 'grace': 2, 'inshah': 2, 'sharing': 2, 'salam': 2, 'field': 2, 'shipped': 2, 'burning': 2, 'loxahatchee': 2, 'wld': 2, 'darlings': 2, 'fav': 2, 'slightly': 2, 'box334sk38ch': 2, 'whatsup': 2, 'goal': 2, '80086': 2, 'txttowin': 2, 'mobno': 2, 'ads': 2, 'name1': 2, 'adam': 2, 'name2': 2, '07123456789': 2, 'txtno': 2, 'siva': 2, 'expression': 2, 'speaking': 2, '3650': 2, '09066382422': 2, 'bcm4284': 2, '300603': 2, 'applebees': 2, 'cricketer': 2, 'bhaji': 2, 'improve': 2, 'oreo': 2, 'truffles': 2, 'amy': 2, 'coping': 2, 'decisions': 2, 'individual': 2, '26th': 2, '153': 2, 'position': 2, 'language': 2, '09061743806': 2, 'box326': 2, 'screamed': 2, 'removed': 2, 'infront': 2, 'broken': 2, 'tension': 2, 'taste': 2, '07781482378': 2, 'trade': 2, '7ish': 2, 'rec': 2, 'b4280703': 2, '08718727868': 2, '09050002311': 2, 'hyde': 2, 'anthony': 2, 'scrounge': 2, 'forgiven': 2, 'slide': 2, 'renewal': 2, 'transport': 2, 'definite': 2, 'nos': 2, 'ebay': 2, 'pickle': 2, 'tacos': 2, '872': 2, '24hrs': 2, 'pg': 2, 'channel': 2, '08718738001': 2, 'web': 2, '2stop': 2, 'ability': 2, 'develop': 2, 'recovery': 2, 'cali': 2, 'cutting': 2, 'reminding': 2, 'owns': 2, 'faggy': 2, 'demand': 2, 'fo': 2, 'loose': 2, 'perhaps': 2, 'mei': 2, 'geeeee': 2, 'ey': 2, 'oooh': 2, 'call09050000327': 2, 'claims': 2, 'dancing': 2, 'snake': 2, 'bite': 2, 'hardly': 2, 'promo': 2, 'ag': 2, '08712402050': 2, '10ppm': 2, '0825': 2, 'tsunamis': 2, 'soiree': 2, '22': 2, 'ques': 2, 'suits': 2, 'reaction': 2, 'shock': 2, 'grow': 2, 'useful': 2, 'officially': 2, '89693': 2, 'textbuddy': 2, 'gaytextbuddy': 2, '09064019014': 2, '4882': 2, 'hundred': 2, 'expressoffer': 2, 'sweetheart': 2, 'biola': 2, 'effects': 2, 'wee': 2, 'trains': 2, 'ham': 2, 'jolly': 2, '40533': 2, 'sw7': 2, '3ss': 2, 'rstm': 2, 'panic': 2, 'dealer': 2, 'impatient': 2, 'river': 2, 'premium': 2, 'lays': 2, 'posts': 2, 'yelling': 2, 'hex': 2, 'cochin': 2, '4d': 2, 'poop': 2, 'gpu': 2, 'hurried': 2, 'aeroplane': 2, 'calld': 2, 'professors': 2, 'wer': 2, 'aeronautics': 2, 'datz': 2, 'dorm': 2, 'mobilesvary': 2, '050703': 2, 'callcost': 2, '1250': 2, '09071512433': 2, 'cookies': 2, 'correction': 2, 'admit': 2, 'ba': 2, 'spring': 2, 'mtmsg': 2, 'ctxt': 2, 'nokia6650': 2, 'attached': 2, '930': 2, 'helpline': 2, '08706091795': 2, 'gist': 2, 'thousands': 2, '40': 2, 'premier': 2, 'lip': 2, 'confused': 2, 'spare': 2, 'faith': 2, 'acting': 2, 'schools': 2, 'inch': 2, 'begging': 2, '0578': 2, 'opening': 2, 'pole': 2, 'thot': 2, 'petey': 2, 'nic': 2, '8077': 2, 'cashto': 2, 'getstop': 2, '88222': 2, 'php': 2, '08000407165': 2, 'imp': 2, 'bec': 2, 'nervous': 2, 'hint': 2, 'borrow': 2, 'dobby': 2, 'galileo': 2, 'enjoyin': 2, 'loveme': 2, 'cappuccino': 2, 'mojibiola': 2, '07821230901': 2, '09065174042': 2, 'hol': 2, 'kz': 2, 'aburo': 2, 'ultimatum': 2, 'countin': 2, 'skyped': 2, '08002888812': 2, 'inconsiderate': 2, 'hence': 2, 'recession': 2, 'nag': 2, 'soo': 2, '09066350750': 2, 'warning': 2, 'shoes': 2, 'lovejen': 2, 'discreet': 2, 'worlds': 2, 'named': 2, 'genius': 2, 'connections': 2, 'lotta': 2, 'lately': 2, 'virgin': 2, 'mystery': 2, 'approx': 2, 'smsco': 2, 'peaceful': 2, 'consider': 2, 'walls': 2, '41685': 2, '07': 2, 'fixedline': 2, '5k': 2, '09064011000': 2, 'cr01327bt': 2, 'castor': 2, '09058094565': 2, '08': 2, 'stopsms': 2, '09065171142': 2, 'downloaded': 2, 'ear': 2, 'oil': 2, 'usb': 2, 'mac': 2, 'gibbs': 2, 'unbelievable': 2, 'superb': 2, 'several': 2, 'worst': 2, 'charles': 2, 'stores': 2, 'peak': 2, '08709222922': 2, '8p': 2, 'sweets': 2, 'chip': 2, 'addicted': 2, 'yck': 2, 'ashley': 2, 'lux': 2, 'jeans': 2, 'tons': 2, 'scores': 2, 'application': 2, 'ms': 2, 'filthy': 2, 'simpler': 2, '09050001808': 2, 'm95': 2, 'necklace': 2, 'rice': 2, 'racing': 2, 'closes': 2, 'crap': 2, 'borin': 2, 'chocolate': 2, 'reckon': 2, 'tech': 2, '65': 2, 'sd': 2, 'blessed': 2, 'quiet': 2, 'aunts': 2, 'helen': 2, 'fan': 2, 'lovers': 2, 'drove': 2, 'exe': 2, 'pen': 2, 'anniversary': 2, 'datebox1282essexcm61xn': 2, 'secretly': 2, 'pattern': 2, 'plm': 2, 'sheffield': 2, 'zoe': 2, 'setting': 2, 'filling': 2, 'sufficient': 2, 'thx': 2, 'gnt': 2, 'rightly': 2, 'viva': 2, 'edison': 2, 'ls15hb': 2, 'educational': 2, 'flirting': 2, 'kickoff': 2, 'sells': 2, 'thesis': 2, 'sends': 2, 'deciding': 2, 'herself': 2, 'compare': 2, 'eastenders': 2, 'tulip': 2, 'wkent': 2, 'violet': 2, '150p16': 2, 'lily': 2, 'prepared': 2, 'm6': 2, '09058091854': 2, 'box385': 2, '6wu': 2, '09050003091': 2, 'c52': 2, 'oi': 2, 'craziest': 2, 'curry': 2, 'thoughts': 2, 'singing': 2, 'breath': 2, 'planet': 2, '28days': 2, '2yr': 2, 'm221bp': 2, 'box177': 2, '09061221061': 2, '99': 2, 'warranty': 2, 'tomorro': 2, 'fret': 2, 'wind': 2, 'depressed': 2, 'math': 2, 'dhoni': 2, 'rocks': 2, 'durban': 2, '08000776320': 2, 'survey': 2, 'difficulties': 2, 'sar': 2, 'tank': 2, 'silently': 2, 'drms': 2, 'itcould': 2, 'wrc': 2, 'rally': 2, 'lucozade': 2, '61200': 2, 'packs': 2, 'toot': 2, 'annoying': 2, 'makin': 2, 'popcorn': 2, 'beneficiary': 2, 'neft': 2, 'subs16': 2, '1win150ppmx3': 2, 'appreciated': 2, 'apart': 2, 'creepy': 2, '08719181513': 2, 'nok': 2, 'invest': 2, 'delay': 2, '1hr': 2, 'purse': 2, 'europe': 2, 'flip': 2, 'accounts': 2, 'jd': 2, 'weirdest': 2, 'l8tr': 2, 'minmoremobsemspobox45po139wa': 2, 'tee': 2, 'control': 2, 'dough': 2, 'irritates': 2, 'fails': 2, 'jerry': 2, 'drinkin': 2, '5pm': 2, 'birthdate': 2, 'nydc': 2, 'ola': 2, 'items': 2, 'garbage': 2, 'logos': 2, 'gold': 2, 'lionp': 2, 'lionm': 2, 'lions': 2, 'jokin': 2, 'colours': 2, 'whenevr': 2, 'remembr': 2, 'harry': 2, 'potter': 2, 'phoenix': 2, 'readers': 2, 'canada': 2, 'goodnoon': 2, 'patty': 2, 'interest': 2, 'george': 2, '89080': 2, 'free2day': 2, '0870241182716': 2, 'theres': 2, 'tmrw': 2, 'soul': 2, 'ned': 2, 'main': 2, 'hurting': 2, 'sweetie': 2, '4a': 2, 'whn': 2, 'dance': 2, 'bar': 2, '08718730666': 2, 'bears': 2, 'juan': 2, 'lf56': 2, 'tlk': 2, 'front': 2, 'ideal': 2, 'arm': 2, 'tirunelvali': 2, 'effect': 2, 'bk': 2, 'kidding': 2, 'stretch': 2, 'urn': 2, 'sinco': 2, 'disclose': 2, 'frauds': 2, 'payee': 2, 'icicibank': 2, 'kaiez': 2, 'babies': 2, 'practicing': 2, 'pale': 2, 'beneath': 2, 'silver': 2, 'silence': 2, 'revision': 2, 'exeter': 2, 'whose': 2, 'condition': 2, 'arsenal': 2, 'missin': 2, 'tues': 2, 'restaurant': 2, 'textpod': 2, 'desperate': 2, 'monkeys': 2, 'practical': 2, 'mails': 2, 'claire': 2, 'costing': 2, 'ke': 2, 'program': 2, '09066362231': 2, 'ny': 2, 'lotr': 2, 'modules': 2, 'musthu': 2, 'testing': 2, 'nit': 2, 'format': 2, 'sarcasm': 2, 'forum': 2, 'aunt': 2, 'unfortunately': 2, 'wihtuot': 2, 'exmpel': 2, 'jsut': 2, 'tihs': 2, 'waht': 2, 'yuo': 2, 'splleing': 2, 'evrey': 2, 'wrnog': 2, 'raed': 2, 'sitll': 2, 'mitsake': 2, 'rael': 2, 'gving': 2, 'ayn': 2, 'konw': 2, 'ow': 2, 'finance': 2, 'joining': 2, 'filled': 2, 'jia': 2, 'sux': 2, 'kegger': 2, 'adventure': 2, 'pack': 2, 'wifi': 2, 'rumour': 2, '7250': 2, 'boyfriend': 2, 'driver': 2, 'kicks': 2, 'falling': 2, 'smeone': 2, 'fire': 2, 'propose': 2, 'gods': 2, 'gifted': 2, 'lovingly': 2, 'tomeandsaid': 2, 'itwhichturnedinto': 2, 'dippeditinadew': 2, 'ringtoneking': 2, 'batch': 2, 'flaky': 2, 'sooooo': 2, 'tooo': 2, '09058094599': 2, 'confuses': 2, 'wating': 2, 'sw73ss': 2, 'british': 2, 'hotels': 2, 'adoring': 2, 'ghost': 2, 'dracula': 2, 'addamsfa': 2, 'munsters': 2, 'exorcist': 2, 'twilight': 2, 'cared': 2, 'constant': 2, 'allow': 2, 'hlp': 2, '2rcv': 2, '82242': 2, 'msg150p': 2, '08712317606': 2, 'fly': 2, 'event': 2, 'movietrivia': 2, '08712405022': 2, '80608': 2, 'partnership': 2, 'mostly': 2, 'mornin': 2, 'jas': 2, 'poker': 2, 'messy': 2, 'slip': 2, 'moves': 2, 'traffic': 2, 'nus': 2, 'wkg': 2, 'keeps': 2, 'gotten': 2, 'promises': 2, 'unknown': 2, 'vu': 2, 'bcm1896wc1n3xx': 2, '09094646899': 2, 'pre': 2, '2007': 2, 'indeed': 2, 'rents': 2, '48': 2, 'alaipayuthe': 2, 'maangalyam': 2, 'easter': 2, 'telephone': 2, '08081560665': 2, 'bahamas': 2, '07786200117': 2, 'callfreefone': 2, 'up4': 2, 'calm': 2, 'habit': 2, 'contacts': 2, 'forgets': 2, 'mandan': 2, 'ibh': 2, '07734396839': 2, 'nokia6600': 2, 'invaders': 2, 'orig': 2, 'console': 2, 'recharge': 2, 'transfr': 2, '82050': 2, 'prizes': 2, 'foley': 2, 'fake': 2, 'desparate': 2, '3100': 2, 'combine': 2, 'sian': 2, 'g696ga': 2, 'joanna': 2, 'replacement': 2, 'telly': 2, 'tooth': 2, '12mths': 2, 'mth': 2, 'wipro': 2, 'laundry': 2, 'underwear': 2, 'delete': 2, 'waheed': 2, 'pushes': 2, 'beyond': 2, 'avoiding': 2, '0776xxxxxxx': 2, '326': 2, 'uh': 2, 'heads': 2, 'vday': 2, 'build': 2, 'snowman': 2, 'fights': 2, 'prescription': 2, 'electricity': 2, 'fujitsu': 2, 'scold': 2, 'se': 2, 'prompts': 2, '09066358152': 2, 'disturbing': 2, 'flies': 2, 'woken': 2, 'aka': 2, 'delhi': 2, 'held': 2, 'fringe': 2, 'distract': 2, 'tones2you': 2, '61610': 2, '08712400602450p': 2, 'mel': 2, 'responsibility': 2, '08006344447': 2, 'kid': 2, 'affair': 2, 'parco': 2, 'nb': 2, 'hallaq': 2, 'bck': 2, 'color': 2, 'lyk': 2, 'gender': 2, 'sleepwell': 2, 'mca': 2, 'vomiting': 2, 'rub': 2, 'clever': 2, '113': 2, 'bray': 2, 'wicklow': 2, 'stamped': 2, 'eire': 2, 'ryan': 2, 'idew': 2, 'manage': 2, 'xam': 2, 'diamonds': 2, 'shitload': 2, 'mcat': 2, '27': 2, 'beg': 2, 'sacrifice': 2, 'stayin': 2, 'satisfy': 2, 'cld': 2, 'miles': 2, 'killed': 2, 'smashed': 2, 'ps': 2, 'tok': 2, 'specific': 2, 'figures': 2, 'cousin': 2, 'excuses': 2, 'neck': 2, 'continue': 2, 'holy': 2, 'billion': 2, 'classes': 2, 'turning': 2, 'youre': 2, 'belive': 2, 'slots': 2, 'discussed': 2, 'prem': 2, '2morro': 2, 'spoiled': 2, 'complaint': 2, 'sales': 2, 'lk': 2, 'lov': 2, 'comfort': 2, '300p': 2, '8552': 2, '2end': 2, '01223585334': 2, '2c': 2, 'shagged': 2, '88066': 2, 'bedrm': 2, '700': 2, 'waited': 2, 'huge': 2, 'mids': 2, 'upd8': 2, 'oranges': 2, 'annie': 2, 'messaging': 2, 'retrieve': 2, 'mailbox': 2, '09056242159': 2, '21870000': 2, 'hrishi': 2, 'nothin': 2, 'poem': 2, 'duchess': 2, '008704050406': 2, 'dan': 2, 'aww': 2, 'staring': 2, 'cm': 2, 'unnecessarily': 2, '08701417012150p': 2, 'weigh': 2, 'scoring': 2, 'active': 2, '250k': 2, '88088': 2, 'gamestar': 2, 'expired': 2, 'opinions': 2, 'lv': 2, 'lived': 2, 'propsd': 2, 'happily': 2, '2gthr': 2, 'gv': 2, 'speeding': 2, 'thy': 2, 'lttrs': 2, 'aproach': 2, 'threw': 2, 'evrydy': 2, 'truck': 2, 'dt': 2, 'paragon': 2, 'arent': 2, 'bluff': 2, 'sary': 2, 'piece': 2, 'scotch': 2, 'yarasu': 2, 'shampain': 2, 'brandy': 2, 'vaazhthukkal': 2, 'gin': 2, 'kudi': 2, 'dhina': 2, 'rum': 2, 'wiskey': 2, 'kg': 2, 'dumb': 2, 'dressed': 2, 'kills': 2, 'kay': 2, 'nasty': 2, 'wasted': 2, 'christ': 2, 'tears': 2, 'push': 2, 'answered': 2, 'rgds': 2, '8pm': 2, 'wrote': 2, 'rights': 2, 'jobs': 2, 'lane': 2, 'donno': 2, 'properly': 2, '630': 2, 'lock': 2, 'furniture': 2, 'shoving': 2, 'papers': 2, 'strange': 2, 'acl03530150pm': 2, 'indyarocks': 2, 'resume': 2, 'bids': 2, 'whr': 2, 'yunny': 2, '83383': 2, 'mmmmm': 2, 'relatives': 2, 'benefits': 2, 'environment': 2, 'terrific': 2, 'dr': 2, 'superior': 2, 'vid': 2, 'ruin': 2, 'conform': 2, 'department': 2, 'bc': 2, 'toshiba': 2, 'wrk': 2, 'innocent': 2, 'mental': 2, 'hoped': 2, 'bills': 2, '2marrow': 2, 'treated': 2, 'wks': 2, 'fab': 2, 'tiwary': 2, 'battle': 2, 'bang': 2, 'pap': 2, 'arts': 2, 'pandy': 2, 'edu': 2, 'secretary': 2, 'dollar': 2, 'pull': 2, 'amongst': 2, '69696': 2, 'nalla': 2, 'pouts': 2, 'stomps': 2, 'northampton': 2, 'abj': 2, 'serving': 2, 'smith': 2, 'nagar': 2, 'anna': 2, 'sports': 2, 'evr': 2, 'hugs': 2, 'neither': 2, 'snogs': 2, 'west': 2, 'growing': 2, 'fastest': 2, 'chase': 2, 'steam': 2, 'reg': 2, 'canary': 2, 'sleepy': 2, 'mag': 2, 'diwali': 2, 'tick': 2, 'onion': 2, 'thgt': 2, 'lower': 2, 'exhaust': 2, 'pee': 2, 'contents': 2, 'success': 2, 'division': 2, 'creep': 2, 'lies': 2, 'property': 2, '7876150ppm': 2, '09058099801': 2, 'b4190604': 2, 'bbd': 2, 'pimples': 2, 'yellow': 2, 'frog': 2, '88888': 2, 'doubt': 2, 'japanese': 2, 'proverb': 2, 'freedom': 2, 'seat': 2, 'twenty': 2, 'painting': 2, 'nowadays': 2, 'talks': 2, 'probs': 2, 'swatch': 2, 'ganesh': 2, 'trips': 2, 'helloooo': 2, 'welcomes': 2, '54': 2, '2geva': 2, 'wuld': 2, 'solved': 2, 'ing': 2, 'sake': 2, 'bruce': 2, 'teaching': 2, 'chest': 2, 'covers': 2, 'hang': 2, 'reboot': 2, 'pt2': 2, 'phoned': 2, 'improved': 2, 'hm': 2, 'salon': 2, 'evenings': 2, 'raj': 2, 'payment': 2, 'clearing': 2, 'shore': 2, 'range': 2, 'changes': 2, 'topic': 2, 'admin': 2, 'visionsms': 2, 'andros': 2, 'meets': 2, 'penis': 2, 'foot': 2, 'sigh': 2, 'eveb': 2, 'window': 2, 'removal': 2, '08708034412': 2, 'cancelled': 2, 'neway': 2, 'xxxxx': 2, 'count': 2, 'otside': 2, 'size': 2, '08712101358': 2, 'tight': 2, 'av': 2, 'everyday': 2, 'curious': 2, 'postcard': 2, 'bread': 2, 'mahal': 2, 'luvs': 2, 'ding': 2, 'allowed': 2, 'shared': 2, 'watever': 2, 'necessary': 2, 'messaged': 2, 'deus': 2, 'broad': 2, 'canal': 2, 'spile': 2, 'tap': 2, 'engin': 2, 'edge': 2, 'east': 2, 'howard': 2, 'cooked': 2, 'cheat': 2, 'block': 2, 'ruining': 2, 'easily': 2, 'selfish': 2, 'custom': 2, 'sac': 2, 'jiayin': 2, 'pobox45w2tg150p': 2, 'forgotten': 2, 'reverse': 2, 'cheating': 2, 'mathematics': 2, '2waxsto': 2, 'minimum': 2, 'elaine': 2, 'drunken': 2, 'mess': 2, 'crisis': 2, '____': 2, 'ias': 2, 'mb': 2, '600': 2, 'desires': 2, '1030': 2, 'careers': 2, '447797706009': 2, 'bloomberg': 2, 'priscilla': 2, 'kent': 2, 'vale': 2, '83049': 2, 'unkempt': 2, 'westlife': 2, 'unbreakable': 2, 'untamed': 2, 'wan2': 2, 'prince': 2, 'cdgt': 2, 'granite': 2, 'nasdaq': 2, 'explosive': 2, 'base': 2, 'placement': 2, 'sumthin': 2, 'lion': 2, 'devouring': 2, 'airtel': 2, 'processed': 2, '69669': 2, 'jaya': 2, 'forums': 2, 'incredible': 2, '18p': 2, 'o2fwd': 2, 'ship': 2, 'maturity': 2, 'kavalan': 2, 'causing': 2, 'blank': 2, 'tonights': 2, 'xin': 2, 'lib': 2, 'difference': 2, 'despite': 2, 'swoop': 2, 'langport': 2, 'mistakes': 2, 'lou': 2, 'pool': 2, '09065989182': 2, 'x49': 2, 'ibn': 2, 'verified': 2, 'confirmd': 2, 'terrorist': 2, 'cnn': 2, 'disconnect': 2, 'hppnss': 2, 'goodfriend': 2, 'sorrow': 2, 'stayed': 2, 'stone': 2, 'help08718728876': 2, 'increments': 2, 'mila': 2, '69866': 2, '30pp': 2, 'blonde': 2, '5free': 2, 'mtalk': 2, 'age23': 2, 'atlast': 2, 'desert': 2, 'funk': 2, 'tones2u': 2, 'funeral': 2, 'vivek': 2, 'tnc': 2, 'brah': 2, 'passwords': 2, 'sensitive': 2, 'protect': 2, 'sib': 2, 'ipad': 2, 'bird': 2, 'cheese': 2, 'widelive': 2, 'index': 2, 'tms': 2, 'wml': 2, 'hsbc': 2, 'asp': 2, '09061702893': 2, 'eek': 2, 'melt': 2, '09061743386': 2, '674': 2, 'eta': 2, '0871750': 2, '77': 2, 'landlines': 2, 'housewives': 2, 'dial': 2, '09066364311': 2, 'literally': 2, 'kothi': 2, 'sem': 2, 'student': 2, 'actual': 2, 'dealing': 2, 'reasonable': 2, 'kappa': 2, 'piss': 2, 'receipts': 2, 'guessing': 2, 'royal': 2, 'sticky': 2, 'indicate': 2, 'repeat': 2, 'calculation': 2, 'clothes': 2, 'lush': 2, '2find': 2, 'courage': 2, 'defeat': 2, 'greatest': 2, 'bear': 2, 'fucked': 2, 'beauty': 2, 'natalja': 2, '440': 2, 'nat27081980': 2, 'moving': 2, 'jogging': 2, 'shelf': 2, 'captain': 2, 'mokka': 2, 'polyh': 2, '09061744553': 2, 'bone': 2, 'steve': 2, 'epsilon': 2, 'mesages': 2, 'lst': 2, 'massive': 2, 'absolutly': 2, 'forms': 2, '373': 2, 'w1j': 2, '6hl': 2, 'polo': 2, 'coast': 2, 'suppose': 2, '02073162414': 2, 'secs': 2, 'explicit': 2, 'clearly': 2, 'gain': 2, 'realise': 2, 'mnths': 2, 'subscribe6gbp': 2, '86888': 2, '3hrs': 2, 'txtstop': 2, 'managed': 2, 'capital': 2, 'acted': 2, '09066380611': 2, 'loyal': 2, 'customers': 2, 'print': 2, 'dokey': 2, 'error': 2, 'sleepin': 2, 'minor': 2, 'woulda': 2, 'santa': 2, 'miserable': 2, 'shoppin': 2, '08718726270': 2, 'celebration': 2, 'warner': 2, 'select': 2, 'sarasota': 2, '13': 2, 'cherish': 2, 'slp': 2, 'muah': 2, '4eva': 2, 'garden': 2, 'notxt': 2, 'seeds': 2, 'bulbs': 2, 'scotsman': 2, 'go2': 2, 'replace': 2, 'reduce': 2, 'limiting': 2, 'gastroenteritis': 2, 'illness': 2, '09061213237': 2, 'm227xy': 2, '177': 2, 'respectful': 2, 'pride': 2, 'bottle': 2, 'amused': 2, 'mega': 2, 'island': 2, 'amore': 1, 'jurong': 1, 'chgs': 1, 'patent': 1, 'aids': 1, 'cried': 1, 'breather': 1, 'granted': 1, 'fulfil': 1, 'xxxmobilemovieclub': 1, 'qjkgighjjgcbl': 1, 'gota': 1, 'poboxox36504w45wq': 1, 'macedonia': 1, 'u1': 1, 'ffffffffff': 1, 'forced': 1, 'packing': 1, 'ahhh': 1, 'vaguely': 1, 'actin': 1, 'badly': 1, 'apologetic': 1, 'fallen': 1, 'spoilt': 1, 'housework': 1, 'cuppa': 1, 'fainting': 1, 'timings': 1, 'watts': 1, 'steed': 1, 'arabian': 1, 'rodger': 1, '07732584351': 1, 'endowed': 1, 'hep': 1, 'immunisation': 1, 'sucker': 1, 'suckers': 1, 'stubborn': 1, 'thinked': 1, 'smarter': 1, 'crashing': 1, 'accomodations': 1, 'offered': 1, 'embarassing': 1, 'cave': 1, 'wings': 1, 'incorrect': 1, 'jersey': 1, 'devils': 1, 'sptv': 1, 'sherawat': 1, 'mallika': 1, 'gauti': 1, 'sehwag': 1, 'seekers': 1, 'barbie': 1, 'ken': 1, 'performed': 1, 'peoples': 1, 'operate': 1, 'multis': 1, 'factory': 1, 'casualty': 1, 'stuff42moro': 1, 'includes': 1, 'hairdressers': 1, 'beforehand': 1, 'ams': 1, '4the': 1, 'signin': 1, 'memorable': 1, 'server': 1, 'minecraft': 1, 'ip': 1, 'grumpy': 1, 'lying': 1, 'plural': 1, 'formal': 1, 'openin': 1, '0871277810910p': 1, 'ratio': 1, '09064019788': 1, 'box42wr29c': 1, 'malarky': 1, 'apples': 1, 'pairs': 1, '4041': 1, '7548': 1, 'sao': 1, 'predict': 1, 'involve': 1, 'imposed': 1, 'lucyxx': 1, 'tmorrow': 1, 'accomodate': 1, 'gravel': 1, 'hotmail': 1, 'svc': 1, '69988': 1, 'nver': 1, 'ummma': 1, 'sindu': 1, 'nevering': 1, 'typical': 1, 'chores': 1, 'mist': 1, 'dirt': 1, 'exist': 1, 'hail': 1, 'aaooooright': 1, '07046744435': 1, 'annoncement': 1, 'envy': 1, 'excited': 1, '32': 1, 'bootydelious': 1, 'bangb': 1, 'bangbabes': 1, 'cultures': 1, 's89': 1, '09061701939': 1, 'missunderstding': 1, 'bridge': 1, 'lager': 1, 'axis': 1, 'surname': 1, 'clue': 1, 'begins': 1, 'hopes': 1, 'lifted': 1, 'approaches': 1, 'finding': 1, 'handsome': 1, 'areyouunique': 1, '30th': 1, 'league': 1, 'stool': 1, 'ors': 1, '1pm': 1, 'babyjontet': 1, 'enc': 1, 'ga': 1, 'alter': 1, 'dogg': 1, 'dats': 1, 'refund': 1, 'prediction': 1, 'os': 1, 'ubandu': 1, 'disk': 1, 'scenery': 1, 'aries': 1, 'flyng': 1, 'horo': 1, 'mudyadhu': 1, 'elama': 1, 'conducts': 1, 'strict': 1, 'gandhipuram': 1, 'rubber': 1, 'thirtyeight': 1, 'hearing': 1, 'pleassssssseeeeee': 1, 'sportsx': 1, 'watches': 1, 'baig': 1, '3days': 1, 'usps': 1, 'nipost': 1, 'bribe': 1, 'ups': 1, 'luton': 1, '0125698789': 1, '69698': 1, 'sometme': 1, 'club4': 1, 'box1146': 1, 'club4mobiles': 1, '87070': 1, 'evo': 1, 'narcotics': 1, 'objection': 1, 'theater': 1, 'mack': 1, 'rob': 1, 'celebrations': 1, 'gdeve': 1, 'ahold': 1, 'cruisin': 1, 'raksha': 1, 'varunnathu': 1, 'edukkukayee': 1, 'ollu': 1, 'resend': 1, '28thfeb': 1, 'gurl': 1, 'appropriate': 1, 'diesel': 1, 'fridge': 1, 'womdarfull': 1, 'rodds1': 1, 'icmb3cktz8r7': 1, 'aberdeen': 1, 'img': 1, 'kingdom': 1, 'united': 1, 'blind': 1, 'remb': 1, 'jos': 1, 'bookshelf': 1, '85222': 1, 'gbp1': 1, '84': 1, 'winnersclub': 1, 'mylife': 1, 'l8': 1, 'gon': 1, 'guild': 1, 'evaporated': 1, 'stealing': 1, 'employer': 1, 'daaaaa': 1, 'dined': 1, 'wined': 1, 'hiding': 1, 'huiming': 1, 'prestige': 1, 'xxuk': 1, 'sextextuk': 1, 'shag': 1, '69876': 1, 'jeremiah': 1, 'iphone': 1, 'apeshit': 1, 'safely': 1, 'onam': 1, 'sirji': 1, 'tata': 1, 'aig': 1, '08708800282': 1, 'unemployed': 1, 'andrews': 1, 'db': 1, 'dawns': 1, 'refreshed': 1, 'f4q': 1, 'regalportfolio': 1, 'rp176781': 1, '08717205546': 1, 'uniform': 1, 'spoil': 1, '09057039994': 1, 't91': 1, 'lindsay': 1, 'bars': 1, 'heron': 1, 'payasam': 1, 'rinu': 1, 'prabu': 1, 'verifying': 1, 'becaus': 1, 'taught': 1, 'followin': 1, 'repairs': 1, 'wallet': 1, '945': 1, 'owl': 1, 'kickboxing': 1, 'lap': 1, 'performance': 1, 'calculated': 1, 'visitor': 1, 'wahleykkum': 1, 'administrator': 1, '2814032': 1, '150pw': 1, '3x': 1, 'stoners': 1, 'disastrous': 1, 'busetop': 1, 'iron': 1, 'blah': 1, 'okies': 1, 'wendy': 1, '09064012103': 1, 'pobox12n146tf150p': 1, '09111032124': 1, '09058094455': 1, 'attractive': 1, 'rowdy': 1, 'sentiment': 1, 'attitude': 1, 'urination': 1, 'hillsborough': 1, 'shoul': 1, 'hasnt': 1, 'monkeespeople': 1, 'werethe': 1, 'jobyet': 1, 'howu': 1, 'monkeyaround': 1, 'howdy': 1, 'foundurself': 1, 'sausage': 1, 'exercise': 1, 'blimey': 1, 'concentration': 1, 'hanks': 1, 'lotsly': 1, 'detail': 1, 'optimistic': 1, 'practicum': 1, 'consistently': 1, 'links': 1, 'ears': 1, 'wavering': 1, 'heal': 1, '9153': 1, 'upgrdcentre': 1, 'oral': 1, 'slippery': 1, 'bike': 1, 'okmail': 1, 'differ': 1, 'enters': 1, '69888nyt': 1, 'machi': 1, 'mcr': 1, 'falconerf': 1, 'thuglyfe': 1, 'jaykwon': 1, 'glory': 1, 'ralphs': 1, 'faded': 1, 'reunion': 1, 'accenture': 1, 'jackson': 1, 'reache': 1, 'nuerologist': 1, 'lolnice': 1, 'westshore': 1, 'significance': 1, 'ammo': 1, 'ak': 1, 'toxic': 1, 'poly3': 1, 'jamz': 1, 'boltblue': 1, 'topped': 1, 'tgxxrz': 1, 'bubbletext': 1, 'problematic': 1, 'abnormally': 1, 'adults': 1, 'unconscious': 1, '9755': 1, 'teletext': 1, 'recieve': 1, 'faggot': 1, '07815296484': 1, '41782': 1, 'bani': 1, 'leads': 1, 'buttons': 1, 'max6': 1, 'csc': 1, 'applausestore': 1, 'monthlysubscription': 1, 'famous': 1, 'temper': 1, 'unconditionally': 1, 'bash': 1, 'oclock': 1, 'cooped': 1, 'weddin': 1, 'invitation': 1, 'alibi': 1, 'paces': 1, 'surrounded': 1, 'cuck': 1, 'sink': 1, 'cage': 1, 'deficient': 1, 'acknowledgement': 1, 'tactless': 1, 'astoundingly': 1, 'oath': 1, 'magic': 1, 'pan': 1, 'silly': 1, 'mutations': 1, 'uv': 1, 'causes': 1, 'sunscreen': 1, 'thesedays': 1, 'sugardad': 1, 'bao': 1, 'brownie': 1, 'ninish': 1, 'freek': 1, 'icky': 1, 'ridden': 1, 'missy': 1, 'goggles': 1, 'arguing': 1, '09050005321': 1, 'unfortuntly': 1, 'arngd': 1, 'frnt': 1, 'walkin': 1, 'sayin': 1, 'bites': 1, 'textand': 1, '08002988890': 1, 'tendencies': 1, 'jjc': 1, 'gotany': 1, 'meive': 1, 'yi': 1, 'srsly': 1, '07753741225': 1, '08715203677': 1, '42478': 1, 'prix': 1, 'nitz': 1, 'stands': 1, 'occur': 1, 'rajnikant': 1, 'blastin': 1, 'ocean': 1, 'speciale': 1, 'roses': 1, '07880867867': 1, 'zouk': 1, 'clubsaisai': 1, '07946746291': 1, 'xclusive': 1, 'banter': 1, 'bridgwater': 1, 'dependents': 1, 'cer': 1, 'thanx4': 1, 'beauties': 1, 'hundreds': 1, 'aunties': 1, 'handsomes': 1, 'friendships': 1, 'dismay': 1, 'concerned': 1, 'tootsie': 1, 'seventeen': 1, 'ml': 1, 'fetching': 1, 'restock': 1, 'brighten': 1, 'braved': 1, 'allo': 1, 'triumphed': 1, 'uncomfortable': 1, '08715203694': 1, 'rough': 1, 'sonetimes': 1, 'wesleys': 1, 'cloud': 1, 'wikipedia': 1, '08718711108': 1, '89034': 1, '88800': 1, 'repent': 1, 'sutra': 1, 'kama': 1, 'positions': 1, 'nange': 1, 'bakra': 1, 'kalstiya': 1, 'lakhs': 1, 'sun0819': 1, '08452810071': 1, 'ditto': 1, 'wetherspoons': 1, 'piggy': 1, 'freaky': 1, 'scrappy': 1, 'sdryb8i': 1, '1da': 1, 'lapdancer': 1, '150ppmsg': 1, 'sue': 1, 'g2': 1, 'tomorw': 1, 'imprtant': 1, 'crying': 1, 'bfore': 1, 'tmorow': 1, 'cherthala': 1, 'engaged': 1, '08712404000': 1, '448712404000': 1, '1680': 1, '1405': 1, '1843': 1, 'entrepreneurs': 1, 'corporation': 1, 'fluids': 1, 'dehydration': 1, 'prevent': 1, 'trek': 1, 'harri': 1, 'deck': 1, 'cnupdates': 1, 'gage': 1, 'alerts': 1, 'newsletter': 1, 'shitstorm': 1, 'attributed': 1, '08714712388': 1, '449071512431': 1, 'specs': 1, 'sth': 1, 'px3748': 1, '08714712394': 1, 'mindset': 1, 'macha': 1, 'wondar': 1, 'flim': 1, 'jelly': 1, 'scrumptious': 1, 'dao': 1, 'half8th': 1, 'visiting': 1, 'jide': 1, 'alertfrom': 1, 'drvgsto': 1, 'stewartsize': 1, 'jeri': 1, 'prescripiton': 1, '2kbsubject': 1, 'steak': 1, 'neglect': 1, 'prayers': 1, 'wahay': 1, 'hadn': 1, 'clocks': 1, 'realised': 1, 'gaze': 1, '82324': 1, 'tattoos': 1, 'caveboy': 1, 'vibrate': 1, '79': 1, '08704439680ts': 1, 'hungover': 1, 'grandmas': 1, 'closingdate04': 1, 'm39m51': 1, 'claimcode': 1, 'mobypobox734ls27yf': 1, '09066368327': 1, '50pmmorefrommobile2bremoved': 1, 'unclaimed': 1, 'gua': 1, 'faber': 1, 'dramatic': 1, 'hunting': 1, 'drunkard': 1, 'weaseling': 1, 'idc': 1, 'trash': 1, 'punish': 1, 'beerage': 1, 'randomlly': 1, 'fixes': 1, 'spelling': 1, '100p': 1, '087018728737': 1, 'tune': 1, 'toppoly': 1, 'fondly': 1, 'dogbreath': 1, 'sounding': 1, 'weighed': 1, 'woohoo': 1, 'uncountable': 1, '14thmarch': 1, 'availa': 1, '9996': 1, 'canlove': 1, 'whereare': 1, 'thekingshead': 1, 'friendsare': 1, 'rg21': 1, '4jx': 1, 'dled': 1, 'smokin': 1, 'boooo': 1, 'costumes': 1, 'yowifes': 1, 'notifications': 1, 'outbid': 1, 'plyr': 1, 'simonwatson5120': 1, 'shinco': 1, 'smsrewards': 1, 'youi': 1, 'yourjob': 1, 'soonlots': 1, 'llspeak': 1, 'starshine': 1, 'sips': 1, 'yourinclusive': 1, 'smsservices': 1, 'bits': 1, 'turned': 1, 'burial': 1, 'rvx': 1, 'rv': 1, 'comprehensive': 1, 'prashanthettan': 1, 'doug': 1, 'realizes': 1, 'guitar': 1, 'samantha': 1, 'impress': 1, 'trauma': 1, 'swear': 1, 'inner': 1, 'tigress': 1, 'overdose': 1, 'urfeeling': 1, 'bettersn': 1, 'probthat': 1, '83110': 1, 'ana': 1, 'rto': 1, 'sathy': 1, 'spoons': 1, 'corvettes': 1, '50pm': 1, '09061104283': 1, 'bunkers': 1, '07808': 1, 'xxxxxx': 1, '08719899217': 1, 'posh': 1, 'dob': 1, 'chaps': 1, 'prods': 1, 'trial': 1, 'champneys': 1, '0721072': 1, 'hole': 1, 'philosophical': 1, 'shakespeare': 1, 'atleast': 1, 'mymoby': 1, 'doit': 1, 'curfew': 1, 'getsleep': 1, 'gibe': 1, 'studdying': 1, 'woul': 1, 'massages': 1, 'yoyyooo': 1, 'permissions': 1, 'hussey': 1, 'mike': 1, 'faglord': 1, 'ctter': 1, 'cttargg': 1, 'ie': 1, 'cttergg': 1, 'ctagg': 1, 'cutter': 1, 'ctargg': 1, 'nutter': 1, 'thus': 1, 'grateful': 1, 'happier': 1, 'experiment': 1, 'agents': 1, 'invoices': 1, 'smell': 1, 'tobacco': 1, 'assumed': 1, 'racal': 1, 'dizzee': 1, 'bookmark': 1, 'stereophonics': 1, 'marley': 1, 'strokes': 1, 'libertines': 1, 'lastest': 1, 'nookii': 1, 'grinule': 1, 'oreos': 1, 'fudge': 1, 'zaher': 1, 'dieting': 1, 'nauseous': 1, 'hollalater': 1, 'avalarr': 1, 'rounds': 1, 'blogging': 1, 'magicalsongs': 1, 'blogspot': 1, 'slices': 1, 'kvb': 1, 'w1t1jy': 1, 'box403': 1, 'ppt150x3': 1, '1million': 1, 'alternative': 1, 'owo': 1, 'fro': 1, 'ore': 1, 'samus': 1, 'shoulders': 1, '09063440451': 1, 'ppm150': 1, 'matthew': 1, 'box334': 1, 'vomitin': 1, '528': 1, '1yf': 1, 'hp20': 1, '09061749602': 1, 'writhing': 1, 'bleh': 1, 'stuffed': 1, 'pockets': 1, 'paypal': 1, 'voila': 1, 'theyre': 1, 'folks': 1, 'sorta': 1, 'blown': 1, 'secondary': 1, 'applying': 1, 'ogunrinde': 1, 'sophas': 1, 'lodging': 1, 'chk': 1, 'dict': 1, 'shb': 1, 'stories': 1, 'retired': 1, 'natwest': 1, 'chad': 1, 'gymnastics': 1, 'christians': 1, 'token': 1, 'liking': 1, 'aptitude': 1, 'horse': 1, 'wrongly': 1, 'boggy': 1, 'biatch': 1, 'weakness': 1, 'hesitate': 1, 'notebook': 1, 'carpark': 1, 'eightish': 1, '5wkg': 1, 'cres': 1, 'ere': 1, 'ubi': 1, '67441233': 1, 'irene': 1, '61': 1, 'bus8': 1, '382': 1, '6ph': 1, '66': 1, '7am': 1, '5ish': 1, 'relaxing': 1, 'stripes': 1, 'skirt': 1, 'escalator': 1, 'beth': 1, 'charlie': 1, 'syllabus': 1, 'panasonic': 1, 'bluetoothhdset': 1, 'doubletxt': 1, 'doublemins': 1, '30pm': 1, 'kolathupalayam': 1, 'unjalur': 1, 'poyyarikatur': 1, 'erode': 1, 'apt': 1, 'hero': 1, 'meat': 1, 'supreme': 1, 'cudnt': 1, 'ctla': 1, 'ishtamayoo': 1, 'bakrid': 1, 'ente': 1, 'images': 1, 'fond': 1, 'finds': 1, 'cougar': 1, 'coaxing': 1, 'souveniers': 1, 'glorious': 1, '09065394514': 1, 'scratches': 1, 'nanny': 1, 'hardest': 1, 'shitin': 1, 'lekdog': 1, 'defo': 1, 'millions': 1, 'blankets': 1, 'atten': 1, '09058097218': 1, 'analysis': 1, 'data': 1, 'belligerent': 1, 'rudi': 1, 'les': 1, 'snoring': 1, 'ink': 1, '515': 1, 'throwing': 1, 'finalise': 1, 'flirtparty': 1, 'replys150': 1, 'dentist': 1, 'shes': 1, 'oyea': 1, 'nurses': 1, 'lul': 1, 'obese': 1, 'ami': 1, 'parchi': 1, 'kicchu': 1, 'korte': 1, 'tul': 1, 'korche': 1, 'iccha': 1, 'kaaj': 1, 'copies': 1, 'sculpture': 1, 'surya': 1, 'pokkiri': 1, 'sorrows': 1, 'praises': 1, 'sambar': 1, 'makiing': 1, 'attraction': 1, 'proove': 1, 'ndship': 1, '4few': 1, 'conected': 1, 'needle': 1, 'spatula': 1, 'outrageous': 1, 'complexities': 1, 'freely': 1, 'taxes': 1, 'ryder': 1, 'presleys': 1, 'elvis': 1, 'postal': 1, 'strips': 1, 'gifts': 1, 'cliff': 1, 'wrking': 1, 'sittin': 1, 'drops': 1, 'hen': 1, 'smoked': 1, 'teju': 1, 'hourish': 1, 'amla': 1, 'convenience': 1, 'evaluation': 1, '09050000301': 1, '449050000301': 1, '80155': 1, 'chat80155': 1, 'rcd': 1, 'speedchat': 1, 'swap': 1, 'chatter': 1, 'cheyyamo': 1, '80160': 1, 'txt43': 1, 'brothers': 1, 'throws': 1, 'hmv1': 1, 'errors': 1, 'piah': 1, 'tau': 1, '1stchoice': 1, '08707808226': 1, 'shade': 1, 'copied': 1, 'notified': 1, 'marketing': 1, '08450542832': 1, '84122': 1, 'theirs': 1, 'sexual': 1, 'virgins': 1, '69911': 1, '4fil': 1, 'kaitlyn': 1, 'sitter': 1, 'peeps': 1, 'danger': 1, 'comment': 1, 'veggie': 1, 'neighbors': 1, 'computerless': 1, 'balloon': 1, 'melody': 1, 'macs': 1, 'hme': 1, 'velachery': 1, 'flippin': 1, 'breaking': 1, 'cstore': 1, 'hangin': 1, 'lodge': 1, 'worrying': 1, 'quizzes': 1, '087016248': 1, '08719181503': 1, 'thin': 1, 'arguments': 1, 'fed': 1, 'himso': 1, 'semi': 1, 'exp': 1, '30apr': 1, 'maaaan': 1, 'guessin': 1, 'wuldnt': 1, 'personally': 1, 'ilol': 1, 'lunchtime': 1, 'organise': 1, '5years': 1, 'passable': 1, 'phd': 1, 'prakesh': 1, 'products': 1, 'betta': 1, 'aging': 1, 'global': 1, '08700435505150p': 1, 'accommodation': 1, 'phb1': 1, 'submitting': 1, 'snatch': 1, 'dancce': 1, 'basq': 1, 'pthis': 1, 'senrd': 1, 'ihave': 1, '0quit': 1, 'edrunk': 1, 'xxxxxxx': 1, 'iff': 1, 'ros': 1, 'drivby': 1, 'drum': 1, 'dnot': 1, '2nhite': 1, 'westonzoyland': 1, 'relieved': 1, 'greatness': 1, 'goin2bed': 1, 'only1more': 1, 'mc': 1, 'ifink': 1, 'everythin': 1, 'ava': 1, 'every1': 1, 'melnite': 1, 'oli': 1, 'goodtime': 1, 'l8rs': 1, '08712402779': 1, 'shun': 1, 'exhibition': 1, 'glass': 1, 'bian': 1, 'nino': 1, 'himself': 1, 'el': 1, 'downstem': 1, '08718730555': 1, 'wahala': 1, 'insects': 1, 'listening2the': 1, 'evil': 1, 'plumbing': 1, 'leafcutter': 1, 'inperialmusic': 1, 'molested': 1, 'acid': 1, 'remixed': 1, 'didntgive': 1, 'jenxxx': 1, 'thepub': 1, 'bellearlier': 1, 'bedbut': 1, 'uwana': 1, '09096102316': 1, 'cheery': 1, 'weirdo': 1, 'profiles': 1, 'stalk': 1, 'pax': 1, 'deposit': 1, '95': 1, 'jap': 1, 'disappeared': 1, 'certificate': 1, 'publish': 1, 'wheellock': 1, 'destination': 1, 'fifty': 1, 'happenin': 1, 'settling': 1, 'ipads': 1, 'worthless': 1, 'novelty': 1, 'cocksuckers': 1, 'janx': 1, 'dads': 1, 'developer': 1, 'designation': 1, 'musicnews': 1, 'videosounds': 1, '09701213186': 1, 'videosound': 1, 'spirit': 1, 'shattered': 1, 'girlie': 1, 'darker': 1, 'styling': 1, 'listn': 1, 'gray': 1, 'watevr': 1, 'paragraphs': 1, 'minus': 1, 'coveragd': 1, 'vasai': 1, 'retard': 1, 'bathroom': 1, 'sang': 1, 'uptown': 1, '80': 1, 'icic': 1, 'syria': 1, 'gauge': 1, 'completing': 1, 'ax': 1, 'unfolds': 1, 'emergency': 1, 'surgical': 1, 'korean': 1, 'fredericksburg': 1, 'pases': 1, 'que': 1, 'buen': 1, 'tiempo': 1, 'compass': 1, 'way2sms': 1, 'gnun': 1, 'youuuuu': 1, 'misss': 1, 'baaaaabe': 1, 'convince': 1, 'witot': 1, 'buyer': 1, 'undrstndng': 1, 'suffer': 1, 'becz': 1, 'avoids': 1, 'steamboat': 1, 'forgive': 1, 'tp': 1, '6ish': 1, 'bbq': 1, 'panicks': 1, 'everyso': 1, 'types': 1, 'nick': 1, 'auntie': 1, 'huai': 1, 'path': 1, 'paths': 1, 'appear': 1, 'thirunelvali': 1, 'reserve': 1, 'tackle': 1, 'tonght': 1, 'ironing': 1, 'pile': 1, 'chinky': 1, 'ploughing': 1, 'wi': 1, 'nz': 1, 'aust': 1, 'recharged': 1, 'papa': 1, 'detailed': 1, 'losers': 1, 'beta': 1, 'noncomittal': 1, 'snickering': 1, 'chords': 1, 'win150ppmx3age16': 1, 'boyf': 1, 'interviw': 1, 'entire': 1, 'determine': 1, 'spreadsheet': 1, 'trebles': 1, 'dartboard': 1, 'doubles': 1, 'coat': 1, 'recognises': 1, 'wisheds': 1, 'duo': 1, 'intrepid': 1, 'breeze': 1, 'fresh': 1, 'twittering': 1, 'chinchillas': 1, 'ducking': 1, 'function': 1, 'headstart': 1, 'rummer': 1, 'flying': 1, 'charts': 1, 'bbc': 1, 'optin': 1, 'thanks2': 1, 'rajini': 1, 'summers': 1, 'matched': 1, 'help08714742804': 1, 'spys': 1, '09099725823': 1, 'offering': 1, 'meow': 1, 'edhae': 1, 'bilo': 1, 'innu': 1, 'astne': 1, 'vargu': 1, 'lyfu': 1, 'halla': 1, 'yalru': 1, 'lyf': 1, 'mundhe': 1, 'ali': 1, 'ovr': 1, 'prone': 1, 'usa': 1, 'msgrcvd18': 1, '07801543489': 1, 'latests': 1, 'llc': 1, 'permission': 1, '09099726395': 1, 'meetins': 1, 'cumin': 1, 'lucy': 1, 'tablet': 1, 'dose': 1, 'incomm': 1, 'maps': 1, 'concentrating': 1, 'tiring': 1, 'browsin': 1, 'compulsory': 1, 'investigate': 1, 'vitamin': 1, 'crucial': 1, '2channel': 1, 'psychic': 1, 'jsco': 1, 'skills': 1, 'leadership': 1, 'host': 1, 'systems': 1, 'linux': 1, 'based': 1, 'idps': 1, 'converter': 1, 'sayy': 1, 'leanne': 1, 'disc': 1, 'glasgow': 1, 'champ': 1, 'lovin': 1, 'browse': 1, 'artists': 1, 'install': 1, 'speling': 1, 'corect': 1, '4719': 1, '523': 1, 'employee': 1, 'cts': 1, 'nike': 1, 'sooo': 1, 'shouting': 1, 'dang': 1, 'earliest': 1, 'nordstrom': 1, 'conference': 1, 'degree': 1, 'bleak': 1, 'shant': 1, 'nearer': 1, 'raiden': 1, 'totes': 1, 'cardin': 1, 'pierre': 1, 'establish': 1, 'rhythm': 1, 'truro': 1, 'ext': 1, 'cloth': 1, 'sunroof': 1, 'blanked': 1, 'image': 1, 'kalainar': 1, 'thenampet': 1, 'freaked': 1, 'reacting': 1, 'nosy': 1, 'imposter': 1, 'destiny': 1, 'satanic': 1, 'pudunga': 1, 'chef': 1, 'exterminator': 1, 'pest': 1, 'aaniye': 1, 'sympathetic': 1, 'venaam': 1, 'psychologist': 1, 'athletic': 1, 'determined': 1, 'companion': 1, 'stylist': 1, 'healer': 1, 'courageous': 1, 'organizer': 1, 'dependable': 1, 'listener': 1, 'psychiatrist': 1, 'chez': 1, 'jules': 1, 'nig': 1, 'hhahhaahahah': 1, 'leonardo': 1, 'dime': 1, 'strain': 1, '2years': 1, 'withdraw': 1, 'anyhow': 1, 'millers': 1, 'rawring': 1, 'xoxo': 1, 'spark': 1, 'flame': 1, 'crushes': 1, 'somewhr': 1, 'honeymoon': 1, 'outfit': 1, '08719899230': 1, 'cheque': 1, 'olympics': 1, 'leo': 1, 'haul': 1, 'want2come': 1, 'wizzle': 1, 'wildlife': 1, 'that2worzels': 1, 'cya': 1, 'shanghai': 1, '645': 1, 'redeemable': 1, 'rt': 1, '08701237397': 1, 'pro': 1, 'thnx': 1, 'anjie': 1, 'sef': 1, 'fring': 1, 'nte': 1, '526': 1, 'bx': 1, '02072069400': 1, 'talents': 1, 'animal': 1, 'warming': 1, 'shiny': 1, 'fooled': 1, 'french': 1, 'responsible': 1, 'companies': 1, 'guarantee': 1, 'suppliers': 1, '0a': 1, 'lnly': 1, 'keen': 1, 'dammit': 1, 'wright': 1, 'wrecked': 1, 'somewhat': 1, 'laden': 1, 'goodevening': 1, 'spontaneously': 1, 'rgent': 1, 'busty': 1, 'daytime': 1, '09099726429': 1, 'janinexx': 1, 'spageddies': 1, 'fourth': 1, 'dimension': 1, 'phasing': 1, 'compromised': 1, 'meaningful': 1, '09050001295': 1, 'a21': 1, '391784': 1, 'mobsi': 1, 'dub': 1, 'je': 1, 'toughest': 1, 'unspoken': 1, 'squatting': 1, 'digits': 1, '0089': 1, '09063442151': 1, 'sonathaya': 1, 'soladha': 1, 'raping': 1, 'dudes': 1, 'weightloss': 1, 'embarrassed': 1, 'mushy': 1, 'stash': 1, 'priya': 1, 'kilos': 1, 'accidant': 1, 'tookplace': 1, 'ghodbandar': 1, 'slovely': 1, 'sc': 1, 'wad': 1, 'specialise': 1, 'desparately': 1, 'mi': 1, 'stereo': 1, 'classmates': 1, 'fires': 1, 'vipclub4u': 1, 'trackmarque': 1, 'missionary': 1, 'entertaining': 1, 'hugh': 1, 'stick': 1, 'laurie': 1, 'praps': 1, 'jon': 1, 'dinero': 1, 'spain': 1, '000pes': 1, 'complaining': 1, 'mandy': 1, '09041940223': 1, 'transferred': 1, 'fm': 1, 'hotmix': 1, 'sullivan': 1, 'finn': 1, 'thew': 1, 'theacusations': 1, 'iwana': 1, 'wotu': 1, 'haventcn': 1, 'downon': 1, 'itxt': 1, 'nething': 1, 'dine': 1, '09111030116': 1, 'conacted': 1, 'pobox12n146tf15': 1, 'inspection': 1, 'nursery': 1, 'becomes': 1, 'panren': 1, 'paru': 1, 'trainners': 1, 'carryin': 1, 'bac': 1, 'chuckin': 1, 'dhanush': 1, 'needing': 1, 'habba': 1, 'dileep': 1, 'venugopal': 1, 'muchand': 1, 'mentioned': 1, 'remembrs': 1, 'everytime': 1, 'edition': 1, 'algorithms': 1, 'textbook': 1, '3230': 1, 'cro1327': 1, '09064018838': 1, 'iwas': 1, 'urmom': 1, 'careabout': 1, 'itried2tell': 1, 'marine': 1, 'intend': 1, 'learned': 1, 'traveling': 1, 'honest': 1, 'afghanistan': 1, 'stable': 1, 'iraq': 1, '50award': 1, '1225': 1, 'pai': 1, 'seh': 1, 'parts': 1, 'walsall': 1, 'terry': 1, 'tue': 1, 'ccna': 1, 'shrek': 1, 'fellow': 1, 'dying': 1, 'lifting': 1, 'teresa': 1, 'aid': 1, 'ld': 1, 'bam': 1, 'dec': 1, 'usmle': 1, 'squishy': 1, 'mwahs': 1, 'hottest': 1, 'prominent': 1, 'cheek': 1, 'september': 1, 'bcm': 1, 'neo69': 1, 'backdoor': 1, '8027': 1, 'hack': 1, 'subscribe': 1, 'fraction': 1, 'dps': 1, '09050280520': 1, 'comingdown': 1, 'murali': 1, 'sts': 1, 'kissing': 1, 'elliot': 1, 'mia': 1, 'engalnd': 1, 'd3wv': 1, '100txt': 1, '2price': 1, 'matric': 1, '650': 1, '850': 1, '08718726970': 1, 'payments': 1, 'fedex': 1, 'reception': 1, 'consensus': 1, 'entertain': 1, 'pillows': 1, 'strewn': 1, 'bras': 1, 'tag': 1, 'exposes': 1, 'weaknesses': 1, 'knee': 1, 'pulls': 1, 'wicked': 1, 'supports': 1, 'srt': 1, 'ps3': 1, 'jontin': 1, 'banned': 1, 'biro': 1, '09058094594': 1, 'unconsciously': 1, 'unhappy': 1, 'shell': 1, 'jog': 1, '09061743811': 1, 'lark': 1, 'sic': 1, '0870753331018': 1, '7mp': 1, '09090900040': 1, 'extreme': 1, 'wild': 1, 'fones': 1, 'stop2stop': 1, 'lim': 1, 'parachute': 1, 'placed': 1, 'lambda': 1, 'snowball': 1, 'angels': 1, 'ello': 1, 'duffer': 1, 'ofice': 1, 'pharmacy': 1, 'grr': 1, 'cnl': 1, '08715500022': 1, 'rpl': 1, 'nor': 1, 'fffff': 1, 'lifebook': 1, 'zhong': 1, 'qing': 1, 'act': 1, 'hypertension': 1, 'annoyin': 1, '08702490080': 1, 'vpod': 1, 'nigro': 1, 'scratching': 1, 'anyplaces': 1, 'priority': 1, 'ecstasy': 1, 'minded': 1, '09090204448': 1, 'ls278bb': 1, 'minapn': 1, 'hittng': 1, 'reflex': 1, 'egbon': 1, 'adewale': 1, 'mary': 1, 'deduct': 1, 'wrks': 1, 'asshole': 1, 'monkey': 1, 'grab': 1, 'sliding': 1, '09065394973': 1, 'payback': 1, 'tescos': 1, 'bowa': 1, 'feathery': 1, 'infra': 1, 'gep': 1, 'fifa': 1, '2006': 1, 'shhhhh': 1, 'arul': 1, 'related': 1, 'amk': 1, '09061743810': 1, 'length': 1, 'corrct': 1, 'antha': 1, 'dane': 1, 'basket': 1, 'rupaul': 1, 'curtsey': 1, 'practising': 1, '4my': 1, 'havebeen': 1, '2i': 1, 'feelingood': 1, 'ordinator': 1, 'preschoolco': 1, 'rise': 1, 'havbeen': 1, 'payed2day': 1, 'memory': 1, 'converted': 1, 'soil': 1, 'african': 1, 'outreach': 1, 'roles': 1, '8lb': 1, 'brilliantly': 1, '7oz': 1, 'forwarding': 1, 'visitors': 1, 'intention': 1, 'bend': 1, 'rules': 1, 'thia': 1, 'inlude': 1, 'previews': 1, 'madurai': 1, 'marrge': 1, 'dha': 1, 'ambrith': 1, 'kitty': 1, 'shaved': 1, 'tactful': 1, 'pert': 1, 'crammed': 1, 'satsgettin': 1, '47per': 1, 'apologize': 1, 'pei': 1, 'subtoitles': 1, 'jot': 1, 'cereals': 1, 'gari': 1, 'bold2': 1, '1er': 1, 'm60': 1, 'cast': 1, 'aom': 1, '09094100151': 1, 'gbp5': 1, 'box61': 1, 'thkin': 1, 'resubbing': 1, 'shadow': 1, 'breadstick': 1, 'saeed': 1, '09066362220': 1, 'purple': 1, 'brown': 1, 'yelow': 1, 'arranging': 1, 'eldest': 1, 'drugdealer': 1, 'wither': 1, '23f': 1, '23g': 1, 'wondarfull': 1, 'txt250': 1, 'web2mobile': 1, 'txtx': 1, 'box139': 1, 'la32wu': 1, 'onbus': 1, 'donyt': 1, 'latelyxxx': 1, '85233': 1, 'endof': 1, 'justthought': 1, '2hook': 1, 'uwant': 1, 'offdam': 1, 'nevamind': 1, 'sayhey': 1, 'stressed': 1, 'provider': 1, 'soooo': 1, 'tming': 1, 'cutest': 1, 'dice': 1, '08700469649': 1, 'box420': 1, 'mathe': 1, 'samachara': 1, 'howda': 1, 'autocorrect': 1, 'audrie': 1, 'readiness': 1, 'simulate': 1, 'lara': 1, 'supplies': 1, 'attach': 1, 'guesses': 1, '087123002209am': 1, 'nickey': 1, 'nobbing': 1, 'platt': 1, 'washob': 1, 'sterling': 1, 'spotty': 1, 'province': 1, 'hall': 1, 'hesitation': 1, 'ponnungale': 1, 'intha': 1, 'ipaditan': 1, 'rejected': 1, 'noisy': 1, 'needa': 1, 'reset': 1, 'troubleshooting': 1, 'manual': 1, 'b4utele': 1, 'marsms': 1, 'b4u': 1, '08717168528': 1, 'stifled': 1, 'creativity': 1, 'strongly': 1, 'requirements': 1, '2getha': 1, 'qlynnbv': 1, 'help08700621170150p': 1, 'buffy': 1, 'nosh': 1, 'lololo': 1, 'waaaat': 1, 'occupied': 1, 'documents': 1, 'stapati': 1, 'submitted': 1, 'hills': 1, 'cutie': 1, 'honesty': 1, 'beggar': 1, 'shakara': 1, 'specialisation': 1, 'labor': 1, 'dent': 1, 'crickiting': 1, 'isv': 1, 'urgoin': 1, 'tome': 1, 'reallyneed': 1, '2docd': 1, 'dontignore': 1, 'mycalls': 1, 'imin': 1, 'outl8r': 1, 'dontmatter': 1, 'thecd': 1, 'dontplease': 1, 'yavnt': 1, 'ibuprofens': 1, 'popping': 1, 'sip': 1, 'grown': 1, 'chinatown': 1, 'claypot': 1, 'beehoon': 1, 'fishhead': 1, 'yam': 1, 'porridge': 1, 'jaklin': 1, 'cliffs': 1, 'nearby': 1, 'bundle': 1, 'mf': 1, 'deals': 1, '49': 1, 'avble': 1, '4got': 1, 'weds': 1, 'moseley': 1, 'ooh': 1, 'thankyou': 1, 'ternal': 1, 'ntimate': 1, 'namous': 1, 'aluable': 1, 'atural': 1, 'oble': 1, 'ffectionate': 1, 'oveable': 1, 'ruthful': 1, 'textin': 1, 'burn': 1, 'amigos': 1, 'progress': 1, 'weren': 1, 'tryin': 1, 'collages': 1, 'arty': 1, '2hrs': 1, 'waliking': 1, 'cartons': 1, 'shelves': 1, '08714712379': 1, 'mirror': 1, '09065069120': 1, 'k718': 1, 'keris': 1, 'smidgin': 1, 'jod': 1, 'intentions': 1, 'accordin': 1, 'knocking': 1, 'como': 1, 'abel': 1, 'listened2the': 1, 'air1': 1, 'braindance': 1, 'plaid': 1, 'ofstuff': 1, 'hilarious': 1, 'hav2hear': 1, 'aphex': 1, 'nelson': 1, 'unmits': 1, 'newspapers': 1, 'yummmm': 1, 'puzzeles': 1, 'scammers': 1, '4goten': 1, '09099726481': 1, 'passion': 1, 'dena': 1, '09065069154': 1, 'r836': 1, 'shifad': 1, 'raised': 1, 'doctors': 1, 'reminds': 1, 'tolerat': 1, 'bcs': 1, 'subscrition': 1, 'splashmobile': 1, 'dust': 1, '88877': 1, '3pound': 1, 'watchin': 1, 'meaningless': 1, 'jones': 1, 'brdget': 1, 'inever': 1, 'hype': 1, 'studio': 1, 'velly': 1, 'marking': 1, '2stoptx': 1, '08718738034': 1, 'va': 1, 'hanger': 1, 'arrow': 1, 'blanket': 1, '08718726971': 1, 'tddnewsletter': 1, 'dozens': 1, 'thedailydraw': 1, 'emc1': 1, 'prizeswith': 1, 'significant': 1, 'waqt': 1, 'ko': 1, 'pehle': 1, 'wo': 1, 'naseeb': 1, 'jeetey': 1, 'nahi': 1, 'kisi': 1, 'kuch': 1, 'jo': 1, 'zyada': 1, 'milta': 1, 'hum': 1, 'zindgi': 1, 'sochte': 1, 'stalking': 1, 'reminded': 1, 'elaya': 1, 'varaya': 1, '09066368753': 1, '97n7qp': 1, 'anand': 1, 'expected': 1, 'beach': 1, 'workand': 1, 'jez': 1, 'whilltake': 1, 'todo': 1, 'zogtorius': 1, 'financial': 1, 'alian': 1, 'or2optout': 1, 'hv9d': 1, 'century': 1, 'frwd': 1, 'posible': 1, 'affectionate': 1, 'sorts': 1, 'restrictions': 1, 'buddys': 1, '08712402902': 1, 'owned': 1, 'possessive': 1, 'clarification': 1, 'coimbatore': 1, 'stream': 1, '0871212025016': 1, 'monos': 1, 'monoc': 1, 'polyc': 1, 'categories': 1, 'transcribing': 1, 'ethnicity': 1, 'census': 1, 'cakes': 1, 'draws': 1, 'asusual': 1, 'franyxxxxx': 1, 'goodmate': 1, 'cheered': 1, 'batt': 1, 'pobox1': 1, 'becausethey': 1, 'w14rg': 1, '09058098002': 1, 'gained': 1, 'limits': 1, 'pressure': 1, 'doke': 1, 'laying': 1, 'neshanth': 1, 'byatch': 1, 'whassup': 1, 'cl': 1, 'slo': 1, '4msgs': 1, 'filthyguys': 1, 'chiong': 1, 'reltnship': 1, 'wipe': 1, 'dialogue': 1, 'dryer': 1, 'comb': 1, 'pose': 1, 'fps': 1, 'computational': 1, 'disturbance': 1, 'premarica': 1, 'dlf': 1, 'gotto': 1, '220cm2': 1, 'err': 1, 'bloo': 1, 'hitter': 1, 'offline': 1, 'anjola': 1, 'asjesus': 1, 'ki': 1, 'imf': 1, 'corrupt': 1, 'deposited': 1, 'projects': 1, 'pura': 1, 'karo': 1, 'crore': 1, 'padhe': 1, 'suply': 1, 'lac': 1, 'blocked': 1, 'itna': 1, 'directors': 1, 'politicians': 1, 'swiss': 1, 'taxless': 1, 'torrents': 1, 'slowing': 1, 'particularly': 1, 'commit': 1, '83370': 1, 'trivia': 1, 'rightio': 1, 'brum': 1, 'scorable': 1, 'paranoid': 1, 'sheet': 1, 'brin': 1, 'bsnl': 1, 'complain': 1, 'offc': 1, 'bettr': 1, 'payed': 1, 'suganya': 1, 'dessert': 1, 'abeg': 1, 'sponsors': 1, 'onum': 1, 'imagination': 1, 'poet': 1, 'rr': 1, 'famamus': 1, 'locks': 1, 'jenne': 1, 'easiest': 1, 'barcelona': 1, 'sppok': 1, 'complementary': 1, '2px': 1, 'wa14': 1, 'pansy': 1, 'jungle': 1, 'kanji': 1, 'srs': 1, 'drizzling': 1, 'appointments': 1, 'excused': 1, 'reppurcussions': 1, 'necessity': 1, 'drama': 1, 'struggling': 1, 'ego': 1, 'cosign': 1, '09061701444': 1, 'hvae': 1, 'requires': 1, 'suman': 1, 'telephonic': 1, 'hcl': 1, 'freshers': 1, 'reliant': 1, 'fwiw': 1, 'afford': 1, 'arrival': 1, 'sq825': 1, 'citylink': 1, 'props': 1, 'statements': 1, 'pleasant': 1, 'pobox114': 1, '14tcr': 1, '6230': 1, 'splendid': 1, 'bognor': 1, 'ktv': 1, 'misplaced': 1, 'computers': 1, 'permanent': 1, 'registration': 1, 'begun': 1, 'residency': 1, 'risks': 1, 'predicting': 1, 'accumulation': 1, 'programs': 1, 'grief': 1, 'belongs': 1, 'shoranur': 1, 'prior': 1, 'fuelled': 1, 'fated': 1, 'concern': 1, 'txt82228': 1, 'text82228': 1, 'promptly': 1, 'honestly': 1, 'burnt': 1, 'quizclub': 1, '80122300p': 1, 'rwm': 1, '08704050406': 1, 'snap': 1, 'connected': 1, 'gmw': 1, 'someplace': 1, 'goods': 1, 'pressies': 1, 'ultimately': 1, 'achieve': 1, 'motive': 1, 'tui': 1, 'tor': 1, 'korli': 1, 'dock': 1, 'newscaster': 1, 'rolled': 1, 'flute': 1, 'wheel': 1, 'dabbles': 1, 'picsfree1': 1, 'keyword': 1, 'the4th': 1, 'october': 1, '83435': 1, 'safety': 1, 'aspects': 1, 'elaborating': 1, '85555': 1, 'tarot': 1, 'ours': 1, 'horniest': 1, 'cysts': 1, 'flow': 1, 'shrink': 1, 'ovarian': 1, 'developed': 1, 'grams': 1, 'upping': 1, 'timin': 1, 'apes': 1, 'ibm': 1, 'hp': 1, 'gosh': 1, 'spose': 1, 'rimac': 1, 'dosomething': 1, 'arestaurant': 1, 'squid': 1, 'dabooks': 1, 'eachother': 1, 'luckily': 1, 'starring': 1, 'restocked': 1, 'stoptxtstop': 1, 'knock': 1, 'tkls': 1, 'challenging': 1, 'smoothly': 1, 'breakfast': 1, 'hamper': 1, 'cc100p': 1, 'above': 1, '0870737910216yrs': 1, 'unni': 1, 'dramastorm': 1, 'particular': 1, 'lacking': 1, 'forfeit': 1, 'coupla': 1, 'digi': 1, '077xxx': 1, '09066362206': 1, 'sundayish': 1, 'prasad': 1, 'rcb': 1, 'kochi': 1, 'smear': 1, 'checkup': 1, 'gobi': 1, '4w': 1, 'technologies': 1, 'olowoyey': 1, 'argentina': 1, 'taxt': 1, 'lool': 1, 'tie': 1, 'massage': 1, 'pos': 1, 'shaking': 1, 'scarcasim': 1, 'naal': 1, 'eruku': 1, 'w4': 1, '5wq': 1, 'sensible': 1, 'impressively': 1, 'obedient': 1, 'ft': 1, 'combination': 1, 'needy': 1, 'playng': 1, 'jorge': 1, 'mcfly': 1, 'sara': 1, 'ab': 1, 'yupz': 1, 'ericson': 1, 'luks': 1, 'modl': 1, 'der': 1, 'frosty': 1, 'cheesy': 1, 'witin': 1, '0870141701216': 1, '120p': 1, 'fans': 1, '09050000555': 1, 'ba128nnfwfly150ppm': 1, '10th': 1, 'themed': 1, 'nudist': 1, 'pump': 1, 'signal': 1, 'unusual': 1, 'palm': 1, 'handing': 1, 'printing': 1, '83021': 1, 'stated': 1, 'perpetual': 1, 'dd': 1, 'flung': 1, 'pract': 1, 'brains': 1, 'justbeen': 1, 'overa': 1, 'mush': 1, 'tunde': 1, 'missions': 1, '20m12aq': 1, 'eh74rr': 1, 'avo': 1, 'cuddled': 1, 'crashed': 1, 'chachi': 1, 'pl': 1, 'tiz': 1, 'kanagu': 1, 'prices': 1, 'ringing': 1, 'houseful': 1, 'pulling': 1, 'brats': 1, 'derp': 1, 'abusers': 1, 'lipo': 1, 'netflix': 1, 'clash': 1, 'arr': 1, 'oscar': 1, 'rebtel': 1, 'firefox': 1, 'bcmsfwc1n3xx': 1, '69969': 1, 'impressed': 1, 'funs': 1, 'footy': 1, 'coca': 1, 'stadium': 1, 'cola': 1, 'large': 1, 'teenager': 1, 'replacing': 1, 'paracetamol': 1, 'mittelschmertz': 1, 'arrived': 1, 'references': 1, 'cthen': 1, 'conclusion': 1, 'instant': 1, '08715203028': 1, '9th': 1, 'rugby': 1, 'courtroom': 1, 'twiggs': 1, 'affidavit': 1, 'showers': 1, 'possessiveness': 1, 'golden': 1, 'poured': 1, 'lasting': 1, 'mobs': 1, 'ymca': 1, 'crazyin': 1, 'sleepingwith': 1, 'finest': 1, 'breathe1': 1, 'pobox365o4w45wq': 1, 'wtc': 1, 'weiyi': 1, '505060': 1, 'flowers': 1, 'interflora': 1, 'paining': 1, 'outgoing': 1, 'romcapspam': 1, 'presence': 1, 'mee': 1, 'maggi': 1, '08712103738': 1, 'cough': 1, 'pooja': 1, 'sweatter': 1, 'ambitious': 1, 'miiiiiiissssssssss': 1, 'tunji': 1, 'frndz': 1, '6missed': 1, 'misscall': 1, 'mad1': 1, 'mad2': 1, 'tall': 1, 'avenge': 1, 'robs': 1, 'gudni8': 1, 'choices': 1, 'toss': 1, 'coin': 1, 'dancin': 1, 'nora': 1, 'explicitly': 1, 'gayle': 1, 'crucify': 1, 'butting': 1, 'vs': 1, 'cedar': 1, 'reserved': 1, 'durham': 1, '69855': 1, 'sf': 1, 'stopbcm': 1, 'wall': 1, 'groovying': 1, 'printer': 1, 'groovy': 1, 'acnt': 1, 'harish': 1, 'transfred': 1, 'shaping': 1, 'showrooms': 1, 'attending': 1, 'doinat': 1, 'callon': 1, 'yhl': 1, 'pdate_now': 1, 'configure': 1, 'anal': 1, 'pears': 1, 'such': 1, 'oooooh': 1, '09058094454': 1, 'expiry': 1, 'resubmit': 1, 'mint': 1, 'humans': 1, 'studyn': 1, 'everyboy': 1, 'xxxxxxxx': 1, 'answr': 1, '1thing': 1, 'liquor': 1, 'loko': 1, '730': 1, 'lined': 1, 'laughs': 1, 'fireplace': 1, 'icon': 1, '08712400200': 1, 'weasels': 1, 'fifth': 1, 'woozles': 1, '08718723815': 1, 'machines': 1, 'fucks': 1, 'ignorant': 1, 'mys': 1, 'downs': 1, 'fletcher': 1, '08714714011': 1, 'bowls': 1, 'cozy': 1, 'shake': 1, 'buzzzz': 1, 'vibrator': 1, 'pros': 1, 'jet': 1, 'nuclear': 1, 'cons': 1, 'trends': 1, 'brief': 1, 'iter': 1, 'description': 1, 'fusion': 1, 'shitinnit': 1, 'ikno': 1, 'nowhere': 1, 'doesdiscount': 1, 'jabo': 1, 'slower': 1, 'maniac': 1, 'manege': 1, 'swalpa': 1, 'hogidhe': 1, 'chinnu': 1, 'sapna': 1, 'agidhane': 1, 'footbl': 1, 'crckt': 1, 'swell': 1, 'bollox': 1, 'tim': 1, 'tol': 1, 'ingredients': 1, 'pocy': 1, 'non': 1, '4qf2': 1, 'senor': 1, 'person2die': 1, 'possibly': 1, 'nvq': 1, 'giggle': 1, 'professional': 1, 'tiger': 1, 'woods': 1, 'grinder': 1, 'buyers': 1, 'figuring': 1, 'entirely': 1, 'disconnected': 1, 'onluy': 1, 'offcampus': 1, 'matters': 1, 'riley': 1, 'ew': 1, 'wesley': 1, 'lingo': 1, '400mins': 1, 'j5q': 1, 'chrgd': 1, '69200': 1, '2exit': 1, 'independence': 1, 'afternoons': 1, 'ugadi': 1, 'sankranti': 1, 'dasara': 1, 'rememberi': 1, 'teachers': 1, 'republic': 1, 'fools': 1, 'festival': 1, 'childrens': 1, 'approaching': 1, 'shivratri': 1, 'mornings': 1, 'joys': 1, 'greeting': 1, 'somewheresomeone': 1, 'daywith': 1, 'tosend': 1, 'lifeis': 1, 'selflessness': 1, 'initiate': 1, 'tallent': 1, 'wasting': 1, 'portal': 1, 't4get2text': 1, 'lennon': 1, 'bothering': 1, 'crab': 1, 'fox': 1, 'waves': 1, 'footprints': 1, 'frndsship': 1, 'dwn': 1, 'summon': 1, 'slaaaaave': 1, 'appendix': 1, 'slob': 1, 'smiled': 1, 'webpage': 1, 'yeesh': 1, 'gotbabes': 1, 'subscriptions': 1, 'hunks': 1, 'unsubscribed': 1, 'participate': 1, 'gopalettan': 1, 'stopcost': 1, '08712400603': 1, 'abroad': 1, 'xxsp': 1, 'mat': 1, 'agent': 1, 'goodies': 1, 'ay': 1, 'steal': 1, 'isaiah': 1, 'expert': 1, 'thinl': 1, 'importantly': 1, 'tightly': 1, 'fals': 1, 'pretsovru': 1, 'nav': 1, 'wnevr': 1, 'vth': 1, 'pretsorginta': 1, 'yen': 1, 'fal': 1, 'alwa': 1, 'madodu': 1, 'nammanna': 1, 'stdtxtrate': 1, 'soundtrack': 1, 'lord': 1, 'rings': 1, 'pc1323': 1, 'phyhcmk': 1, 'sg': 1, 'emigrated': 1, 'hopeful': 1, 'olol': 1, 'victors': 1, 'winterstone': 1, 'stagwood': 1, 'jp': 1, 'mofo': 1, 'maraikara': 1, 'pathaya': 1, 'enketa': 1, 'priest': 1, 'reserves': 1, 'intrude': 1, 'walkabout': 1, 'cashed': 1, 'announced': 1, 'blog': 1, '28th': 1, 'neville': 1, 'footie': 1, 'phil': 1, 'abbey': 1, 'returning': 1, 'punj': 1, 'str8': 1, 'classic': 1, '200p': 1, 'sacked': 1, 'mmsto': 1, '35p': 1, 'lookatme': 1, 'clip': 1, '32323': 1, 'twat': 1, 'punch': 1, 'barred': 1, 'decking': 1, 'dungerees': 1, 'mentionned': 1, 'vat': 1, 'grl': 1, 'madstini': 1, 'eerulli': 1, 'agalla': 1, 'kodstini': 1, 'hogli': 1, 'kodthini': 1, 'mutai': 1, 'hogolo': 1, 'messed': 1, 'illspeak': 1, 'thasa': 1, 'shudvetold': 1, 'urgran': 1, 'u2moro': 1, 'updat': 1, 'okden': 1, 'likeyour': 1, 'mecause': 1, 'werebored': 1, 'countinlots': 1, 'uin': 1, 'tex': 1, 'gr8fun': 1, 'tagged': 1, 'hdd': 1, 'casing': 1, 'opened': 1, 'describe': 1, '140ppm': 1, '08718725756': 1, '09053750005': 1, '310303': 1, 'asus': 1, 'reformat': 1, 'plumbers': 1, 'wrench': 1, 'bcum': 1, 'appeal': 1, 'thriller': 1, 'director': 1, 'shove': 1, 'um': 1, 'elephant': 1, 'cr': 1, 'pookie': 1, 'nri': 1, 'x2': 1, 'deserve': 1, 'neighbor': 1, 'toothpaste': 1, 'diddy': 1, 'poking': 1, 'coccooning': 1, 'mus': 1, 'talkin': 1, 'newquay': 1, '1im': 1, 'windy': 1, 'y87': 1, '09066358361': 1, 'tirunelvai': 1, 'dusk': 1, 'puzzles': 1, '09065989180': 1, 'x29': 1, 'phews': 1, 'stairs': 1, 'earning': 1, 'recycling': 1, 'toledo': 1, 'reservations': 1, 'tai': 1, 'feng': 1, 'swimsuit': 1, 'frndshp': 1, 'luvd': 1, 'squeeeeeze': 1, 'hurricanes': 1, 'disasters': 1, 'erupt': 1, 'sway': 1, 'aroundn': 1, 'arise': 1, 'volcanoes': 1, 'lighters': 1, 'lasagna': 1, 'woould': 1, 'chickened': 1, '08718726978': 1, '7732584351': 1, '44': 1, 'raviyog': 1, 'bhayandar': 1, 'peripherals': 1, 'sunoco': 1, 'musical': 1, 'leftovers': 1, 'plate': 1, 'starving': 1, 'fatty': 1, 'badrith': 1, 'owe': 1, 'checkin': 1, 'swann': 1, 'armenia': 1, '09058097189': 1, '1205': 1, '330': 1, '1120': 1, 'justify': 1, 'hunt': 1, 'hava': 1, '1131': 1, '5226': 1, 'adrian': 1, 'thnq': 1, 'rct': 1, 'vatian': 1, 'babysitting': 1, 'everyones': 1, 'buttheres': 1, 'ofsi': 1, 'aboutas': 1, 'gonnamissu': 1, 'yaxx': 1, 'breakin': 1, 'merememberin': 1, 'asthere': 1, 'neglet': 1, 'ramaduth': 1, 'siguviri': 1, 'mahaveer': 1, 'nalli': 1, 'ee': 1, 'problum': 1, 'dodda': 1, 'pavanaputra': 1, 'ondu': 1, 'keluviri': 1, 'maruti': 1, 'maretare': 1, 'poortiyagi': 1, 'bajarangabali': 1, 'sankatmochan': 1, 'hanumanji': 1, 'olage': 1, 'hanuman': 1, 'kalisidare': 1, 'odalebeku': 1, 'janarige': 1, 'idu': 1, 'inde': 1, 'ivatte': 1, 'matra': 1, 'ijust': 1, 'talked': 1, 'opps': 1, 'dl': 1, 'gei': 1, 'tron': 1, 'workage': 1, 'spiffing': 1, 'craving': 1, 'babysit': 1, 'supose': 1, 'embassy': 1, 'spaces': 1, 'lightly': 1, 'checkboxes': 1, 'batsman': 1, 'yetty': 1, '09050000928': 1, 'yifeng': 1, 'emailed': 1, 'slurp': 1, '3miles': 1, 'doll': 1, 'barolla': 1, 'brainless': 1, 'sariyag': 1, 'vehicle': 1, 'madoke': 1, '07090201529': 1, 'postponed': 1, 'stocked': 1, 'tiime': 1, 'afternon': 1, 'interviews': 1, 'resizing': 1, '09066364349': 1, 'box434sk38wp150ppm18': 1, 'opposed': 1, '08081263000': 1, '83332': 1, 'shortcode': 1, 'refunded': 1, 'somerset': 1, 'nigpun': 1, 'overtime': 1, 'dismissial': 1, 'screwd': 1, '08712402972': 1, 'bull': 1, 'floating': 1, '09058095201': 1, 'heehee': 1, 'percentages': 1, 'arithmetic': 1, 'chillaxin': 1, 'iknow': 1, 'peril': 1, 'das': 1, 'studentfinancial': 1, 'wellda': 1, 'monster': 1, 'obey': 1, 'uhhhhrmm': 1, 'deltomorrow': 1, '09066368470': 1, '24m': 1, 'subscriptn3gbp': 1, '68866': 1, '08448714184': 1, 'landlineonly': 1, 'smartcall': 1, 'orno': 1, 'minmobsmore': 1, 'fink': 1, 'lkpobox177hp51fl': 1, 'carlie': 1, 'promised': 1, '09099726553': 1, 'youwanna': 1, 'youphone': 1, 'athome': 1, 'jack': 1, 'hypotheticalhuagauahahuagahyuhagga': 1, 'pretend': 1, 'helpful': 1, 'brainy': 1, 'reflection': 1, 'occasion': 1, 'traditions': 1, 'values': 1, 'affections': 1, 'celebrated': 1, 'katexxx': 1, 'myparents': 1, 'cantdo': 1, 'anythingtomorrow': 1, 'outfor': 1, 'aretaking': 1, 'level': 1, 'gate': 1, '89105': 1, 'lingerie': 1, 'weddingfriend': 1, 'petticoatdreams': 1, 'bridal': 1, 'inst': 1, 'overheating': 1, 'board': 1, 'reslove': 1, 'western': 1, 'oblisingately': 1, 'bambling': 1, 'notixiquating': 1, 'champlaxigating': 1, 'laxinorficated': 1, 'opted': 1, 'masteriastering': 1, 'wotz': 1, 'atrocious': 1, 'entropication': 1, 'amplikater': 1, 'fidalfication': 1, 'junna': 1, 'knickers': 1, 'nikiyu4': 1, '01223585236': 1, 'divert': 1, 'a30': 1, 'wadebridge': 1, 'orc': 1, 'vill': 1, 'seeking': 1, 'wherre': 1, 'phone750': 1, 'resolution': 1, 'frank': 1, 'logoff': 1, 'parkin': 1, 'asa': 1, '09050000878': 1, 'charming': 1, 'served': 1, 'mention': 1, 'arnt': 1, 'xxxxxxxxxxxxxx': 1, 'dorothy': 1, 'kiefer': 1, 'allalo': 1, 'mone': 1, 'alle': 1, 'eppolum': 1, 'fundamentals': 1, 'whoever': 1, 'dooms': 1, '5digital': 1, '3optical': 1, '1mega': 1, 'pixels': 1, 'js': 1, 'noi': 1, 'captaining': 1, 'burgundy': 1, 'amrita': 1, 'bpo': 1, 'profile': 1, 'persevered': 1, 'nighters': 1, 'regretted': 1, 'spouse': 1, 'pmt': 1, 'shldxxxx': 1, '4give': 1, 'scenario': 1, 'nytho': 1, 'frmcloud': 1, 'spun': 1, '2mwen': 1, 'fonin': 1, 'tx': 1, 'wrld': 1, '09071517866': 1, '150ppmpobox10183bhamb64xe': 1, 'pounded': 1, 'broadband': 1, 'installation': 1, 'tensed': 1, 'coughing': 1, 'warned': 1, 'sprint': 1, 'gower': 1, 'morrow': 1, '450p': 1, 'filth': 1, '9yt': 1, 'stop2': 1, 'e14': 1, '08701752560': 1, 'saristar': 1, 'chik': 1, '420': 1, '9061100010': 1, 'mobcudb': 1, '1st4terms': 1, 'wire3': 1, 'sabarish': 1, '09050000460': 1, 'j89': 1, 'box245c2150pm': 1, 'flea': 1, 'inpersonation': 1, 'banneduk': 1, 'maximum': 1, 'highest': 1, '71': 1, 'hari': 1, 'wifes': 1, 'mumtaz': 1, 'facts': 1, 'taj': 1, 'arises': 1, 'known': 1, 'shahjahan': 1, 'lesser': 1, '69101': 1, 'rtf': 1, 'sphosting': 1, 'webadres': 1, 'geting': 1, 'passport': 1, 'independently': 1, 'showed': 1, 'multiply': 1, 'twins': 1, '02085076972': 1, 'strt': 1, 'ltdhelpdesk': 1, 'pesky': 1, 'cyclists': 1, 'uneventful': 1, 'equally': 1, 'nattil': 1, 'adi': 1, 'entey': 1, 'kittum': 1, 'hitman': 1, 'hire': 1, '2309': 1, '09066660100': 1, 'outages': 1, 'conserve': 1, 'cps': 1, 'voted': 1, 'epi': 1, 'bare': 1, 'bhaskar': 1, 'gong': 1, 'kaypoh': 1, 'outdoors': 1, 'basketball': 1, 'interfued': 1, 'listed': 1, 'apology': 1, 'harlem': 1, 'forth': 1, 'hustle': 1, 'fats': 1, 'workout': 1, 'zac': 1, 'hui': 1, 'versus': 1, 'underdtand': 1, 'locaxx': 1, 'muchxxlove': 1, '07090298926': 1, '9307622': 1, 'winds': 1, 'skateboarding': 1, 'thrown': 1, 'bandages': 1, 'html': 1, '1146': 1, 'mfl': 1, 'gbp4': 1, 'hectic': 1, 'dogs': 1, 'doggin': 1, 'wamma': 1, 'virtual': 1, 'apnt': 1, 'pants': 1, 'lanka': 1, 'go2sri': 1, 'gudnyt': 1, 'relationship': 1, 'wherevr': 1, 'merely': 1, 'smacks': 1, 'plum': 1, 'alot': 1, '50s': 1, 'formatting': 1, 'attracts': 1, '8714714': 1, 'promotion': 1, 'vegas': 1, 'lancaster': 1, 'soc': 1, 'advising': 1, 'bsn': 1, 'lobby': 1, 'showered': 1, 'ything': 1, 'lubly': 1, 'vewy': 1, '087147123779am': 1, 'catches': 1, 'domain': 1, 'specify': 1, 'nusstu': 1, 'bari': 1, 'hudgi': 1, 'yorge': 1, 'ertini': 1, 'pataistha': 1, 'hoops': 1, 'hasbro': 1, 'jump': 1, 'ummifying': 1, 'associate': 1, 'uterus': 1, 'rip': 1, 'jacuzzi': 1, 'txtstar': 1, 'uve': 1, '2nights': 1, 'wildest': 1, 'aldrine': 1, 'rtm': 1, 'unhappiness': 1, 'sources': 1, 'events': 1, 'functions': 1, 'irritated': 1, '4wrd': 1, 'colleg': 1, 'necesity': 1, 'wthout': 1, 'witout': 1, 'espe': 1, 'wth': 1, 'takecare': 1, 'univ': 1, 'rajas': 1, 'burrito': 1, 'stitch': 1, 'trouser': 1, '146tf150p': 1, 'cheetos': 1, 'synced': 1, 'shangela': 1, 'passes': 1, '08704439680': 1, 'poo': 1, 'uup': 1, 'gloucesterroad': 1, 'ouch': 1, 'fruit': 1, 'forgiveness': 1, 'glo': 1, '09058095107': 1, 's3xy': 1, 'wlcome': 1, 'timi': 1, 'strtd': 1, 'sack': 1, '1stone': 1, 'throwin': 1, 'fishrman': 1, 'stones': 1, '08717895698': 1, 'mobstorequiz10ppm': 1, 'physics': 1, 'delicious': 1, 'praveesh': 1, 'beers': 1, 'salad': 1, 'whore': 1, 'scallies': 1, 'twinks': 1, 'skins': 1, '08712466669': 1, 'jocks': 1, 'flood': 1, 'beads': 1, 'section': 1, 'nitro': 1, 'wishlist': 1, 'sold': 1, 'creative': 1, 'reffering': 1, 'getiing': 1, 'weirdy': 1, 'brownies': 1, '12hours': 1, 'k61': 1, '09061701851': 1, 'restrict': 1, '74355': 1, 'greece': 1, 'recorded': 1, 'someday': 1, 'grandfather': 1, 'november': 1, '75max': 1, 'blu': 1, '09061104276': 1, 'yuou': 1, 'spot': 1, 'lotto': 1, 'bunch': 1, 'purchases': 1, 'authorise': 1, '45pm': 1, 'goss': 1, 'gimmi': 1, 'ystrday': 1, 'chile': 1, 'subletting': 1, 'steering': 1, 'ammae': 1, 'sleeps': 1, 'required': 1, 'rounder': 1, 'batchlor': 1, 'lambu': 1, 'ji': 1, 'zoom': 1, '08717890890': 1, 'stopcs': 1, '62220cncl': 1, '0430': 1, 'true18': 1, '37819': 1, '1b6a5ecef91ff9': 1, 'jul': 1, 'chg': 1, 'cst': 1, 'xafter': 1, 'pure': 1, 'hearted': 1, 'enemies': 1, 'smiley': 1, 'yaxxx': 1, 'gail': 1, 'theoretically': 1, 'hooked': 1, 'formally': 1, 'multimedia': 1, 'housing': 1, 'accounting': 1, 'agency': 1, 'delayed': 1, 'renting': 1, 'vague': 1, 'presents': 1, 'nicky': 1, 'gumby': 1, 'wave': 1, '44345': 1, 'alto18': 1, 'sized': 1, 'springs': 1, 'tarpon': 1, 'steps': 1, 'cab': 1, 'limited': 1, 'hf8': 1, '08719181259': 1, 'radiator': 1, 'tongued': 1, 'proper': 1, 'shorts': 1, 'qi': 1, 'suddenly': 1, 'flurries': 1, 'webeburnin': 1, 'babygoodbye': 1, 'golddigger': 1, 'dontcha': 1, 'pushbutton': 1, 'real1': 1, 'perform': 1, 'cards': 1, 'rebooting': 1, 'nigh': 1, 'cable': 1, 'outage': 1, 'nooooooo': 1, 'sos': 1, 'playin': 1, 'guoyang': 1, 'rahul': 1, 'dengra': 1, 'fieldof': 1, 'selfindependence': 1, 'contention': 1, 'antelope': 1, 'toplay': 1, 'gnarls': 1, 'barkleys': 1, 'borderline': 1, '545': 1, 'nightnight': 1, 'possibility': 1, 'grooved': 1, 'mising': 1, '6669': 1, 'unsecured': 1, '195': 1, 'secured': 1, 'fakeye': 1, 'eckankar': 1, 'lanre': 1, '3000': 1, 'heater': 1, 'degrees': 1, 'dodgey': 1, 'asssssholeeee': 1, 'seing': 1, 'dreamz': 1, 'blokes': 1, 'rebel': 1, 'buddy': 1, 'ceri': 1, '84484': 1, 'nationwide': 1, 'newport': 1, 'juliana': 1, 'nachos': 1, 'dizzamn': 1, 'suitemates': 1, 'nimbomsons': 1, 'continent': 1, '087104711148': 1, 'emerging': 1, 'fiend': 1, 'impede': 1, 'hesitant': 1, '400thousad': 1, '60': 1, 'essay': 1, 'nose': 1, 'tram': 1, 'vic': 1, 'coherently': 1, 'echo': 1, 'triple': 1, 'cusoon': 1, 'afew': 1, 'honi': 1, 'onlyfound': 1, 'gran': 1, 'bx526': 1, 'southern': 1, 'rayan': 1, 'macleran': 1, 'balls': 1, 'olave': 1, 'mandara': 1, 'trishul': 1, 'woo': 1, 'hoo': 1, 'panties': 1, 'thout': 1, 'pints': 1, 'flatter': 1, 'carlin': 1, 'ciao': 1, 'starve': 1, 'impression': 1, 'darkness': 1, 'motivate': 1, 'wknd': 1, 'heltini': 1, 'trusting': 1, 'uttered': 1, 'yalrigu': 1, 'iyo': 1, 'noice': 1, 'esaplanade': 1, '139': 1, 'accessible': 1, '08709501522': 1, 'la3': 1, '2wu': 1, 'occurs': 1, 'enna': 1, 'kalaachutaarama': 1, 'prof': 1, 'coco': 1, 'sporadically': 1, 'pobox75ldns7': 1, '09064017305': 1, '38': 1, 'persolvo': 1, 'tbs': 1, 'manchester': 1, 'kath': 1, 'burden': 1, 'noworriesloans': 1, '08717111821': 1, 'harder': 1, 'nbme': 1, 'sickness': 1, 'villa': 1, 'sathya': 1, 'gam': 1, 'smash': 1, 'religiously': 1, 'tips': 1, 'heroes': 1, '08715203649': 1, '07973788240': 1, 'penny': 1, 'muhommad': 1, 'fiting': 1, 'load': 1, 'mj': 1, 'unconvinced': 1, 'willpower': 1, 'elaborate': 1, 'absence': 1, 'answerin': 1, 'evey': 1, 'prin': 1, '08714342399': 1, 'gigolo': 1, 'spam': 1, 'mens': 1, '50rcvd': 1, 'gsoh': 1, 'oncall': 1, 'mjzgroup': 1, 'ashwini': 1, '08707500020': 1, '09061790125': 1, 'ukp': 1, 'skinny': 1, 'casting': 1, 'thet': 1, 'elections': 1, '116': 1, 'serena': 1, 'amrca': 1, 'hlday': 1, 'camp': 1, 'prescribed': 1, 'meatballs': 1, 'approve': 1, 'panalam': 1, 'spjanuary': 1, 'fortune': 1, 'allday': 1, 'perf': 1, 'outsider': 1, '98321561': 1, 'familiar': 1, 'depression': 1, 'infact': 1, 'band': 1, 'simpsons': 1, 'kip': 1, 'shite': 1, 'hont': 1, 'upgrading': 1, 'amanda': 1, 'renewing': 1, 'subject': 1, 'regard': 1, 'nannys': 1, 'perspective': 1, 'puts': 1, 'conveying': 1, 'debating': 1, 'wtlp': 1, 'jb': 1, 'florida': 1, 'hidden': 1, 'teams': 1, 'swhrt': 1, 'po19': 1, '2ez': 1, '47': 1, '0906346330': 1, 'general': 1, 'jetton': 1, 'cmon': 1, 'replies': 1, 'lunsford': 1, 'enjoying': 1, '0796xxxxxx': 1, 'prizeawaiting': 1, 'gravy': 1, 'meals': 1, 'kfc': 1, '07008009200': 1, 'attended': 1, 'mw': 1, 'tuth': 1, 'eviction': 1, 'michael': 1, 'spiral': 1, 'riddance': 1, 'suffers': 1, 'cricket': 1, 'edward': 1, 'closeby': 1, 'raglan': 1, 'skye': 1, 'bookedthe': 1, 'hut': 1, 'drastic': 1, '3750': 1, 'garments': 1, 'sez': 1, 'evry1': 1, 'arab': 1, 'eshxxxxxxxxxxx': 1, 'lay': 1, 'bimbo': 1, 'ugo': 1, '3lions': 1, 'portege': 1, 'm100': 1, 'semiobscure': 1, 'gprs': 1, 'loosu': 1, 'careless': 1, 'freaking': 1, 'myspace': 1, 'logged': 1, 'method': 1, 'blur': 1, 'jewelry': 1, 'deluxe': 1, 'bbdeluxe': 1, 'features': 1, 'breaker': 1, 'graphics': 1, 'fumbling': 1, 'weekdays': 1, 'nails': 1, 'asia': 1, 'tobed': 1, '430': 1, 'stil': 1, 'asthma': 1, 'attack': 1, 'ball': 1, 'spin': 1, 'million': 1, 'haiyoh': 1, 'saves': 1, 'prsn': 1, 'sunlight': 1, 'relocate': 1, 'audiitions': 1, 'pocked': 1, 'motivating': 1, 'brison': 1, 'caps': 1, 'bullshit': 1, 'spelled': 1, 'motherfucker': 1, 'ig11': 1, '1013': 1, 'kit': 1, 'oja': 1, 'strip': 1, '08712402578': 1, 'thesmszone': 1, 'masked': 1, 'anonymous': 1, 'abuse': 1, 'parish': 1, 'magazine': 1, 'woodland': 1, 'avenue': 1, 'billy': 1, 'awww': 1, 'useless': 1, 'loo': 1, 'ed': 1, 'glands': 1, 'swollen': 1, 'bcaz': 1, 'truble': 1, 'evone': 1, 'hates': 1, 'stu': 1, 'view': 1, 'gays': 1, 'dual': 1, 'hostile': 1, 'breezy': 1, 'haircut': 1, '1tulsi': 1, 'leaf': 1, 'diseases': 1, 'litres': 1, 'watr': 1, 'problms': 1, '1apple': 1, '1lemon': 1, 'snd': 1, '1cup': 1, 'lavender': 1, 'manky': 1, 'inmind': 1, 'travelling': 1, 'scouse': 1, 'recreation': 1, 'judgemental': 1, 'fridays': 1, 'waheeda': 1, 'notes': 1, 'bot': 1, 'eventually': 1, 'hits': 1, 'tolerance': 1, '0789xxxxxxx': 1, 'hellogorgeous': 1, 'nitw': 1, 'texd': 1, 'jaz': 1, '4ward': 1, 'hopeu': 1, '09058091870': 1, 'exorcism': 1, 'emily': 1, 'prayrs': 1, 'othrwise': 1, 'evry': 1, 'emotion': 1, 'dsn': 1, 'sandiago': 1, 'parantella': 1, 'ujhhhhhhh': 1, 'hugging': 1, 'mango': 1, 'sweater': 1, 'involved': 1, 'bob': 1, 'barry': 1, '83738': 1, 'landmark': 1, 'consent': 1, 'clubzed': 1, 'tonexs': 1, 'renewed': 1, 'billing': 1, 'mathews': 1, 'anderson': 1, 'edwards': 1, 'tait': 1, 'promoting': 1, 'haunt': 1, 'crowd': 1, '8000930705': 1, 'snowboarding': 1, 'christmassy': 1, 'recpt': 1, 'baaaaaaaabe': 1, 'ignoring': 1, 'zealand': 1, 'education': 1, 'academic': 1, 'completes': 1, 'sagamu': 1, 'vital': 1, 'lautech': 1, 'shola': 1, 'qet': 1, 'browser': 1, 'surf': 1, 'subscribers': 1, 'conversations': 1, 'overemphasise': 1, 'senses': 1, 'convinced': 1, 'adp': 1, 'headset': 1, 'internal': 1, 'extract': 1, 'immed': 1, 'bevies': 1, 'waz': 1, 'fancied': 1, 'spoon': 1, 'comfey': 1, 'watchng': 1, 'othrs': 1, 'skint': 1, 'least5times': 1, 'quitting': 1, 'wudn': 1, 'frequently': 1, 'cupboard': 1, 'route': 1, '2mro': 1, 'floppy': 1, 'snappy': 1, 'risk': 1, 'grasp': 1, 'flavour': 1, 'laready': 1, 'denying': 1, 'dom': 1, 'ffffuuuuuuu': 1, 'julianaland': 1, 'oblivious': 1, 'dehydrated': 1, 'dogwood': 1, 'tiny': 1, 'mapquest': 1, 'archive': 1, '08719839835': 1, 'mgs': 1, '89123': 1, 'behalf': 1, 'lengths': 1, 'stunning': 1, 'visa': 1, 'gucci': 1, 'talkbut': 1, 'culdnt': 1, 'wenwecan': 1, 'sozi': 1, 'wannatell': 1, 'smsing': 1, 'efficient': 1, '15pm': 1, 'thandiyachu': 1, 'erutupalam': 1, 'invention': 1, 'lyrics': 1, 'somone': 1, 'nevr': 1, 'undrstnd': 1, 'definitly': 1, 'unrecognized': 1, 'valuing': 1, 'toking': 1, 'ger': 1, 'syd': 1, 'dhorte': 1, 'lage': 1, 'kintu': 1, 'opponenter': 1, 'khelate': 1, 'spares': 1, 'looovvve': 1, 'fried': 1, 'warwick': 1, 'tmw': 1, 'canceled': 1, 'havn': 1, 'tops': 1, 'grandma': 1, 'parade': 1, 'norcorp': 1, 'proze': 1, 'posting': 1, '7cfca1a': 1, 'grumble': 1, 'algebra': 1, 'linear': 1, 'decorating': 1, '946': 1, 'wining': 1, 'roomate': 1, 'graduated': 1, 'adjustable': 1, 'allows': 1, 'cooperative': 1, 'nottingham': 1, '40mph': 1, '63miles': 1, 'thanku': 1, 'guessed': 1, '50ea': 1, 'la1': 1, 'strings': 1, '7ws': 1, '89938': 1, 'otbox': 1, '731': 1, 'beside': 1, 'walks': 1, 'brisk': 1, 'dirtiest': 1, 'sexiest': 1, '89070': 1, 'tellmiss': 1, 'contribute': 1, 'greatly': 1, 'duvet': 1, 'smells': 1, 'urgh': 1, 'predictive': 1, 'coach': 1, 'w8in': 1, '4utxt': 1, '24th': 1, 'pist': 1, 'beverage': 1, 'surrender': 1, 'symptoms': 1, 'rdy': 1, 'backwards': 1, 'abstract': 1, 'avin': 1, 'africa': 1, 'chit': 1, '4217': 1, 'logon': 1, '6zf': 1, 'w1a': 1, '118p': 1, '8883': 1, 'cu': 1, 'quiteamuzing': 1, '4brekkie': 1, 'scool': 1, 'lrg': 1, 'psxtra': 1, 'satthen': 1, 'probpop': 1, 'portions': 1, '1000call': 1, '09071512432': 1, '300603t': 1, 'callcost150ppmmobilesvary': 1, 'rows': 1, 'njan': 1, 'fixd': 1, 'sudn': 1, 'engagement': 1, 'vilikkam': 1, 'maths': 1, 'chapter': 1, 'chop': 1, 'noooooooo': 1, '08718727870150ppm': 1, 'firsg': 1, 'split': 1, 'wasnt': 1, 'applyed': 1, 'heat': 1, 'sumfing': 1, 'ithink': 1, 'amnow': 1, 'bedreal': 1, 'layin': 1, 'tonsolitusaswell': 1, 'hopeso': 1, 'lotsof': 1, 'hiphop': 1, 'oxygen': 1, 'resort': 1, 'roller': 1, 'australia': 1, 'recorder': 1, 'canname': 1, 'mquiz': 1, 'showr': 1, 'upon': 1, 'ceiling': 1, 'ennal': 1, 'prakasam': 1, 'bcz': 1, 'prakasamanu': 1, 'presnts': 1, 'sneham': 1, 'mns': 1, 'jeevithathile': 1, 'neekunna': 1, 'irulinae': 1, 'mis': 1, 'blowing': 1, '7634': 1, '7684': 1, 'firmware': 1, 'vijaykanth': 1, 'anythiing': 1, 'clubmoby': 1, '08717509990': 1, 'ripped': 1, 'keypad': 1, 'btwn': 1, 'expects': 1, 'decades': 1, 'goverment': 1, 'spice': 1, 'ettans': 1, 'prasanth': 1, '08718738002': 1, '48922': 1, 'fizz': 1, 'contains': 1, 'appy': 1, 'genus': 1, 'robinson': 1, 'outs': 1, 'soz': 1, 'mums': 1, 'imat': 1, 'freinds': 1, 'sometext': 1, '9280114': 1, '07099833605': 1, 'chloe': 1, '130': 1, 'wewa': 1, 'iriver': 1, '255': 1, '128': 1, 'bw': 1, 'surly': 1, '9758': 1, '07808726822': 1, 'snuggles': 1, 'contented': 1, 'whispers': 1, 'mmmmmmm': 1, 'healthy': 1, '2bold': 1, 'barrel': 1, 'scraped': 1, 'misfits': 1, 'clearer': 1, 'sections': 1, 'peach': 1, 'tasts': 1, 'termsapply': 1, 'golf': 1, 'rayman': 1, 'activ8': 1, 'shindig': 1, 'phonebook': 1, 'ashes': 1, 'rocking': 1, 'shijutta': 1, 'offense': 1, 'dvg': 1, 'vinobanagar': 1, 'ovulate': 1, '3wks': 1, 'realising': 1, 'woah': 1, 'orh': 1, 'n8': 1, 'secrets': 1, 'hides': 1, 'akon': 1, 'axel': 1, 'eyed': 1, 'cashbin': 1, 'canteen': 1, 'stressfull': 1, 'adds': 1, 'continued': 1, 'president': 1, '180': 1, '140': 1, 'pleasured': 1, 'providing': 1, 'assistance': 1, 'whens': 1, '1172': 1, 'memories': 1, 'lonlines': 1, 'built': 1, 'lotz': 1, 'gailxx': 1, 'complacent': 1, 'denis': 1, 'mina': 1, 'miwa': 1, '09066649731from': 1, 'opposite': 1, 'heavily': 1, 'swayze': 1, 'patrick': 1, 'dolls': 1, '09077818151': 1, '30s': 1, 'santacalling': 1, 'calls1': 1, '50ppm': 1, 'quarter': 1, 'fired': 1, 'limping': 1, 'aa': 1, '08719180219': 1, '078498': 1, 'oga': 1, 'punishment': 1, 'poorly': 1, 'brb': 1, 'kill': 1, 'predicte': 1, 'situations': 1, 'loosing': 1, 'capacity': 1, 'smaller': 1, 'fgkslpo': 1, 'videos': 1, 'netun': 1, 'shsex': 1, 'fgkslpopw': 1, '0871277810710p': 1, 'defer': 1, 'admission': 1, 'checkmate': 1, 'maat': 1, 'shah': 1, 'chess': 1, 'persian': 1, 'phrase': 1, 'rats': 1, 'themes': 1, 'photoshop': 1, 'manageable': 1, '08715203652': 1, '42810': 1, 'increase': 1, 'carolina': 1, 'north': 1, 'texas': 1, 'gre': 1, 'bomb': 1, 'breathing': 1, 'powerful': 1, 'weapon': 1, 'lovly': 1, 'customercare': 1, 'msgrcvd': 1, 'clas': 1, 'lit': 1, 'couch': 1, 'loooooool': 1, 'swashbuckling': 1, 'terror': 1, 'cruel': 1, 'decent': 1, 'joker': 1, 'dip': 1, 'gek1510': 1, 'nuther': 1, 'lyricalladie': 1, 'hmmross': 1, '910': 1, 'differences': 1, 'happiest': 1, 'characters': 1, 'lists': 1, 'infections': 1, 'antibiotic': 1, 'gynae': 1, 'abdomen': 1, '6times': 1, 'exposed': 1, 'device': 1, 'beatings': 1, 'chastity': 1, 'uses': 1, 'wrenching': 1, 'gut': 1, 'tallahassee': 1, 'ou': 1, 'taka': 1, 'nr31': 1, '450pw': 1, 'pobox202': 1, '7zs': 1, 'ritten': 1, 'fold': 1, 'colin': 1, 'kiosk': 1, 'swat': 1, 'mre': 1, 'farrell': 1, '83118': 1, 'solihull': 1, 'terminated': 1, 'nhs': 1, '2b': 1, 'inconvenience': 1, 'dentists': 1, 'margin': 1, 'bergkamp': 1, 'yards': 1, 'henry': 1, '78': 1, 'parent': 1, 'unintentional': 1, 'snot': 1, 'nonetheless': 1, 'knees': 1, 'toaday': 1, 'grazed': 1, 'splat': 1, 'hooch': 1, 'deny': 1, 'hearin': 1, 'yah': 1, 'torture': 1, 'hopeing': 1, 'sisters': 1, 'sexychat': 1, 'lips': 1, 'congratulation': 1, 'court': 1, 'frontierville': 1, 'chapel': 1, 'mountain': 1, 'deer': 1, 'varma': 1, 'mailed': 1, 'secure': 1, 'parties': 1, 'farting': 1, 'ortxt': 1, 'dialling': 1, '402': 1, 'advisors': 1, 'trained': 1, 'stuffing': 1, 'woot': 1, 'ahhhh': 1, 'dining': 1, 'vouch4me': 1, 'etlp': 1, 'experiencehttp': 1, 'kaila': 1, '09058094507': 1, 'tsunami': 1, 'disaster': 1, 'donate': 1, 'unicef': 1, 'asian': 1, 'fund': 1, '864233': 1, 'cme': 1, 'hos': 1, 'collapsed': 1, 'cumming': 1, 'jade': 1, 'paul': 1, 'barmed': 1, 'thinkthis': 1, 'dangerous': 1, '762': 1, 'goldviking': 1, 'rushing': 1, 'coulda': 1, 'phony': 1, 'okday': 1, 'petexxx': 1, 'fromwrk': 1, 'adrink': 1, 'bthere': 1, 'buz': 1, 'outsomewhere': 1, 'wedlunch': 1, '2watershd': 1, 'hmph': 1, 'baller': 1, 'punto': 1, 'travelled': 1, 'ayo': 1, '125': 1, 'freeentry': 1, 'xt': 1, 'toyota': 1, 'olayiwola': 1, 'landing': 1, 'camry': 1, 'mileage': 1, 'clover': 1, 'amma': 1, 'achan': 1, 'rencontre': 1, 'mountains': 1, '08714712412': 1, 'puppy': 1, 'noise': 1, 'meg': 1, '08715203685': 1, '4xx26': 1, 'crossing': 1, '09094646631': 1, 'deepest': 1, 'darkest': 1, 'inconvenient': 1, 'adsense': 1, 'approved': 1, 'dudette': 1, 'perumbavoor': 1, 'stage': 1, 'clarify': 1, 'preponed': 1, 'natalie2k9': 1, '165': 1, 'natalie': 1, 'younger': 1, '08701213186': 1, 'liver': 1, 'opener': 1, 'guides': 1, 'watched': 1, 'loneliness': 1, 'skyving': 1, 'onwords': 1, 'mtnl': 1, 'mumbai': 1, 'mustprovide': 1, '62735': 1, '83039': 1, 'accommodationvouchers': 1, '450': 1, '15541': 1, 'rajitha': 1, 'ranju': 1, 'styles': 1, '1winawk': 1, 'tscs08714740323': 1, '50perweeksub': 1, '09066361921': 1, 'disagreeable': 1, 'afterwards': 1, 'vivekanand': 1, 'uawake': 1, 'deviousbitch': 1, 'aletter': 1, 'thatmum': 1, '4thnov': 1, 'gotmarried': 1, 'fuckinnice': 1, 'ourbacks': 1, 'justfound': 1, 'feellikw': 1, 'election': 1, 'rearrange': 1, 'eleven': 1, 'dormitory': 1, 'hitler': 1, 'starer': 1, 'recount': 1, 'astronomer': 1, 'worms': 1, 'suffering': 1, 'dysentry': 1, 'virgil': 1, 'andre': 1, 'gokila': 1, 'uncut': 1, 'dino': 1, 'diamond': 1, 'shanil': 1, 'exchanged': 1, 'kotees': 1, 'zebra': 1, 'sugababes': 1, 'panther': 1, 'badass': 1, 'hoody': 1, 'resent': 1, 'queries': 1, 'customersqueries': 1, 'netvision': 1, 'haughaighgtujhyguj': 1, 'andres': 1, 'hassling': 1, 'londn': 1, 'fassyole': 1, 'blacko': 1, 'responsibilities': 1, '08715205273': 1, 'vco': 1, 'humanities': 1, 'reassurance': 1, 'albi': 1, 'mahfuuz': 1, 'beeen': 1, 'tohar': 1, 'aslamalaikkum': 1, 'mufti': 1, 'muht': 1, '078': 1, 'tocall': 1, 'enufcredeit': 1, 'ileave': 1, 'treats': 1, 'okors': 1, 'ibored': 1, 'adding': 1, 'savings': 1, 'zeros': 1, 'goigng': 1, 'perfume': 1, 'sday': 1, 'grocers': 1, 'pubs': 1, 'bennys': 1, 'frankie': 1, 'owed': 1, 'diapers': 1, 'changing': 1, 'unlike': 1, 'turkeys': 1, 'patients': 1, 'princes': 1, 'helens': 1, 'unintentionally': 1, 'wenever': 1, 'stability': 1, 'vibrant': 1, 'colourful': 1, 'tranquility': 1, 'failing': 1, 'failure': 1, 'bawling': 1, 'velusamy': 1, 'facilities': 1, 'karnan': 1, 'bluray': 1, 'salt': 1, 'wounds': 1, 'logging': 1, 'geoenvironmental': 1, 'implications': 1, 'fuuuuck': 1, 'salmon': 1, 'uploaded': 1, 'wrkin': 1, 'ree': 1, 'compensation': 1, 'awkward': 1, 'splash': 1, 'musta': 1, 'leg': 1, 'overdid': 1, 'telediscount': 1, 'foned': 1, 'chuck': 1, 'port': 1, 'stuffs': 1, 'juswoke': 1, 'docks': 1, 'spinout': 1, 'boatin': 1, '08715203656': 1, '42049': 1, 'uworld': 1, 'assessment': 1, 'qbank': 1, 'someonone': 1, '09064015307': 1, 'tke': 1, 'temales': 1, 'finishd': 1, 'dull': 1, 'studies': 1, 'anyones': 1, 'craigslist': 1, 'treadmill': 1, 'absolutely': 1, 'swan': 1, 'hehe': 1, 'shexy': 1, 'sall': 1, 'lamp': 1, 'foward': 1, '09061790126': 1, 'misundrstud': 1, '2u2': 1, 'genes': 1, 'com1win150ppmx3age16subscription': 1, 'resuming': 1, 'reapply': 1, 'treatin': 1, 'treacle': 1, 'mumhas': 1, 'beendropping': 1, 'theplace': 1, 'adress': 1, 'favorite': 1, 'rumbling': 1, 'sashimi': 1, 'oyster': 1, 'marandratha': 1, 'correctly': 1, 'alaikkum': 1, 'heaven': 1, 'pisces': 1, 'aquarius': 1, '2yrs': 1, 'wicket': 1, 'steyn': 1, 'sterm': 1, 'resolved': 1, 'wheat': 1, 'chex': 1, 'hannaford': 1, 'jam': 1, 'grownup': 1, 'costume': 1, 'jerk': 1, 'stink': 1, 'subsequent': 1, 'follows': 1, 'openings': 1, 'upcharge': 1, 'guai': 1, 'astrology': 1, 'slacking': 1, 'mentor': 1, 'percent': 1, 'erotic': 1, 'ecstacy': 1, '09095350301': 1, 'dept': 1, '08717507382': 1, 'coincidence': 1, 'sane': 1, 'helping': 1, 'pause': 1, '151': 1, 'leading': 1, '8800': 1, 'psp': 1, 'gr8prizes': 1, 'spacebucks': 1, '083': 1, '6089': 1, 'squeezed': 1, 'maintaining': 1, 'dreading': 1, 'thou': 1, 'suggestion': 1, 'forgt': 1, 'lands': 1, 'helps': 1, 'ajith': 1, 'yoville': 1, 'ooooooh': 1, 'asda': 1, 'counts': 1, 'officer': 1, 'carly': 1, 'bffs': 1, '\u3028ud': 1, 'seperated': 1, 'franxx': 1, 'brolly': 1, 'syrup': 1, '5mls': 1, 'feed': 1, 'prometazine': 1, 'singapore': 1, 'shu': 1, 'victoria': 1, 'pocay': 1, '2morrowxxxx': 1, 'wocay': 1, 'ramen': 1, 'broth': 1, 'fowler': 1, 'tats': 1, 'flew': 1, '09058094583': 1, 'attention': 1, 'tix': 1, 'fne': 1, 'youdoing': 1, 'foregate': 1, 'shrub': 1, 'worc': 1, 'get4an18th': 1, '32000': 1, 'efreefone': 1, 'legitimat': 1, 'pendent': 1, 'toilet': 1, 'cops': 1, 'stolen': 1, 'navigate': 1, 'require': 1, 'choosing': 1, 'hu': 1, 'guidance': 1, 'chick': 1, 'boobs': 1, 'revealing': 1, 'org': 1, '2025050': 1, '0121': 1, 'shortbreaks': 1, 'sparkling': 1, 'breaks': 1, '45': 1, 'gyno': 1, 'belong': 1, 'gamb': 1, 'treasure': 1, '820554ad0a1705572711': 1, '09050000332': 1, 'negative': 1, 'positive': 1, 'hmmmm': 1, 'command': 1, 'stressful': 1, 'holby': 1, '09064017295': 1, 'li': 1, 'lecturer': 1, 'repeating': 1, 'motor': 1, 'yeovil': 1, 'rhode': 1, 'bong': 1, 'ofcourse': 1, '08448350055': 1, 'planettalkinstant': 1, '2p': 1, 'spider': 1, 'marvel': 1, 'ultimate': 1, '8ball': 1, '83338': 1, 'tamilnadu': 1, 'tip': 1, '07808247860': 1, '40411': 1, '08719899229': 1, 'identification': 1, 'boundaries': 1, 'limit': 1, 'endless': 1, 'reassuring': 1, 'young': 1, 'referin': 1, 'saibaba': 1, 'colany': 1, 'declare': 1, 'chic': 1, '49557': 1, 'disappointment': 1, 'irritation': 1, 'tantrum': 1, 'compliments': 1, 'adventuring': 1, 'chief': 1, 'gsex': 1, 'wc1n': 1, '2667': 1, '3xx': 1, 'l8er': 1, 'bailiff': 1, 'inclu': 1, '3mobile': 1, 'servs': 1, 'chatlines': 1, 'mouse': 1, 'desk': 1, 'childporn': 1, 'jumpers': 1, 'belt': 1, 'cribbs': 1, 'hat': 1, 'spiritual': 1, 'barring': 1, 'influx': 1, 'sudden': 1, 'kane': 1, 'shud': 1, 'pshew': 1, '4years': 1, 'units': 1, 'accent': 1, 'dental': 1, 'nmde': 1, 'dump': 1, 'heap': 1, 'lowes': 1, 'salesman': 1, '087187272008': 1, 'now1': 1, 'suggestions': 1, 'pity': 1, 'bitching': 1}), 'lowercase': True, 'n': 5574, 'ngram_range': (1, 1), 'normalize': True, 'on': None, 'preprocessor': None, 'processing_steps': [<function strip_accents_unicode at 0x7ff10730dc10>, <method 'lower' of 'str' objects>, <built-in method findall of re.Pattern object at 0x7ff107682440>], 'strip_accents': True, 'tokenizer': <built-in method findall of re.Pattern object at 0x7ff107682440>} RandomUnderSampler(BernoulliNB) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': True, '_rng': RandomState(MT19937) at 0x7FF107657940, 'classifier': BernoulliNB ( alpha=0 true_threshold=0. ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } Now let's try to use logistic regression to classify messages. We will use different tips to make my model perform better. As in the previous example, we rebalance the classes of our dataset. The logistics regression will be fed from a TF-IDF. from river import linear_model from river import optim from river import preprocessing X_y = datasets . SMSSpam () model = ( extract_body | feature_extraction . TFIDF () | preprocessing . Normalizer () | imblearn . RandomUnderSampler ( classifier = linear_model . LogisticRegression ( optimizer = optim . SGD ( .9 ), loss = optim . losses . Log () ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.946039 The confusion matrix: cm False True False 4655 172 True 54 693 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] TFIDF {'dfs': Counter({'to': 1687, 'you': 1591, 'the': 1035, 'in': 810, 'and': 795, 'is': 752, 'me': 690, 'for': 624, 'it': 614, 'my': 613, 'your': 587, 'call': 551, 'of': 550, 'have': 531, 'that': 512, 'on': 488, 'now': 481, 'are': 445, 'so': 433, 'can': 425, 'but': 422, 'not': 418, 'or': 398, 'at': 378, 'get': 368, 'just': 365, 'do': 359, 'be': 355, 'will': 354, 'with': 350, 'if': 350, 'we': 350, 'no': 343, 'this': 319, 'ur': 310, 'up': 297, 'how': 293, 'ok': 280, 'what': 279, 'from': 273, 'when': 272, 'go': 264, 'out': 262, 'all': 261, 'll': 258, 'know': 247, 'gt': 242, 'lt': 242, 'good': 233, 'then': 232, 'got': 229, 'free': 229, 'like': 229, 'come': 217, 'there': 214, 'am': 214, 'day': 212, 'only': 211, 'its': 210, 'time': 210, 'was': 196, 'send': 195, 'want': 188, 'love': 178, 'text': 174, 'he': 171, 'going': 167, 'txt': 165, 'home': 165, 'one': 163, 'by': 162, 'need': 162, 'today': 159, 'see': 158, 'as': 156, 'still': 155, 'about': 151, 'sorry': 151, 'don': 148, 'back': 147, 'lor': 145, 'our': 143, 'take': 140, 'dont': 139, 'stop': 138, 'reply': 135, 'da': 134, 'tell': 134, 'new': 133, 'later': 132, 'please': 131, 'any': 131, 'think': 129, 'mobile': 128, 'been': 126, 'phone': 124, 'here': 122, 'hi': 120, 'some': 119, 'did': 119, 'well': 119, 'she': 117, 'they': 116, 're': 114, 'where': 114, 'hope': 112, 'hey': 111, 'dear': 111, 'week': 110, 'oh': 110, 'pls': 110, 'has': 109, 'much': 109, 'night': 109, 'great': 108, 'claim': 108, 'an': 108, 'too': 107, 'msg': 106, 'more': 103, 'yes': 103, 'wat': 102, 'had': 101, 'him': 101, 'www': 100, 'her': 100, 'make': 99, 'work': 98, 'give': 98, 'way': 97, 've': 95, 'won': 95, 'who': 95, 'message': 93, 'number': 92, 'tomorrow': 90, 'after': 90, 'say': 89, 'already': 89, 'should': 89, 'doing': 88, 'right': 86, 'yeah': 86, 'happy': 86, 'prize': 84, 'why': 84, 'really': 83, 'ask': 83, 'find': 81, 'meet': 80, 'said': 80, 'cash': 78, 'im': 78, 'last': 78, 'let': 77, 'morning': 76, 'babe': 76, 'them': 76, 'lol': 74, 'thanks': 74, 'miss': 73, 'cos': 73, 'care': 73, 'anything': 72, '150p': 71, 'amp': 71, 'uk': 71, 'pick': 71, 'also': 71, 'win': 70, 'very': 70, 'urgent': 69, 'com': 69, 'would': 69, 'sure': 68, 'something': 68, 'contact': 68, 'over': 67, 'life': 67, 'sent': 67, 'again': 66, 'keep': 66, 'wait': 65, 'every': 65, 'his': 64, 'cant': 64, 'first': 62, 'us': 62, 'buy': 62, 'gud': 62, 'before': 62, 'thing': 62, 'even': 61, 'min': 61, 'soon': 60, 'next': 60, 'place': 60, 'off': 60, '50': 59, 'nice': 59, 'which': 59, 'customer': 58, 'tonight': 58, 'always': 58, 'service': 58, 'around': 57, 'per': 57, 'were': 57, 'late': 57, 'someone': 57, 'gonna': 56, 'feel': 56, 'could': 56, 'money': 56, 'help': 55, 'down': 55, 'sms': 55, 'sleep': 55, 'leave': 55, 'co': 54, '16': 54, 'other': 54, 'many': 54, 'wan': 54, 'nokia': 53, 'went': 53, 'told': 53, 'friends': 52, 'try': 52, 'waiting': 52, 'dun': 51, '18': 51, 'chat': 51, 'friend': 51, 'may': 50, 'fine': 50, 'guaranteed': 50, 'ya': 50, 'coming': 50, 'getting': 49, 'done': 49, 'special': 49, 'yet': 49, 'people': 49, 'haha': 49, 'use': 48, 'year': 48, 'same': 48, 'mins': 48, 'wish': 48, 'didn': 47, 'things': 47, 'holiday': 47, 'thk': 47, 'name': 46, 'man': 46, 'best': 46, 'thought': 46, 'talk': 45, 'bit': 45, 'hello': 44, 'draw': 44, 'few': 44, '500': 44, 'person': 44, 'cs': 44, 'stuff': 43, 'yup': 43, 'trying': 43, 'meeting': 43, 'thats': 43, 'job': 43, 'line': 43, 'heart': 43, 'being': 42, 'class': 42, 'never': 42, 'cool': 42, 'long': 42, 'better': 42, 'ill': 42, 'having': 42, 'days': 42, 'tone': 42, 'cost': 41, '100': 41, 'car': 41, 'live': 41, '1000': 41, 'house': 41, 'ready': 41, 'mind': 41, 'finish': 40, 'enjoy': 40, 'lunch': 39, 'half': 39, 'play': 39, 'check': 39, '10': 39, 'real': 39, 'lot': 39, 'dat': 39, 'chance': 39, 'god': 39, 'word': 38, 'awarded': 38, 'wanna': 38, 'box': 38, 'nothing': 38, 'guess': 38, 'sir': 38, 'lar': 37, 'latest': 37, 'end': 37, 'another': 37, 'liao': 37, 'guys': 37, 'than': 37, 'dinner': 36, 'month': 36, 'sweet': 36, 'ah': 36, 'shows': 36, 'big': 36, 'into': 36, 'shit': 36, '1st': 36, 'world': 35, 'xxx': 35, 'eat': 35, 'po': 35, 'account': 35, 'bt': 35, 'might': 35, 'problem': 35, 'quite': 35, 'receive': 34, 'camera': 34, 'watching': 34, 'smile': 34, '150ppm': 34, 'landline': 34, 'because': 34, 'start': 34, 'speak': 33, 'wont': 33, 'room': 33, 'yo': 33, 'wk': 33, 'aight': 33, 'luv': 33, 'tv': 33, 'offer': 33, 'called': 33, 'two': 33, 'probably': 33, 'rate': 32, 'apply': 32, 'remember': 32, 'left': 32, 'weekend': 32, 'once': 32, 'forgot': 32, 'jus': 32, 'watch': 32, 'plan': 32, 'actually': 32, 'bad': 32, 'princess': 32, 'early': 31, 'code': 31, 'does': 31, 'look': 31, 'maybe': 31, 'hear': 31, 'between': 31, 'easy': 31, 'reach': 31, 'thanx': 31, 'video': 31, 'shopping': 31, 'shall': 31, 'dunno': 31, 'minutes': 31, 'office': 31, 'fun': 30, '2nd': 30, 'part': 30, 'anyway': 30, 'didnt': 30, 'hour': 30, 'baby': 30, 'ever': 30, 'sat': 30, 'network': 29, 'selected': 29, 'enough': 29, 'thank': 29, 'bus': 29, 'ringtone': 29, 'pa': 29, 'looking': 29, 'bed': 29, 'birthday': 29, 'girl': 29, 'little': 29, 'working': 29, 'leh': 29, 'made': 29, 'orange': 29, 'put': 29, 'dad': 29, 'town': 29, 'pay': 28, 'calls': 28, 'afternoon': 28, 'those': 28, 'evening': 28, 'collect': 28, 'everything': 28, 'asked': 28, 'true': 28, 'texts': 28, 'den': 28, 'while': 28, 'kiss': 28, 'until': 27, 'though': 27, '000': 27, 'since': 27, 'pain': 27, 'dis': 27, 'came': 27, 'okay': 27, 'must': 27, 'join': 27, 'tmr': 27, 'details': 27, 'fuck': 27, 'wif': 26, 'wanted': 26, 'most': 26, 'means': 26, 'says': 26, 'mail': 26, 'able': 26, 'important': 26, 'wake': 26, 'collection': 26, 'goes': 25, 'times': 25, 'mob': 25, 'haven': 25, '5000': 25, 'show': 25, 'price': 25, 'school': 25, '2000': 25, 'sexy': 25, 'til': 25, 'guy': 25, 'away': 25, 'valid': 24, 'alright': 24, 'messages': 24, 'missed': 24, 'saw': 24, 'yesterday': 24, 'wen': 24, 'havent': 24, 'abt': 24, 'else': 24, 'juz': 24, 'years': 24, 'hav': 24, 'weekly': 24, 'wot': 24, 'bring': 24, 'attempt': 24, 'yours': 23, 'run': 23, 'making': 23, 'worry': 23, 'haf': 23, 'coz': 23, 'id': 23, 'oso': 23, '10p': 23, 'music': 23, 'stay': 23, 'bored': 23, 'these': 23, 'wife': 23, 'gift': 23, 'plus': 23, 'lei': 23, 'question': 22, 'colour': 22, 'net': 22, 'words': 22, 'national': 22, 'tried': 22, 'yourself': 22, 'address': 22, 'food': 22, 'top': 22, 'without': 22, 'boy': 22, 'decimal': 22, 'shop': 22, 'nite': 22, 'hot': 22, 'book': 22, 'friendship': 22, 'dude': 22, 'change': 22, 'feeling': 22, 'either': 22, '800': 22, 'online': 22, 'family': 22, 'de': 22, 'entry': 21, 'hours': 21, 'http': 21, 'ard': 21, 'till': 21, 'delivery': 21, 'hair': 21, 'bonus': 21, 'test': 21, 'driving': 21, 'busy': 21, 'todays': 21, 'answer': 21, 'nt': 21, 'xmas': 21, 'both': 21, 'vouchers': 21, 'full': 21, 'plz': 21, 'tones': 21, 'calling': 21, 'tot': 21, 'sae': 21, 'together': 21, 'wants': 21, 'goin': 21, 'sad': 21, 'brother': 20, 'set': 20, 'date': 20, 'trip': 20, 'comes': 20, 'movie': 20, 'mean': 20, 'old': 20, 'points': 20, 'award': 20, 'leaving': 20, 'order': 20, 'believe': 20, 'story': 20, 'sleeping': 20, 'noe': 20, 'happen': 20, 'face': 20, 'wid': 20, 'ring': 20, 'huh': 20, 'sch': 20, 'game': 20, 'makes': 20, 'await': 20, 'pounds': 19, 'news': 19, 'aft': 19, 'doesn': 19, 'tomo': 19, 'congrats': 19, 'took': 19, 'double': 19, 'finished': 19, 'started': 19, 'private': 19, 'gr8': 19, 'minute': 19, 'awesome': 19, 'wil': 19, '750': 19, '86688': 19, 'hurt': 19, 'row': 19, 'pm': 19, 'head': 19, 'eve': 19, 'beautiful': 19, 'thinking': 19, 'mum': 19, 'saying': 19, 'rite': 19, 'available': 18, 'final': 18, 'update': 18, 'tho': 18, 'xx': 18, 'close': 18, 'cause': 18, 'services': 18, 'taking': 18, 'missing': 18, 'touch': 18, 'walk': 18, 'unsubscribe': 18, 'charge': 18, 'lets': 18, 'post': 18, 'drink': 18, '250': 18, 'land': 18, 'gd': 18, '150': 18, 'mine': 18, 'pics': 18, 'pub': 18, 'email': 18, 'drive': 18, 'drop': 18, 'dreams': 18, '11': 17, 'lesson': 17, 'second': 17, 'lucky': 17, 'search': 17, '12hrs': 17, 'statement': 17, 'expires': 17, 'msgs': 17, 'open': 17, 'whats': 17, 'lots': 17, 'everyone': 17, 'carlos': 17, 'worth': 17, 'sis': 17, 'sounds': 17, 'company': 17, 'choose': 17, 'club': 17, 'okie': 17, 'card': 17, 'sister': 17, 'chikku': 17, 'poly': 17, 'dating': 17, 'opt': 17, 'neva': 17, 'anyone': 17, 'loving': 17, 'alone': 17, 'treat': 16, 'winner': 16, 'info': 16, 'pobox': 16, 'saturday': 16, 'decided': 16, 'forget': 16, '08000930705': 16, 'girls': 16, 'smiling': 16, 'prob': 16, 'gone': 16, 'happened': 16, 'identifier': 16, 'type': 16, 'ni8': 16, 'ltd': 16, 'hard': 16, 'each': 16, 'boytoy': 16, 'found': 16, 'college': 16, 'break': 16, 'anytime': 16, 'far': 16, 'games': 16, 'mobileupd8': 16, 'bout': 16, 'kind': 16, 'visit': 16, 'fast': 16, 'voucher': 16, 'sun': 16, '8007': 16, 'hows': 16, 'wonderful': 15, 'smth': 15, 'mom': 15, 'camcorder': 15, 'used': 15, 'hit': 15, 'operator': 15, 'friday': 15, 'quiz': 15, 'player': 15, 'parents': 15, 'frnd': 15, 'finally': 15, 'darlin': 15, 'goodmorning': 15, 'oredi': 15, 'secret': 15, 'congratulations': 15, 'hold': 15, 'takes': 15, 'read': 15, 'suite342': 15, '2lands': 15, '08000839402': 15, 'fucking': 15, 'nope': 15, 'outside': 15, 'pretty': 15, 'sea': 15, 'whatever': 15, 'weeks': 15, 'lovely': 15, 'mates': 15, 'wrong': 15, 'party': 15, 'nyt': 15, 'pic': 15, 'crazy': 14, 'wkly': 14, 'freemsg': 14, 'credit': 14, 'seeing': 14, 'whole': 14, 'frnds': 14, 'isn': 14, 'hmm': 14, 'mu': 14, 'their': 14, 'content': 14, 'fancy': 14, 'log': 14, 'course': 14, 'mrng': 14, 'tc': 14, 'thinks': 14, 'case': 14, 'tel': 14, 'meant': 14, 'fr': 14, 'angry': 14, 'light': 14, 'jay': 14, 'project': 14, 'reason': 14, 'ten': 14, 'welcome': 14, 'cum': 14, 'b4': 14, 'mate': 14, 'least': 14, 'earlier': 14, 'chennai': 14, '30': 14, 'point': 13, 'press': 13, 'valued': 13, 'hungry': 13, 'almost': 13, 'hee': 13, '0800': 13, 'felt': 13, 'invited': 13, '03': 13, 'caller': 13, 'numbers': 13, 'yr': 13, 'tired': 13, 'wit': 13, 'needs': 13, 'hmmm': 13, 'mr': 13, 'smoke': 13, 'balance': 13, 'march': 13, 'side': 13, '87066': 13, 'dnt': 13, 'unlimited': 13, 'fone': 13, 'stupid': 13, 'bslvyl': 13, 'lost': 13, 'reading': 13, 'txts': 13, 'ago': 13, 'currently': 13, 'motorola': 13, 'talking': 13, 'couple': 13, 'phones': 13, 'ass': 13, 'park': 13, 'frm': 13, 'fri': 13, 'offers': 13, 'within': 13, '2003': 13, 'un': 13, 'listen': 13, 'yar': 13, 'knw': 13, 'sex': 13, 'mayb': 13, 'understand': 13, 'knew': 13, 'gas': 13, 'comp': 12, '12': 12, 'mobiles': 12, '20': 12, 'eh': 12, 'confirm': 12, 'telling': 12, 'wow': 12, 'correct': 12, 'pass': 12, 'etc': 12, 'complimentary': 12, 'gotta': 12, 'loads': 12, 'computer': 12, 'mah': 12, 'askd': 12, 'uncle': 12, 'sending': 12, 'direct': 12, 'age': 12, 'hand': 12, 'bank': 12, 'bcoz': 12, 'laptop': 12, 'questions': 12, 'swing': 12, 'ge': 12, 'ends': 12, 'die': 12, '200': 12, 'via': 12, 'met': 12, 'call2optout': 12, 'seen': 12, 'rental': 12, 'india': 12, 'doin': 12, 'lose': 12, 'ipod': 12, '04': 12, 'redeemed': 12, 'through': 12, 'gym': 12, 'happiness': 12, 'snow': 12, 'area': 12, 'sound': 12, 'picking': 12, 'ugh': 12, 'extra': 12, 'heard': 12, 'support': 12, 'surprise': 12, 'information': 12, 'grins': 12, 'luck': 12, 'enter': 12, 'auction': 12, 'difficult': 12, 'wasn': 12, 'std': 11, 'usf': 11, 'sunday': 11, 'eg': 11, 'comin': 11, 'charged': 11, 'abiola': 11, 'crave': 11, 'gets': 11, 'ac': 11, 'move': 11, 'checking': 11, 'cut': 11, 'rply': 11, 'download': 11, 'shower': 11, 'entered': 11, 'match': 11, '350': 11, 'txting': 11, 'lovable': 11, 'wine': 11, 'safe': 11, 'orchard': 11, 'kate': 11, 'rs': 11, 'semester': 11, 'wana': 11, 'somebody': 11, 'rest': 11, 'christmas': 11, 'pete': 11, 'plans': 11, 'small': 11, 'ex': 11, 'w1j6hl': 11, 'hg': 11, 'discount': 11, 'slow': 11, 'yep': 11, 'th': 11, 'supposed': 11, 'asking': 11, 'remove': 11, 'monday': 11, 'simple': 11, 'noon': 11, 'darren': 11, 'ans': 11, 'store': 11, 'wonder': 11, 'sort': 11, 'asap': 11, 'na': 11, 'nobody': 11, 'nah': 10, '900': 10, 'months': 10, 'link': 10, 'ha': 10, 'worried': 10, 'myself': 10, 'knows': 10, 'oops': 10, 'hospital': 10, 'red': 10, 'reached': 10, 'forever': 10, 'song': 10, 'save': 10, 'tickets': 10, 'il': 10, 'representative': 10, 'gave': 10, 'rates': 10, 'del': 10, 'sony': 10, 'pray': 10, 'dream': 10, 'spend': 10, 'muz': 10, 'bath': 10, 'bathe': 10, 'study': 10, 'exam': 10, 'street': 10, 'reveal': 10, 'admirer': 10, 'deep': 10, 'own': 10, 'leaves': 10, 'blue': 10, 'usual': 10, 'somewhere': 10, 'normal': 10, 'merry': 10, 'immediately': 10, 'custcare': 10, 'weed': 10, 'rakhesh': 10, 'moment': 10, 'st': 10, 'woke': 10, 'mm': 10, 'voice': 10, 'ldn': 10, 'booked': 10, 'different': 10, 'terms': 10, 'water': 10, 'sub': 10, '00': 10, 'across': 10, 'warm': 10, 'cheap': 10, 'clean': 10, 'em': 10, 'ts': 10, 'drugs': 10, 'laugh': 10, 'fantastic': 10, 'glad': 10, 'wishing': 10, 'getzed': 10, 'whenever': 10, 'otherwise': 10, 'ntt': 10, 'truth': 10, 'gn': 10, 'convey': 10, 'film': 10, '2nite': 10, 'write': 10, 'fact': 10, 'loved': 10, 'slowly': 10, 'cup': 9, 'copy': 9, 'reward': 9, 'england': 9, 'seriously': 9, 'sick': 9, 'catch': 9, 'decide': 9, 'ice': 9, 'situation': 9, 'short': 9, 'rain': 9, 'coffee': 9, 'men': 9, 'boss': 9, 'specially': 9, 'ending': 9, 'sunshine': 9, 'lazy': 9, 'completely': 9, 'staying': 9, 'doesnt': 9, 'especially': 9, 'studying': 9, 'trust': 9, 'using': 9, 'deal': 9, 'itself': 9, 'dead': 9, 'mrt': 9, 'bill': 9, 'lessons': 9, 'goodnight': 9, 'cd': 9, 'ldew': 9, 'lover': 9, 'disturb': 9, 'credits': 9, 'worries': 9, 'tonite': 9, 'unless': 9, '4u': 9, '2day': 9, '11mths': 9, 'valentines': 9, 'urself': 9, 'bluetooth': 9, 'rock': 9, 'starts': 9, 'kinda': 9, 'loan': 9, 'meh': 9, 'near': 9, 'rent': 9, 'silent': 9, 'less': 9, 'children': 9, 'hoping': 9, 'age16': 9, 'self': 9, 'train': 9, 'forwarded': 9, 'starting': 9, 'paper': 9, 'seems': 9, 'sell': 9, 'eyes': 9, 'possible': 9, 'ones': 9, 'gettin': 9, 'poor': 9, 'tampa': 9, 'user': 9, 'mo': 9, 'against': 9, 'hiya': 9, 'doctor': 9, 'mon': 9, 'john': 9, 'mode': 9, 'others': 9, 'wondering': 9, 'ringtones': 9, 'bb': 9, 'tht': 9, '20p': 9, 'moral': 9, 'excellent': 9, 'father': 9, 'thinkin': 9, 'sitting': 9, 'sofa': 9, 'request': 8, 'entitled': 8, 'anymore': 8, 'promise': 8, 'wap': 8, 'pizza': 8, 'mark': 8, 'cheers': 8, 'quick': 8, 'replying': 8, 'nigeria': 8, 'cinema': 8, 'ip4': 8, '5we': 8, 'stand': 8, 'spent': 8, 'loves': 8, 'hurts': 8, 'trouble': 8, 'planning': 8, 'ave': 8, 'wishes': 8, 'weekends': 8, 'apartment': 8, 'inc': 8, 'paying': 8, '2004': 8, 'buying': 8, 'bak': 8, 'sp': 8, 'dvd': 8, 'dogging': 8, 'swt': 8, 'joy': 8, 'goto': 8, 'freephone': 8, 'joined': 8, 'however': 8, 'ive': 8, 'slept': 8, 'sign': 8, 'kick': 8, 'lemme': 8, 'rose': 8, 'cake': 8, 'fixed': 8, 'rcvd': 8, 'interested': 8, 'round': 8, 'figure': 8, 'reference': 8, 'mistake': 8, 'facebook': 8, 'al': 8, 'yahoo': 8, 'aha': 8, '3030': 8, 'funny': 8, 'giving': 8, 'din': 8, 'thru': 8, 'style': 8, 'opinion': 8, '02': 8, 'savamob': 8, 'member': 8, 'fingers': 8, '50p': 8, 'blood': 8, 'daddy': 8, 'door': 8, 'kids': 8, 'pound': 8, 'ar': 8, 'alex': 8, 'longer': 8, '25p': 8, 'pc': 8, 'xy': 8, 'bedroom': 8, 'king': 8, 'idea': 8, 'add': 8, 'library': 8, 'slave': 8, 'omg': 8, 'no1': 8, 'polys': 8, 'moon': 8, 'training': 8, 'gay': 8, 'sale': 8, '08712460324': 8, 'registered': 8, 'miracle': 8, 'during': 8, 'movies': 8, 'digital': 8, 'black': 8, 'awaiting': 8, 'cancel': 8, 'cute': 8, 'energy': 8, 'complete': 8, 'honey': 8, 'picked': 8, 'vl': 8, 'frens': 8, 'reaching': 8, '0870': 8, 'cover': 8, '06': 8, 'south': 8, 'inside': 8, 'hw': 8, 'wednesday': 8, 'pix': 8, 'mood': 8, 'la': 7, 'bugis': 7, 'cine': 7, 'naughty': 7, 'sucks': 7, 'tea': 7, 'eating': 7, 'learn': 7, 'ahead': 7, 'kept': 7, 'liked': 7, 'bx420': 7, 'joke': 7, 'wun': 7, 'following': 7, 'ta': 7, 'pleasure': 7, '10am': 7, 'password': 7, 'cuz': 7, 'page': 7, 'umma': 7, 'weight': 7, 'bother': 7, 'country': 7, '82277': 7, 'yijue': 7, 'lect': 7, 'persons': 7, 'sometimes': 7, 'become': 7, '62468': 7, 'internet': 7, 'waste': 7, 'hell': 7, 'experience': 7, 'towards': 7, 'bucks': 7, 'past': 7, 'biz': 7, 'appreciate': 7, 'road': 7, 'battery': 7, '25': 7, 'kallis': 7, 'cal': 7, 'showing': 7, 'naked': 7, 'horny': 7, 'quality': 7, 'definitely': 7, 'sense': 7, 'sim': 7, 'loyalty': 7, 'high': 7, 'advance': 7, 'power': 7, 'return': 7, 'access': 7, '08718720201': 7, 'wiv': 7, 'fault': 7, 'maximize': 7, 'cold': 7, 'forward': 7, 'happening': 7, 'lift': 7, 'tough': 7, 'tenerife': 7, 'notice': 7, '8th': 7, 'depends': 7, 'realy': 7, 'mp3': 7, '85023': 7, 'unsub': 7, 'single': 7, 'fat': 7, 'married': 7, 'rather': 7, 'hotel': 7, 'omw': 7, 'hurry': 7, 'workin': 7, 'gee': 7, 'izzit': 7, 'spree': 7, 'present': 7, 'valentine': 7, 'future': 7, 'shuhui': 7, 'weather': 7, 'login': 7, 'tuesday': 7, 'ho': 7, 'awake': 7, 'bold': 7, 'looks': 7, 'dey': 7, 'sit': 7, '7pm': 7, 'holla': 7, 'summer': 7, 'damn': 7, 'space': 7, '36504': 7, 'bag': 7, 'model': 7, 'mother': 7, 'yrs': 7, 'mid': 7, 'midnight': 7, 'january': 7, 'iam': 7, 'photo': 7, 'sk38xh': 7, 'recently': 7, 'feels': 7, 'heavy': 7, 'nxt': 7, '3g': 7, 'o2': 7, 'onto': 7, 'station': 7, 'tuition': 7, 'strong': 7, 'cell': 7, 'dog': 7, 'alrite': 7, 'shd': 7, 'croydon': 7, 'cr9': 7, '5wb': 7, '1327': 7, 'meaning': 7, 'players': 7, 'share': 7, 'lmao': 7, 'except': 7, 'arrive': 7, 'instead': 7, 'holding': 7, 'list': 7, 'thnk': 7, 'excuse': 7, 'costa': 7, 'sol': 7, 'including': 7, 'vikky': 7, 'colleagues': 7, 'tear': 7, 'worse': 7, 'murderer': 7, 'maid': 7, 'murdered': 7, 'happens': 7, 'feb': 7, 'planned': 7, 'joking': 6, 'hl': 6, 'click': 6, 'team': 6, 'texting': 6, 'tyler': 6, 'usually': 6, 'fyi': 6, '150pm': 6, 'review': 6, 'pleased': 6, 'kano': 6, 'simply': 6, 'changed': 6, 'eatin': 6, 'flights': 6, 'directly': 6, 'informed': 6, 'app': 6, 'standard': 6, '08712300220': 6, 'shouldn': 6, 'replied': 6, 'local': 6, 'qatar': 6, 'arrange': 6, 'inviting': 6, 'turns': 6, 'spoke': 6, 'personal': 6, 'nights': 6, 'system': 6, 'partner': 6, 'died': 6, 'website': 6, 'tncs': 6, 'childish': 6, 'handset': 6, 'dint': 6, 'sunny': 6, 'ended': 6, 'anybody': 6, 'imagine': 6, 'babes': 6, 'sport': 6, 'accept': 6, 'kb': 6, 'yoga': 6, 'track': 6, 'ish': 6, 'cc': 6, 'posted': 6, 'air': 6, 'willing': 6, 'body': 6, 'relax': 6, 'pilates': 6, 'putting': 6, 'fullonsms': 6, 'competition': 6, 'aathi': 6, 'wnt': 6, 'vry': 6, 'vary': 6, 'askin': 6, 'group': 6, 'ttyl': 6, 'isnt': 6, 'gives': 6, 'moan': 6, 'fb': 6, 'activate': 6, 'character': 6, 'jst': 6, 'tat': 6, '40gb': 6, 'pin': 6, 'campus': 6, 'lady': 6, 'l8r': 6, 'aiyo': 6, 'barely': 6, 'scream': 6, 'marriage': 6, 'announcement': 6, 'indian': 6, 'ladies': 6, '28': 6, 'imma': 6, 'daily': 6, 'paid': 6, 'vodafone': 6, 'matches': 6, 'holder': 6, 'evng': 6, 'earth': 6, 'under': 6, 'torch': 6, 'aftr': 6, 'exactly': 6, 'yay': 6, 'txtauction': 6, 'yest': 6, 'closed': 6, 'wats': 6, 'couldn': 6, 'pobox84': 6, 'norm150p': 6, 'w45wq': 6, 'hunny': 6, 'boo': 6, 'teasing': 6, 'zed': 6, 'green': 6, 'surely': 6, 'five': 6, 'wed': 6, 'fall': 6, 'sup': 6, 'murder': 6, 'due': 6, 'teach': 6, 'ate': 6, 'wherever': 6, 'medical': 6, 'brand': 6, 'contract': 6, 'kerala': 6, 'asleep': 6, 'loverboy': 6, 'serious': 6, 'april': 6, 'flower': 6, 'process': 6, 'works': 6, 'regards': 6, 'sipix': 6, 'aiyah': 6, 'urawinner': 6, 'gal': 6, 'howz': 6, 'raining': 6, 'thts': 6, 'tour': 6, 'super': 6, 'marry': 6, 'problems': 6, 'fantasies': 6, '08707509020': 6, 'walking': 6, 'cafe': 6, 'bought': 6, '4th': 6, 'nature': 6, 'keeping': 6, 'screaming': 6, '86021': 6, 'london': 6, 'lookin': 6, 'exciting': 6, 'toclaim': 6, 'max10mins': 6, 'pobox334': 6, '09050090044': 6, 'stockport': 6, 'theatre': 6, 'ahmad': 6, 'official': 6, 'armand': 6, 'nimya': 6, 'sed': 6, 'role': 6, 'checked': 6, 'added': 6, 'pussy': 6, 'budget': 6, 'random': 6, 'er': 6, 'hr': 6, 'hrs': 6, 'cancer': 6, 'tariffs': 6, 'meds': 6, 'darling': 5, 'callers': 5, 'callertune': 5, 'searching': 5, 'wet': 5, '87077': 5, 'stock': 5, 'egg': 5, 'subscription': 5, 'roommate': 5, 'hopefully': 5, 'ride': 5, 'respect': 5, 'urgnt': 5, '530': 5, 'truly': 5, 'scared': 5, 'cabin': 5, 'voda': 5, 'quoting': 5, 'ec2a': 5, 'laid': 5, 'locations': 5, 'rooms': 5, 'begin': 5, 'shirt': 5, 'menu': 5, 'hop': 5, 'discuss': 5, 'bye': 5, '9am': 5, 'transaction': 5, 'cannot': 5, 'straight': 5, 'connection': 5, 'sen': 5, 'atm': 5, 'romantic': 5, '2optout': 5, 'flirt': 5, 'sam': 5, 'argument': 5, 'wins': 5, 'fix': 5, 'singles': 5, 'rays': 5, 'bf': 5, '21': 5, 'themob': 5, 'selection': 5, 'aren': 5, 'pongal': 5, 'december': 5, 'ppl': 5, 'cud': 5, 'report': 5, 'surfing': 5, 'num': 5, 'basically': 5, 'allah': 5, 'sonyericsson': 5, 'geeee': 5, 'sighs': 5, 'brings': 5, 'guide': 5, 'intro': 5, 'current': 5, 'pictures': 5, 'none': 5, 'yan': 5, 'jiu': 5, 'logo': 5, 'pobox36504w45wq': 5, 'contacted': 5, 'hostel': 5, 'hv': 5, 'amt': 5, 'respond': 5, 'ticket': 5, 'dollars': 5, 'acc': 5, 'woman': 5, 'flat': 5, 'sec': 5, 'conditions': 5, 'fighting': 5, 'some1': 5, 'spl': 5, 'stylish': 5, '83355': 5, 'returns': 5, 'quote': 5, 'english': 5, 'btw': 5, '2mrw': 5, 'smiles': 5, 'jazz': 5, 'yogasana': 5, '1x150p': 5, 'stopped': 5, 'somethin': 5, 'results': 5, 'euro2004': 5, 'drinks': 5, '80062': 5, 'thursday': 5, 'listening': 5, 'cartoon': 5, 'drug': 5, 'fetch': 5, 'belly': 5, 'lonely': 5, 'law': 5, 'gap': 5, 'timing': 5, 'running': 5, 'mad': 5, 'twice': 5, 'opportunity': 5, 'gals': 5, 'city': 5, 'tis': 5, 'living': 5, 'polyphonic': 5, 'xxxx': 5, 'comuk': 5, 'ages': 5, 'sura': 5, 'playing': 5, 'matter': 5, 'sn': 5, 'hai': 5, 'records': 5, 'cds': 5, 'birds': 5, 'travel': 5, 'lead': 5, 'unsold': 5, 'derek': 5, 'greet': 5, 'white': 5, 'cheaper': 5, 'ym': 5, 'pissed': 5, 'ma': 5, 'wear': 5, 'expensive': 5, 'photos': 5, 'site': 5, 'boring': 5, 'ad': 5, 'salary': 5, 'noline': 5, 'dload': 5, 'videochat': 5, 'videophones': 5, 'rentl': 5, 'java': 5, 'dropped': 5, 'yun': 5, 'jesus': 5, 'gm': 5, '3rd': 5, 'bitch': 5, 'revealed': 5, 'xchat': 5, 'hands': 5, 'receipt': 5, 'interesting': 5, 'uni': 5, 'italian': 5, 'adult': 5, 'oz': 5, 'horrible': 5, 'nw': 5, 'jordan': 5, 'choice': 5, 'mite': 5, 'chinese': 5, 'hun': 5, 'cbe': 5, 'broke': 5, 'original': 5, 'pple': 5, 'arrested': 5, 'linerental': 5, 'vote': 5, 'tells': 5, 'totally': 5, 'rem': 5, 'exams': 5, 'everybody': 5, 'optout': 5, 'google': 5, 'vomit': 5, 'centre': 5, 'airport': 5, 'costs': 5, 'buzz': 5, 'eerie': 5, 'waking': 5, 'ran': 5, '60p': 5, 'boys': 5, 'bin': 5, 'social': 5, 'buns': 5, 'created': 5, 'beer': 5, 'season': 5, 'nvm': 5, 'moms': 5, 'obviously': 5, 'flag': 5, 'eng': 5, 'inclusive': 5, 'looked': 5, 'expecting': 5, 'latr': 5, 'minuts': 5, 'unable': 5, 'remind': 5, 'whether': 5, 'fantasy': 5, 'brilliant': 5, 'police': 5, 'ru': 5, 'cars': 5, 'plenty': 5, 'amount': 5, 'advice': 5, 'behind': 5, 'amazing': 5, 'issues': 5, 'ignore': 5, 'thurs': 5, 'wouldn': 5, 'lik': 5, 'asks': 5, '300': 5, '3510i': 5, '400': 5, 'mths': 5, 'common': 5, 'oni': 4, '87121': 4, 'tkts': 4, 'lives': 4, 'tb': 4, 'oru': 4, 'six': 4, '87575': 4, 'membership': 4, 'str': 4, 'sooner': 4, 'turn': 4, 'child': 4, 'letter': 4, 'inches': 4, 'weak': 4, 'seemed': 4, 'url': 4, 'series': 4, 'iq': 4, 'wah': 4, 'machan': 4, 'becoz': 4, '9pm': 4, 'fml': 4, 'appointment': 4, 'hols': 4, 'legal': 4, 'nyc': 4, 'considering': 4, 'jokes': 4, 'research': 4, 'tt': 4, 'needed': 4, '786': 4, 'unredeemed': 4, 'hasn': 4, 'yetunde': 4, 'ansr': 4, 'tyrone': 4, 'largest': 4, 'befor': 4, 'activities': 4, 'biggest': 4, 'netcollex': 4, 'deleted': 4, 'interview': 4, '434': 4, 'escape': 4, 'bloody': 4, 'anyways': 4, '4742': 4, '145': 4, '0808': 4, '11pm': 4, 'radio': 4, 'unique': 4, 'settled': 4, 'shoot': 4, 'files': 4, 'career': 4, 'cross': 4, 'recd': 4, 'closer': 4, 'argue': 4, 'theory': 4, 'com1win150ppmx3age16': 4, 'affection': 4, 'manda': 4, 'kettoda': 4, 'bcums': 4, 'expect': 4, 'mmm': 4, 'bay': 4, 'hmv': 4, 'passed': 4, 'throw': 4, 'cam': 4, 'accidentally': 4, 'cry': 4, 'def': 4, 'meal': 4, 'dates': 4, 'hanging': 4, 'belovd': 4, 'enemy': 4, 'smart': 4, 'afraid': 4, '08002986906': 4, 'kisses': 4, 'waitin': 4, '83600': 4, '1000s': 4, 'practice': 4, 'wtf': 4, 'further': 4, 'sometime': 4, 'cream': 4, 'tree': 4, 'esplanade': 4, 'fifteen': 4, '3mins': 4, 'wc1n3xx': 4, 'journey': 4, 'jen': 4, 'gorgeous': 4, 'purpose': 4, 'tenants': 4, 'refused': 4, 'ure': 4, 'intelligent': 4, 'result': 4, 'reasons': 4, 'receiving': 4, 'cw25wx': 4, 'tcs': 4, 'charges': 4, 'dry': 4, 'center': 4, 'village': 4, 'bringing': 4, 'jada': 4, 'matured': 4, 'kusruthi': 4, 'prabha': 4, 'mtmsgrcvd18': 4, 'bday': 4, 'rude': 4, 'mas': 4, 'passionate': 4, 'confidence': 4, 'losing': 4, 'three': 4, 'milk': 4, 'essential': 4, 'lab': 4, 'quit': 4, '08715705022': 4, '24': 4, 'grand': 4, '542': 4, 'pie': 4, 'answers': 4, 'often': 4, 'uncles': 4, 'leona': 4, 'bud': 4, 'taken': 4, 'church': 4, 'temple': 4, 'gentle': 4, 'hella': 4, 'bet': 4, 'envelope': 4, 'prepare': 4, 'seem': 4, 'explain': 4, 'purchase': 4, 'weird': 4, 'drivin': 4, 'students': 4, 'height': 4, 'max': 4, 'assume': 4, '81151': 4, '4t': 4, 'faster': 4, 'spoken': 4, 'mt': 4, 'skilgme': 4, '88039': 4, 'meetin': 4, 'apparently': 4, 'smokes': 4, 'sing': 4, 'perfect': 4, '08718727870': 4, 'enjoyed': 4, 'dictionary': 4, 'm263uz': 4, 'appt': 4, '3d': 4, 'ain': 4, 'ache': 4, '3qxj9': 4, '08702840625': 4, '9ae': 4, 'profit': 4, 'cust': 4, 'ppm': 4, 'ibiza': 4, 'meanwhile': 4, 'suite': 4, 'version': 4, 'careful': 4, 'spk': 4, 'saved': 4, 'played': 4, 'wanting': 4, 'pig': 4, 'attend': 4, 'term': 4, 'fever': 4, 'places': 4, 'w1': 4, 'carefully': 4, 'gravity': 4, 'bowl': 4, 'decision': 4, 'sore': 4, 'regret': 4, 'throat': 4, 'lecture': 4, 'raise': 4, 'fool': 4, 'june': 4, 'technical': 4, 'bathing': 4, 'vijay': 4, 'dem': 4, 'fight': 4, 'subscriber': 4, 'aiyar': 4, 'wearing': 4, 'shame': 4, 'credited': 4, 'understanding': 4, 'delivered': 4, 'arms': 4, 'easier': 4, 'txtin': 4, '4info': 4, '08712405020': 4, 'songs': 4, 'exact': 4, '2moro': 4, '80488': 4, 'favour': 4, '3gbp': 4, 'idiot': 4, 'february': 4, 'rush': 4, 'blackberry': 4, 'moji': 4, 'fill': 4, 'gently': 4, '4get': 4, 'msgrcvdhg': 4, 'aiya': 4, 'pod': 4, '7th': 4, '6th': 4, '5th': 4, 'wonders': 4, 'personality': 4, 'purity': 4, 'aint': 4, 'sha': 4, 'total': 4, 'along': 4, 'file': 4, 'shortly': 4, 'ron': 4, '7250i': 4, 'w1jhl': 4, 'preferably': 4, 'idk': 4, 'whom': 4, 'laughing': 4, 'title': 4, 'brought': 4, 'surprised': 4, 'comedy': 4, 'moby': 4, 'action': 4, 'sight': 4, 'remain': 4, 'received': 4, 'ordered': 4, 'rd': 4, 'queen': 4, 'fren': 4, 'connect': 4, 'hook': 4, '05': 4, 'schedule': 4, 'selling': 4, 'settings': 4, 'alert': 4, 'atlanta': 4, 'gaps': 4, 'fills': 4, 'takin': 4, 'answering': 4, 'jess': 4, 'dirty': 4, 'package': 4, '08001950382': 4, 'upto': 4, 'hate': 4, 'skype': 4, 'nearly': 4, 'masters': 4, 'cook': 4, 'cleaning': 4, 'cat': 4, 'hip': 4, '87239': 4, 'freefone': 4, 'lie': 4, 'infernal': 4, 'giv': 4, '84199': 4, 'w111wx': 4, 'yer': 4, 'box39822': 4, 'subs': 4, 'med': 4, 'kidz': 4, 'ntwk': 4, 'frndship': 4, 'freak': 4, 'ref': 4, 'empty': 4, 'wkend': 4, 'letters': 4, 'football': 4, 'happend': 4, 'sugar': 4, 'thangam': 4, 'roger': 4, 'solve': 4, 'cooking': 4, 'key': 4, 'released': 4, 'deliver': 4, 'spending': 4, 'sept': 4, 'public': 4, 'instituitions': 4, 'govt': 4, 'dare': 4, 'teeth': 4, 'iz': 4, 'handle': 4, 'note': 4, 'celebrate': 4, 'tm': 4, 'abi': 4, 'hill': 4, 'relation': 4, 'fromm': 4, '09061221066': 4, 'wylie': 4, 'basic': 4, 'outta': 4, 'inform': 4, 'texted': 4, '26': 4, 'doc': 4, 'taunton': 4, 'loss': 4, '2005': 3, '21st': 3, 'nurungu': 3, 'vettam': 3, 'melle': 3, 'minnaminunginte': 3, 'spell': 3, 'wales': 3, 'scotland': 3, 'clear': 3, 'caught': 3, 'fear': 3, 'xuhui': 3, 'invite': 3, 'yummy': 3, 'fair': 3, 'runs': 3, 'embarassed': 3, 'realized': 3, 'matrix3': 3, '09061209465': 3, 'starwars3': 3, 'suprman': 3, 'roommates': 3, 'dresser': 3, '1500': 3, 'advise': 3, 'recent': 3, 'valuable': 3, 'plane': 3, 'gentleman': 3, 'dignity': 3, 'shy': 3, 'coins': 3, 'requests': 3, 'sheets': 3, 'sum1': 3, 'lido': 3, 'collected': 3, 'verify': 3, 'mix': 3, 'four': 3, 'boston': 3, 'vava': 3, 'loud': 3, 'k52': 3, 'sentence': 3, 'wa': 3, 'anythin': 3, '45239': 3, 'apologise': 3, 'hardcore': 3, 'dot': 3, 'staff': 3, 'female': 3, 'birla': 3, 'soft': 3, 'floor': 3, 'spanish': 3, 'mall': 3, 'maneesha': 3, 'toll': 3, 'satisfied': 3, 'mummy': 3, 'finishes': 3, 'august': 3, 'suggest': 3, 'successfully': 3, 'register': 3, 'mtmsg18': 3, '087187262701': 3, '89545': 3, '50gbp': 3, 'followed': 3, 'pence': 3, 'loses': 3, 'tomarrow': 3, 'avent': 3, 'touched': 3, 'slippers': 3, 'innings': 3, 'bat': 3, 'dearly': 3, '125gift': 3, 'ranjith': 3, '5min': 3, 'networks': 3, 'mini': 3, 'parked': 3, 'jealous': 3, 'flash': 3, 'sorting': 3, '100percent': 3, 'genuine': 3, 'handed': 3, 'gautham': 3, 'buzy': 3, 'upgrade': 3, '0845': 3, 'tease': 3, 'scary': 3, 'gossip': 3, 'fit': 3, 'newest': 3, 'keys': 3, 'garage': 3, 'bstfrnd': 3, 'lifpartnr': 3, 'jstfrnd': 3, 'dear1': 3, 'best1': 3, 'clos1': 3, 'lvblefrnd': 3, 'cutefrnd': 3, 'swtheart': 3, '3uz': 3, 'm26': 3, 'gona': 3, 'flight': 3, 'women': 3, 'record': 3, 'germany': 3, 'supervisor': 3, 'lifetime': 3, 'bless': 3, 'favourite': 3, 'stranger': 3, 'gudnite': 3, 'slap': 3, 'alcohol': 3, 'remembered': 3, 'insha': 3, 'insurance': 3, 'alive': 3, 'gbp': 3, 'ptbo': 3, 'tests': 3, '6months': 3, '08000938767': 3, '4mths': 3, 'or2stoptxt': 3, 'mobilesdirect': 3, 'shut': 3, 'period': 3, 'business': 3, 'picture': 3, 'quickly': 3, 'nd': 3, '87131': 3, 'chechi': 3, 'sender': 3, 'skip': 3, 'names': 3, 'irritating': 3, 'ful': 3, 'lacs': 3, 'bmw': 3, 'arng': 3, 'shortage': 3, 'urgently': 3, 'source': 3, 'iouri': 3, 'sachin': 3, 'oic': 3, 'transfer': 3, '1956669': 3, 'homeowners': 3, 'previously': 3, '75': 3, 'si': 3, 'july': 3, '0207': 3, 'railway': 3, 'doggy': 3, 'fave': 3, 'roads': 3, 'dave': 3, 'transfered': 3, 'banks': 3, '9ja': 3, '9t': 3, 'wise': 3, 'boye': 3, 'dificult': 3, 'fightng': 3, 'fish': 3, '123': 3, '1450': 3, 'fees': 3, 'sory': 3, 'soryda': 3, 'ldnw15h': 3, 'ibhltd': 3, 'cha': 3, 'mono': 3, 'booking': 3, 'behave': 3, 'elsewhere': 3, '09': 3, 'box95qu': 3, '0871': 3, '08717898035': 3, 'malaria': 3, 'ummmmmaah': 3, 'tirupur': 3, 'bloke': 3, 'cock': 3, 'generally': 3, 'likely': 3, 'callin': 3, 'american': 3, 'dick': 3, 'headache': 3, '80878': 3, 'lines': 3, 'exhausted': 3, 'swimming': 3, '2morow': 3, 'paris': 3, 'nichols': 3, '83222': 3, 'market': 3, 'pop': 3, 'postcode': 3, 'seven': 3, 'tlp': 3, 'thanksgiving': 3, '31': 3, 'en': 3, 'peace': 3, '89555': 3, 'textoperator': 3, 'map': 3, 'building': 3, 'accordingly': 3, 'farm': 3, 'ws': 3, 'stress': 3, 'csbcm4235wc1n3xx': 3, 'upset': 3, 'low': 3, 'shouted': 3, 'shorter': 3, 'subscribed': 3, 'realize': 3, 'gimme': 3, '50perwksub': 3, 'tscs087147403231winawk': 3, 'anywhere': 3, 'diff': 3, 'community': 3, 'subpoly': 3, '81618': 3, 'bein': 3, 'jan': 3, 'pieces': 3, 'bid': 3, 'responding': 3, '2u': 3, 'cm2': 3, '220': 3, '08701417012': 3, 'charity': 3, 'alfie': 3, 'm8s': 3, 'nokias': 3, 'brain': 3, 'hahaha': 3, 'given': 3, 'successful': 3, '2morrow': 3, 'sk3': 3, '8wp': 3, 'seconds': 3, 'xavier': 3, 'stomach': 3, 'returned': 3, 'vip': 3, 'supply': 3, 'nap': 3, 'cuddle': 3, 'shesil': 3, '10k': 3, 'liverpool': 3, 'reminder': 3, 'failed': 3, 'outstanding': 3, 'taylor': 3, 'male': 3, '5p': 3, 'msging': 3, 'diet': 3, '88600': 3, 'moments': 3, '114': 3, '14': 3, 'tcr': 3, 'magical': 3, 'welp': 3, 'valid12hrs': 3, '15': 3, 'chicken': 3, 'potential': 3, 'talent': 3, '09063458130': 3, 'polyph': 3, 'fuckin': 3, 'butt': 3, 'terrible': 3, 'prey': 3, 'fancies': 3, 'foreign': 3, 'stamps': 3, 'speechless': 3, 'roast': 3, 'concentrate': 3, 'chatting': 3, 'walked': 3, 'euro': 3, 'drunk': 3, '84025': 3, 'networking': 3, 'juicy': 3, 'dearer': 3, 'itz': 3, 'alwys': 3, 'evn': 3, 'clock': 3, '09061790121': 3, 'ne': 3, 'ground': 3, 'speed': 3, 'catching': 3, 'falls': 3, 'whos': 3, 'le': 3, 'bigger': 3, 'islands': 3, 'celeb': 3, 'pocketbabe': 3, 'voicemail': 3, '2go': 3, 'walmart': 3, 'score': 3, '87021': 3, 'apps': 3, 'anti': 3, 'rofl': 3, 'ph': 3, 'various': 3, 'textcomp': 3, '84128': 3, 'morn': 3, 'docs': 3, 'havin': 3, 'rang': 3, 'sorted': 3, 'executive': 3, 'jane': 3, 'express': 3, 'fran': 3, 'knackered': 3, 'software': 3, 'jamster': 3, 'among': 3, 'cares': 3, '6hrs': 3, 'chill': 3, 'chillin': 3, 'saucy': 3, 'chain': 3, 'suntec': 3, 'messenger': 3, 'screen': 3, 'upload': 3, 'tom': 3, 'shot': 3, 'wt': 3, 'storming': 3, 'invnted': 3, 'margaret': 3, 'telphone': 3, 'phne': 3, 'girlfrnd': 3, 'grahmbell': 3, 'popped': 3, 'shld': 3, 'beware': 3, 'caring': 3, 'option': 3, 'goodnite': 3, 'painful': 3, 'guilty': 3, 'cardiff': 3, 'addie': 3, 'bright': 3, 'certainly': 3, 'twelve': 3, 'aah': 3, '07xxxxxxxxx': 3, 'hubby': 3, 'minmobsmorelkpobox177hp51fl': 3, 'blake': 3, 'karaoke': 3, 'stars': 3, 'eight': 3, 'ese': 3, 'prospects': 3, 'bishan': 3, 'buff': 3, 'gang': 3, 'tablets': 3, 'finishing': 3, 'doors': 3, 'chasing': 3, 'brothas': 3, 'force': 3, 'blame': 3, 'blessings': 3, 'freezing': 3, 'winning': 3, '6pm': 3, 'titles': 3, 'feelin': 3, 'switch': 3, 'monthly': 3, 'ideas': 3, 'maintain': 3, 'sh': 3, 'cramps': 3, 'nan': 3, '81303': 3, 'dislikes': 3, 'likes': 3, 'album': 3, '121': 3, 'standing': 3, 'james': 3, '29': 3, 'chosen': 3, 'di': 3, 'cruise': 3, 'follow': 3, 'stuck': 3, 'regarding': 3, 'adore': 3, 'arcade': 3, 'arun': 3, 'philosophy': 3, 'eye': 3, 'husband': 3, 'norm': 3, 'toa': 3, 'payoh': 3, 'fathima': 3, 'mmmm': 3, '18yrs': 3, 'abta': 3, '80182': 3, '08452810073': 3, 'table': 3, 'ikea': 3, 'cn': 3, 'kadeem': 3, 'wud': 3, 'carry': 3, 'avatar': 3, 'stops': 3, 'constantly': 3, 'lousy': 3, 'ic': 3, 'sweetest': 3, 'honeybee': 3, 'laughed': 3, 'havnt': 3, 'crack': 3, 'boat': 3, 'proof': 3, 'provided': 3, 'yeh': 3, 'members': 3, 'downloads': 3, 'major': 3, 'birth': 3, 'rule': 3, 'natural': 3, 'onwards': 3, '150ppermesssubscription': 3, 'skillgame': 3, 'tscs': 3, '1winaweek': 3, 'eggs': 3, 'boost': 3, 'calicut': 3, 'box97n7qp': 3, 'pink': 3, 'normally': 3, 'rich': 3, 'm8': 3, 'yor': 3, 'jason': 3, 'art': 3, 'feet': 3, 'argh': 3, 'favor': 3, 'tessy': 3, 'shijas': 3, 'aunty': 3, 'china': 3, 'morphine': 3, 'prefer': 3, 'kindly': 3, 'pages': 3, 'pending': 3, 'raji': 3, 'legs': 3, 'distance': 3, 'temp': 3, 'display': 3, 'soup': 3, 'management': 3, 'include': 3, 'regular': 3, 'threats': 3, 'lounge': 3, 'u4': 3, 'cheer': 3, 'cornwall': 3, 'bags': 3, 'iscoming': 3, '80082': 3, 'spook': 3, 'halloween': 3, 'issue': 3, 'sky': 3, 'measure': 3, 'thm': 3, 'instantly': 3, 'drinking': 3, 'wn': 3, 'impossible': 3, 'responce': 3, 'vodka': 3, 'okey': 3, 'neighbour': 3, 'questioned': 3, 'gardener': 3, 'vegetables': 3, 'science': 3, 'madam': 3, 'settle': 3, 'citizen': 3, 'indians': 3, 'sry': 3, '09066612661': 3, 'greetings': 3, 'dai': 3, 'maga': 3, 'medicine': 3, 'violence': 3, 'incident': 3, 'erm': 3, 'instructions': 3, '3lp': 3, 'death': 3, 'hon': 3, 'reality': 3, 'usc': 3, 'booty': 3, 'lil': 3, 'remains': 3, 'bros': 3, 'bro': 3, 'response': 3, 'shirts': 3, 'petrol': 3, 'uks': 3, '2stoptxt': 3, 'luxury': 3, 'ben': 3, 'middle': 3, 'dark': 3, 'enuff': 3, 'strike': 3, 'moved': 3, 'porn': 3, 'dress': 3, 'collecting': 3, 'flaked': 3, 'gary': 3, 'history': 3, 'bell': 3, 'understood': 3, 'bottom': 3, '33': 3, 'books': 3, 'prove': 3, 'blow': 3, 'knowing': 3, 'challenge': 3, 'randomly': 3, 'tape': 3, 'films': 3, 'lick': 3, 'auto': 3, 'praying': 3, 'hug': 3, 'deliveredtomorrow': 3, 'smoking': 3, 'in2': 3, 'billed': 3, 'ths': 3, 'callback': 3, 'wedding': 3, 'accident': 3, 'wisdom': 3, 'cann': 3, 'symbol': 3, 'prolly': 3, 'confirmed': 3, 'dubsack': 3, 'macho': 3, 'audition': 3, 'fell': 3, 'senthil': 3, 'forevr': 3, 'eaten': 3, 'nat': 3, 'possession': 3, 'concert': 3, 'born': 3, 'affairs': 3, 'california': 3, 'university': 3, 'value': 3, 'mnth': 3, 'tog': 3, 'haiz': 3, 'previous': 3, 'parking': 3, 'wallpaper': 3, 'step': 3, 'buffet': 2, 'fa': 2, '08452810075over18': 2, 'hor': 2, 'rcv': 2, 'kl341': 2, 'receivea': 2, '09061701461': 2, '08002986030': 2, 'tsandcs': 2, 'csh11': 2, '6days': 2, 'chances': 2, '4403ldnw1a7rw18': 2, 'jackpot': 2, 'dbuk': 2, 'lccltd': 2, '81010': 2, 'blessing': 2, 'goals': 2, '4txt': 2, 'slice': 2, 'convincing': 2, 'frying': 2, 'sarcastic': 2, '8am': 2, 'mmmmmm': 2, 'burns': 2, 'hospitals': 2, 'gram': 2, 'eighth': 2, 'detroit': 2, 'hockey': 2, 'odi': 2, 'killing': 2, 'burger': 2, '09066364589': 2, 'dedicated': 2, 'dedicate': 2, 'eurodisinc': 2, 'shracomorsglsuplt': 2, 'entry41': 2, '3aj': 2, 'trav': 2, 'ls1': 2, 'aco': 2, 'morefrmmob': 2, 'divorce': 2, 'earn': 2, 'jacket': 2, 'nitros': 2, 'ela': 2, 'pours': 2, '169': 2, '6031': 2, '92h': 2, 'usher': 2, 'britney': 2, '450ppw': 2, '5249': 2, 'mk17': 2, '85069': 2, 'telugu': 2, 'loans': 2, 'animation': 2, 'location': 2, 'noun': 2, 'gent': 2, '09064012160': 2, 'puttin': 2, 'goodo': 2, 'potato': 2, 'tortilla': 2, '08719180248': 2, '07742676969': 2, 'sum': 2, 'algarve': 2, '69888': 2, '31p': 2, 'msn': 2, 'pouch': 2, 'hearts': 2, 'somtimes': 2, 'occupy': 2, '08700621170150p': 2, 'randy': 2, 'flowing': 2, 'plaza': 2, 'everywhere': 2, 'windows': 2, 'mouth': 2, '0871277810810': 2, 'module': 2, 'avoid': 2, 'beloved': 2, 'clark': 2, 'form': 2, 'utter': 2, 'completed': 2, 'stays': 2, 'wishin': 2, 'hamster': 2, 'keralacircle': 2, 'inr': 2, 'refilled': 2, 'kr': 2, 'prepaid': 2, 'ericsson': 2, 'bruv': 2, 'rewarding': 2, 'heading': 2, 'installing': 2, 'repair': 2, 'star': 2, 'teacher': 2, 'teaches': 2, 'upstairs': 2, 'printed': 2, '09058094597': 2, '447801259231': 2, 'signing': 2, 'shining': 2, 'although': 2, 'commercial': 2, 'drpd': 2, 'deepak': 2, 'deeraj': 2, '2wks': 2, 'lag': 2, 'shipping': 2, 'headin': 2, 'necessarily': 2, 'jolt': 2, 'suzy': 2, 'mk45': 2, '2wt': 2, 'chart': 2, 'gf': 2, 'tool': 2, 'jenny': 2, '021': 2, '3680': 2, 'grave': 2, 'taxi': 2, 'crash': 2, 'shocking': 2, 'actor': 2, 'hide': 2, 'thread': 2, '82468': 2, 'funky': 2, 'anot': 2, 'lo': 2, 'tahan': 2, 'buses': 2, 'bristol': 2, 'apo': 2, '861': 2, '85': 2, 'prepayment': 2, '0844': 2, 'paperwork': 2, 'violated': 2, 'privacy': 2, 'caroline': 2, 'cleared': 2, 'misbehaved': 2, 'tissco': 2, 'tayseer': 2, 'audrey': 2, 'status': 2, 'breathe': 2, 'update_now': 2, 'cuddling': 2, 'agree': 2, 'recognise': 2, 'hes': 2, 'ovulation': 2, 'n9dx': 2, 'licks': 2, '30ish': 2, 'grace': 2, 'inshah': 2, 'sharing': 2, 'salam': 2, 'field': 2, 'shipped': 2, 'burning': 2, 'loxahatchee': 2, 'wld': 2, 'darlings': 2, 'fav': 2, 'slightly': 2, 'box334sk38ch': 2, 'whatsup': 2, 'goal': 2, '80086': 2, 'txttowin': 2, 'mobno': 2, 'ads': 2, 'name1': 2, 'adam': 2, 'name2': 2, '07123456789': 2, 'txtno': 2, 'siva': 2, 'expression': 2, 'speaking': 2, '3650': 2, '09066382422': 2, 'bcm4284': 2, '300603': 2, 'applebees': 2, 'cricketer': 2, 'bhaji': 2, 'improve': 2, 'oreo': 2, 'truffles': 2, 'amy': 2, 'coping': 2, 'decisions': 2, 'individual': 2, '26th': 2, '153': 2, 'position': 2, 'language': 2, '09061743806': 2, 'box326': 2, 'screamed': 2, 'removed': 2, 'infront': 2, 'broken': 2, 'tension': 2, 'taste': 2, '07781482378': 2, 'trade': 2, '7ish': 2, 'rec': 2, 'b4280703': 2, '08718727868': 2, '09050002311': 2, 'hyde': 2, 'anthony': 2, 'scrounge': 2, 'forgiven': 2, 'slide': 2, 'renewal': 2, 'transport': 2, 'definite': 2, 'nos': 2, 'ebay': 2, 'pickle': 2, 'tacos': 2, '872': 2, '24hrs': 2, 'pg': 2, 'channel': 2, '08718738001': 2, 'web': 2, '2stop': 2, 'ability': 2, 'develop': 2, 'recovery': 2, 'cali': 2, 'cutting': 2, 'reminding': 2, 'owns': 2, 'faggy': 2, 'demand': 2, 'fo': 2, 'loose': 2, 'perhaps': 2, 'mei': 2, 'geeeee': 2, 'ey': 2, 'oooh': 2, 'call09050000327': 2, 'claims': 2, 'dancing': 2, 'snake': 2, 'bite': 2, 'hardly': 2, 'promo': 2, 'ag': 2, '08712402050': 2, '10ppm': 2, '0825': 2, 'tsunamis': 2, 'soiree': 2, '22': 2, 'ques': 2, 'suits': 2, 'reaction': 2, 'shock': 2, 'grow': 2, 'useful': 2, 'officially': 2, '89693': 2, 'textbuddy': 2, 'gaytextbuddy': 2, '09064019014': 2, '4882': 2, 'hundred': 2, 'expressoffer': 2, 'sweetheart': 2, 'biola': 2, 'effects': 2, 'wee': 2, 'trains': 2, 'ham': 2, 'jolly': 2, '40533': 2, 'sw7': 2, '3ss': 2, 'rstm': 2, 'panic': 2, 'dealer': 2, 'impatient': 2, 'river': 2, 'premium': 2, 'lays': 2, 'posts': 2, 'yelling': 2, 'hex': 2, 'cochin': 2, '4d': 2, 'poop': 2, 'gpu': 2, 'hurried': 2, 'aeroplane': 2, 'calld': 2, 'professors': 2, 'wer': 2, 'aeronautics': 2, 'datz': 2, 'dorm': 2, 'mobilesvary': 2, '050703': 2, 'callcost': 2, '1250': 2, '09071512433': 2, 'cookies': 2, 'correction': 2, 'admit': 2, 'ba': 2, 'spring': 2, 'mtmsg': 2, 'ctxt': 2, 'nokia6650': 2, 'attached': 2, '930': 2, 'helpline': 2, '08706091795': 2, 'gist': 2, 'thousands': 2, '40': 2, 'premier': 2, 'lip': 2, 'confused': 2, 'spare': 2, 'faith': 2, 'acting': 2, 'schools': 2, 'inch': 2, 'begging': 2, '0578': 2, 'opening': 2, 'pole': 2, 'thot': 2, 'petey': 2, 'nic': 2, '8077': 2, 'cashto': 2, 'getstop': 2, '88222': 2, 'php': 2, '08000407165': 2, 'imp': 2, 'bec': 2, 'nervous': 2, 'hint': 2, 'borrow': 2, 'dobby': 2, 'galileo': 2, 'enjoyin': 2, 'loveme': 2, 'cappuccino': 2, 'mojibiola': 2, '07821230901': 2, '09065174042': 2, 'hol': 2, 'kz': 2, 'aburo': 2, 'ultimatum': 2, 'countin': 2, 'skyped': 2, '08002888812': 2, 'inconsiderate': 2, 'hence': 2, 'recession': 2, 'nag': 2, 'soo': 2, '09066350750': 2, 'warning': 2, 'shoes': 2, 'lovejen': 2, 'discreet': 2, 'worlds': 2, 'named': 2, 'genius': 2, 'connections': 2, 'lotta': 2, 'lately': 2, 'virgin': 2, 'mystery': 2, 'approx': 2, 'smsco': 2, 'peaceful': 2, 'consider': 2, 'walls': 2, '41685': 2, '07': 2, 'fixedline': 2, '5k': 2, '09064011000': 2, 'cr01327bt': 2, 'castor': 2, '09058094565': 2, '08': 2, 'stopsms': 2, '09065171142': 2, 'downloaded': 2, 'ear': 2, 'oil': 2, 'usb': 2, 'mac': 2, 'gibbs': 2, 'unbelievable': 2, 'superb': 2, 'several': 2, 'worst': 2, 'charles': 2, 'stores': 2, 'peak': 2, '08709222922': 2, '8p': 2, 'sweets': 2, 'chip': 2, 'addicted': 2, 'yck': 2, 'ashley': 2, 'lux': 2, 'jeans': 2, 'tons': 2, 'scores': 2, 'application': 2, 'ms': 2, 'filthy': 2, 'simpler': 2, '09050001808': 2, 'm95': 2, 'necklace': 2, 'rice': 2, 'racing': 2, 'closes': 2, 'crap': 2, 'borin': 2, 'chocolate': 2, 'reckon': 2, 'tech': 2, '65': 2, 'sd': 2, 'blessed': 2, 'quiet': 2, 'aunts': 2, 'helen': 2, 'fan': 2, 'lovers': 2, 'drove': 2, 'exe': 2, 'pen': 2, 'anniversary': 2, 'datebox1282essexcm61xn': 2, 'secretly': 2, 'pattern': 2, 'plm': 2, 'sheffield': 2, 'zoe': 2, 'setting': 2, 'filling': 2, 'sufficient': 2, 'thx': 2, 'gnt': 2, 'rightly': 2, 'viva': 2, 'edison': 2, 'ls15hb': 2, 'educational': 2, 'flirting': 2, 'kickoff': 2, 'sells': 2, 'thesis': 2, 'sends': 2, 'deciding': 2, 'herself': 2, 'compare': 2, 'eastenders': 2, 'tulip': 2, 'wkent': 2, 'violet': 2, '150p16': 2, 'lily': 2, 'prepared': 2, 'm6': 2, '09058091854': 2, 'box385': 2, '6wu': 2, '09050003091': 2, 'c52': 2, 'oi': 2, 'craziest': 2, 'curry': 2, 'thoughts': 2, 'singing': 2, 'breath': 2, 'planet': 2, '28days': 2, '2yr': 2, 'm221bp': 2, 'box177': 2, '09061221061': 2, '99': 2, 'warranty': 2, 'tomorro': 2, 'fret': 2, 'wind': 2, 'depressed': 2, 'math': 2, 'dhoni': 2, 'rocks': 2, 'durban': 2, '08000776320': 2, 'survey': 2, 'difficulties': 2, 'sar': 2, 'tank': 2, 'silently': 2, 'drms': 2, 'itcould': 2, 'wrc': 2, 'rally': 2, 'lucozade': 2, '61200': 2, 'packs': 2, 'toot': 2, 'annoying': 2, 'makin': 2, 'popcorn': 2, 'beneficiary': 2, 'neft': 2, 'subs16': 2, '1win150ppmx3': 2, 'appreciated': 2, 'apart': 2, 'creepy': 2, '08719181513': 2, 'nok': 2, 'invest': 2, 'delay': 2, '1hr': 2, 'purse': 2, 'europe': 2, 'flip': 2, 'accounts': 2, 'jd': 2, 'weirdest': 2, 'l8tr': 2, 'minmoremobsemspobox45po139wa': 2, 'tee': 2, 'control': 2, 'dough': 2, 'irritates': 2, 'fails': 2, 'jerry': 2, 'drinkin': 2, '5pm': 2, 'birthdate': 2, 'nydc': 2, 'ola': 2, 'items': 2, 'garbage': 2, 'logos': 2, 'gold': 2, 'lionp': 2, 'lionm': 2, 'lions': 2, 'jokin': 2, 'colours': 2, 'whenevr': 2, 'remembr': 2, 'harry': 2, 'potter': 2, 'phoenix': 2, 'readers': 2, 'canada': 2, 'goodnoon': 2, 'patty': 2, 'interest': 2, 'george': 2, '89080': 2, 'free2day': 2, '0870241182716': 2, 'theres': 2, 'tmrw': 2, 'soul': 2, 'ned': 2, 'main': 2, 'hurting': 2, 'sweetie': 2, '4a': 2, 'whn': 2, 'dance': 2, 'bar': 2, '08718730666': 2, 'bears': 2, 'juan': 2, 'lf56': 2, 'tlk': 2, 'front': 2, 'ideal': 2, 'arm': 2, 'tirunelvali': 2, 'effect': 2, 'bk': 2, 'kidding': 2, 'stretch': 2, 'urn': 2, 'sinco': 2, 'disclose': 2, 'frauds': 2, 'payee': 2, 'icicibank': 2, 'kaiez': 2, 'babies': 2, 'practicing': 2, 'pale': 2, 'beneath': 2, 'silver': 2, 'silence': 2, 'revision': 2, 'exeter': 2, 'whose': 2, 'condition': 2, 'arsenal': 2, 'missin': 2, 'tues': 2, 'restaurant': 2, 'textpod': 2, 'desperate': 2, 'monkeys': 2, 'practical': 2, 'mails': 2, 'claire': 2, 'costing': 2, 'ke': 2, 'program': 2, '09066362231': 2, 'ny': 2, 'lotr': 2, 'modules': 2, 'musthu': 2, 'testing': 2, 'nit': 2, 'format': 2, 'sarcasm': 2, 'forum': 2, 'aunt': 2, 'unfortunately': 2, 'wihtuot': 2, 'exmpel': 2, 'jsut': 2, 'tihs': 2, 'waht': 2, 'yuo': 2, 'splleing': 2, 'evrey': 2, 'wrnog': 2, 'raed': 2, 'sitll': 2, 'mitsake': 2, 'rael': 2, 'gving': 2, 'ayn': 2, 'konw': 2, 'ow': 2, 'finance': 2, 'joining': 2, 'filled': 2, 'jia': 2, 'sux': 2, 'kegger': 2, 'adventure': 2, 'pack': 2, 'wifi': 2, 'rumour': 2, '7250': 2, 'boyfriend': 2, 'driver': 2, 'kicks': 2, 'falling': 2, 'smeone': 2, 'fire': 2, 'propose': 2, 'gods': 2, 'gifted': 2, 'lovingly': 2, 'tomeandsaid': 2, 'itwhichturnedinto': 2, 'dippeditinadew': 2, 'ringtoneking': 2, 'batch': 2, 'flaky': 2, 'sooooo': 2, 'tooo': 2, '09058094599': 2, 'confuses': 2, 'wating': 2, 'sw73ss': 2, 'british': 2, 'hotels': 2, 'adoring': 2, 'ghost': 2, 'dracula': 2, 'addamsfa': 2, 'munsters': 2, 'exorcist': 2, 'twilight': 2, 'cared': 2, 'constant': 2, 'allow': 2, 'hlp': 2, '2rcv': 2, '82242': 2, 'msg150p': 2, '08712317606': 2, 'fly': 2, 'event': 2, 'movietrivia': 2, '08712405022': 2, '80608': 2, 'partnership': 2, 'mostly': 2, 'mornin': 2, 'jas': 2, 'poker': 2, 'messy': 2, 'slip': 2, 'moves': 2, 'traffic': 2, 'nus': 2, 'wkg': 2, 'keeps': 2, 'gotten': 2, 'promises': 2, 'unknown': 2, 'vu': 2, 'bcm1896wc1n3xx': 2, '09094646899': 2, 'pre': 2, '2007': 2, 'indeed': 2, 'rents': 2, '48': 2, 'alaipayuthe': 2, 'maangalyam': 2, 'easter': 2, 'telephone': 2, '08081560665': 2, 'bahamas': 2, '07786200117': 2, 'callfreefone': 2, 'up4': 2, 'calm': 2, 'habit': 2, 'contacts': 2, 'forgets': 2, 'mandan': 2, 'ibh': 2, '07734396839': 2, 'nokia6600': 2, 'invaders': 2, 'orig': 2, 'console': 2, 'recharge': 2, 'transfr': 2, '82050': 2, 'prizes': 2, 'foley': 2, 'fake': 2, 'desparate': 2, '3100': 2, 'combine': 2, 'sian': 2, 'g696ga': 2, 'joanna': 2, 'replacement': 2, 'telly': 2, 'tooth': 2, '12mths': 2, 'mth': 2, 'wipro': 2, 'laundry': 2, 'underwear': 2, 'delete': 2, 'waheed': 2, 'pushes': 2, 'beyond': 2, 'avoiding': 2, '0776xxxxxxx': 2, '326': 2, 'uh': 2, 'heads': 2, 'vday': 2, 'build': 2, 'snowman': 2, 'fights': 2, 'prescription': 2, 'electricity': 2, 'fujitsu': 2, 'scold': 2, 'se': 2, 'prompts': 2, '09066358152': 2, 'disturbing': 2, 'flies': 2, 'woken': 2, 'aka': 2, 'delhi': 2, 'held': 2, 'fringe': 2, 'distract': 2, 'tones2you': 2, '61610': 2, '08712400602450p': 2, 'mel': 2, 'responsibility': 2, '08006344447': 2, 'kid': 2, 'affair': 2, 'parco': 2, 'nb': 2, 'hallaq': 2, 'bck': 2, 'color': 2, 'lyk': 2, 'gender': 2, 'sleepwell': 2, 'mca': 2, 'vomiting': 2, 'rub': 2, 'clever': 2, '113': 2, 'bray': 2, 'wicklow': 2, 'stamped': 2, 'eire': 2, 'ryan': 2, 'idew': 2, 'manage': 2, 'xam': 2, 'diamonds': 2, 'shitload': 2, 'mcat': 2, '27': 2, 'beg': 2, 'sacrifice': 2, 'stayin': 2, 'satisfy': 2, 'cld': 2, 'miles': 2, 'killed': 2, 'smashed': 2, 'ps': 2, 'tok': 2, 'specific': 2, 'figures': 2, 'cousin': 2, 'excuses': 2, 'neck': 2, 'continue': 2, 'holy': 2, 'billion': 2, 'classes': 2, 'turning': 2, 'youre': 2, 'belive': 2, 'slots': 2, 'discussed': 2, 'prem': 2, '2morro': 2, 'spoiled': 2, 'complaint': 2, 'sales': 2, 'lk': 2, 'lov': 2, 'comfort': 2, '300p': 2, '8552': 2, '2end': 2, '01223585334': 2, '2c': 2, 'shagged': 2, '88066': 2, 'bedrm': 2, '700': 2, 'waited': 2, 'huge': 2, 'mids': 2, 'upd8': 2, 'oranges': 2, 'annie': 2, 'messaging': 2, 'retrieve': 2, 'mailbox': 2, '09056242159': 2, '21870000': 2, 'hrishi': 2, 'nothin': 2, 'poem': 2, 'duchess': 2, '008704050406': 2, 'dan': 2, 'aww': 2, 'staring': 2, 'cm': 2, 'unnecessarily': 2, '08701417012150p': 2, 'weigh': 2, 'scoring': 2, 'active': 2, '250k': 2, '88088': 2, 'gamestar': 2, 'expired': 2, 'opinions': 2, 'lv': 2, 'lived': 2, 'propsd': 2, 'happily': 2, '2gthr': 2, 'gv': 2, 'speeding': 2, 'thy': 2, 'lttrs': 2, 'aproach': 2, 'threw': 2, 'evrydy': 2, 'truck': 2, 'dt': 2, 'paragon': 2, 'arent': 2, 'bluff': 2, 'sary': 2, 'piece': 2, 'scotch': 2, 'yarasu': 2, 'shampain': 2, 'brandy': 2, 'vaazhthukkal': 2, 'gin': 2, 'kudi': 2, 'dhina': 2, 'rum': 2, 'wiskey': 2, 'kg': 2, 'dumb': 2, 'dressed': 2, 'kills': 2, 'kay': 2, 'nasty': 2, 'wasted': 2, 'christ': 2, 'tears': 2, 'push': 2, 'answered': 2, 'rgds': 2, '8pm': 2, 'wrote': 2, 'rights': 2, 'jobs': 2, 'lane': 2, 'donno': 2, 'properly': 2, '630': 2, 'lock': 2, 'furniture': 2, 'shoving': 2, 'papers': 2, 'strange': 2, 'acl03530150pm': 2, 'indyarocks': 2, 'resume': 2, 'bids': 2, 'whr': 2, 'yunny': 2, '83383': 2, 'mmmmm': 2, 'relatives': 2, 'benefits': 2, 'environment': 2, 'terrific': 2, 'dr': 2, 'superior': 2, 'vid': 2, 'ruin': 2, 'conform': 2, 'department': 2, 'bc': 2, 'toshiba': 2, 'wrk': 2, 'innocent': 2, 'mental': 2, 'hoped': 2, 'bills': 2, '2marrow': 2, 'treated': 2, 'wks': 2, 'fab': 2, 'tiwary': 2, 'battle': 2, 'bang': 2, 'pap': 2, 'arts': 2, 'pandy': 2, 'edu': 2, 'secretary': 2, 'dollar': 2, 'pull': 2, 'amongst': 2, '69696': 2, 'nalla': 2, 'pouts': 2, 'stomps': 2, 'northampton': 2, 'abj': 2, 'serving': 2, 'smith': 2, 'nagar': 2, 'anna': 2, 'sports': 2, 'evr': 2, 'hugs': 2, 'neither': 2, 'snogs': 2, 'west': 2, 'growing': 2, 'fastest': 2, 'chase': 2, 'steam': 2, 'reg': 2, 'canary': 2, 'sleepy': 2, 'mag': 2, 'diwali': 2, 'tick': 2, 'onion': 2, 'thgt': 2, 'lower': 2, 'exhaust': 2, 'pee': 2, 'contents': 2, 'success': 2, 'division': 2, 'creep': 2, 'lies': 2, 'property': 2, '7876150ppm': 2, '09058099801': 2, 'b4190604': 2, 'bbd': 2, 'pimples': 2, 'yellow': 2, 'frog': 2, '88888': 2, 'doubt': 2, 'japanese': 2, 'proverb': 2, 'freedom': 2, 'seat': 2, 'twenty': 2, 'painting': 2, 'nowadays': 2, 'talks': 2, 'probs': 2, 'swatch': 2, 'ganesh': 2, 'trips': 2, 'helloooo': 2, 'welcomes': 2, '54': 2, '2geva': 2, 'wuld': 2, 'solved': 2, 'ing': 2, 'sake': 2, 'bruce': 2, 'teaching': 2, 'chest': 2, 'covers': 2, 'hang': 2, 'reboot': 2, 'pt2': 2, 'phoned': 2, 'improved': 2, 'hm': 2, 'salon': 2, 'evenings': 2, 'raj': 2, 'payment': 2, 'clearing': 2, 'shore': 2, 'range': 2, 'changes': 2, 'topic': 2, 'admin': 2, 'visionsms': 2, 'andros': 2, 'meets': 2, 'penis': 2, 'foot': 2, 'sigh': 2, 'eveb': 2, 'window': 2, 'removal': 2, '08708034412': 2, 'cancelled': 2, 'neway': 2, 'xxxxx': 2, 'count': 2, 'otside': 2, 'size': 2, '08712101358': 2, 'tight': 2, 'av': 2, 'everyday': 2, 'curious': 2, 'postcard': 2, 'bread': 2, 'mahal': 2, 'luvs': 2, 'ding': 2, 'allowed': 2, 'shared': 2, 'watever': 2, 'necessary': 2, 'messaged': 2, 'deus': 2, 'broad': 2, 'canal': 2, 'spile': 2, 'tap': 2, 'engin': 2, 'edge': 2, 'east': 2, 'howard': 2, 'cooked': 2, 'cheat': 2, 'block': 2, 'ruining': 2, 'easily': 2, 'selfish': 2, 'custom': 2, 'sac': 2, 'jiayin': 2, 'pobox45w2tg150p': 2, 'forgotten': 2, 'reverse': 2, 'cheating': 2, 'mathematics': 2, '2waxsto': 2, 'minimum': 2, 'elaine': 2, 'drunken': 2, 'mess': 2, 'crisis': 2, '____': 2, 'ias': 2, 'mb': 2, '600': 2, 'desires': 2, '1030': 2, 'careers': 2, '447797706009': 2, 'bloomberg': 2, 'priscilla': 2, 'kent': 2, 'vale': 2, '83049': 2, 'unkempt': 2, 'westlife': 2, 'unbreakable': 2, 'untamed': 2, 'wan2': 2, 'prince': 2, 'cdgt': 2, 'granite': 2, 'nasdaq': 2, 'explosive': 2, 'base': 2, 'placement': 2, 'sumthin': 2, 'lion': 2, 'devouring': 2, 'airtel': 2, 'processed': 2, '69669': 2, 'jaya': 2, 'forums': 2, 'incredible': 2, '18p': 2, 'o2fwd': 2, 'ship': 2, 'maturity': 2, 'kavalan': 2, 'causing': 2, 'blank': 2, 'tonights': 2, 'xin': 2, 'lib': 2, 'difference': 2, 'despite': 2, 'swoop': 2, 'langport': 2, 'mistakes': 2, 'lou': 2, 'pool': 2, '09065989182': 2, 'x49': 2, 'ibn': 2, 'verified': 2, 'confirmd': 2, 'terrorist': 2, 'cnn': 2, 'disconnect': 2, 'hppnss': 2, 'goodfriend': 2, 'sorrow': 2, 'stayed': 2, 'stone': 2, 'help08718728876': 2, 'increments': 2, 'mila': 2, '69866': 2, '30pp': 2, 'blonde': 2, '5free': 2, 'mtalk': 2, 'age23': 2, 'atlast': 2, 'desert': 2, 'funk': 2, 'tones2u': 2, 'funeral': 2, 'vivek': 2, 'tnc': 2, 'brah': 2, 'passwords': 2, 'sensitive': 2, 'protect': 2, 'sib': 2, 'ipad': 2, 'bird': 2, 'cheese': 2, 'widelive': 2, 'index': 2, 'tms': 2, 'wml': 2, 'hsbc': 2, 'asp': 2, '09061702893': 2, 'eek': 2, 'melt': 2, '09061743386': 2, '674': 2, 'eta': 2, '0871750': 2, '77': 2, 'landlines': 2, 'housewives': 2, 'dial': 2, '09066364311': 2, 'literally': 2, 'kothi': 2, 'sem': 2, 'student': 2, 'actual': 2, 'dealing': 2, 'reasonable': 2, 'kappa': 2, 'piss': 2, 'receipts': 2, 'guessing': 2, 'royal': 2, 'sticky': 2, 'indicate': 2, 'repeat': 2, 'calculation': 2, 'clothes': 2, 'lush': 2, '2find': 2, 'courage': 2, 'defeat': 2, 'greatest': 2, 'bear': 2, 'fucked': 2, 'beauty': 2, 'natalja': 2, '440': 2, 'nat27081980': 2, 'moving': 2, 'jogging': 2, 'shelf': 2, 'captain': 2, 'mokka': 2, 'polyh': 2, '09061744553': 2, 'bone': 2, 'steve': 2, 'epsilon': 2, 'mesages': 2, 'lst': 2, 'massive': 2, 'absolutly': 2, 'forms': 2, '373': 2, 'w1j': 2, '6hl': 2, 'polo': 2, 'coast': 2, 'suppose': 2, '02073162414': 2, 'secs': 2, 'explicit': 2, 'clearly': 2, 'gain': 2, 'realise': 2, 'mnths': 2, 'subscribe6gbp': 2, '86888': 2, '3hrs': 2, 'txtstop': 2, 'managed': 2, 'capital': 2, 'acted': 2, '09066380611': 2, 'loyal': 2, 'customers': 2, 'print': 2, 'dokey': 2, 'error': 2, 'sleepin': 2, 'minor': 2, 'woulda': 2, 'santa': 2, 'miserable': 2, 'shoppin': 2, '08718726270': 2, 'celebration': 2, 'warner': 2, 'select': 2, 'sarasota': 2, '13': 2, 'cherish': 2, 'slp': 2, 'muah': 2, '4eva': 2, 'garden': 2, 'notxt': 2, 'seeds': 2, 'bulbs': 2, 'scotsman': 2, 'go2': 2, 'replace': 2, 'reduce': 2, 'limiting': 2, 'gastroenteritis': 2, 'illness': 2, '09061213237': 2, 'm227xy': 2, '177': 2, 'respectful': 2, 'pride': 2, 'bottle': 2, 'amused': 2, 'mega': 2, 'island': 2, 'amore': 1, 'jurong': 1, 'chgs': 1, 'patent': 1, 'aids': 1, 'cried': 1, 'breather': 1, 'granted': 1, 'fulfil': 1, 'xxxmobilemovieclub': 1, 'qjkgighjjgcbl': 1, 'gota': 1, 'poboxox36504w45wq': 1, 'macedonia': 1, 'u1': 1, 'ffffffffff': 1, 'forced': 1, 'packing': 1, 'ahhh': 1, 'vaguely': 1, 'actin': 1, 'badly': 1, 'apologetic': 1, 'fallen': 1, 'spoilt': 1, 'housework': 1, 'cuppa': 1, 'fainting': 1, 'timings': 1, 'watts': 1, 'steed': 1, 'arabian': 1, 'rodger': 1, '07732584351': 1, 'endowed': 1, 'hep': 1, 'immunisation': 1, 'sucker': 1, 'suckers': 1, 'stubborn': 1, 'thinked': 1, 'smarter': 1, 'crashing': 1, 'accomodations': 1, 'offered': 1, 'embarassing': 1, 'cave': 1, 'wings': 1, 'incorrect': 1, 'jersey': 1, 'devils': 1, 'sptv': 1, 'sherawat': 1, 'mallika': 1, 'gauti': 1, 'sehwag': 1, 'seekers': 1, 'barbie': 1, 'ken': 1, 'performed': 1, 'peoples': 1, 'operate': 1, 'multis': 1, 'factory': 1, 'casualty': 1, 'stuff42moro': 1, 'includes': 1, 'hairdressers': 1, 'beforehand': 1, 'ams': 1, '4the': 1, 'signin': 1, 'memorable': 1, 'server': 1, 'minecraft': 1, 'ip': 1, 'grumpy': 1, 'lying': 1, 'plural': 1, 'formal': 1, 'openin': 1, '0871277810910p': 1, 'ratio': 1, '09064019788': 1, 'box42wr29c': 1, 'malarky': 1, 'apples': 1, 'pairs': 1, '4041': 1, '7548': 1, 'sao': 1, 'predict': 1, 'involve': 1, 'imposed': 1, 'lucyxx': 1, 'tmorrow': 1, 'accomodate': 1, 'gravel': 1, 'hotmail': 1, 'svc': 1, '69988': 1, 'nver': 1, 'ummma': 1, 'sindu': 1, 'nevering': 1, 'typical': 1, 'chores': 1, 'mist': 1, 'dirt': 1, 'exist': 1, 'hail': 1, 'aaooooright': 1, '07046744435': 1, 'annoncement': 1, 'envy': 1, 'excited': 1, '32': 1, 'bootydelious': 1, 'bangb': 1, 'bangbabes': 1, 'cultures': 1, 's89': 1, '09061701939': 1, 'missunderstding': 1, 'bridge': 1, 'lager': 1, 'axis': 1, 'surname': 1, 'clue': 1, 'begins': 1, 'hopes': 1, 'lifted': 1, 'approaches': 1, 'finding': 1, 'handsome': 1, 'areyouunique': 1, '30th': 1, 'league': 1, 'stool': 1, 'ors': 1, '1pm': 1, 'babyjontet': 1, 'enc': 1, 'ga': 1, 'alter': 1, 'dogg': 1, 'dats': 1, 'refund': 1, 'prediction': 1, 'os': 1, 'ubandu': 1, 'disk': 1, 'scenery': 1, 'aries': 1, 'flyng': 1, 'horo': 1, 'mudyadhu': 1, 'elama': 1, 'conducts': 1, 'strict': 1, 'gandhipuram': 1, 'rubber': 1, 'thirtyeight': 1, 'hearing': 1, 'pleassssssseeeeee': 1, 'sportsx': 1, 'watches': 1, 'baig': 1, '3days': 1, 'usps': 1, 'nipost': 1, 'bribe': 1, 'ups': 1, 'luton': 1, '0125698789': 1, '69698': 1, 'sometme': 1, 'club4': 1, 'box1146': 1, 'club4mobiles': 1, '87070': 1, 'evo': 1, 'narcotics': 1, 'objection': 1, 'theater': 1, 'mack': 1, 'rob': 1, 'celebrations': 1, 'gdeve': 1, 'ahold': 1, 'cruisin': 1, 'raksha': 1, 'varunnathu': 1, 'edukkukayee': 1, 'ollu': 1, 'resend': 1, '28thfeb': 1, 'gurl': 1, 'appropriate': 1, 'diesel': 1, 'fridge': 1, 'womdarfull': 1, 'rodds1': 1, 'icmb3cktz8r7': 1, 'aberdeen': 1, 'img': 1, 'kingdom': 1, 'united': 1, 'blind': 1, 'remb': 1, 'jos': 1, 'bookshelf': 1, '85222': 1, 'gbp1': 1, '84': 1, 'winnersclub': 1, 'mylife': 1, 'l8': 1, 'gon': 1, 'guild': 1, 'evaporated': 1, 'stealing': 1, 'employer': 1, 'daaaaa': 1, 'dined': 1, 'wined': 1, 'hiding': 1, 'huiming': 1, 'prestige': 1, 'xxuk': 1, 'sextextuk': 1, 'shag': 1, '69876': 1, 'jeremiah': 1, 'iphone': 1, 'apeshit': 1, 'safely': 1, 'onam': 1, 'sirji': 1, 'tata': 1, 'aig': 1, '08708800282': 1, 'unemployed': 1, 'andrews': 1, 'db': 1, 'dawns': 1, 'refreshed': 1, 'f4q': 1, 'regalportfolio': 1, 'rp176781': 1, '08717205546': 1, 'uniform': 1, 'spoil': 1, '09057039994': 1, 't91': 1, 'lindsay': 1, 'bars': 1, 'heron': 1, 'payasam': 1, 'rinu': 1, 'prabu': 1, 'verifying': 1, 'becaus': 1, 'taught': 1, 'followin': 1, 'repairs': 1, 'wallet': 1, '945': 1, 'owl': 1, 'kickboxing': 1, 'lap': 1, 'performance': 1, 'calculated': 1, 'visitor': 1, 'wahleykkum': 1, 'administrator': 1, '2814032': 1, '150pw': 1, '3x': 1, 'stoners': 1, 'disastrous': 1, 'busetop': 1, 'iron': 1, 'blah': 1, 'okies': 1, 'wendy': 1, '09064012103': 1, 'pobox12n146tf150p': 1, '09111032124': 1, '09058094455': 1, 'attractive': 1, 'rowdy': 1, 'sentiment': 1, 'attitude': 1, 'urination': 1, 'hillsborough': 1, 'shoul': 1, 'hasnt': 1, 'monkeespeople': 1, 'werethe': 1, 'jobyet': 1, 'howu': 1, 'monkeyaround': 1, 'howdy': 1, 'foundurself': 1, 'sausage': 1, 'exercise': 1, 'blimey': 1, 'concentration': 1, 'hanks': 1, 'lotsly': 1, 'detail': 1, 'optimistic': 1, 'practicum': 1, 'consistently': 1, 'links': 1, 'ears': 1, 'wavering': 1, 'heal': 1, '9153': 1, 'upgrdcentre': 1, 'oral': 1, 'slippery': 1, 'bike': 1, 'okmail': 1, 'differ': 1, 'enters': 1, '69888nyt': 1, 'machi': 1, 'mcr': 1, 'falconerf': 1, 'thuglyfe': 1, 'jaykwon': 1, 'glory': 1, 'ralphs': 1, 'faded': 1, 'reunion': 1, 'accenture': 1, 'jackson': 1, 'reache': 1, 'nuerologist': 1, 'lolnice': 1, 'westshore': 1, 'significance': 1, 'ammo': 1, 'ak': 1, 'toxic': 1, 'poly3': 1, 'jamz': 1, 'boltblue': 1, 'topped': 1, 'tgxxrz': 1, 'bubbletext': 1, 'problematic': 1, 'abnormally': 1, 'adults': 1, 'unconscious': 1, '9755': 1, 'teletext': 1, 'recieve': 1, 'faggot': 1, '07815296484': 1, '41782': 1, 'bani': 1, 'leads': 1, 'buttons': 1, 'max6': 1, 'csc': 1, 'applausestore': 1, 'monthlysubscription': 1, 'famous': 1, 'temper': 1, 'unconditionally': 1, 'bash': 1, 'oclock': 1, 'cooped': 1, 'weddin': 1, 'invitation': 1, 'alibi': 1, 'paces': 1, 'surrounded': 1, 'cuck': 1, 'sink': 1, 'cage': 1, 'deficient': 1, 'acknowledgement': 1, 'tactless': 1, 'astoundingly': 1, 'oath': 1, 'magic': 1, 'pan': 1, 'silly': 1, 'mutations': 1, 'uv': 1, 'causes': 1, 'sunscreen': 1, 'thesedays': 1, 'sugardad': 1, 'bao': 1, 'brownie': 1, 'ninish': 1, 'freek': 1, 'icky': 1, 'ridden': 1, 'missy': 1, 'goggles': 1, 'arguing': 1, '09050005321': 1, 'unfortuntly': 1, 'arngd': 1, 'frnt': 1, 'walkin': 1, 'sayin': 1, 'bites': 1, 'textand': 1, '08002988890': 1, 'tendencies': 1, 'jjc': 1, 'gotany': 1, 'meive': 1, 'yi': 1, 'srsly': 1, '07753741225': 1, '08715203677': 1, '42478': 1, 'prix': 1, 'nitz': 1, 'stands': 1, 'occur': 1, 'rajnikant': 1, 'blastin': 1, 'ocean': 1, 'speciale': 1, 'roses': 1, '07880867867': 1, 'zouk': 1, 'clubsaisai': 1, '07946746291': 1, 'xclusive': 1, 'banter': 1, 'bridgwater': 1, 'dependents': 1, 'cer': 1, 'thanx4': 1, 'beauties': 1, 'hundreds': 1, 'aunties': 1, 'handsomes': 1, 'friendships': 1, 'dismay': 1, 'concerned': 1, 'tootsie': 1, 'seventeen': 1, 'ml': 1, 'fetching': 1, 'restock': 1, 'brighten': 1, 'braved': 1, 'allo': 1, 'triumphed': 1, 'uncomfortable': 1, '08715203694': 1, 'rough': 1, 'sonetimes': 1, 'wesleys': 1, 'cloud': 1, 'wikipedia': 1, '08718711108': 1, '89034': 1, '88800': 1, 'repent': 1, 'sutra': 1, 'kama': 1, 'positions': 1, 'nange': 1, 'bakra': 1, 'kalstiya': 1, 'lakhs': 1, 'sun0819': 1, '08452810071': 1, 'ditto': 1, 'wetherspoons': 1, 'piggy': 1, 'freaky': 1, 'scrappy': 1, 'sdryb8i': 1, '1da': 1, 'lapdancer': 1, '150ppmsg': 1, 'sue': 1, 'g2': 1, 'tomorw': 1, 'imprtant': 1, 'crying': 1, 'bfore': 1, 'tmorow': 1, 'cherthala': 1, 'engaged': 1, '08712404000': 1, '448712404000': 1, '1680': 1, '1405': 1, '1843': 1, 'entrepreneurs': 1, 'corporation': 1, 'fluids': 1, 'dehydration': 1, 'prevent': 1, 'trek': 1, 'harri': 1, 'deck': 1, 'cnupdates': 1, 'gage': 1, 'alerts': 1, 'newsletter': 1, 'shitstorm': 1, 'attributed': 1, '08714712388': 1, '449071512431': 1, 'specs': 1, 'sth': 1, 'px3748': 1, '08714712394': 1, 'mindset': 1, 'macha': 1, 'wondar': 1, 'flim': 1, 'jelly': 1, 'scrumptious': 1, 'dao': 1, 'half8th': 1, 'visiting': 1, 'jide': 1, 'alertfrom': 1, 'drvgsto': 1, 'stewartsize': 1, 'jeri': 1, 'prescripiton': 1, '2kbsubject': 1, 'steak': 1, 'neglect': 1, 'prayers': 1, 'wahay': 1, 'hadn': 1, 'clocks': 1, 'realised': 1, 'gaze': 1, '82324': 1, 'tattoos': 1, 'caveboy': 1, 'vibrate': 1, '79': 1, '08704439680ts': 1, 'hungover': 1, 'grandmas': 1, 'closingdate04': 1, 'm39m51': 1, 'claimcode': 1, 'mobypobox734ls27yf': 1, '09066368327': 1, '50pmmorefrommobile2bremoved': 1, 'unclaimed': 1, 'gua': 1, 'faber': 1, 'dramatic': 1, 'hunting': 1, 'drunkard': 1, 'weaseling': 1, 'idc': 1, 'trash': 1, 'punish': 1, 'beerage': 1, 'randomlly': 1, 'fixes': 1, 'spelling': 1, '100p': 1, '087018728737': 1, 'tune': 1, 'toppoly': 1, 'fondly': 1, 'dogbreath': 1, 'sounding': 1, 'weighed': 1, 'woohoo': 1, 'uncountable': 1, '14thmarch': 1, 'availa': 1, '9996': 1, 'canlove': 1, 'whereare': 1, 'thekingshead': 1, 'friendsare': 1, 'rg21': 1, '4jx': 1, 'dled': 1, 'smokin': 1, 'boooo': 1, 'costumes': 1, 'yowifes': 1, 'notifications': 1, 'outbid': 1, 'plyr': 1, 'simonwatson5120': 1, 'shinco': 1, 'smsrewards': 1, 'youi': 1, 'yourjob': 1, 'soonlots': 1, 'llspeak': 1, 'starshine': 1, 'sips': 1, 'yourinclusive': 1, 'smsservices': 1, 'bits': 1, 'turned': 1, 'burial': 1, 'rvx': 1, 'rv': 1, 'comprehensive': 1, 'prashanthettan': 1, 'doug': 1, 'realizes': 1, 'guitar': 1, 'samantha': 1, 'impress': 1, 'trauma': 1, 'swear': 1, 'inner': 1, 'tigress': 1, 'overdose': 1, 'urfeeling': 1, 'bettersn': 1, 'probthat': 1, '83110': 1, 'ana': 1, 'rto': 1, 'sathy': 1, 'spoons': 1, 'corvettes': 1, '50pm': 1, '09061104283': 1, 'bunkers': 1, '07808': 1, 'xxxxxx': 1, '08719899217': 1, 'posh': 1, 'dob': 1, 'chaps': 1, 'prods': 1, 'trial': 1, 'champneys': 1, '0721072': 1, 'hole': 1, 'philosophical': 1, 'shakespeare': 1, 'atleast': 1, 'mymoby': 1, 'doit': 1, 'curfew': 1, 'getsleep': 1, 'gibe': 1, 'studdying': 1, 'woul': 1, 'massages': 1, 'yoyyooo': 1, 'permissions': 1, 'hussey': 1, 'mike': 1, 'faglord': 1, 'ctter': 1, 'cttargg': 1, 'ie': 1, 'cttergg': 1, 'ctagg': 1, 'cutter': 1, 'ctargg': 1, 'nutter': 1, 'thus': 1, 'grateful': 1, 'happier': 1, 'experiment': 1, 'agents': 1, 'invoices': 1, 'smell': 1, 'tobacco': 1, 'assumed': 1, 'racal': 1, 'dizzee': 1, 'bookmark': 1, 'stereophonics': 1, 'marley': 1, 'strokes': 1, 'libertines': 1, 'lastest': 1, 'nookii': 1, 'grinule': 1, 'oreos': 1, 'fudge': 1, 'zaher': 1, 'dieting': 1, 'nauseous': 1, 'hollalater': 1, 'avalarr': 1, 'rounds': 1, 'blogging': 1, 'magicalsongs': 1, 'blogspot': 1, 'slices': 1, 'kvb': 1, 'w1t1jy': 1, 'box403': 1, 'ppt150x3': 1, '1million': 1, 'alternative': 1, 'owo': 1, 'fro': 1, 'ore': 1, 'samus': 1, 'shoulders': 1, '09063440451': 1, 'ppm150': 1, 'matthew': 1, 'box334': 1, 'vomitin': 1, '528': 1, '1yf': 1, 'hp20': 1, '09061749602': 1, 'writhing': 1, 'bleh': 1, 'stuffed': 1, 'pockets': 1, 'paypal': 1, 'voila': 1, 'theyre': 1, 'folks': 1, 'sorta': 1, 'blown': 1, 'secondary': 1, 'applying': 1, 'ogunrinde': 1, 'sophas': 1, 'lodging': 1, 'chk': 1, 'dict': 1, 'shb': 1, 'stories': 1, 'retired': 1, 'natwest': 1, 'chad': 1, 'gymnastics': 1, 'christians': 1, 'token': 1, 'liking': 1, 'aptitude': 1, 'horse': 1, 'wrongly': 1, 'boggy': 1, 'biatch': 1, 'weakness': 1, 'hesitate': 1, 'notebook': 1, 'carpark': 1, 'eightish': 1, '5wkg': 1, 'cres': 1, 'ere': 1, 'ubi': 1, '67441233': 1, 'irene': 1, '61': 1, 'bus8': 1, '382': 1, '6ph': 1, '66': 1, '7am': 1, '5ish': 1, 'relaxing': 1, 'stripes': 1, 'skirt': 1, 'escalator': 1, 'beth': 1, 'charlie': 1, 'syllabus': 1, 'panasonic': 1, 'bluetoothhdset': 1, 'doubletxt': 1, 'doublemins': 1, '30pm': 1, 'kolathupalayam': 1, 'unjalur': 1, 'poyyarikatur': 1, 'erode': 1, 'apt': 1, 'hero': 1, 'meat': 1, 'supreme': 1, 'cudnt': 1, 'ctla': 1, 'ishtamayoo': 1, 'bakrid': 1, 'ente': 1, 'images': 1, 'fond': 1, 'finds': 1, 'cougar': 1, 'coaxing': 1, 'souveniers': 1, 'glorious': 1, '09065394514': 1, 'scratches': 1, 'nanny': 1, 'hardest': 1, 'shitin': 1, 'lekdog': 1, 'defo': 1, 'millions': 1, 'blankets': 1, 'atten': 1, '09058097218': 1, 'analysis': 1, 'data': 1, 'belligerent': 1, 'rudi': 1, 'les': 1, 'snoring': 1, 'ink': 1, '515': 1, 'throwing': 1, 'finalise': 1, 'flirtparty': 1, 'replys150': 1, 'dentist': 1, 'shes': 1, 'oyea': 1, 'nurses': 1, 'lul': 1, 'obese': 1, 'ami': 1, 'parchi': 1, 'kicchu': 1, 'korte': 1, 'tul': 1, 'korche': 1, 'iccha': 1, 'kaaj': 1, 'copies': 1, 'sculpture': 1, 'surya': 1, 'pokkiri': 1, 'sorrows': 1, 'praises': 1, 'sambar': 1, 'makiing': 1, 'attraction': 1, 'proove': 1, 'ndship': 1, '4few': 1, 'conected': 1, 'needle': 1, 'spatula': 1, 'outrageous': 1, 'complexities': 1, 'freely': 1, 'taxes': 1, 'ryder': 1, 'presleys': 1, 'elvis': 1, 'postal': 1, 'strips': 1, 'gifts': 1, 'cliff': 1, 'wrking': 1, 'sittin': 1, 'drops': 1, 'hen': 1, 'smoked': 1, 'teju': 1, 'hourish': 1, 'amla': 1, 'convenience': 1, 'evaluation': 1, '09050000301': 1, '449050000301': 1, '80155': 1, 'chat80155': 1, 'rcd': 1, 'speedchat': 1, 'swap': 1, 'chatter': 1, 'cheyyamo': 1, '80160': 1, 'txt43': 1, 'brothers': 1, 'throws': 1, 'hmv1': 1, 'errors': 1, 'piah': 1, 'tau': 1, '1stchoice': 1, '08707808226': 1, 'shade': 1, 'copied': 1, 'notified': 1, 'marketing': 1, '08450542832': 1, '84122': 1, 'theirs': 1, 'sexual': 1, 'virgins': 1, '69911': 1, '4fil': 1, 'kaitlyn': 1, 'sitter': 1, 'peeps': 1, 'danger': 1, 'comment': 1, 'veggie': 1, 'neighbors': 1, 'computerless': 1, 'balloon': 1, 'melody': 1, 'macs': 1, 'hme': 1, 'velachery': 1, 'flippin': 1, 'breaking': 1, 'cstore': 1, 'hangin': 1, 'lodge': 1, 'worrying': 1, 'quizzes': 1, '087016248': 1, '08719181503': 1, 'thin': 1, 'arguments': 1, 'fed': 1, 'himso': 1, 'semi': 1, 'exp': 1, '30apr': 1, 'maaaan': 1, 'guessin': 1, 'wuldnt': 1, 'personally': 1, 'ilol': 1, 'lunchtime': 1, 'organise': 1, '5years': 1, 'passable': 1, 'phd': 1, 'prakesh': 1, 'products': 1, 'betta': 1, 'aging': 1, 'global': 1, '08700435505150p': 1, 'accommodation': 1, 'phb1': 1, 'submitting': 1, 'snatch': 1, 'dancce': 1, 'basq': 1, 'pthis': 1, 'senrd': 1, 'ihave': 1, '0quit': 1, 'edrunk': 1, 'xxxxxxx': 1, 'iff': 1, 'ros': 1, 'drivby': 1, 'drum': 1, 'dnot': 1, '2nhite': 1, 'westonzoyland': 1, 'relieved': 1, 'greatness': 1, 'goin2bed': 1, 'only1more': 1, 'mc': 1, 'ifink': 1, 'everythin': 1, 'ava': 1, 'every1': 1, 'melnite': 1, 'oli': 1, 'goodtime': 1, 'l8rs': 1, '08712402779': 1, 'shun': 1, 'exhibition': 1, 'glass': 1, 'bian': 1, 'nino': 1, 'himself': 1, 'el': 1, 'downstem': 1, '08718730555': 1, 'wahala': 1, 'insects': 1, 'listening2the': 1, 'evil': 1, 'plumbing': 1, 'leafcutter': 1, 'inperialmusic': 1, 'molested': 1, 'acid': 1, 'remixed': 1, 'didntgive': 1, 'jenxxx': 1, 'thepub': 1, 'bellearlier': 1, 'bedbut': 1, 'uwana': 1, '09096102316': 1, 'cheery': 1, 'weirdo': 1, 'profiles': 1, 'stalk': 1, 'pax': 1, 'deposit': 1, '95': 1, 'jap': 1, 'disappeared': 1, 'certificate': 1, 'publish': 1, 'wheellock': 1, 'destination': 1, 'fifty': 1, 'happenin': 1, 'settling': 1, 'ipads': 1, 'worthless': 1, 'novelty': 1, 'cocksuckers': 1, 'janx': 1, 'dads': 1, 'developer': 1, 'designation': 1, 'musicnews': 1, 'videosounds': 1, '09701213186': 1, 'videosound': 1, 'spirit': 1, 'shattered': 1, 'girlie': 1, 'darker': 1, 'styling': 1, 'listn': 1, 'gray': 1, 'watevr': 1, 'paragraphs': 1, 'minus': 1, 'coveragd': 1, 'vasai': 1, 'retard': 1, 'bathroom': 1, 'sang': 1, 'uptown': 1, '80': 1, 'icic': 1, 'syria': 1, 'gauge': 1, 'completing': 1, 'ax': 1, 'unfolds': 1, 'emergency': 1, 'surgical': 1, 'korean': 1, 'fredericksburg': 1, 'pases': 1, 'que': 1, 'buen': 1, 'tiempo': 1, 'compass': 1, 'way2sms': 1, 'gnun': 1, 'youuuuu': 1, 'misss': 1, 'baaaaabe': 1, 'convince': 1, 'witot': 1, 'buyer': 1, 'undrstndng': 1, 'suffer': 1, 'becz': 1, 'avoids': 1, 'steamboat': 1, 'forgive': 1, 'tp': 1, '6ish': 1, 'bbq': 1, 'panicks': 1, 'everyso': 1, 'types': 1, 'nick': 1, 'auntie': 1, 'huai': 1, 'path': 1, 'paths': 1, 'appear': 1, 'thirunelvali': 1, 'reserve': 1, 'tackle': 1, 'tonght': 1, 'ironing': 1, 'pile': 1, 'chinky': 1, 'ploughing': 1, 'wi': 1, 'nz': 1, 'aust': 1, 'recharged': 1, 'papa': 1, 'detailed': 1, 'losers': 1, 'beta': 1, 'noncomittal': 1, 'snickering': 1, 'chords': 1, 'win150ppmx3age16': 1, 'boyf': 1, 'interviw': 1, 'entire': 1, 'determine': 1, 'spreadsheet': 1, 'trebles': 1, 'dartboard': 1, 'doubles': 1, 'coat': 1, 'recognises': 1, 'wisheds': 1, 'duo': 1, 'intrepid': 1, 'breeze': 1, 'fresh': 1, 'twittering': 1, 'chinchillas': 1, 'ducking': 1, 'function': 1, 'headstart': 1, 'rummer': 1, 'flying': 1, 'charts': 1, 'bbc': 1, 'optin': 1, 'thanks2': 1, 'rajini': 1, 'summers': 1, 'matched': 1, 'help08714742804': 1, 'spys': 1, '09099725823': 1, 'offering': 1, 'meow': 1, 'edhae': 1, 'bilo': 1, 'innu': 1, 'astne': 1, 'vargu': 1, 'lyfu': 1, 'halla': 1, 'yalru': 1, 'lyf': 1, 'mundhe': 1, 'ali': 1, 'ovr': 1, 'prone': 1, 'usa': 1, 'msgrcvd18': 1, '07801543489': 1, 'latests': 1, 'llc': 1, 'permission': 1, '09099726395': 1, 'meetins': 1, 'cumin': 1, 'lucy': 1, 'tablet': 1, 'dose': 1, 'incomm': 1, 'maps': 1, 'concentrating': 1, 'tiring': 1, 'browsin': 1, 'compulsory': 1, 'investigate': 1, 'vitamin': 1, 'crucial': 1, '2channel': 1, 'psychic': 1, 'jsco': 1, 'skills': 1, 'leadership': 1, 'host': 1, 'systems': 1, 'linux': 1, 'based': 1, 'idps': 1, 'converter': 1, 'sayy': 1, 'leanne': 1, 'disc': 1, 'glasgow': 1, 'champ': 1, 'lovin': 1, 'browse': 1, 'artists': 1, 'install': 1, 'speling': 1, 'corect': 1, '4719': 1, '523': 1, 'employee': 1, 'cts': 1, 'nike': 1, 'sooo': 1, 'shouting': 1, 'dang': 1, 'earliest': 1, 'nordstrom': 1, 'conference': 1, 'degree': 1, 'bleak': 1, 'shant': 1, 'nearer': 1, 'raiden': 1, 'totes': 1, 'cardin': 1, 'pierre': 1, 'establish': 1, 'rhythm': 1, 'truro': 1, 'ext': 1, 'cloth': 1, 'sunroof': 1, 'blanked': 1, 'image': 1, 'kalainar': 1, 'thenampet': 1, 'freaked': 1, 'reacting': 1, 'nosy': 1, 'imposter': 1, 'destiny': 1, 'satanic': 1, 'pudunga': 1, 'chef': 1, 'exterminator': 1, 'pest': 1, 'aaniye': 1, 'sympathetic': 1, 'venaam': 1, 'psychologist': 1, 'athletic': 1, 'determined': 1, 'companion': 1, 'stylist': 1, 'healer': 1, 'courageous': 1, 'organizer': 1, 'dependable': 1, 'listener': 1, 'psychiatrist': 1, 'chez': 1, 'jules': 1, 'nig': 1, 'hhahhaahahah': 1, 'leonardo': 1, 'dime': 1, 'strain': 1, '2years': 1, 'withdraw': 1, 'anyhow': 1, 'millers': 1, 'rawring': 1, 'xoxo': 1, 'spark': 1, 'flame': 1, 'crushes': 1, 'somewhr': 1, 'honeymoon': 1, 'outfit': 1, '08719899230': 1, 'cheque': 1, 'olympics': 1, 'leo': 1, 'haul': 1, 'want2come': 1, 'wizzle': 1, 'wildlife': 1, 'that2worzels': 1, 'cya': 1, 'shanghai': 1, '645': 1, 'redeemable': 1, 'rt': 1, '08701237397': 1, 'pro': 1, 'thnx': 1, 'anjie': 1, 'sef': 1, 'fring': 1, 'nte': 1, '526': 1, 'bx': 1, '02072069400': 1, 'talents': 1, 'animal': 1, 'warming': 1, 'shiny': 1, 'fooled': 1, 'french': 1, 'responsible': 1, 'companies': 1, 'guarantee': 1, 'suppliers': 1, '0a': 1, 'lnly': 1, 'keen': 1, 'dammit': 1, 'wright': 1, 'wrecked': 1, 'somewhat': 1, 'laden': 1, 'goodevening': 1, 'spontaneously': 1, 'rgent': 1, 'busty': 1, 'daytime': 1, '09099726429': 1, 'janinexx': 1, 'spageddies': 1, 'fourth': 1, 'dimension': 1, 'phasing': 1, 'compromised': 1, 'meaningful': 1, '09050001295': 1, 'a21': 1, '391784': 1, 'mobsi': 1, 'dub': 1, 'je': 1, 'toughest': 1, 'unspoken': 1, 'squatting': 1, 'digits': 1, '0089': 1, '09063442151': 1, 'sonathaya': 1, 'soladha': 1, 'raping': 1, 'dudes': 1, 'weightloss': 1, 'embarrassed': 1, 'mushy': 1, 'stash': 1, 'priya': 1, 'kilos': 1, 'accidant': 1, 'tookplace': 1, 'ghodbandar': 1, 'slovely': 1, 'sc': 1, 'wad': 1, 'specialise': 1, 'desparately': 1, 'mi': 1, 'stereo': 1, 'classmates': 1, 'fires': 1, 'vipclub4u': 1, 'trackmarque': 1, 'missionary': 1, 'entertaining': 1, 'hugh': 1, 'stick': 1, 'laurie': 1, 'praps': 1, 'jon': 1, 'dinero': 1, 'spain': 1, '000pes': 1, 'complaining': 1, 'mandy': 1, '09041940223': 1, 'transferred': 1, 'fm': 1, 'hotmix': 1, 'sullivan': 1, 'finn': 1, 'thew': 1, 'theacusations': 1, 'iwana': 1, 'wotu': 1, 'haventcn': 1, 'downon': 1, 'itxt': 1, 'nething': 1, 'dine': 1, '09111030116': 1, 'conacted': 1, 'pobox12n146tf15': 1, 'inspection': 1, 'nursery': 1, 'becomes': 1, 'panren': 1, 'paru': 1, 'trainners': 1, 'carryin': 1, 'bac': 1, 'chuckin': 1, 'dhanush': 1, 'needing': 1, 'habba': 1, 'dileep': 1, 'venugopal': 1, 'muchand': 1, 'mentioned': 1, 'remembrs': 1, 'everytime': 1, 'edition': 1, 'algorithms': 1, 'textbook': 1, '3230': 1, 'cro1327': 1, '09064018838': 1, 'iwas': 1, 'urmom': 1, 'careabout': 1, 'itried2tell': 1, 'marine': 1, 'intend': 1, 'learned': 1, 'traveling': 1, 'honest': 1, 'afghanistan': 1, 'stable': 1, 'iraq': 1, '50award': 1, '1225': 1, 'pai': 1, 'seh': 1, 'parts': 1, 'walsall': 1, 'terry': 1, 'tue': 1, 'ccna': 1, 'shrek': 1, 'fellow': 1, 'dying': 1, 'lifting': 1, 'teresa': 1, 'aid': 1, 'ld': 1, 'bam': 1, 'dec': 1, 'usmle': 1, 'squishy': 1, 'mwahs': 1, 'hottest': 1, 'prominent': 1, 'cheek': 1, 'september': 1, 'bcm': 1, 'neo69': 1, 'backdoor': 1, '8027': 1, 'hack': 1, 'subscribe': 1, 'fraction': 1, 'dps': 1, '09050280520': 1, 'comingdown': 1, 'murali': 1, 'sts': 1, 'kissing': 1, 'elliot': 1, 'mia': 1, 'engalnd': 1, 'd3wv': 1, '100txt': 1, '2price': 1, 'matric': 1, '650': 1, '850': 1, '08718726970': 1, 'payments': 1, 'fedex': 1, 'reception': 1, 'consensus': 1, 'entertain': 1, 'pillows': 1, 'strewn': 1, 'bras': 1, 'tag': 1, 'exposes': 1, 'weaknesses': 1, 'knee': 1, 'pulls': 1, 'wicked': 1, 'supports': 1, 'srt': 1, 'ps3': 1, 'jontin': 1, 'banned': 1, 'biro': 1, '09058094594': 1, 'unconsciously': 1, 'unhappy': 1, 'shell': 1, 'jog': 1, '09061743811': 1, 'lark': 1, 'sic': 1, '0870753331018': 1, '7mp': 1, '09090900040': 1, 'extreme': 1, 'wild': 1, 'fones': 1, 'stop2stop': 1, 'lim': 1, 'parachute': 1, 'placed': 1, 'lambda': 1, 'snowball': 1, 'angels': 1, 'ello': 1, 'duffer': 1, 'ofice': 1, 'pharmacy': 1, 'grr': 1, 'cnl': 1, '08715500022': 1, 'rpl': 1, 'nor': 1, 'fffff': 1, 'lifebook': 1, 'zhong': 1, 'qing': 1, 'act': 1, 'hypertension': 1, 'annoyin': 1, '08702490080': 1, 'vpod': 1, 'nigro': 1, 'scratching': 1, 'anyplaces': 1, 'priority': 1, 'ecstasy': 1, 'minded': 1, '09090204448': 1, 'ls278bb': 1, 'minapn': 1, 'hittng': 1, 'reflex': 1, 'egbon': 1, 'adewale': 1, 'mary': 1, 'deduct': 1, 'wrks': 1, 'asshole': 1, 'monkey': 1, 'grab': 1, 'sliding': 1, '09065394973': 1, 'payback': 1, 'tescos': 1, 'bowa': 1, 'feathery': 1, 'infra': 1, 'gep': 1, 'fifa': 1, '2006': 1, 'shhhhh': 1, 'arul': 1, 'related': 1, 'amk': 1, '09061743810': 1, 'length': 1, 'corrct': 1, 'antha': 1, 'dane': 1, 'basket': 1, 'rupaul': 1, 'curtsey': 1, 'practising': 1, '4my': 1, 'havebeen': 1, '2i': 1, 'feelingood': 1, 'ordinator': 1, 'preschoolco': 1, 'rise': 1, 'havbeen': 1, 'payed2day': 1, 'memory': 1, 'converted': 1, 'soil': 1, 'african': 1, 'outreach': 1, 'roles': 1, '8lb': 1, 'brilliantly': 1, '7oz': 1, 'forwarding': 1, 'visitors': 1, 'intention': 1, 'bend': 1, 'rules': 1, 'thia': 1, 'inlude': 1, 'previews': 1, 'madurai': 1, 'marrge': 1, 'dha': 1, 'ambrith': 1, 'kitty': 1, 'shaved': 1, 'tactful': 1, 'pert': 1, 'crammed': 1, 'satsgettin': 1, '47per': 1, 'apologize': 1, 'pei': 1, 'subtoitles': 1, 'jot': 1, 'cereals': 1, 'gari': 1, 'bold2': 1, '1er': 1, 'm60': 1, 'cast': 1, 'aom': 1, '09094100151': 1, 'gbp5': 1, 'box61': 1, 'thkin': 1, 'resubbing': 1, 'shadow': 1, 'breadstick': 1, 'saeed': 1, '09066362220': 1, 'purple': 1, 'brown': 1, 'yelow': 1, 'arranging': 1, 'eldest': 1, 'drugdealer': 1, 'wither': 1, '23f': 1, '23g': 1, 'wondarfull': 1, 'txt250': 1, 'web2mobile': 1, 'txtx': 1, 'box139': 1, 'la32wu': 1, 'onbus': 1, 'donyt': 1, 'latelyxxx': 1, '85233': 1, 'endof': 1, 'justthought': 1, '2hook': 1, 'uwant': 1, 'offdam': 1, 'nevamind': 1, 'sayhey': 1, 'stressed': 1, 'provider': 1, 'soooo': 1, 'tming': 1, 'cutest': 1, 'dice': 1, '08700469649': 1, 'box420': 1, 'mathe': 1, 'samachara': 1, 'howda': 1, 'autocorrect': 1, 'audrie': 1, 'readiness': 1, 'simulate': 1, 'lara': 1, 'supplies': 1, 'attach': 1, 'guesses': 1, '087123002209am': 1, 'nickey': 1, 'nobbing': 1, 'platt': 1, 'washob': 1, 'sterling': 1, 'spotty': 1, 'province': 1, 'hall': 1, 'hesitation': 1, 'ponnungale': 1, 'intha': 1, 'ipaditan': 1, 'rejected': 1, 'noisy': 1, 'needa': 1, 'reset': 1, 'troubleshooting': 1, 'manual': 1, 'b4utele': 1, 'marsms': 1, 'b4u': 1, '08717168528': 1, 'stifled': 1, 'creativity': 1, 'strongly': 1, 'requirements': 1, '2getha': 1, 'qlynnbv': 1, 'help08700621170150p': 1, 'buffy': 1, 'nosh': 1, 'lololo': 1, 'waaaat': 1, 'occupied': 1, 'documents': 1, 'stapati': 1, 'submitted': 1, 'hills': 1, 'cutie': 1, 'honesty': 1, 'beggar': 1, 'shakara': 1, 'specialisation': 1, 'labor': 1, 'dent': 1, 'crickiting': 1, 'isv': 1, 'urgoin': 1, 'tome': 1, 'reallyneed': 1, '2docd': 1, 'dontignore': 1, 'mycalls': 1, 'imin': 1, 'outl8r': 1, 'dontmatter': 1, 'thecd': 1, 'dontplease': 1, 'yavnt': 1, 'ibuprofens': 1, 'popping': 1, 'sip': 1, 'grown': 1, 'chinatown': 1, 'claypot': 1, 'beehoon': 1, 'fishhead': 1, 'yam': 1, 'porridge': 1, 'jaklin': 1, 'cliffs': 1, 'nearby': 1, 'bundle': 1, 'mf': 1, 'deals': 1, '49': 1, 'avble': 1, '4got': 1, 'weds': 1, 'moseley': 1, 'ooh': 1, 'thankyou': 1, 'ternal': 1, 'ntimate': 1, 'namous': 1, 'aluable': 1, 'atural': 1, 'oble': 1, 'ffectionate': 1, 'oveable': 1, 'ruthful': 1, 'textin': 1, 'burn': 1, 'amigos': 1, 'progress': 1, 'weren': 1, 'tryin': 1, 'collages': 1, 'arty': 1, '2hrs': 1, 'waliking': 1, 'cartons': 1, 'shelves': 1, '08714712379': 1, 'mirror': 1, '09065069120': 1, 'k718': 1, 'keris': 1, 'smidgin': 1, 'jod': 1, 'intentions': 1, 'accordin': 1, 'knocking': 1, 'como': 1, 'abel': 1, 'listened2the': 1, 'air1': 1, 'braindance': 1, 'plaid': 1, 'ofstuff': 1, 'hilarious': 1, 'hav2hear': 1, 'aphex': 1, 'nelson': 1, 'unmits': 1, 'newspapers': 1, 'yummmm': 1, 'puzzeles': 1, 'scammers': 1, '4goten': 1, '09099726481': 1, 'passion': 1, 'dena': 1, '09065069154': 1, 'r836': 1, 'shifad': 1, 'raised': 1, 'doctors': 1, 'reminds': 1, 'tolerat': 1, 'bcs': 1, 'subscrition': 1, 'splashmobile': 1, 'dust': 1, '88877': 1, '3pound': 1, 'watchin': 1, 'meaningless': 1, 'jones': 1, 'brdget': 1, 'inever': 1, 'hype': 1, 'studio': 1, 'velly': 1, 'marking': 1, '2stoptx': 1, '08718738034': 1, 'va': 1, 'hanger': 1, 'arrow': 1, 'blanket': 1, '08718726971': 1, 'tddnewsletter': 1, 'dozens': 1, 'thedailydraw': 1, 'emc1': 1, 'prizeswith': 1, 'significant': 1, 'waqt': 1, 'ko': 1, 'pehle': 1, 'wo': 1, 'naseeb': 1, 'jeetey': 1, 'nahi': 1, 'kisi': 1, 'kuch': 1, 'jo': 1, 'zyada': 1, 'milta': 1, 'hum': 1, 'zindgi': 1, 'sochte': 1, 'stalking': 1, 'reminded': 1, 'elaya': 1, 'varaya': 1, '09066368753': 1, '97n7qp': 1, 'anand': 1, 'expected': 1, 'beach': 1, 'workand': 1, 'jez': 1, 'whilltake': 1, 'todo': 1, 'zogtorius': 1, 'financial': 1, 'alian': 1, 'or2optout': 1, 'hv9d': 1, 'century': 1, 'frwd': 1, 'posible': 1, 'affectionate': 1, 'sorts': 1, 'restrictions': 1, 'buddys': 1, '08712402902': 1, 'owned': 1, 'possessive': 1, 'clarification': 1, 'coimbatore': 1, 'stream': 1, '0871212025016': 1, 'monos': 1, 'monoc': 1, 'polyc': 1, 'categories': 1, 'transcribing': 1, 'ethnicity': 1, 'census': 1, 'cakes': 1, 'draws': 1, 'asusual': 1, 'franyxxxxx': 1, 'goodmate': 1, 'cheered': 1, 'batt': 1, 'pobox1': 1, 'becausethey': 1, 'w14rg': 1, '09058098002': 1, 'gained': 1, 'limits': 1, 'pressure': 1, 'doke': 1, 'laying': 1, 'neshanth': 1, 'byatch': 1, 'whassup': 1, 'cl': 1, 'slo': 1, '4msgs': 1, 'filthyguys': 1, 'chiong': 1, 'reltnship': 1, 'wipe': 1, 'dialogue': 1, 'dryer': 1, 'comb': 1, 'pose': 1, 'fps': 1, 'computational': 1, 'disturbance': 1, 'premarica': 1, 'dlf': 1, 'gotto': 1, '220cm2': 1, 'err': 1, 'bloo': 1, 'hitter': 1, 'offline': 1, 'anjola': 1, 'asjesus': 1, 'ki': 1, 'imf': 1, 'corrupt': 1, 'deposited': 1, 'projects': 1, 'pura': 1, 'karo': 1, 'crore': 1, 'padhe': 1, 'suply': 1, 'lac': 1, 'blocked': 1, 'itna': 1, 'directors': 1, 'politicians': 1, 'swiss': 1, 'taxless': 1, 'torrents': 1, 'slowing': 1, 'particularly': 1, 'commit': 1, '83370': 1, 'trivia': 1, 'rightio': 1, 'brum': 1, 'scorable': 1, 'paranoid': 1, 'sheet': 1, 'brin': 1, 'bsnl': 1, 'complain': 1, 'offc': 1, 'bettr': 1, 'payed': 1, 'suganya': 1, 'dessert': 1, 'abeg': 1, 'sponsors': 1, 'onum': 1, 'imagination': 1, 'poet': 1, 'rr': 1, 'famamus': 1, 'locks': 1, 'jenne': 1, 'easiest': 1, 'barcelona': 1, 'sppok': 1, 'complementary': 1, '2px': 1, 'wa14': 1, 'pansy': 1, 'jungle': 1, 'kanji': 1, 'srs': 1, 'drizzling': 1, 'appointments': 1, 'excused': 1, 'reppurcussions': 1, 'necessity': 1, 'drama': 1, 'struggling': 1, 'ego': 1, 'cosign': 1, '09061701444': 1, 'hvae': 1, 'requires': 1, 'suman': 1, 'telephonic': 1, 'hcl': 1, 'freshers': 1, 'reliant': 1, 'fwiw': 1, 'afford': 1, 'arrival': 1, 'sq825': 1, 'citylink': 1, 'props': 1, 'statements': 1, 'pleasant': 1, 'pobox114': 1, '14tcr': 1, '6230': 1, 'splendid': 1, 'bognor': 1, 'ktv': 1, 'misplaced': 1, 'computers': 1, 'permanent': 1, 'registration': 1, 'begun': 1, 'residency': 1, 'risks': 1, 'predicting': 1, 'accumulation': 1, 'programs': 1, 'grief': 1, 'belongs': 1, 'shoranur': 1, 'prior': 1, 'fuelled': 1, 'fated': 1, 'concern': 1, 'txt82228': 1, 'text82228': 1, 'promptly': 1, 'honestly': 1, 'burnt': 1, 'quizclub': 1, '80122300p': 1, 'rwm': 1, '08704050406': 1, 'snap': 1, 'connected': 1, 'gmw': 1, 'someplace': 1, 'goods': 1, 'pressies': 1, 'ultimately': 1, 'achieve': 1, 'motive': 1, 'tui': 1, 'tor': 1, 'korli': 1, 'dock': 1, 'newscaster': 1, 'rolled': 1, 'flute': 1, 'wheel': 1, 'dabbles': 1, 'picsfree1': 1, 'keyword': 1, 'the4th': 1, 'october': 1, '83435': 1, 'safety': 1, 'aspects': 1, 'elaborating': 1, '85555': 1, 'tarot': 1, 'ours': 1, 'horniest': 1, 'cysts': 1, 'flow': 1, 'shrink': 1, 'ovarian': 1, 'developed': 1, 'grams': 1, 'upping': 1, 'timin': 1, 'apes': 1, 'ibm': 1, 'hp': 1, 'gosh': 1, 'spose': 1, 'rimac': 1, 'dosomething': 1, 'arestaurant': 1, 'squid': 1, 'dabooks': 1, 'eachother': 1, 'luckily': 1, 'starring': 1, 'restocked': 1, 'stoptxtstop': 1, 'knock': 1, 'tkls': 1, 'challenging': 1, 'smoothly': 1, 'breakfast': 1, 'hamper': 1, 'cc100p': 1, 'above': 1, '0870737910216yrs': 1, 'unni': 1, 'dramastorm': 1, 'particular': 1, 'lacking': 1, 'forfeit': 1, 'coupla': 1, 'digi': 1, '077xxx': 1, '09066362206': 1, 'sundayish': 1, 'prasad': 1, 'rcb': 1, 'kochi': 1, 'smear': 1, 'checkup': 1, 'gobi': 1, '4w': 1, 'technologies': 1, 'olowoyey': 1, 'argentina': 1, 'taxt': 1, 'lool': 1, 'tie': 1, 'massage': 1, 'pos': 1, 'shaking': 1, 'scarcasim': 1, 'naal': 1, 'eruku': 1, 'w4': 1, '5wq': 1, 'sensible': 1, 'impressively': 1, 'obedient': 1, 'ft': 1, 'combination': 1, 'needy': 1, 'playng': 1, 'jorge': 1, 'mcfly': 1, 'sara': 1, 'ab': 1, 'yupz': 1, 'ericson': 1, 'luks': 1, 'modl': 1, 'der': 1, 'frosty': 1, 'cheesy': 1, 'witin': 1, '0870141701216': 1, '120p': 1, 'fans': 1, '09050000555': 1, 'ba128nnfwfly150ppm': 1, '10th': 1, 'themed': 1, 'nudist': 1, 'pump': 1, 'signal': 1, 'unusual': 1, 'palm': 1, 'handing': 1, 'printing': 1, '83021': 1, 'stated': 1, 'perpetual': 1, 'dd': 1, 'flung': 1, 'pract': 1, 'brains': 1, 'justbeen': 1, 'overa': 1, 'mush': 1, 'tunde': 1, 'missions': 1, '20m12aq': 1, 'eh74rr': 1, 'avo': 1, 'cuddled': 1, 'crashed': 1, 'chachi': 1, 'pl': 1, 'tiz': 1, 'kanagu': 1, 'prices': 1, 'ringing': 1, 'houseful': 1, 'pulling': 1, 'brats': 1, 'derp': 1, 'abusers': 1, 'lipo': 1, 'netflix': 1, 'clash': 1, 'arr': 1, 'oscar': 1, 'rebtel': 1, 'firefox': 1, 'bcmsfwc1n3xx': 1, '69969': 1, 'impressed': 1, 'funs': 1, 'footy': 1, 'coca': 1, 'stadium': 1, 'cola': 1, 'large': 1, 'teenager': 1, 'replacing': 1, 'paracetamol': 1, 'mittelschmertz': 1, 'arrived': 1, 'references': 1, 'cthen': 1, 'conclusion': 1, 'instant': 1, '08715203028': 1, '9th': 1, 'rugby': 1, 'courtroom': 1, 'twiggs': 1, 'affidavit': 1, 'showers': 1, 'possessiveness': 1, 'golden': 1, 'poured': 1, 'lasting': 1, 'mobs': 1, 'ymca': 1, 'crazyin': 1, 'sleepingwith': 1, 'finest': 1, 'breathe1': 1, 'pobox365o4w45wq': 1, 'wtc': 1, 'weiyi': 1, '505060': 1, 'flowers': 1, 'interflora': 1, 'paining': 1, 'outgoing': 1, 'romcapspam': 1, 'presence': 1, 'mee': 1, 'maggi': 1, '08712103738': 1, 'cough': 1, 'pooja': 1, 'sweatter': 1, 'ambitious': 1, 'miiiiiiissssssssss': 1, 'tunji': 1, 'frndz': 1, '6missed': 1, 'misscall': 1, 'mad1': 1, 'mad2': 1, 'tall': 1, 'avenge': 1, 'robs': 1, 'gudni8': 1, 'choices': 1, 'toss': 1, 'coin': 1, 'dancin': 1, 'nora': 1, 'explicitly': 1, 'gayle': 1, 'crucify': 1, 'butting': 1, 'vs': 1, 'cedar': 1, 'reserved': 1, 'durham': 1, '69855': 1, 'sf': 1, 'stopbcm': 1, 'wall': 1, 'groovying': 1, 'printer': 1, 'groovy': 1, 'acnt': 1, 'harish': 1, 'transfred': 1, 'shaping': 1, 'showrooms': 1, 'attending': 1, 'doinat': 1, 'callon': 1, 'yhl': 1, 'pdate_now': 1, 'configure': 1, 'anal': 1, 'pears': 1, 'such': 1, 'oooooh': 1, '09058094454': 1, 'expiry': 1, 'resubmit': 1, 'mint': 1, 'humans': 1, 'studyn': 1, 'everyboy': 1, 'xxxxxxxx': 1, 'answr': 1, '1thing': 1, 'liquor': 1, 'loko': 1, '730': 1, 'lined': 1, 'laughs': 1, 'fireplace': 1, 'icon': 1, '08712400200': 1, 'weasels': 1, 'fifth': 1, 'woozles': 1, '08718723815': 1, 'machines': 1, 'fucks': 1, 'ignorant': 1, 'mys': 1, 'downs': 1, 'fletcher': 1, '08714714011': 1, 'bowls': 1, 'cozy': 1, 'shake': 1, 'buzzzz': 1, 'vibrator': 1, 'pros': 1, 'jet': 1, 'nuclear': 1, 'cons': 1, 'trends': 1, 'brief': 1, 'iter': 1, 'description': 1, 'fusion': 1, 'shitinnit': 1, 'ikno': 1, 'nowhere': 1, 'doesdiscount': 1, 'jabo': 1, 'slower': 1, 'maniac': 1, 'manege': 1, 'swalpa': 1, 'hogidhe': 1, 'chinnu': 1, 'sapna': 1, 'agidhane': 1, 'footbl': 1, 'crckt': 1, 'swell': 1, 'bollox': 1, 'tim': 1, 'tol': 1, 'ingredients': 1, 'pocy': 1, 'non': 1, '4qf2': 1, 'senor': 1, 'person2die': 1, 'possibly': 1, 'nvq': 1, 'giggle': 1, 'professional': 1, 'tiger': 1, 'woods': 1, 'grinder': 1, 'buyers': 1, 'figuring': 1, 'entirely': 1, 'disconnected': 1, 'onluy': 1, 'offcampus': 1, 'matters': 1, 'riley': 1, 'ew': 1, 'wesley': 1, 'lingo': 1, '400mins': 1, 'j5q': 1, 'chrgd': 1, '69200': 1, '2exit': 1, 'independence': 1, 'afternoons': 1, 'ugadi': 1, 'sankranti': 1, 'dasara': 1, 'rememberi': 1, 'teachers': 1, 'republic': 1, 'fools': 1, 'festival': 1, 'childrens': 1, 'approaching': 1, 'shivratri': 1, 'mornings': 1, 'joys': 1, 'greeting': 1, 'somewheresomeone': 1, 'daywith': 1, 'tosend': 1, 'lifeis': 1, 'selflessness': 1, 'initiate': 1, 'tallent': 1, 'wasting': 1, 'portal': 1, 't4get2text': 1, 'lennon': 1, 'bothering': 1, 'crab': 1, 'fox': 1, 'waves': 1, 'footprints': 1, 'frndsship': 1, 'dwn': 1, 'summon': 1, 'slaaaaave': 1, 'appendix': 1, 'slob': 1, 'smiled': 1, 'webpage': 1, 'yeesh': 1, 'gotbabes': 1, 'subscriptions': 1, 'hunks': 1, 'unsubscribed': 1, 'participate': 1, 'gopalettan': 1, 'stopcost': 1, '08712400603': 1, 'abroad': 1, 'xxsp': 1, 'mat': 1, 'agent': 1, 'goodies': 1, 'ay': 1, 'steal': 1, 'isaiah': 1, 'expert': 1, 'thinl': 1, 'importantly': 1, 'tightly': 1, 'fals': 1, 'pretsovru': 1, 'nav': 1, 'wnevr': 1, 'vth': 1, 'pretsorginta': 1, 'yen': 1, 'fal': 1, 'alwa': 1, 'madodu': 1, 'nammanna': 1, 'stdtxtrate': 1, 'soundtrack': 1, 'lord': 1, 'rings': 1, 'pc1323': 1, 'phyhcmk': 1, 'sg': 1, 'emigrated': 1, 'hopeful': 1, 'olol': 1, 'victors': 1, 'winterstone': 1, 'stagwood': 1, 'jp': 1, 'mofo': 1, 'maraikara': 1, 'pathaya': 1, 'enketa': 1, 'priest': 1, 'reserves': 1, 'intrude': 1, 'walkabout': 1, 'cashed': 1, 'announced': 1, 'blog': 1, '28th': 1, 'neville': 1, 'footie': 1, 'phil': 1, 'abbey': 1, 'returning': 1, 'punj': 1, 'str8': 1, 'classic': 1, '200p': 1, 'sacked': 1, 'mmsto': 1, '35p': 1, 'lookatme': 1, 'clip': 1, '32323': 1, 'twat': 1, 'punch': 1, 'barred': 1, 'decking': 1, 'dungerees': 1, 'mentionned': 1, 'vat': 1, 'grl': 1, 'madstini': 1, 'eerulli': 1, 'agalla': 1, 'kodstini': 1, 'hogli': 1, 'kodthini': 1, 'mutai': 1, 'hogolo': 1, 'messed': 1, 'illspeak': 1, 'thasa': 1, 'shudvetold': 1, 'urgran': 1, 'u2moro': 1, 'updat': 1, 'okden': 1, 'likeyour': 1, 'mecause': 1, 'werebored': 1, 'countinlots': 1, 'uin': 1, 'tex': 1, 'gr8fun': 1, 'tagged': 1, 'hdd': 1, 'casing': 1, 'opened': 1, 'describe': 1, '140ppm': 1, '08718725756': 1, '09053750005': 1, '310303': 1, 'asus': 1, 'reformat': 1, 'plumbers': 1, 'wrench': 1, 'bcum': 1, 'appeal': 1, 'thriller': 1, 'director': 1, 'shove': 1, 'um': 1, 'elephant': 1, 'cr': 1, 'pookie': 1, 'nri': 1, 'x2': 1, 'deserve': 1, 'neighbor': 1, 'toothpaste': 1, 'diddy': 1, 'poking': 1, 'coccooning': 1, 'mus': 1, 'talkin': 1, 'newquay': 1, '1im': 1, 'windy': 1, 'y87': 1, '09066358361': 1, 'tirunelvai': 1, 'dusk': 1, 'puzzles': 1, '09065989180': 1, 'x29': 1, 'phews': 1, 'stairs': 1, 'earning': 1, 'recycling': 1, 'toledo': 1, 'reservations': 1, 'tai': 1, 'feng': 1, 'swimsuit': 1, 'frndshp': 1, 'luvd': 1, 'squeeeeeze': 1, 'hurricanes': 1, 'disasters': 1, 'erupt': 1, 'sway': 1, 'aroundn': 1, 'arise': 1, 'volcanoes': 1, 'lighters': 1, 'lasagna': 1, 'woould': 1, 'chickened': 1, '08718726978': 1, '7732584351': 1, '44': 1, 'raviyog': 1, 'bhayandar': 1, 'peripherals': 1, 'sunoco': 1, 'musical': 1, 'leftovers': 1, 'plate': 1, 'starving': 1, 'fatty': 1, 'badrith': 1, 'owe': 1, 'checkin': 1, 'swann': 1, 'armenia': 1, '09058097189': 1, '1205': 1, '330': 1, '1120': 1, 'justify': 1, 'hunt': 1, 'hava': 1, '1131': 1, '5226': 1, 'adrian': 1, 'thnq': 1, 'rct': 1, 'vatian': 1, 'babysitting': 1, 'everyones': 1, 'buttheres': 1, 'ofsi': 1, 'aboutas': 1, 'gonnamissu': 1, 'yaxx': 1, 'breakin': 1, 'merememberin': 1, 'asthere': 1, 'neglet': 1, 'ramaduth': 1, 'siguviri': 1, 'mahaveer': 1, 'nalli': 1, 'ee': 1, 'problum': 1, 'dodda': 1, 'pavanaputra': 1, 'ondu': 1, 'keluviri': 1, 'maruti': 1, 'maretare': 1, 'poortiyagi': 1, 'bajarangabali': 1, 'sankatmochan': 1, 'hanumanji': 1, 'olage': 1, 'hanuman': 1, 'kalisidare': 1, 'odalebeku': 1, 'janarige': 1, 'idu': 1, 'inde': 1, 'ivatte': 1, 'matra': 1, 'ijust': 1, 'talked': 1, 'opps': 1, 'dl': 1, 'gei': 1, 'tron': 1, 'workage': 1, 'spiffing': 1, 'craving': 1, 'babysit': 1, 'supose': 1, 'embassy': 1, 'spaces': 1, 'lightly': 1, 'checkboxes': 1, 'batsman': 1, 'yetty': 1, '09050000928': 1, 'yifeng': 1, 'emailed': 1, 'slurp': 1, '3miles': 1, 'doll': 1, 'barolla': 1, 'brainless': 1, 'sariyag': 1, 'vehicle': 1, 'madoke': 1, '07090201529': 1, 'postponed': 1, 'stocked': 1, 'tiime': 1, 'afternon': 1, 'interviews': 1, 'resizing': 1, '09066364349': 1, 'box434sk38wp150ppm18': 1, 'opposed': 1, '08081263000': 1, '83332': 1, 'shortcode': 1, 'refunded': 1, 'somerset': 1, 'nigpun': 1, 'overtime': 1, 'dismissial': 1, 'screwd': 1, '08712402972': 1, 'bull': 1, 'floating': 1, '09058095201': 1, 'heehee': 1, 'percentages': 1, 'arithmetic': 1, 'chillaxin': 1, 'iknow': 1, 'peril': 1, 'das': 1, 'studentfinancial': 1, 'wellda': 1, 'monster': 1, 'obey': 1, 'uhhhhrmm': 1, 'deltomorrow': 1, '09066368470': 1, '24m': 1, 'subscriptn3gbp': 1, '68866': 1, '08448714184': 1, 'landlineonly': 1, 'smartcall': 1, 'orno': 1, 'minmobsmore': 1, 'fink': 1, 'lkpobox177hp51fl': 1, 'carlie': 1, 'promised': 1, '09099726553': 1, 'youwanna': 1, 'youphone': 1, 'athome': 1, 'jack': 1, 'hypotheticalhuagauahahuagahyuhagga': 1, 'pretend': 1, 'helpful': 1, 'brainy': 1, 'reflection': 1, 'occasion': 1, 'traditions': 1, 'values': 1, 'affections': 1, 'celebrated': 1, 'katexxx': 1, 'myparents': 1, 'cantdo': 1, 'anythingtomorrow': 1, 'outfor': 1, 'aretaking': 1, 'level': 1, 'gate': 1, '89105': 1, 'lingerie': 1, 'weddingfriend': 1, 'petticoatdreams': 1, 'bridal': 1, 'inst': 1, 'overheating': 1, 'board': 1, 'reslove': 1, 'western': 1, 'oblisingately': 1, 'bambling': 1, 'notixiquating': 1, 'champlaxigating': 1, 'laxinorficated': 1, 'opted': 1, 'masteriastering': 1, 'wotz': 1, 'atrocious': 1, 'entropication': 1, 'amplikater': 1, 'fidalfication': 1, 'junna': 1, 'knickers': 1, 'nikiyu4': 1, '01223585236': 1, 'divert': 1, 'a30': 1, 'wadebridge': 1, 'orc': 1, 'vill': 1, 'seeking': 1, 'wherre': 1, 'phone750': 1, 'resolution': 1, 'frank': 1, 'logoff': 1, 'parkin': 1, 'asa': 1, '09050000878': 1, 'charming': 1, 'served': 1, 'mention': 1, 'arnt': 1, 'xxxxxxxxxxxxxx': 1, 'dorothy': 1, 'kiefer': 1, 'allalo': 1, 'mone': 1, 'alle': 1, 'eppolum': 1, 'fundamentals': 1, 'whoever': 1, 'dooms': 1, '5digital': 1, '3optical': 1, '1mega': 1, 'pixels': 1, 'js': 1, 'noi': 1, 'captaining': 1, 'burgundy': 1, 'amrita': 1, 'bpo': 1, 'profile': 1, 'persevered': 1, 'nighters': 1, 'regretted': 1, 'spouse': 1, 'pmt': 1, 'shldxxxx': 1, '4give': 1, 'scenario': 1, 'nytho': 1, 'frmcloud': 1, 'spun': 1, '2mwen': 1, 'fonin': 1, 'tx': 1, 'wrld': 1, '09071517866': 1, '150ppmpobox10183bhamb64xe': 1, 'pounded': 1, 'broadband': 1, 'installation': 1, 'tensed': 1, 'coughing': 1, 'warned': 1, 'sprint': 1, 'gower': 1, 'morrow': 1, '450p': 1, 'filth': 1, '9yt': 1, 'stop2': 1, 'e14': 1, '08701752560': 1, 'saristar': 1, 'chik': 1, '420': 1, '9061100010': 1, 'mobcudb': 1, '1st4terms': 1, 'wire3': 1, 'sabarish': 1, '09050000460': 1, 'j89': 1, 'box245c2150pm': 1, 'flea': 1, 'inpersonation': 1, 'banneduk': 1, 'maximum': 1, 'highest': 1, '71': 1, 'hari': 1, 'wifes': 1, 'mumtaz': 1, 'facts': 1, 'taj': 1, 'arises': 1, 'known': 1, 'shahjahan': 1, 'lesser': 1, '69101': 1, 'rtf': 1, 'sphosting': 1, 'webadres': 1, 'geting': 1, 'passport': 1, 'independently': 1, 'showed': 1, 'multiply': 1, 'twins': 1, '02085076972': 1, 'strt': 1, 'ltdhelpdesk': 1, 'pesky': 1, 'cyclists': 1, 'uneventful': 1, 'equally': 1, 'nattil': 1, 'adi': 1, 'entey': 1, 'kittum': 1, 'hitman': 1, 'hire': 1, '2309': 1, '09066660100': 1, 'outages': 1, 'conserve': 1, 'cps': 1, 'voted': 1, 'epi': 1, 'bare': 1, 'bhaskar': 1, 'gong': 1, 'kaypoh': 1, 'outdoors': 1, 'basketball': 1, 'interfued': 1, 'listed': 1, 'apology': 1, 'harlem': 1, 'forth': 1, 'hustle': 1, 'fats': 1, 'workout': 1, 'zac': 1, 'hui': 1, 'versus': 1, 'underdtand': 1, 'locaxx': 1, 'muchxxlove': 1, '07090298926': 1, '9307622': 1, 'winds': 1, 'skateboarding': 1, 'thrown': 1, 'bandages': 1, 'html': 1, '1146': 1, 'mfl': 1, 'gbp4': 1, 'hectic': 1, 'dogs': 1, 'doggin': 1, 'wamma': 1, 'virtual': 1, 'apnt': 1, 'pants': 1, 'lanka': 1, 'go2sri': 1, 'gudnyt': 1, 'relationship': 1, 'wherevr': 1, 'merely': 1, 'smacks': 1, 'plum': 1, 'alot': 1, '50s': 1, 'formatting': 1, 'attracts': 1, '8714714': 1, 'promotion': 1, 'vegas': 1, 'lancaster': 1, 'soc': 1, 'advising': 1, 'bsn': 1, 'lobby': 1, 'showered': 1, 'ything': 1, 'lubly': 1, 'vewy': 1, '087147123779am': 1, 'catches': 1, 'domain': 1, 'specify': 1, 'nusstu': 1, 'bari': 1, 'hudgi': 1, 'yorge': 1, 'ertini': 1, 'pataistha': 1, 'hoops': 1, 'hasbro': 1, 'jump': 1, 'ummifying': 1, 'associate': 1, 'uterus': 1, 'rip': 1, 'jacuzzi': 1, 'txtstar': 1, 'uve': 1, '2nights': 1, 'wildest': 1, 'aldrine': 1, 'rtm': 1, 'unhappiness': 1, 'sources': 1, 'events': 1, 'functions': 1, 'irritated': 1, '4wrd': 1, 'colleg': 1, 'necesity': 1, 'wthout': 1, 'witout': 1, 'espe': 1, 'wth': 1, 'takecare': 1, 'univ': 1, 'rajas': 1, 'burrito': 1, 'stitch': 1, 'trouser': 1, '146tf150p': 1, 'cheetos': 1, 'synced': 1, 'shangela': 1, 'passes': 1, '08704439680': 1, 'poo': 1, 'uup': 1, 'gloucesterroad': 1, 'ouch': 1, 'fruit': 1, 'forgiveness': 1, 'glo': 1, '09058095107': 1, 's3xy': 1, 'wlcome': 1, 'timi': 1, 'strtd': 1, 'sack': 1, '1stone': 1, 'throwin': 1, 'fishrman': 1, 'stones': 1, '08717895698': 1, 'mobstorequiz10ppm': 1, 'physics': 1, 'delicious': 1, 'praveesh': 1, 'beers': 1, 'salad': 1, 'whore': 1, 'scallies': 1, 'twinks': 1, 'skins': 1, '08712466669': 1, 'jocks': 1, 'flood': 1, 'beads': 1, 'section': 1, 'nitro': 1, 'wishlist': 1, 'sold': 1, 'creative': 1, 'reffering': 1, 'getiing': 1, 'weirdy': 1, 'brownies': 1, '12hours': 1, 'k61': 1, '09061701851': 1, 'restrict': 1, '74355': 1, 'greece': 1, 'recorded': 1, 'someday': 1, 'grandfather': 1, 'november': 1, '75max': 1, 'blu': 1, '09061104276': 1, 'yuou': 1, 'spot': 1, 'lotto': 1, 'bunch': 1, 'purchases': 1, 'authorise': 1, '45pm': 1, 'goss': 1, 'gimmi': 1, 'ystrday': 1, 'chile': 1, 'subletting': 1, 'steering': 1, 'ammae': 1, 'sleeps': 1, 'required': 1, 'rounder': 1, 'batchlor': 1, 'lambu': 1, 'ji': 1, 'zoom': 1, '08717890890': 1, 'stopcs': 1, '62220cncl': 1, '0430': 1, 'true18': 1, '37819': 1, '1b6a5ecef91ff9': 1, 'jul': 1, 'chg': 1, 'cst': 1, 'xafter': 1, 'pure': 1, 'hearted': 1, 'enemies': 1, 'smiley': 1, 'yaxxx': 1, 'gail': 1, 'theoretically': 1, 'hooked': 1, 'formally': 1, 'multimedia': 1, 'housing': 1, 'accounting': 1, 'agency': 1, 'delayed': 1, 'renting': 1, 'vague': 1, 'presents': 1, 'nicky': 1, 'gumby': 1, 'wave': 1, '44345': 1, 'alto18': 1, 'sized': 1, 'springs': 1, 'tarpon': 1, 'steps': 1, 'cab': 1, 'limited': 1, 'hf8': 1, '08719181259': 1, 'radiator': 1, 'tongued': 1, 'proper': 1, 'shorts': 1, 'qi': 1, 'suddenly': 1, 'flurries': 1, 'webeburnin': 1, 'babygoodbye': 1, 'golddigger': 1, 'dontcha': 1, 'pushbutton': 1, 'real1': 1, 'perform': 1, 'cards': 1, 'rebooting': 1, 'nigh': 1, 'cable': 1, 'outage': 1, 'nooooooo': 1, 'sos': 1, 'playin': 1, 'guoyang': 1, 'rahul': 1, 'dengra': 1, 'fieldof': 1, 'selfindependence': 1, 'contention': 1, 'antelope': 1, 'toplay': 1, 'gnarls': 1, 'barkleys': 1, 'borderline': 1, '545': 1, 'nightnight': 1, 'possibility': 1, 'grooved': 1, 'mising': 1, '6669': 1, 'unsecured': 1, '195': 1, 'secured': 1, 'fakeye': 1, 'eckankar': 1, 'lanre': 1, '3000': 1, 'heater': 1, 'degrees': 1, 'dodgey': 1, 'asssssholeeee': 1, 'seing': 1, 'dreamz': 1, 'blokes': 1, 'rebel': 1, 'buddy': 1, 'ceri': 1, '84484': 1, 'nationwide': 1, 'newport': 1, 'juliana': 1, 'nachos': 1, 'dizzamn': 1, 'suitemates': 1, 'nimbomsons': 1, 'continent': 1, '087104711148': 1, 'emerging': 1, 'fiend': 1, 'impede': 1, 'hesitant': 1, '400thousad': 1, '60': 1, 'essay': 1, 'nose': 1, 'tram': 1, 'vic': 1, 'coherently': 1, 'echo': 1, 'triple': 1, 'cusoon': 1, 'afew': 1, 'honi': 1, 'onlyfound': 1, 'gran': 1, 'bx526': 1, 'southern': 1, 'rayan': 1, 'macleran': 1, 'balls': 1, 'olave': 1, 'mandara': 1, 'trishul': 1, 'woo': 1, 'hoo': 1, 'panties': 1, 'thout': 1, 'pints': 1, 'flatter': 1, 'carlin': 1, 'ciao': 1, 'starve': 1, 'impression': 1, 'darkness': 1, 'motivate': 1, 'wknd': 1, 'heltini': 1, 'trusting': 1, 'uttered': 1, 'yalrigu': 1, 'iyo': 1, 'noice': 1, 'esaplanade': 1, '139': 1, 'accessible': 1, '08709501522': 1, 'la3': 1, '2wu': 1, 'occurs': 1, 'enna': 1, 'kalaachutaarama': 1, 'prof': 1, 'coco': 1, 'sporadically': 1, 'pobox75ldns7': 1, '09064017305': 1, '38': 1, 'persolvo': 1, 'tbs': 1, 'manchester': 1, 'kath': 1, 'burden': 1, 'noworriesloans': 1, '08717111821': 1, 'harder': 1, 'nbme': 1, 'sickness': 1, 'villa': 1, 'sathya': 1, 'gam': 1, 'smash': 1, 'religiously': 1, 'tips': 1, 'heroes': 1, '08715203649': 1, '07973788240': 1, 'penny': 1, 'muhommad': 1, 'fiting': 1, 'load': 1, 'mj': 1, 'unconvinced': 1, 'willpower': 1, 'elaborate': 1, 'absence': 1, 'answerin': 1, 'evey': 1, 'prin': 1, '08714342399': 1, 'gigolo': 1, 'spam': 1, 'mens': 1, '50rcvd': 1, 'gsoh': 1, 'oncall': 1, 'mjzgroup': 1, 'ashwini': 1, '08707500020': 1, '09061790125': 1, 'ukp': 1, 'skinny': 1, 'casting': 1, 'thet': 1, 'elections': 1, '116': 1, 'serena': 1, 'amrca': 1, 'hlday': 1, 'camp': 1, 'prescribed': 1, 'meatballs': 1, 'approve': 1, 'panalam': 1, 'spjanuary': 1, 'fortune': 1, 'allday': 1, 'perf': 1, 'outsider': 1, '98321561': 1, 'familiar': 1, 'depression': 1, 'infact': 1, 'band': 1, 'simpsons': 1, 'kip': 1, 'shite': 1, 'hont': 1, 'upgrading': 1, 'amanda': 1, 'renewing': 1, 'subject': 1, 'regard': 1, 'nannys': 1, 'perspective': 1, 'puts': 1, 'conveying': 1, 'debating': 1, 'wtlp': 1, 'jb': 1, 'florida': 1, 'hidden': 1, 'teams': 1, 'swhrt': 1, 'po19': 1, '2ez': 1, '47': 1, '0906346330': 1, 'general': 1, 'jetton': 1, 'cmon': 1, 'replies': 1, 'lunsford': 1, 'enjoying': 1, '0796xxxxxx': 1, 'prizeawaiting': 1, 'gravy': 1, 'meals': 1, 'kfc': 1, '07008009200': 1, 'attended': 1, 'mw': 1, 'tuth': 1, 'eviction': 1, 'michael': 1, 'spiral': 1, 'riddance': 1, 'suffers': 1, 'cricket': 1, 'edward': 1, 'closeby': 1, 'raglan': 1, 'skye': 1, 'bookedthe': 1, 'hut': 1, 'drastic': 1, '3750': 1, 'garments': 1, 'sez': 1, 'evry1': 1, 'arab': 1, 'eshxxxxxxxxxxx': 1, 'lay': 1, 'bimbo': 1, 'ugo': 1, '3lions': 1, 'portege': 1, 'm100': 1, 'semiobscure': 1, 'gprs': 1, 'loosu': 1, 'careless': 1, 'freaking': 1, 'myspace': 1, 'logged': 1, 'method': 1, 'blur': 1, 'jewelry': 1, 'deluxe': 1, 'bbdeluxe': 1, 'features': 1, 'breaker': 1, 'graphics': 1, 'fumbling': 1, 'weekdays': 1, 'nails': 1, 'asia': 1, 'tobed': 1, '430': 1, 'stil': 1, 'asthma': 1, 'attack': 1, 'ball': 1, 'spin': 1, 'million': 1, 'haiyoh': 1, 'saves': 1, 'prsn': 1, 'sunlight': 1, 'relocate': 1, 'audiitions': 1, 'pocked': 1, 'motivating': 1, 'brison': 1, 'caps': 1, 'bullshit': 1, 'spelled': 1, 'motherfucker': 1, 'ig11': 1, '1013': 1, 'kit': 1, 'oja': 1, 'strip': 1, '08712402578': 1, 'thesmszone': 1, 'masked': 1, 'anonymous': 1, 'abuse': 1, 'parish': 1, 'magazine': 1, 'woodland': 1, 'avenue': 1, 'billy': 1, 'awww': 1, 'useless': 1, 'loo': 1, 'ed': 1, 'glands': 1, 'swollen': 1, 'bcaz': 1, 'truble': 1, 'evone': 1, 'hates': 1, 'stu': 1, 'view': 1, 'gays': 1, 'dual': 1, 'hostile': 1, 'breezy': 1, 'haircut': 1, '1tulsi': 1, 'leaf': 1, 'diseases': 1, 'litres': 1, 'watr': 1, 'problms': 1, '1apple': 1, '1lemon': 1, 'snd': 1, '1cup': 1, 'lavender': 1, 'manky': 1, 'inmind': 1, 'travelling': 1, 'scouse': 1, 'recreation': 1, 'judgemental': 1, 'fridays': 1, 'waheeda': 1, 'notes': 1, 'bot': 1, 'eventually': 1, 'hits': 1, 'tolerance': 1, '0789xxxxxxx': 1, 'hellogorgeous': 1, 'nitw': 1, 'texd': 1, 'jaz': 1, '4ward': 1, 'hopeu': 1, '09058091870': 1, 'exorcism': 1, 'emily': 1, 'prayrs': 1, 'othrwise': 1, 'evry': 1, 'emotion': 1, 'dsn': 1, 'sandiago': 1, 'parantella': 1, 'ujhhhhhhh': 1, 'hugging': 1, 'mango': 1, 'sweater': 1, 'involved': 1, 'bob': 1, 'barry': 1, '83738': 1, 'landmark': 1, 'consent': 1, 'clubzed': 1, 'tonexs': 1, 'renewed': 1, 'billing': 1, 'mathews': 1, 'anderson': 1, 'edwards': 1, 'tait': 1, 'promoting': 1, 'haunt': 1, 'crowd': 1, '8000930705': 1, 'snowboarding': 1, 'christmassy': 1, 'recpt': 1, 'baaaaaaaabe': 1, 'ignoring': 1, 'zealand': 1, 'education': 1, 'academic': 1, 'completes': 1, 'sagamu': 1, 'vital': 1, 'lautech': 1, 'shola': 1, 'qet': 1, 'browser': 1, 'surf': 1, 'subscribers': 1, 'conversations': 1, 'overemphasise': 1, 'senses': 1, 'convinced': 1, 'adp': 1, 'headset': 1, 'internal': 1, 'extract': 1, 'immed': 1, 'bevies': 1, 'waz': 1, 'fancied': 1, 'spoon': 1, 'comfey': 1, 'watchng': 1, 'othrs': 1, 'skint': 1, 'least5times': 1, 'quitting': 1, 'wudn': 1, 'frequently': 1, 'cupboard': 1, 'route': 1, '2mro': 1, 'floppy': 1, 'snappy': 1, 'risk': 1, 'grasp': 1, 'flavour': 1, 'laready': 1, 'denying': 1, 'dom': 1, 'ffffuuuuuuu': 1, 'julianaland': 1, 'oblivious': 1, 'dehydrated': 1, 'dogwood': 1, 'tiny': 1, 'mapquest': 1, 'archive': 1, '08719839835': 1, 'mgs': 1, '89123': 1, 'behalf': 1, 'lengths': 1, 'stunning': 1, 'visa': 1, 'gucci': 1, 'talkbut': 1, 'culdnt': 1, 'wenwecan': 1, 'sozi': 1, 'wannatell': 1, 'smsing': 1, 'efficient': 1, '15pm': 1, 'thandiyachu': 1, 'erutupalam': 1, 'invention': 1, 'lyrics': 1, 'somone': 1, 'nevr': 1, 'undrstnd': 1, 'definitly': 1, 'unrecognized': 1, 'valuing': 1, 'toking': 1, 'ger': 1, 'syd': 1, 'dhorte': 1, 'lage': 1, 'kintu': 1, 'opponenter': 1, 'khelate': 1, 'spares': 1, 'looovvve': 1, 'fried': 1, 'warwick': 1, 'tmw': 1, 'canceled': 1, 'havn': 1, 'tops': 1, 'grandma': 1, 'parade': 1, 'norcorp': 1, 'proze': 1, 'posting': 1, '7cfca1a': 1, 'grumble': 1, 'algebra': 1, 'linear': 1, 'decorating': 1, '946': 1, 'wining': 1, 'roomate': 1, 'graduated': 1, 'adjustable': 1, 'allows': 1, 'cooperative': 1, 'nottingham': 1, '40mph': 1, '63miles': 1, 'thanku': 1, 'guessed': 1, '50ea': 1, 'la1': 1, 'strings': 1, '7ws': 1, '89938': 1, 'otbox': 1, '731': 1, 'beside': 1, 'walks': 1, 'brisk': 1, 'dirtiest': 1, 'sexiest': 1, '89070': 1, 'tellmiss': 1, 'contribute': 1, 'greatly': 1, 'duvet': 1, 'smells': 1, 'urgh': 1, 'predictive': 1, 'coach': 1, 'w8in': 1, '4utxt': 1, '24th': 1, 'pist': 1, 'beverage': 1, 'surrender': 1, 'symptoms': 1, 'rdy': 1, 'backwards': 1, 'abstract': 1, 'avin': 1, 'africa': 1, 'chit': 1, '4217': 1, 'logon': 1, '6zf': 1, 'w1a': 1, '118p': 1, '8883': 1, 'cu': 1, 'quiteamuzing': 1, '4brekkie': 1, 'scool': 1, 'lrg': 1, 'psxtra': 1, 'satthen': 1, 'probpop': 1, 'portions': 1, '1000call': 1, '09071512432': 1, '300603t': 1, 'callcost150ppmmobilesvary': 1, 'rows': 1, 'njan': 1, 'fixd': 1, 'sudn': 1, 'engagement': 1, 'vilikkam': 1, 'maths': 1, 'chapter': 1, 'chop': 1, 'noooooooo': 1, '08718727870150ppm': 1, 'firsg': 1, 'split': 1, 'wasnt': 1, 'applyed': 1, 'heat': 1, 'sumfing': 1, 'ithink': 1, 'amnow': 1, 'bedreal': 1, 'layin': 1, 'tonsolitusaswell': 1, 'hopeso': 1, 'lotsof': 1, 'hiphop': 1, 'oxygen': 1, 'resort': 1, 'roller': 1, 'australia': 1, 'recorder': 1, 'canname': 1, 'mquiz': 1, 'showr': 1, 'upon': 1, 'ceiling': 1, 'ennal': 1, 'prakasam': 1, 'bcz': 1, 'prakasamanu': 1, 'presnts': 1, 'sneham': 1, 'mns': 1, 'jeevithathile': 1, 'neekunna': 1, 'irulinae': 1, 'mis': 1, 'blowing': 1, '7634': 1, '7684': 1, 'firmware': 1, 'vijaykanth': 1, 'anythiing': 1, 'clubmoby': 1, '08717509990': 1, 'ripped': 1, 'keypad': 1, 'btwn': 1, 'expects': 1, 'decades': 1, 'goverment': 1, 'spice': 1, 'ettans': 1, 'prasanth': 1, '08718738002': 1, '48922': 1, 'fizz': 1, 'contains': 1, 'appy': 1, 'genus': 1, 'robinson': 1, 'outs': 1, 'soz': 1, 'mums': 1, 'imat': 1, 'freinds': 1, 'sometext': 1, '9280114': 1, '07099833605': 1, 'chloe': 1, '130': 1, 'wewa': 1, 'iriver': 1, '255': 1, '128': 1, 'bw': 1, 'surly': 1, '9758': 1, '07808726822': 1, 'snuggles': 1, 'contented': 1, 'whispers': 1, 'mmmmmmm': 1, 'healthy': 1, '2bold': 1, 'barrel': 1, 'scraped': 1, 'misfits': 1, 'clearer': 1, 'sections': 1, 'peach': 1, 'tasts': 1, 'termsapply': 1, 'golf': 1, 'rayman': 1, 'activ8': 1, 'shindig': 1, 'phonebook': 1, 'ashes': 1, 'rocking': 1, 'shijutta': 1, 'offense': 1, 'dvg': 1, 'vinobanagar': 1, 'ovulate': 1, '3wks': 1, 'realising': 1, 'woah': 1, 'orh': 1, 'n8': 1, 'secrets': 1, 'hides': 1, 'akon': 1, 'axel': 1, 'eyed': 1, 'cashbin': 1, 'canteen': 1, 'stressfull': 1, 'adds': 1, 'continued': 1, 'president': 1, '180': 1, '140': 1, 'pleasured': 1, 'providing': 1, 'assistance': 1, 'whens': 1, '1172': 1, 'memories': 1, 'lonlines': 1, 'built': 1, 'lotz': 1, 'gailxx': 1, 'complacent': 1, 'denis': 1, 'mina': 1, 'miwa': 1, '09066649731from': 1, 'opposite': 1, 'heavily': 1, 'swayze': 1, 'patrick': 1, 'dolls': 1, '09077818151': 1, '30s': 1, 'santacalling': 1, 'calls1': 1, '50ppm': 1, 'quarter': 1, 'fired': 1, 'limping': 1, 'aa': 1, '08719180219': 1, '078498': 1, 'oga': 1, 'punishment': 1, 'poorly': 1, 'brb': 1, 'kill': 1, 'predicte': 1, 'situations': 1, 'loosing': 1, 'capacity': 1, 'smaller': 1, 'fgkslpo': 1, 'videos': 1, 'netun': 1, 'shsex': 1, 'fgkslpopw': 1, '0871277810710p': 1, 'defer': 1, 'admission': 1, 'checkmate': 1, 'maat': 1, 'shah': 1, 'chess': 1, 'persian': 1, 'phrase': 1, 'rats': 1, 'themes': 1, 'photoshop': 1, 'manageable': 1, '08715203652': 1, '42810': 1, 'increase': 1, 'carolina': 1, 'north': 1, 'texas': 1, 'gre': 1, 'bomb': 1, 'breathing': 1, 'powerful': 1, 'weapon': 1, 'lovly': 1, 'customercare': 1, 'msgrcvd': 1, 'clas': 1, 'lit': 1, 'couch': 1, 'loooooool': 1, 'swashbuckling': 1, 'terror': 1, 'cruel': 1, 'decent': 1, 'joker': 1, 'dip': 1, 'gek1510': 1, 'nuther': 1, 'lyricalladie': 1, 'hmmross': 1, '910': 1, 'differences': 1, 'happiest': 1, 'characters': 1, 'lists': 1, 'infections': 1, 'antibiotic': 1, 'gynae': 1, 'abdomen': 1, '6times': 1, 'exposed': 1, 'device': 1, 'beatings': 1, 'chastity': 1, 'uses': 1, 'wrenching': 1, 'gut': 1, 'tallahassee': 1, 'ou': 1, 'taka': 1, 'nr31': 1, '450pw': 1, 'pobox202': 1, '7zs': 1, 'ritten': 1, 'fold': 1, 'colin': 1, 'kiosk': 1, 'swat': 1, 'mre': 1, 'farrell': 1, '83118': 1, 'solihull': 1, 'terminated': 1, 'nhs': 1, '2b': 1, 'inconvenience': 1, 'dentists': 1, 'margin': 1, 'bergkamp': 1, 'yards': 1, 'henry': 1, '78': 1, 'parent': 1, 'unintentional': 1, 'snot': 1, 'nonetheless': 1, 'knees': 1, 'toaday': 1, 'grazed': 1, 'splat': 1, 'hooch': 1, 'deny': 1, 'hearin': 1, 'yah': 1, 'torture': 1, 'hopeing': 1, 'sisters': 1, 'sexychat': 1, 'lips': 1, 'congratulation': 1, 'court': 1, 'frontierville': 1, 'chapel': 1, 'mountain': 1, 'deer': 1, 'varma': 1, 'mailed': 1, 'secure': 1, 'parties': 1, 'farting': 1, 'ortxt': 1, 'dialling': 1, '402': 1, 'advisors': 1, 'trained': 1, 'stuffing': 1, 'woot': 1, 'ahhhh': 1, 'dining': 1, 'vouch4me': 1, 'etlp': 1, 'experiencehttp': 1, 'kaila': 1, '09058094507': 1, 'tsunami': 1, 'disaster': 1, 'donate': 1, 'unicef': 1, 'asian': 1, 'fund': 1, '864233': 1, 'cme': 1, 'hos': 1, 'collapsed': 1, 'cumming': 1, 'jade': 1, 'paul': 1, 'barmed': 1, 'thinkthis': 1, 'dangerous': 1, '762': 1, 'goldviking': 1, 'rushing': 1, 'coulda': 1, 'phony': 1, 'okday': 1, 'petexxx': 1, 'fromwrk': 1, 'adrink': 1, 'bthere': 1, 'buz': 1, 'outsomewhere': 1, 'wedlunch': 1, '2watershd': 1, 'hmph': 1, 'baller': 1, 'punto': 1, 'travelled': 1, 'ayo': 1, '125': 1, 'freeentry': 1, 'xt': 1, 'toyota': 1, 'olayiwola': 1, 'landing': 1, 'camry': 1, 'mileage': 1, 'clover': 1, 'amma': 1, 'achan': 1, 'rencontre': 1, 'mountains': 1, '08714712412': 1, 'puppy': 1, 'noise': 1, 'meg': 1, '08715203685': 1, '4xx26': 1, 'crossing': 1, '09094646631': 1, 'deepest': 1, 'darkest': 1, 'inconvenient': 1, 'adsense': 1, 'approved': 1, 'dudette': 1, 'perumbavoor': 1, 'stage': 1, 'clarify': 1, 'preponed': 1, 'natalie2k9': 1, '165': 1, 'natalie': 1, 'younger': 1, '08701213186': 1, 'liver': 1, 'opener': 1, 'guides': 1, 'watched': 1, 'loneliness': 1, 'skyving': 1, 'onwords': 1, 'mtnl': 1, 'mumbai': 1, 'mustprovide': 1, '62735': 1, '83039': 1, 'accommodationvouchers': 1, '450': 1, '15541': 1, 'rajitha': 1, 'ranju': 1, 'styles': 1, '1winawk': 1, 'tscs08714740323': 1, '50perweeksub': 1, '09066361921': 1, 'disagreeable': 1, 'afterwards': 1, 'vivekanand': 1, 'uawake': 1, 'deviousbitch': 1, 'aletter': 1, 'thatmum': 1, '4thnov': 1, 'gotmarried': 1, 'fuckinnice': 1, 'ourbacks': 1, 'justfound': 1, 'feellikw': 1, 'election': 1, 'rearrange': 1, 'eleven': 1, 'dormitory': 1, 'hitler': 1, 'starer': 1, 'recount': 1, 'astronomer': 1, 'worms': 1, 'suffering': 1, 'dysentry': 1, 'virgil': 1, 'andre': 1, 'gokila': 1, 'uncut': 1, 'dino': 1, 'diamond': 1, 'shanil': 1, 'exchanged': 1, 'kotees': 1, 'zebra': 1, 'sugababes': 1, 'panther': 1, 'badass': 1, 'hoody': 1, 'resent': 1, 'queries': 1, 'customersqueries': 1, 'netvision': 1, 'haughaighgtujhyguj': 1, 'andres': 1, 'hassling': 1, 'londn': 1, 'fassyole': 1, 'blacko': 1, 'responsibilities': 1, '08715205273': 1, 'vco': 1, 'humanities': 1, 'reassurance': 1, 'albi': 1, 'mahfuuz': 1, 'beeen': 1, 'tohar': 1, 'aslamalaikkum': 1, 'mufti': 1, 'muht': 1, '078': 1, 'tocall': 1, 'enufcredeit': 1, 'ileave': 1, 'treats': 1, 'okors': 1, 'ibored': 1, 'adding': 1, 'savings': 1, 'zeros': 1, 'goigng': 1, 'perfume': 1, 'sday': 1, 'grocers': 1, 'pubs': 1, 'bennys': 1, 'frankie': 1, 'owed': 1, 'diapers': 1, 'changing': 1, 'unlike': 1, 'turkeys': 1, 'patients': 1, 'princes': 1, 'helens': 1, 'unintentionally': 1, 'wenever': 1, 'stability': 1, 'vibrant': 1, 'colourful': 1, 'tranquility': 1, 'failing': 1, 'failure': 1, 'bawling': 1, 'velusamy': 1, 'facilities': 1, 'karnan': 1, 'bluray': 1, 'salt': 1, 'wounds': 1, 'logging': 1, 'geoenvironmental': 1, 'implications': 1, 'fuuuuck': 1, 'salmon': 1, 'uploaded': 1, 'wrkin': 1, 'ree': 1, 'compensation': 1, 'awkward': 1, 'splash': 1, 'musta': 1, 'leg': 1, 'overdid': 1, 'telediscount': 1, 'foned': 1, 'chuck': 1, 'port': 1, 'stuffs': 1, 'juswoke': 1, 'docks': 1, 'spinout': 1, 'boatin': 1, '08715203656': 1, '42049': 1, 'uworld': 1, 'assessment': 1, 'qbank': 1, 'someonone': 1, '09064015307': 1, 'tke': 1, 'temales': 1, 'finishd': 1, 'dull': 1, 'studies': 1, 'anyones': 1, 'craigslist': 1, 'treadmill': 1, 'absolutely': 1, 'swan': 1, 'hehe': 1, 'shexy': 1, 'sall': 1, 'lamp': 1, 'foward': 1, '09061790126': 1, 'misundrstud': 1, '2u2': 1, 'genes': 1, 'com1win150ppmx3age16subscription': 1, 'resuming': 1, 'reapply': 1, 'treatin': 1, 'treacle': 1, 'mumhas': 1, 'beendropping': 1, 'theplace': 1, 'adress': 1, 'favorite': 1, 'rumbling': 1, 'sashimi': 1, 'oyster': 1, 'marandratha': 1, 'correctly': 1, 'alaikkum': 1, 'heaven': 1, 'pisces': 1, 'aquarius': 1, '2yrs': 1, 'wicket': 1, 'steyn': 1, 'sterm': 1, 'resolved': 1, 'wheat': 1, 'chex': 1, 'hannaford': 1, 'jam': 1, 'grownup': 1, 'costume': 1, 'jerk': 1, 'stink': 1, 'subsequent': 1, 'follows': 1, 'openings': 1, 'upcharge': 1, 'guai': 1, 'astrology': 1, 'slacking': 1, 'mentor': 1, 'percent': 1, 'erotic': 1, 'ecstacy': 1, '09095350301': 1, 'dept': 1, '08717507382': 1, 'coincidence': 1, 'sane': 1, 'helping': 1, 'pause': 1, '151': 1, 'leading': 1, '8800': 1, 'psp': 1, 'gr8prizes': 1, 'spacebucks': 1, '083': 1, '6089': 1, 'squeezed': 1, 'maintaining': 1, 'dreading': 1, 'thou': 1, 'suggestion': 1, 'forgt': 1, 'lands': 1, 'helps': 1, 'ajith': 1, 'yoville': 1, 'ooooooh': 1, 'asda': 1, 'counts': 1, 'officer': 1, 'carly': 1, 'bffs': 1, '\u3028ud': 1, 'seperated': 1, 'franxx': 1, 'brolly': 1, 'syrup': 1, '5mls': 1, 'feed': 1, 'prometazine': 1, 'singapore': 1, 'shu': 1, 'victoria': 1, 'pocay': 1, '2morrowxxxx': 1, 'wocay': 1, 'ramen': 1, 'broth': 1, 'fowler': 1, 'tats': 1, 'flew': 1, '09058094583': 1, 'attention': 1, 'tix': 1, 'fne': 1, 'youdoing': 1, 'foregate': 1, 'shrub': 1, 'worc': 1, 'get4an18th': 1, '32000': 1, 'efreefone': 1, 'legitimat': 1, 'pendent': 1, 'toilet': 1, 'cops': 1, 'stolen': 1, 'navigate': 1, 'require': 1, 'choosing': 1, 'hu': 1, 'guidance': 1, 'chick': 1, 'boobs': 1, 'revealing': 1, 'org': 1, '2025050': 1, '0121': 1, 'shortbreaks': 1, 'sparkling': 1, 'breaks': 1, '45': 1, 'gyno': 1, 'belong': 1, 'gamb': 1, 'treasure': 1, '820554ad0a1705572711': 1, '09050000332': 1, 'negative': 1, 'positive': 1, 'hmmmm': 1, 'command': 1, 'stressful': 1, 'holby': 1, '09064017295': 1, 'li': 1, 'lecturer': 1, 'repeating': 1, 'motor': 1, 'yeovil': 1, 'rhode': 1, 'bong': 1, 'ofcourse': 1, '08448350055': 1, 'planettalkinstant': 1, '2p': 1, 'spider': 1, 'marvel': 1, 'ultimate': 1, '8ball': 1, '83338': 1, 'tamilnadu': 1, 'tip': 1, '07808247860': 1, '40411': 1, '08719899229': 1, 'identification': 1, 'boundaries': 1, 'limit': 1, 'endless': 1, 'reassuring': 1, 'young': 1, 'referin': 1, 'saibaba': 1, 'colany': 1, 'declare': 1, 'chic': 1, '49557': 1, 'disappointment': 1, 'irritation': 1, 'tantrum': 1, 'compliments': 1, 'adventuring': 1, 'chief': 1, 'gsex': 1, 'wc1n': 1, '2667': 1, '3xx': 1, 'l8er': 1, 'bailiff': 1, 'inclu': 1, '3mobile': 1, 'servs': 1, 'chatlines': 1, 'mouse': 1, 'desk': 1, 'childporn': 1, 'jumpers': 1, 'belt': 1, 'cribbs': 1, 'hat': 1, 'spiritual': 1, 'barring': 1, 'influx': 1, 'sudden': 1, 'kane': 1, 'shud': 1, 'pshew': 1, '4years': 1, 'units': 1, 'accent': 1, 'dental': 1, 'nmde': 1, 'dump': 1, 'heap': 1, 'lowes': 1, 'salesman': 1, '087187272008': 1, 'now1': 1, 'suggestions': 1, 'pity': 1, 'bitching': 1}), 'lowercase': True, 'n': 5574, 'ngram_range': (1, 1), 'normalize': True, 'on': None, 'preprocessor': None, 'processing_steps': [<function strip_accents_unicode at 0x7ff10730dc10>, <method 'lower' of 'str' objects>, <built-in method findall of re.Pattern object at 0x7ff107682440>], 'strip_accents': True, 'tokenizer': <built-in method findall of re.Pattern object at 0x7ff107682440>} Normalizer {'order': 2} RandomUnderSampler(LogisticRegression) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': True, '_rng': RandomState(MT19937) at 0x7FF107657B40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.9 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } The results of the logistic regression are quite good but still inferior to the naive Bayes model. Let's try to use word embeddings to improve our logistic regression. Word embeddings allow you to represent a word as a vector. Embeddings are developed to build semantically rich vectors. For instance, the vector which represents the word python should be close to the vector which represents the word programming . We will use spaCy to convert our sentence to vectors. spaCy converts a sentence to a vector by calculating the average of the embeddings of the words in the sentence. You can download pre-trained embeddings in many languages. We will use English pre-trained embeddings as our SMS are in English. The command below allows you to download the pre-trained embeddings that spaCy makes available. More informations about spaCy and its installation may be found here here . python -m spacy download en_core_web_sm Here, we create a custom transformer to convert an input sentence to a dict of floats. We will integrate this transformer into our pipeline. import spacy from river.base import Transformer class Embeddings ( Transformer ): \"\"\"My custom transformer, word embedding using spaCy.\"\"\" def __init__ ( self ): self . embeddings = spacy . load ( 'en_core_web_sm' ) def transform_one ( self , x , y = None ): return { dimension : xi for dimension , xi in enumerate ( self . embeddings ( x ) . vector )} Let's train our logistic regression: X_y = datasets . SMSSpam () model = ( extract_body | Embeddings () | preprocessing . Normalizer () | imblearn . RandomOverSampler ( classifier = linear_model . LogisticRegression ( optimizer = optim . SGD ( .5 ), loss = optim . losses . Log () ), desired_dist = { 0 : .5 , 1 : .5 }, seed = 42 ) ) metric = metrics . ROCAUC () cm = metrics . ConfusionMatrix () for x , y in X_y : y_pred = model . predict_one ( x ) metric . update ( y_pred = y_pred , y_true = y ) cm . update ( y_pred = y_pred , y_true = y ) model . learn_one ( x , y ) metric ROCAUC: 0.91568 The confusion matrix: cm False True False 4517 310 True 78 669 model extract_body def extract_body(x): \"\"\"Extract the body of the sms.\"\"\" return x['body'] Embeddings {'embeddings': <spacy.lang.en.English object at 0x7ff1085a5940>} Normalizer {'order': 2} RandomOverSampler(LogisticRegression) {'_actual_dist': Counter({False: 4827, True: 747}), '_pivot': False, '_rng': RandomState(MT19937) at 0x7FF10852DB40, 'classifier': LogisticRegression ( optimizer=SGD ( lr=Constant ( learning_rate=0.5 ) ) loss=Log ( weight_pos=1. weight_neg=1. ) l2=0. intercept_init=0. intercept_lr=Constant ( learning_rate=0.01 ) clip_gradient=1e+12 initializer=Zeros () ), 'desired_dist': {0: 0.5, 1: 0.5}, 'seed': 42} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } The results of the logistic regression using spaCy embeddings are lower than those obtained with TF-IDF values. We could surely improve the results by cleaning up the text. We could also use embeddings more suited to our dataset. However, on this problem, the logistic regression is not better than the Naive Bayes model. No free lunch today.","title":"Sentence classification"},{"location":"examples/the-art-of-using-pipelines/","text":"The art of using pipelines \u00b6 Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems. Both scikit-learn and pandas make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's pipeline module, however the pipe method from pandas is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations. Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not! In this notebook we'll manipulate data from the Kaggle Recruit Restaurants Visitor Forecasting competition . The data is directly available through river 's datasets module. from pprint import pprint from river import datasets for x , y in datasets . Restaurants (): pprint ( x ) pprint ( y ) break {'area_name': 'T\u014dky\u014d-to Nerima-ku Toyotamakita', 'date': datetime.datetime(2016, 1, 1, 0, 0), 'genre_name': 'Izakaya', 'is_holiday': True, 'latitude': 35.7356234, 'longitude': 139.6516577, 'store_id': 'air_04341b588bde96cd'} 10 We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model. from river import feature_extraction from river import linear_model from river import metrics from river import preprocessing from river import stats means = ( feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) scaler = preprocessing . StandardScaler () lin_reg = linear_model . LinearRegression () metric = metrics . MAE () for x , y in datasets . Restaurants (): # Derive date features x [ 'weekday' ] = x [ 'date' ] . weekday () x [ 'is_weekend' ] = x [ 'date' ] . weekday () in ( 5 , 6 ) # Process the rolling means of the target for mean in means : x = { ** x , ** mean . transform_one ( x )} mean . learn_one ( x , y ) # Remove the key/value pairs that aren't features for key in [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ]: x . pop ( key ) # Rescale the data x = scaler . learn_one ( x ) . transform_one ( x ) # Fit the linear regression y_pred = lin_reg . predict_one ( x ) lin_reg . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.465114 We're not using many features. We can print the last x to get an idea of the features (don't forget they've been scaled!) pprint ( x ) {'is_holiday': -0.23103573677646685, 'is_weekend': 1.6249280076334165, 'target_rollingmean_14_by_store_id': -1.4125913815779154, 'target_rollingmean_21_by_store_id': -1.3980979075298519, 'target_rollingmean_7_by_store_id': -1.3502314499809096, 'weekday': 1.0292832579142892} The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as river is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline \u00e0 la sklearn . from river import compose def get_date_features ( x ): weekday = x [ 'date' ] . weekday () return { 'weekday' : weekday , 'is_weekend' : weekday in ( 5 , 6 )} model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) metric = metrics . MAE () for x , y in datasets . Restaurants (): # Make a prediction without using the target y_pred = model . predict_one ( x ) # Update the model using the target model . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.38533 We use a Pipeline to arrange each step in a sequential order. A TransformerUnion is used to merge multiple feature extractors into a single transformer. The for loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called progressive_val_score part of the evaluate module. We can use it to replace the for loop. from river import evaluate model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) evaluate . progressive_val_score ( dataset = datasets . Restaurants (), model = model , metric = metrics . MAE ()) MAE: 8.38533 Notice that you couldn't have used the progressive_val_score method if you wrote the model in a procedural manner. Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However river has some special tricks up it's sleeve to save you from a lot of pain. The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then river automatically infers one. model = compose . Pipeline ( compose . TransformerUnion ( compose . FuncTransformer ( get_date_features ), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Under the hood a Pipeline inherits from collections.OrderedDict . Indeed this makes sense because if you think about it a Pipeline is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a Pipeline the same way you would manipulate an ordinary dict . For instance we can print the name of each step by using the keys method. for name in model . steps : print ( name ) TransformerUnion Discard StandardScaler LinearRegression The first step is a FeatureUnion and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious. The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the + operator to merge Transformer s into a TransformerUnion . model = compose . Pipeline ( compose . FuncTransformer ( get_date_features ) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Likewhise we can use the | operator to assemble steps into a Pipeline . model = ( compose . FuncTransformer ( get_date_features ) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) to_discard = [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ] model = model | compose . Discard ( * to_discard ) | preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a FuncTransformer , which can be quite handy. model = get_date_features for n in [ 7 , 14 , 21 ]: model += feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( n )) model |= compose . Discard ( * to_discard ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own! Before finishing we can take an interactive look at our pipeline. model get_date_features def get_date_features(x): weekday = x['date'].weekday() return {'weekday': weekday, 'is_weekend': weekday in (5, 6)} target_rollingmean_7_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_7_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 31.571429, 'air_0164b9927d20bcc3': RollingMean: 6.428571, 'air_0241aa3964b7f861': RollingMean: 11.428571, 'air_0328696196e46f18': RollingMean: 10., 'air_034a3d5b40d5b1b1': RollingMean: 28.428571, 'air_036d4f1ee7285390': RollingMean: 27.142857, 'air_0382c794b73b51ad': RollingMean: 29.142857, 'air_03963426c9312048': RollingMean: 45.714286, 'air_04341b588bde96cd': RollingMean: 32.857143, 'air_049f6d5b402a31b2': RollingMean: 16.571429, 'air_04cae7c1bc9b2a0b': RollingMean: 18.571429, 'air_0585011fa179bcce': RollingMean: 5.571429, 'air_05c325d315cc17f5': RollingMean: 29.714286, 'air_0647f17b4dc041c8': RollingMean: 29.714286, 'air_064e203265ee5753': RollingMean: 19.142857, 'air_066f0221b8a4d533': RollingMean: 11.428571, 'air_06f95ac5c33aca10': RollingMean: 27.142857, 'air_0728814bd98f7367': RollingMean: 8.285714, 'air_0768ab3910f7967f': RollingMean: 32.571429, 'air_07b314d83059c4d2': RollingMean: 41.285714, 'air_07bb665f9cdfbdfb': RollingMean: 24.142857, 'air_082908692355165e': RollingMean: 48., 'air_083ddc520ea47e1e': RollingMean: 14.857143, 'air_0845d8395f30c6bb': RollingMean: 25.285714, 'air_084d98859256acf0': RollingMean: 12.714286, 'air_0867f7bebad6a649': RollingMean: 19.142857, 'air_08ba8cd01b3ba010': RollingMean: 10., 'air_08cb3c4ee6cd6a22': RollingMean: 10.857143, 'air_08ef81d5b7a0d13f': RollingMean: 12.857143, 'air_08f994758a1e76d4': RollingMean: 29.285714, 'air_09040f6df960ddb8': RollingMean: 17.571429, 'air_0919d54f0c9a24b8': RollingMean: 37., 'air_09661c0f3259cc04': RollingMean: 26.428571, 'air_09a845d5b5944b01': RollingMean: 7.571429, 'air_09fd1f5c58583141': RollingMean: 8.571429, 'air_0a74a5408a0b8642': RollingMean: 28.857143, 'air_0b184ec04c741a6a': RollingMean: 11.857143, 'air_0b1e72d2d4422b20': RollingMean: 20.857143, 'air_0b9038300f8b2b50': RollingMean: 13.428571, 'air_0e1eae99b8723bc1': RollingMean: 12.571429, 'air_0e7c11b9abc50163': RollingMean: 37., 'air_0ead98dd07e7a82a': RollingMean: 10.571429, 'air_0f0cdeee6c9bf3d7': RollingMean: 25.571429, 'air_0f2f96335f274801': RollingMean: 11.428571, 'air_0f60e1576a7d397d': RollingMean: 4.857143, 'air_1033310359ceeac1': RollingMean: 20.857143, 'air_10393f12e9069760': RollingMean: 13.285714, 'air_105a7954e32dba9b': RollingMean: 50.714286, 'air_10713fbf3071c361': RollingMean: 12.857143, 'air_10bbe8acd943d8f6': RollingMean: 28., 'air_12c4fb7a423df20d': RollingMean: 21.142857, 'air_138ee734ac79ff90': RollingMean: 6.285714, 'air_138ff410757b845f': RollingMean: 49.714286, 'air_1408dd53f31a8a65': RollingMean: 24.285714, 'air_142e78ba7001da9c': RollingMean: 14.142857, 'air_1509881b22965b34': RollingMean: 17., 'air_152c1f08d7d20e07': RollingMean: 12., 'air_15ae33469e9ea2dd': RollingMean: 8.285714, 'air_15e6e15c7ea2c162': RollingMean: 19.571429, 'air_16179d43b6ee5fd8': RollingMean: 8.428571, 'air_1653a6c513865af3': RollingMean: 35., 'air_168441ada3e878e1': RollingMean: 54.571429, 'air_16c4cfddeb2cf69b': RollingMean: 9.428571, 'air_16cf0a73233896de': RollingMean: 13., 'air_1707a3f18bb0da07': RollingMean: 23.142857, 'air_17a6ab40f97fd4d8': RollingMean: 6.571429, 'air_17bed6dbf7c8b0fc': RollingMean: 20.428571, 'air_1979eaff8189d086': RollingMean: 9.857143, 'air_1ab60ce33bfed8a8': RollingMean: 10.142857, 'air_1ae94f514a0bce13': RollingMean: 6.571429, 'air_1ba4e87ef7422183': RollingMean: 35.571429, 'air_1c0b150f9e696a5f': RollingMean: 100.714286, 'air_1c95a84924d72500': RollingMean: 9.285714, 'air_1d1e8860ae04f8e9': RollingMean: 15.142857, 'air_1d25ca6c76df48b4': RollingMean: 42.857143, 'air_1d3f797dd1f7cf1c': RollingMean: 39., 'air_1dd8f6f47480d1a2': RollingMean: 39.142857, 'air_1dea9815ccd36620': RollingMean: 10.285714, 'air_1e23210b584540e7': RollingMean: 3.857143, 'air_1e665503b8474c55': RollingMean: 6.142857, 'air_1eeff462acb24fb7': RollingMean: 19.571429, 'air_1f1390a8be2272b3': RollingMean: 19., 'air_1f34e9beded2231a': RollingMean: 9., 'air_1f7f8fa557bc0d55': RollingMean: 3.714286, 'air_2009041dbf9264de': RollingMean: 52.285714, 'air_20619d21192aa571': RollingMean: 13.285714, 'air_20add8092c9bb51d': RollingMean: 33.571429, 'air_2195cd5025a98033': RollingMean: 34.142857, 'air_21f5052d5330528d': RollingMean: 31.857143, 'air_220cba70c890b119': RollingMean: 8.714286, 'air_22682e965418936f': RollingMean: 10.142857, 'air_228f10bec0bda9c8': RollingMean: 17.142857, 'air_229d7e508d9f1b5e': RollingMean: 11.428571, 'air_232dcee6f7c51d37': RollingMean: 6.857143, 'air_234d3dbf7f3d5a50': RollingMean: 6.428571, 'air_23e1b11aee2a1407': RollingMean: 46.857143, 'air_23ee674e91469086': RollingMean: 21.142857, 'air_24b9b2a020826ede': RollingMean: 32.714286, 'air_24e8414b9b07decb': RollingMean: 5.857143, 'air_2545dd3a00f265e2': RollingMean: 56., 'air_256be208a979e023': RollingMean: 8., 'air_2570ccb93badde68': RollingMean: 37.428571, 'air_258ad2619d7bff9a': RollingMean: 38.428571, 'air_258dc112912fc458': RollingMean: 67.571429, 'air_25c583983246b7b0': RollingMean: 29.857143, 'air_25d8e5cc57dd87d9': RollingMean: 26.285714, 'air_25e9888d30b386df': RollingMean: 5.142857, 'air_2634e41551e9807d': RollingMean: 18.428571, 'air_26c5bbeb7bb82bf1': RollingMean: 27.142857, 'air_26f10355d9b4d82a': RollingMean: 33.285714, 'air_2703dcb33192b181': RollingMean: 54.142857, 'air_275732a5db46f4d3': RollingMean: 18.714286, 'air_27e991812b0d9c92': RollingMean: 42., 'air_28064154614b2e6c': RollingMean: 22.857143, 'air_287d2de7d3c93406': RollingMean: 11.857143, 'air_28a9fa1ec0839375': RollingMean: 34.142857, 'air_28dbe91c4c9656be': RollingMean: 33.571429, 'air_290e7a57b390f78e': RollingMean: 14.285714, 'air_298513175efdf261': RollingMean: 24.857143, 'air_2a184c1745274b2b': RollingMean: 3.428571, 'air_2a24aec099333f39': RollingMean: 8.857143, 'air_2a3743e37aab04b4': RollingMean: 17.428571, 'air_2a485b92210c98b5': RollingMean: 22.857143, 'air_2a7f14da7fe0f699': RollingMean: 25., 'air_2aab19554f91ff82': RollingMean: 43.571429, 'air_2ac361b97630e2df': RollingMean: 13.285714, 'air_2b8b29ddfd35018e': RollingMean: 8.714286, 'air_2b9bc9f5f5168ea1': RollingMean: 21., 'air_2bffb19a24d11729': RollingMean: 11., 'air_2c505f9ad67d4635': RollingMean: 16.571429, 'air_2c6c79d597e48096': RollingMean: 14.714286, 'air_2c6fef1ce0e13a5a': RollingMean: 28.714286, 'air_2c989829acbd1c6b': RollingMean: 28.142857, 'air_2cee51fa6fdf6c0d': RollingMean: 17.142857, 'air_2d3afcb91762fe01': RollingMean: 51., 'air_2d78d9a1f4dd02ca': RollingMean: 11.714286, 'air_2e7cb1f1a2a9cd6a': RollingMean: 31.428571, 'air_2f8ced25216df926': RollingMean: 12.714286, 'air_2fc149abe33adcb4': RollingMean: 38.285714, 'air_2fc478dc9f0a6b31': RollingMean: 12.857143, 'air_2fed81034f8834e5': RollingMean: 23.857143, 'air_303bac187b53083a': RollingMean: 9.571429, 'air_310e467e6e625004': RollingMean: 16.714286, 'air_3155ee23d92202da': RollingMean: 14.428571, 'air_31c753b48a657b6c': RollingMean: 21.285714, 'air_32460819c7600037': RollingMean: 46.857143, 'air_324f7c39a8410e7c': RollingMean: 12.428571, 'air_326ca454ef3558bc': RollingMean: 23.714286, 'air_32b02ba5dc2027f4': RollingMean: 29., 'air_32c61b620a766138': RollingMean: 28., 'air_32f5d7cd696e3c4a': RollingMean: 20.714286, 'air_33b01025210d6007': RollingMean: 12., 'air_3440e0ea1b70a99b': RollingMean: 30., 'air_346ade7d29230634': RollingMean: 7., 'air_347be2c4feeb408b': RollingMean: 22.571429, 'air_349278fa964bb12f': RollingMean: 19.714286, 'air_3525f11ef0bf0c35': RollingMean: 44.714286, 'air_35512c42db0868da': RollingMean: 5.142857, 'air_3561fd1c0bce6a95': RollingMean: 11.714286, 'air_35c4732dcbfe31be': RollingMean: 8.714286, 'air_36429b5ca4407b3e': RollingMean: 20., 'air_36bcf77d3382d36e': RollingMean: 31.285714, 'air_37189c92b6c761ec': RollingMean: 20.285714, 'air_375a5241615b5e22': RollingMean: 7.142857, 'air_382f5ace4e2247b8': RollingMean: 8.857143, 'air_383f5b2f8d345a49': RollingMean: 12.714286, 'air_38746ffe9aa20c7e': RollingMean: 4.571429, 'air_396166d47733d5c9': RollingMean: 31., 'air_396942e6423a2145': RollingMean: 24.428571, 'air_397d3f32a7196aa2': RollingMean: 33.142857, 'air_3980af67be35afdb': RollingMean: 20., 'air_3982a2c4ea2ed431': RollingMean: 33.428571, 'air_399904bdb7685ca0': RollingMean: 29., 'air_39dccf7df20b1c6a': RollingMean: 26.142857, 'air_3a8a3f8fb5cd7f88': RollingMean: 22.428571, 'air_3aa839e8e0cb6c87': RollingMean: 29.857143, 'air_3ac24136722e2291': RollingMean: 15.142857, 'air_3b20733899b5287f': RollingMean: 41.857143, 'air_3b6438b125086430': RollingMean: 13.857143, 'air_3bb99a1fe0583897': RollingMean: 39.428571, 'air_3bd49f98ab7f36ab': RollingMean: 15.857143, 'air_3c05c8f26c611eb9': RollingMean: 22.571429, 'air_3c938075889fc059': RollingMean: 27.285714, 'air_3cad29d1a23209d2': RollingMean: 9.142857, 'air_3caef3f76b8f26c5': RollingMean: 25.428571, 'air_3d3a2b509180e798': RollingMean: 17., 'air_3e6cea17a9d2c0f1': RollingMean: 18.714286, 'air_3e93f3c81008696d': RollingMean: 39.714286, 'air_3f91d592acd6cc0b': RollingMean: 21.714286, 'air_401b39f97e56b939': RollingMean: 11.285714, 'air_4043b7ccfbffa732': RollingMean: 47.857143, 'air_4092cfbd95a3ac1b': RollingMean: 27.714286, 'air_40953e2d8b4f2857': RollingMean: 17.428571, 'air_40f6193ea3ed1b91': RollingMean: 17.857143, 'air_414ff459ed18fa48': RollingMean: 14.285714, 'air_41bbf6e1d9814c4b': RollingMean: 7.714286, 'air_421670f21da5ba31': RollingMean: 18.428571, 'air_4254c3fc3ad078bd': RollingMean: 12.285714, 'air_42c9aa6d617c5057': RollingMean: 47., 'air_42d41eb58cad170e': RollingMean: 33.571429, 'air_43b65e4b05bff2d3': RollingMean: 19.571429, 'air_43d577e0c9460e64': RollingMean: 32., 'air_4433ab8e9999915f': RollingMean: 21.142857, 'air_4481a87c1d7c9896': RollingMean: 24.142857, 'air_452100f5305dde64': RollingMean: 7.714286, 'air_45326ebb8dc72cfb': RollingMean: 22.428571, 'air_4570f52104fe0982': RollingMean: 8.571429, 'air_4579cb0669fd411b': RollingMean: 19., 'air_457efe8c3a30ea17': RollingMean: 6.857143, 'air_464a62de0d57be1e': RollingMean: 26.428571, 'air_465bddfed3353b23': RollingMean: 30.285714, 'air_47070be6093f123e': RollingMean: 44.285714, 'air_472b19e3b5bffa41': RollingMean: 14., 'air_473cf23b9e7c0a37': RollingMean: 10.142857, 'air_473f98b212d37b4a': RollingMean: 27.714286, 'air_47beaffd3806c979': RollingMean: 18.571429, 'air_483eba479dc9910d': RollingMean: 19.285714, 'air_48e9fc98b62495a7': RollingMean: 23.142857, 'air_48f4da6223571da4': RollingMean: 21.428571, 'air_48ffd31594bc3263': RollingMean: 4., 'air_49211568cab5fdee': RollingMean: 25.285714, 'air_4974785f48853db9': RollingMean: 7.285714, 'air_4b251b9f8373f1ae': RollingMean: 25.857143, 'air_4b380b4db9d37883': RollingMean: 25.285714, 'air_4b55d8aea1d2b395': RollingMean: 36.142857, 'air_4b9085d0d46a6211': RollingMean: 21.285714, 'air_4beac252540f865e': RollingMean: 47.571429, 'air_4c2ed28f3f19ca52': RollingMean: 14.571429, 'air_4c665a2bfff0da3b': RollingMean: 8.857143, 'air_4c727b55acdee495': RollingMean: 14.285714, 'air_4cab15ad29c0ffbc': RollingMean: 19.142857, 'air_4cab91146e3d1897': RollingMean: 16., 'air_4cca5666eaf5c709': RollingMean: 37.571429, 'air_4ce7b17062a1bf73': RollingMean: 6., 'air_4d21676ed11f0bac': RollingMean: 30.714286, 'air_4d71826793c09b22': RollingMean: 20.857143, 'air_4d90a22572fa1ec9': RollingMean: 25.285714, 'air_4de6d887a7b1c1fc': RollingMean: 16.142857, 'air_4dea8d17f6f59c56': RollingMean: 27.857143, 'air_4e1c38f68f435596': RollingMean: 33.285714, 'air_4f762e840b3996e1': RollingMean: 10., 'air_4feeb8600f131e43': RollingMean: 55.714286, 'air_500641aca4cf673c': RollingMean: 18., 'air_506fe758114df773': RollingMean: 32.571429, 'air_51281cd059d7b89b': RollingMean: 16.571429, 'air_51319e7acf0438cf': RollingMean: 13.857143, 'air_52a08ef3efdb4bb0': RollingMean: 35.428571, 'air_52e2a1fd42bc917a': RollingMean: 11.142857, 'air_536043fcf1a4f8a4': RollingMean: 29.571429, 'air_539d693f7317c62d': RollingMean: 18.571429, 'air_546b353cbea4a45b': RollingMean: 14.285714, 'air_5485912b44f976de': RollingMean: 8., 'air_54d6c25d33f5260e': RollingMean: 45., 'air_54ed43163b7596c4': RollingMean: 14., 'air_55390f784018349a': RollingMean: 48.142857, 'air_55c3627912b9c849': RollingMean: 8.714286, 'air_55e11c33d4758131': RollingMean: 22.571429, 'air_56cd12f31a0afc04': RollingMean: 29.571429, 'air_56cebcbd6906e04c': RollingMean: 24.285714, 'air_56ea46c14b2dd967': RollingMean: 45.142857, 'air_57013002b912772b': RollingMean: 6.285714, 'air_573ecdf81b157d22': RollingMean: 25.857143, 'air_57c9eea1a2b66e65': RollingMean: 14.285714, 'air_57ed725a1930a5b9': RollingMean: 14., 'air_5878b6f2a9da12c1': RollingMean: 14., 'air_59cc9b2b209c6331': RollingMean: 9.285714, 'air_5a9a6cbeeb434c08': RollingMean: 23.714286, 'air_5acc13d655a6e8b2': RollingMean: 23.714286, 'air_5afb1cca48ceaa19': RollingMean: 51.571429, 'air_5b6d18c470bbfaf9': RollingMean: 39.285714, 'air_5b704df317ed1962': RollingMean: 2.142857, 'air_5bd22f9cc1426a90': RollingMean: 37.285714, 'air_5c65468938c07fa5': RollingMean: 11.428571, 'air_5c7489c9ec755e2d': RollingMean: 39.714286, 'air_5c817ef28f236bdf': RollingMean: 45.428571, 'air_5cb030b9f0b91537': RollingMean: 11.142857, 'air_5cfc537125d97f16': RollingMean: 8.428571, 'air_5d7c744c3a2ef624': RollingMean: 32.428571, 'air_5d945ade487cdf4d': RollingMean: 17., 'air_5dea8a7a5bf5eb71': RollingMean: 32.285714, 'air_5e339a1f364cdb00': RollingMean: 13.571429, 'air_5e34c6fe6fabd10e': RollingMean: 18.285714, 'air_5e70fe82f9e4fab6': RollingMean: 17.857143, 'air_5e939e005bd34633': RollingMean: 1.857143, 'air_5ed3198e4a5eed0f': RollingMean: 34.571429, 'air_5f3a3ef4cba110a4': RollingMean: 34.571429, 'air_5f6fa1b897fe80d5': RollingMean: 26., 'air_5fbda8e9302f7c13': RollingMean: 26.714286, 'air_602ca92c0db34f8f': RollingMean: 16.857143, 'air_609050e4e4f79ae1': RollingMean: 10.571429, 'air_60a7057184ec7ec7': RollingMean: 30.428571, 'air_60aa54ecbc602348': RollingMean: 5.714286, 'air_6108821ffafa9b72': RollingMean: 26., 'air_614e2f7e76dff854': RollingMean: 11.571429, 'air_61668cc2b0778898': RollingMean: 9.285714, 'air_61b8d37c33617f21': RollingMean: 28.857143, 'air_61de73b097513f58': RollingMean: 8.714286, 'air_622375b4815cf5cb': RollingMean: 44.857143, 'air_627cabe2fe53f33f': RollingMean: 14.571429, 'air_629d9935273c82ae': RollingMean: 27.142857, 'air_629edf21ea38ac2d': RollingMean: 39.142857, 'air_632ba66e1f75aa28': RollingMean: 20.142857, 'air_638c35eb25e53eea': RollingMean: 23.571429, 'air_63a750d8b4b6a976': RollingMean: 30.142857, 'air_63a88d81295195ed': RollingMean: 29.571429, 'air_63b13c56b7201bd9': RollingMean: 26.285714, 'air_63e28ee0b0c955a7': RollingMean: 25.857143, 'air_640cf4835f0d9ba3': RollingMean: 30., 'air_6411203a47b5ec77': RollingMean: 10., 'air_645cb18b33f938cf': RollingMean: 13.571429, 'air_646b93e336f0dded': RollingMean: 8.142857, 'air_64a5d5c1381837af': RollingMean: 38.428571, 'air_64d4491ad8cdb1c6': RollingMean: 14.714286, 'air_650f9b9de0c5542c': RollingMean: 23.857143, 'air_657a0748462f85de': RollingMean: 8.285714, 'air_65e294f1ae6df9c3': RollingMean: 18.857143, 'air_6607fe3671242ce3': RollingMean: 44.142857, 'air_670a0c1c4108bcea': RollingMean: 27.857143, 'air_671b4bea84dafb67': RollingMean: 26., 'air_673acd9fa5e0dd78': RollingMean: 7.142857, 'air_67483104fa38ef6c': RollingMean: 30.428571, 'air_675aa35cba456fd1': RollingMean: 43.285714, 'air_67f87c159d9e2ee2': RollingMean: 39.857143, 'air_68147db09287bf74': RollingMean: 21.285714, 'air_681b0c56328dd2af': RollingMean: 35.428571, 'air_681f96e6a6595f82': RollingMean: 35.857143, 'air_68301bcb11e2f389': RollingMean: 27.142857, 'air_683371d9baabf410': RollingMean: 31.714286, 'air_6836438b543ba698': RollingMean: 11.571429, 'air_6873982b9e19c7ad': RollingMean: 6.285714, 'air_68c1de82037d87e6': RollingMean: 25., 'air_68cc910e7b307b09': RollingMean: 9.428571, 'air_68d075113f368946': RollingMean: 23.857143, 'air_6902e4ec305b3d08': RollingMean: 38.428571, 'air_694571ea13fb9e0e': RollingMean: 29.285714, 'air_6a15e4eae523189d': RollingMean: 17.857143, 'air_6b15edd1b4fbb96a': RollingMean: 31., 'air_6b2268863b14a2af': RollingMean: 20.285714, 'air_6b65745d432fd77f': RollingMean: 23.428571, 'air_6b7678aae65d2d59': RollingMean: 9., 'air_6b942d5ebbc759c2': RollingMean: 12.857143, 'air_6b9fa44a9cf504a1': RollingMean: 4.857143, 'air_6c1128955c58b690': RollingMean: 14.285714, 'air_6c91a28278a16f64': RollingMean: 9.142857, 'air_6c952e3c6e590945': RollingMean: 15.571429, 'air_6ca1d941c8199a67': RollingMean: 28.571429, 'air_6cbe54f0aa30b615': RollingMean: 13.714286, 'air_6ced51c24fb54262': RollingMean: 9.142857, 'air_6d64dba2edd4fc0c': RollingMean: 5.142857, 'air_6d65542aa43b598b': RollingMean: 30., 'air_6d65dd11d96e00fb': RollingMean: 5.285714, 'air_6e06824d0934dd81': RollingMean: 23.285714, 'air_6e3fd96320d24324': RollingMean: 7.857143, 'air_6e64fb5821402cd2': RollingMean: 8.142857, 'air_6ff5fca957798daa': RollingMean: 7.285714, 'air_707d4b6328f2c2df': RollingMean: 28.857143, 'air_709262d948dd0b6e': RollingMean: 14.714286, 'air_70e9e8cd55879414': RollingMean: 10.857143, 'air_70f834596eb99fee': RollingMean: 21., 'air_710d6537cb7623df': RollingMean: 31.714286, 'air_712dd258f7f91b4b': RollingMean: 20.571429, 'air_71903025d39a4571': RollingMean: 15.142857, 'air_722297e7f26db91d': RollingMean: 12.285714, 'air_728ff578acc6ac6e': RollingMean: 11.857143, 'air_72f5146cf0c49beb': RollingMean: 13., 'air_735bcbe1763d6e98': RollingMean: 10.285714, 'air_73f316e6a18d8aa9': RollingMean: 23.714286, 'air_7420042ff75f9aca': RollingMean: 33.285714, 'air_746211c0b532e8aa': RollingMean: 64.142857, 'air_747f375eb3900e1e': RollingMean: 4.428571, 'air_74cf22153214064c': RollingMean: 13.714286, 'air_7514d90009613cd6': RollingMean: 75.714286, 'air_754ae581ad80cc9f': RollingMean: 10.857143, 'air_75864c80d2fb334a': RollingMean: 11.571429, 'air_75bd5d1b6dc6670d': RollingMean: 11.857143, 'air_764f71040a413d4d': RollingMean: 54.428571, 'air_77488fa378cf98c3': RollingMean: 6.857143, 'air_77dfc83450cbc89c': RollingMean: 42.714286, 'air_7831b00996701c0f': RollingMean: 23.428571, 'air_789103bf53b8096b': RollingMean: 54., 'air_789466e488705c93': RollingMean: 22.714286, 'air_78df4dc6a7e83e41': RollingMean: 16.857143, 'air_79afb3f52b4d062c': RollingMean: 9., 'air_79f528087f49df06': RollingMean: 31., 'air_7a81bd7fadcbf3d8': RollingMean: 4., 'air_7a946aada80376a4': RollingMean: 16.142857, 'air_7bacc4d36fb094c9': RollingMean: 6.571429, 'air_7bc6ca04d7b0f3b8': RollingMean: 8.428571, 'air_7bda6048a4a78837': RollingMean: 24.857143, 'air_7c7774c66fb237f7': RollingMean: 7.571429, 'air_7cc17a324ae5c7dc': RollingMean: 14.714286, 'air_7cf5a02c0e01b647': RollingMean: 33.857143, 'air_7d65049f9d275c0d': RollingMean: 11.571429, 'air_7dacea2f22afccfb': RollingMean: 38.142857, 'air_7db266904cb0d72a': RollingMean: 13.571429, 'air_7e12c5d27f44a8de': RollingMean: 25.571429, 'air_7ef9a5ea5c8fe39f': RollingMean: 10.857143, 'air_7f3dc18494bce98b': RollingMean: 15.428571, 'air_7f9e15afafcf4c75': RollingMean: 41.857143, 'air_7fbf7649eb13ad9b': RollingMean: 19.571429, 'air_800c02226e2e0288': RollingMean: 15.285714, 'air_8093d0b565e9dbdf': RollingMean: 39.142857, 'air_8110d68cc869b85e': RollingMean: 51.857143, 'air_81546875de9c8e78': RollingMean: 5., 'air_81a12d67c22e012f': RollingMean: 19.714286, 'air_81bd68142db76f58': RollingMean: 17.714286, 'air_81c2600146d07d16': RollingMean: 6.142857, 'air_81c5dff692063446': RollingMean: 14.857143, 'air_820d1919cbecaa0a': RollingMean: 32.714286, 'air_82a6ae14151953ba': RollingMean: 41.714286, 'air_831658500aa7c846': RollingMean: 31.571429, 'air_832f9dbe9ee4ebd3': RollingMean: 12.428571, 'air_83db5aff8f50478e': RollingMean: 6., 'air_84060403939d8216': RollingMean: 15.571429, 'air_848616680ef061bd': RollingMean: 29.571429, 'air_84f6876ff7e83ae7': RollingMean: 18.428571, 'air_8523d6a70de49e6c': RollingMean: 31.571429, 'air_859feab8e3c9f98d': RollingMean: 25.285714, 'air_85bd13a49370c392': RollingMean: 12.571429, 'air_86cfbf2624576fad': RollingMean: 7.571429, 'air_86f7b2109e4abd65': RollingMean: 47.571429, 'air_87059630ab6fe47f': RollingMean: 4.285714, 'air_87078cf7903a648c': RollingMean: 6.285714, 'air_87467487d21891dd': RollingMean: 10.142857, 'air_8764b3473ddcceaf': RollingMean: 4.857143, 'air_876d7a23c47811cb': RollingMean: 16.571429, 'air_877f79706adbfb06': RollingMean: 10.285714, 'air_87ca98aa7664de94': RollingMean: 13.714286, 'air_87f9e1024b951f01': RollingMean: 12.142857, 'air_883ca28ef0ed3d55': RollingMean: 13.857143, 'air_88c8e34baa79217b': RollingMean: 35.571429, 'air_88ca84051ba95339': RollingMean: 16.428571, 'air_88f31db64991768a': RollingMean: 8.428571, 'air_890d7e28e8eaaa11': RollingMean: 9.714286, 'air_89e7328af22efe74': RollingMean: 41., 'air_8a1d21fad48374e8': RollingMean: 13.857143, 'air_8a59bb0c497b771e': RollingMean: 23.857143, 'air_8a906e5801eac81c': RollingMean: 21.857143, 'air_8b4a46dc521bfcfe': RollingMean: 31.714286, 'air_8c119d1f16049f20': RollingMean: 29.857143, 'air_8c3175aa5e4fc569': RollingMean: 131.714286, 'air_8cc350fd70ee0757': RollingMean: 37.285714, 'air_8ce035ee1d8a56a6': RollingMean: 19.571429, 'air_8d50c64692322dff': RollingMean: 12.571429, 'air_8d61f49aa0373492': RollingMean: 49.714286, 'air_8e429650fcf7a0ae': RollingMean: 19.428571, 'air_8e4360a64dbd4c50': RollingMean: 20., 'air_8e492076a1179383': RollingMean: 43.571429, 'air_8e8f42f047537154': RollingMean: 28.285714, 'air_8ec47c0f1e2c879e': RollingMean: 32., 'air_8f13ef0f5e8c64dd': RollingMean: 5., 'air_8f273fb9ad2fed6f': RollingMean: 12.857143, 'air_8f3b563416efc6ad': RollingMean: 15., 'air_900d755ebd2f7bbd': RollingMean: 97.285714, 'air_901925b628677c2e': RollingMean: 8.571429, 'air_90213bcae4afa274': RollingMean: 29.571429, 'air_90bd5de52c166767': RollingMean: 21.714286, 'air_90ed0a2f24755533': RollingMean: 50., 'air_90f0efbb702d77b7': RollingMean: 30.857143, 'air_9105a29b0eb250d2': RollingMean: 16.857143, 'air_91236b89d29567af': RollingMean: 27.428571, 'air_9152d9926e5c4a3a': RollingMean: 31.142857, 'air_915558a55c2bc56c': RollingMean: 17.142857, 'air_91beafbba9382b0a': RollingMean: 36.428571, 'air_91d72e16c4bcba55': RollingMean: 15.714286, 'air_9241121af22ff1d6': RollingMean: 27.285714, 'air_929d8513e3cdb423': RollingMean: 8.285714, 'air_931a8a4321b6e7d1': RollingMean: 4., 'air_9352c401d5adb01b': RollingMean: 25.571429, 'air_9387ff95e886ebc7': RollingMean: 11.857143, 'air_938ef91ecdde6878': RollingMean: 27., 'air_939964477035ef0b': RollingMean: 16.857143, 'air_93b9bb641f8fc982': RollingMean: 24.857143, 'air_93dd7070c9bf5453': RollingMean: 29.142857, 'air_93ebe490d4abb8e9': RollingMean: 21.285714, 'air_9438d67241c81314': RollingMean: 31.714286, 'air_947eb2cae4f3e8f2': RollingMean: 36.714286, 'air_9483d65e9cc9a6b7': RollingMean: 15.428571, 'air_950381108f839348': RollingMean: 32.142857, 'air_95d28905941fd4cb': RollingMean: 36.428571, 'air_95e917913cd988f3': RollingMean: 25.714286, 'air_96005f79124e12bf': RollingMean: 42.714286, 'air_965b2e0cf4119003': RollingMean: 40., 'air_96743eee94114261': RollingMean: 15.571429, 'air_96773a6236d279b1': RollingMean: 25.428571, 'air_968d72c12eed09f0': RollingMean: 17.285714, 'air_96929a799b12a43e': RollingMean: 27.428571, 'air_96ec3cfe78cb0652': RollingMean: 21.285714, 'air_97159fc4e90053fe': RollingMean: 24.142857, 'air_97958e7fce98b6a3': RollingMean: 17.857143, 'air_97b2a9f975fc702c': RollingMean: 40.428571, 'air_97cf68dc1a9beac0': RollingMean: 14.571429, 'air_97e0f2feec4d577a': RollingMean: 18.714286, 'air_9828505fefc77d75': RollingMean: 12.714286, 'air_98b54e32ccddd896': RollingMean: 17.428571, 'air_990a642a3deb2903': RollingMean: 33.428571, 'air_99157b6163835eec': RollingMean: 34.428571, 'air_99a5183695b849f9': RollingMean: 25.857143, 'air_99b01136f451fc0e': RollingMean: 43.857143, 'air_99c3eae84130c1cb': RollingMean: 41.857143, 'air_9a30407764f4ff84': RollingMean: 20., 'air_9a6f6e7f623003d2': RollingMean: 2.857143, 'air_9aa32b3db0fab3a5': RollingMean: 15.857143, 'air_9aa92007e3628dbc': RollingMean: 36.571429, 'air_9ae7081cb77dc51e': RollingMean: 32.857143, 'air_9b13c7feb0a0c431': RollingMean: 12.714286, 'air_9b6af3db40da4ae2': RollingMean: 29., 'air_9bbc673495e23532': RollingMean: 4.571429, 'air_9bf0ccac497c4af3': RollingMean: 47.285714, 'air_9bf595ef095572fb': RollingMean: 28.142857, 'air_9c6787aa03a45586': RollingMean: 86., 'air_9ca2767761efff4d': RollingMean: 9.142857, 'air_9cd5e310f488bced': RollingMean: 11.571429, 'air_9cf2f1ba86229773': RollingMean: 34.285714, 'air_9d3482b4680aee88': RollingMean: 9.571429, 'air_9d452a881f7f2bb7': RollingMean: 9.285714, 'air_9d474ec2448c700d': RollingMean: 11.857143, 'air_9d5a980b211e1795': RollingMean: 11.285714, 'air_9d93d95720f2e831': RollingMean: 8.857143, 'air_9dc9483f717d73ee': RollingMean: 4.714286, 'air_9dd7d38b0f1760c4': RollingMean: 3., 'air_9e920b758503ef54': RollingMean: 7.285714, 'air_9efaa7ded03c5a71': RollingMean: 13.857143, 'air_9f277fb7a2c1d842': RollingMean: 9.857143, 'air_9fc607777ad76b26': RollingMean: 16.285714, 'air_a083834e7ffe187e': RollingMean: 20.857143, 'air_a11473cc1eb9a27f': RollingMean: 31.714286, 'air_a17f0778617c76e2': RollingMean: 30.285714, 'air_a1fe8c588c8d2f30': RollingMean: 17.285714, 'air_a218912784bf767d': RollingMean: 13., 'air_a21ffca0bea1661a': RollingMean: 1.142857, 'air_a239a44805932bab': RollingMean: 35.142857, 'air_a24bf50c3e90d583': RollingMean: 18.285714, 'air_a2567267116a3b75': RollingMean: 16.285714, 'air_a257c9749d8d0ff6': RollingMean: 18.142857, 'air_a271c9ba19e81d17': RollingMean: 29.142857, 'air_a2b29aa7feb4e36f': RollingMean: 21.571429, 'air_a304330715435390': RollingMean: 8.142857, 'air_a33461f4392ec62c': RollingMean: 30.142857, 'air_a373500730e2a9e0': RollingMean: 10.142857, 'air_a38f25e3399d1b25': RollingMean: 41.142857, 'air_a41b032371a63427': RollingMean: 9.428571, 'air_a49f1cf0634f13e5': RollingMean: 16.857143, 'air_a510dcfe979f09eb': RollingMean: 12.714286, 'air_a546cbf478a8b6e4': RollingMean: 28.714286, 'air_a55d17bd3f3033cb': RollingMean: 12.142857, 'air_a563896da3777078': RollingMean: 23.142857, 'air_a678e5b144ca24ce': RollingMean: 18.571429, 'air_a7404a854919e990': RollingMean: 8.571429, 'air_a8533b7a992bb0ca': RollingMean: 19.285714, 'air_a85f0c0c889f6b7e': RollingMean: 50.571429, 'air_a85f8c0bfd61889f': RollingMean: 17.142857, 'air_a88ac559064dec08': RollingMean: 28., 'air_a9133955abccf071': RollingMean: 27., 'air_a9178f19da58fe99': RollingMean: 7.714286, 'air_a9a380530c1e121f': RollingMean: 45.285714, 'air_aa0049fe3cc6f4d6': RollingMean: 11.142857, 'air_ab3ae0e410b20069': RollingMean: 16.714286, 'air_ab9746a0f83084b7': RollingMean: 1.142857, 'air_abcdc8115988a010': RollingMean: 9.857143, 'air_abf06fcca748dca5': RollingMean: 9., 'air_ac7a7427c9ae12a5': RollingMean: 63.857143, 'air_ad13e71e21235131': RollingMean: 10.285714, 'air_ad60f6b76c9df7ed': RollingMean: 30.571429, 'air_ad7777590c884721': RollingMean: 9.428571, 'air_add9a575623726c8': RollingMean: 47.571429, 'air_ade6e836ffd1da64': RollingMean: 11.571429, 'air_aed3a8b49abe4a48': RollingMean: 5., 'air_af03c277a167b2bd': RollingMean: 36.285714, 'air_af24e3e817dea1e5': RollingMean: 15.285714, 'air_af63df35857b16e6': RollingMean: 25.714286, 'air_b0a6a4c5e95c74cf': RollingMean: 19.571429, 'air_b162fb07fbbdea33': RollingMean: 15.285714, 'air_b192fb5310436005': RollingMean: 8., 'air_b1a72bf1ebf4b8ef': RollingMean: 46.285714, 'air_b1bb1fae86617d7a': RollingMean: 36.428571, 'air_b1d822f75c9fc615': RollingMean: 10.714286, 'air_b2395df0e874078d': RollingMean: 6.571429, 'air_b23d0f519291247d': RollingMean: 34., 'air_b259b4e4a51a690d': RollingMean: 18.142857, 'air_b28bed4b2e7167b7': RollingMean: 19.142857, 'air_b2a639cc7e02edf1': RollingMean: 20.142857, 'air_b2d8bc9c88b85f96': RollingMean: 8.142857, 'air_b2d97bd2337c5ba7': RollingMean: 36.142857, 'air_b2dcec37b83e2494': RollingMean: 7.857143, 'air_b30fffd7ab1e75a5': RollingMean: 9.428571, 'air_b3180b74332ba886': RollingMean: 14.285714, 'air_b3a824511477a4ed': RollingMean: 6.285714, 'air_b439391e72899756': RollingMean: 20.142857, 'air_b45b8e456f53942a': RollingMean: 9.285714, 'air_b4f32bcc399da2b9': RollingMean: 20.428571, 'air_b5598d12d1b84890': RollingMean: 5.428571, 'air_b5bdd318005d9aa4': RollingMean: 39.428571, 'air_b60cc7d6aee68194': RollingMean: 13.285714, 'air_b711b43ae472cb6b': RollingMean: 22.571429, 'air_b7fa3d2fca744dd2': RollingMean: 41.428571, 'air_b80fed1a07c817d2': RollingMean: 4.142857, 'air_b88192b35ac03c24': RollingMean: 21., 'air_b8925441167c3152': RollingMean: 1.714286, 'air_b8a5ee69e5fdcc5b': RollingMean: 31.571429, 'air_b8d9e1624baaadc2': RollingMean: 6.857143, 'air_b9e27558fb8bd5c4': RollingMean: 13.714286, 'air_ba495cccc8f0f237': RollingMean: 18.714286, 'air_ba937bf13d40fb24': RollingMean: 15.714286, 'air_bac5f4441db21db9': RollingMean: 45.857143, 'air_baf28ac9f13a307d': RollingMean: 14.571429, 'air_bb09595bab7d5cfb': RollingMean: 33.857143, 'air_bb26d6d079594414': RollingMean: 11.428571, 'air_bb4ff06cd661ee9b': RollingMean: 33.285714, 'air_bbe1c1a47e09f161': RollingMean: 1.428571, 'air_bc991c51d6613745': RollingMean: 19.714286, 'air_bc9a129e11a2efe0': RollingMean: 30.857143, 'air_bcce1ea4350b7b72': RollingMean: 26., 'air_bd74a9222edfdfe1': RollingMean: 13.428571, 'air_bdd32aa407c16335': RollingMean: 16.714286, 'air_bebd55ed63ab2422': RollingMean: 10.857143, 'air_bed603c423b7d9d4': RollingMean: 5.857143, 'air_bedd35489e666605': RollingMean: 35.857143, 'air_bf13014b6e3e60ca': RollingMean: 31.857143, 'air_bf21b8350771879b': RollingMean: 22.142857, 'air_bf617aa68d5f1cfa': RollingMean: 7.571429, 'air_bf7591560077332d': RollingMean: 9.142857, 'air_bfafaed35e213fd7': RollingMean: 14.714286, 'air_bfda7731a6c6fc61': RollingMean: 20.285714, 'air_c027e2b560442808': RollingMean: 17.285714, 'air_c0385db498b391e5': RollingMean: 34.285714, 'air_c1d5d165c055b866': RollingMean: 30.142857, 'air_c1ff20617c54fee7': RollingMean: 8.142857, 'air_c225148c0fcc5c72': RollingMean: 39.428571, 'air_c2626f5f86d57342': RollingMean: 16.142857, 'air_c26f027b5bc1f081': RollingMean: 5.857143, 'air_c28983412a7eefcf': RollingMean: 40.571429, 'air_c2c8435bdb3516d4': RollingMean: 35., 'air_c31472d14e29cee8': RollingMean: 16.714286, 'air_c3585b0fba3998d0': RollingMean: 9.285714, 'air_c3bc011cca3bec65': RollingMean: 4.285714, 'air_c3dcaf3aeb18e20e': RollingMean: 15.714286, 'air_c47aa7493b15f297': RollingMean: 23., 'air_c4fa5c562d5409ca': RollingMean: 15.857143, 'air_c52c63c781fe48f6': RollingMean: 29.428571, 'air_c5459218282bedd5': RollingMean: 21.714286, 'air_c66dbd2c37832d00': RollingMean: 16.714286, 'air_c6a164dd4060e960': RollingMean: 15.857143, 'air_c6aa2efba0ffc8eb': RollingMean: 26.428571, 'air_c6ffd6a93e6b68d6': RollingMean: 15.714286, 'air_c73d319ffabf287a': RollingMean: 18.428571, 'air_c759b6abeb552160': RollingMean: 7.142857, 'air_c77ee2b7d36da265': RollingMean: 42., 'air_c7d30ab0e07f31d5': RollingMean: 20.285714, 'air_c7f78b4f3cba33ff': RollingMean: 25.142857, 'air_c8265ecc116f2284': RollingMean: 9.142857, 'air_c88467d88b2c8ecd': RollingMean: 19.428571, 'air_c8a657c8c5c93d69': RollingMean: 9.142857, 'air_c8c0ef02ed72053f': RollingMean: 27.428571, 'air_c8fe396d6c46275d': RollingMean: 18., 'air_c92745dfdd2ec68a': RollingMean: 21.857143, 'air_c9ed65554b6edffb': RollingMean: 12.714286, 'air_c9f6de13be8b8f25': RollingMean: 3.571429, 'air_ca1315af9e073bd1': RollingMean: 43.714286, 'air_ca6ae8d49a2f1eaf': RollingMean: 24.142857, 'air_ca957d3a1529fbd3': RollingMean: 31.285714, 'air_cadf9cfb510a1d78': RollingMean: 29.714286, 'air_caf996ac27206301': RollingMean: 4.714286, 'air_cb083b4789a8d3a2': RollingMean: 18.714286, 'air_cb25551c4cd8d9f3': RollingMean: 5.714286, 'air_cb7467aed805e7fe': RollingMean: 40.428571, 'air_cb935ff8610ba3d3': RollingMean: 4.142857, 'air_cbe139af83feb388': RollingMean: 10.428571, 'air_cbe867adcf44e14f': RollingMean: 15.714286, 'air_cc1a0e985ce63711': RollingMean: 27.285714, 'air_cc35590cd1da8554': RollingMean: 24.571429, 'air_ccd19a5bc5573ae5': RollingMean: 34.857143, 'air_cd4b301d5d3918d8': RollingMean: 5.857143, 'air_cd5f54969be9ed08': RollingMean: 6.285714, 'air_ced6297e5bdf5130': RollingMean: 23.428571, 'air_cf2229e64408d9fe': RollingMean: 20.428571, 'air_cf22e368c1a71d53': RollingMean: 34.857143, 'air_cf5ab75a0afb8af9': RollingMean: 48., 'air_cfcc94797d2b5d3d': RollingMean: 18.285714, 'air_cfdeb326418194ff': RollingMean: 17.285714, 'air_d00161e19f08290b': RollingMean: 28.714286, 'air_d00a15343325e5f7': RollingMean: 21.142857, 'air_d07e57b21109304a': RollingMean: 11.428571, 'air_d0a1e69685259c92': RollingMean: 32., 'air_d0a7bd3339c3d12a': RollingMean: 46.714286, 'air_d0e8a085d8dc83aa': RollingMean: 6.571429, 'air_d138b593ebda55cc': RollingMean: 6., 'air_d1418d6fd6d634f2': RollingMean: 17.714286, 'air_d186b2cb0b9ce022': RollingMean: 14.428571, 'air_d1f20424f76cc78e': RollingMean: 19.857143, 'air_d34c0861a2be94cb': RollingMean: 41.285714, 'air_d3e7b5952cd09ccb': RollingMean: 17.857143, 'air_d44d210d2994f01b': RollingMean: 7., 'air_d473620754bf9fc2': RollingMean: 11., 'air_d477b6339b8ce69f': RollingMean: 11., 'air_d4981cdde163b172': RollingMean: 27.142857, 'air_d4b5a4b04c5f2d04': RollingMean: 15.142857, 'air_d4d218b451f82c3d': RollingMean: 9.428571, 'air_d500b48a8735fbd3': RollingMean: 20.142857, 'air_d54d6fcb116fbed3': RollingMean: 4.714286, 'air_d5e0a20370c325c7': RollingMean: 33.285714, 'air_d63cfa6d6ab78446': RollingMean: 20.857143, 'air_d69b08a175bc0387': RollingMean: 13.571429, 'air_d6b3e67261f07646': RollingMean: 11.714286, 'air_d8abb9e490abf94f': RollingMean: 12.714286, 'air_d97dabf7aae60da5': RollingMean: 34.714286, 'air_d98380a4aeb0290b': RollingMean: 44.285714, 'air_daa7947e1c47f5ed': RollingMean: 31.142857, 'air_dabfbd0ec951925a': RollingMean: 8.142857, 'air_dad0b6a36138f309': RollingMean: 5.142857, 'air_db1233ad855b34d5': RollingMean: 25.857143, 'air_db4b38ebe7a7ceff': RollingMean: 19.714286, 'air_db80363d35f10926': RollingMean: 36.142857, 'air_dbf64f1ce38c7442': RollingMean: 14., 'air_dc0e080ba0a5e5af': RollingMean: 9.428571, 'air_dc71c6cc06cd1aa2': RollingMean: 6.857143, 'air_de692863bb2dd758': RollingMean: 22., 'air_de803f7e324936b8': RollingMean: 24.571429, 'air_de88770300008624': RollingMean: 21.857143, 'air_dea0655f96947922': RollingMean: 37., 'air_df507aec929ce5f6': RollingMean: 24.857143, 'air_df554c4527a1cfe6': RollingMean: 57.285714, 'air_df5cf5cd03eb68d0': RollingMean: 7.285714, 'air_df843e6b22e8d540': RollingMean: 12.285714, 'air_df9355c47c5df9d3': RollingMean: 32.428571, 'air_dfad598ff642dab7': RollingMean: 28.285714, 'air_dfe068a1bf85f395': RollingMean: 41.142857, 'air_e00fe7853c0100d6': RollingMean: 18.142857, 'air_e0118664da63a2d0': RollingMean: 15.428571, 'air_e01d99390355408d': RollingMean: 10.571429, 'air_e053c561f32acc28': RollingMean: 16.428571, 'air_e08b9cf82057a170': RollingMean: 34., 'air_e0aee25b56a069f2': RollingMean: 14.857143, 'air_e0e69668214ff972': RollingMean: 11.142857, 'air_e0f241bd406810c0': RollingMean: 34.714286, 'air_e1b76fcb5208fb6b': RollingMean: 18.714286, 'air_e2208a79e2678432': RollingMean: 47.428571, 'air_e270aff84ac7e4c8': RollingMean: 22.714286, 'air_e3020992d5fe5dfd': RollingMean: 14.285714, 'air_e34c631c766f2766': RollingMean: 24.857143, 'air_e42bdc3377d1eee7': RollingMean: 23.428571, 'air_e483f5b3c4f310e0': RollingMean: 6., 'air_e524c6a9e06cc3a1': RollingMean: 9., 'air_e55abd740f93ecc4': RollingMean: 45.285714, 'air_e57dd6884595f60d': RollingMean: 37.714286, 'air_e58f669b6f1a08ce': RollingMean: 12.142857, 'air_e5cf003abcc5febb': RollingMean: 18.285714, 'air_e64de0a6bf0739af': RollingMean: 38.285714, 'air_e657ca554b0c008c': RollingMean: 28.285714, 'air_e700e390226d9985': RollingMean: 18.714286, 'air_e76a668009c5dabc': RollingMean: 9.285714, 'air_e7d2ac6d53d1b744': RollingMean: 11.714286, 'air_e7fbee4e3cfe65c5': RollingMean: 39., 'air_e88bbe2ede3467aa': RollingMean: 21.285714, 'air_e89735e80d614a7e': RollingMean: 31.428571, 'air_e8ed9335d0c38333': RollingMean: 30.142857, 'air_e9ebf7fc520ac76a': RollingMean: 29.142857, 'air_ea6d0c3acf00b22a': RollingMean: 25.571429, 'air_ea7c16131980c837': RollingMean: 7., 'air_eb120e6d384a17a8': RollingMean: 48.428571, 'air_eb20a89bba7dd3d0': RollingMean: 3.142857, 'air_eb2d2653586315dd': RollingMean: 35.285714, 'air_eb5788dba285e725': RollingMean: 31.142857, 'air_ebd31e812960f517': RollingMean: 30.571429, 'air_ebe02c3090271fa9': RollingMean: 11.285714, 'air_ec0fad2def4dcff0': RollingMean: 16., 'air_eca4a5a191e8d993': RollingMean: 32.571429, 'air_eca5e0064dc9314a': RollingMean: 33.714286, 'air_ecab54b57a71b10d': RollingMean: 13., 'air_eceb97ad6a7d4c07': RollingMean: 30.714286, 'air_ecf7f141339f1d57': RollingMean: 20.142857, 'air_eda179770dfa9f91': RollingMean: 10.428571, 'air_edd5e3d696a5811b': RollingMean: 38.142857, 'air_ee3a01f0c71a769f': RollingMean: 28.142857, 'air_ee3ba9af184c6c82': RollingMean: 21.285714, 'air_eec5e572b9eb9c23': RollingMean: 15.571429, 'air_eeeadee005c006a2': RollingMean: 13.714286, 'air_ef47430bcd6f6a89': RollingMean: 13.857143, 'air_ef789667e2e6fe96': RollingMean: 36.428571, 'air_ef920fa6f4b085f6': RollingMean: 32.142857, 'air_efc80d3f96b3aff7': RollingMean: 8.428571, 'air_efd70b04de878f25': RollingMean: 30., 'air_efef1e3daecce07e': RollingMean: 43.428571, 'air_f068442ebb6c246c': RollingMean: 11.571429, 'air_f0c7272956e62f12': RollingMean: 5.428571, 'air_f0fb0975bdc2cdf9': RollingMean: 10.142857, 'air_f1094dbf2aef85d9': RollingMean: 7.142857, 'air_f180301886c21375': RollingMean: 18.142857, 'air_f183a514cb8ff4fa': RollingMean: 38., 'air_f1f9027d4fa8f653': RollingMean: 22., 'air_f267dd70a6a6b5d3': RollingMean: 31.857143, 'air_f26f36ec4dc5adb0': RollingMean: 39.857143, 'air_f2985de32bb792e0': RollingMean: 24.714286, 'air_f2c5a1f24279c531': RollingMean: 18.571429, 'air_f3602e4fa2f12993': RollingMean: 11., 'air_f3f9824b7d70c3cf': RollingMean: 15.714286, 'air_f4936b91c9addbf0': RollingMean: 16.142857, 'air_f593fa60ac3541e2': RollingMean: 8., 'air_f690c42545146e0a': RollingMean: 11.714286, 'air_f6b2489ccf873c3b': RollingMean: 16.285714, 'air_f6bfd27e2e174d16': RollingMean: 14.142857, 'air_f6cdaf7b7fdc6d78': RollingMean: 12.285714, 'air_f8233ad00755c35c': RollingMean: 28.714286, 'air_f85e21e543cf44f2': RollingMean: 5.714286, 'air_f88898cd09f40496': RollingMean: 7.714286, 'air_f911308e19d64236': RollingMean: 47., 'air_f9168b23fdfc1e52': RollingMean: 14.142857, 'air_f927b2da69a82341': RollingMean: 9.428571, 'air_f957c6d6467d4d90': RollingMean: 9.285714, 'air_f96765e800907c77': RollingMean: 38.428571, 'air_fa12b40b02fecfd8': RollingMean: 14.571429, 'air_fa4ffc9057812fa2': RollingMean: 6., 'air_fab092c35776a9b1': RollingMean: 10.571429, 'air_fb44f566d4f64a4e': RollingMean: 14.857143, 'air_fbadf737162a5ce3': RollingMean: 15., 'air_fc477473134e9ae5': RollingMean: 15.714286, 'air_fcd4492c83f1c6b9': RollingMean: 24.142857, 'air_fcfbdcf7b1f82c6e': RollingMean: 39.857143, 'air_fd154088b1de6fa7': RollingMean: 4.428571, 'air_fd6aac1043520e83': RollingMean: 39.428571, 'air_fdc02ec4a3d21ea4': RollingMean: 8.142857, 'air_fdcfef8bd859f650': RollingMean: 3.714286, 'air_fe22ef5a9cbef123': RollingMean: 24.428571, 'air_fe58c074ec1445ea': RollingMean: 35., 'air_fea5dc9594450608': RollingMean: 17.428571, 'air_fee8dcf4d619598e': RollingMean: 29.571429, 'air_fef9ccb3ba0da2f7': RollingMean: 8.714286, 'air_ffcc2d5087e1b476': RollingMean: 24.714286, 'air_fff68b929994bfbd': RollingMean: 4.142857}), 'how': RollingMean: 0., 'target_name': 'target'} target_rollingmean_14_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_14_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 28.428571, 'air_0164b9927d20bcc3': RollingMean: 6.571429, 'air_0241aa3964b7f861': RollingMean: 9.928571, 'air_0328696196e46f18': RollingMean: 8.142857, 'air_034a3d5b40d5b1b1': RollingMean: 24.285714, 'air_036d4f1ee7285390': RollingMean: 22.714286, 'air_0382c794b73b51ad': RollingMean: 26.571429, 'air_03963426c9312048': RollingMean: 47.142857, 'air_04341b588bde96cd': RollingMean: 32.5, 'air_049f6d5b402a31b2': RollingMean: 13.571429, 'air_04cae7c1bc9b2a0b': RollingMean: 20., 'air_0585011fa179bcce': RollingMean: 4.785714, 'air_05c325d315cc17f5': RollingMean: 28.142857, 'air_0647f17b4dc041c8': RollingMean: 30.928571, 'air_064e203265ee5753': RollingMean: 18.714286, 'air_066f0221b8a4d533': RollingMean: 11.142857, 'air_06f95ac5c33aca10': RollingMean: 29.714286, 'air_0728814bd98f7367': RollingMean: 7.785714, 'air_0768ab3910f7967f': RollingMean: 28.214286, 'air_07b314d83059c4d2': RollingMean: 38.571429, 'air_07bb665f9cdfbdfb': RollingMean: 25.785714, 'air_082908692355165e': RollingMean: 46.071429, 'air_083ddc520ea47e1e': RollingMean: 12.642857, 'air_0845d8395f30c6bb': RollingMean: 22.714286, 'air_084d98859256acf0': RollingMean: 16.142857, 'air_0867f7bebad6a649': RollingMean: 21., 'air_08ba8cd01b3ba010': RollingMean: 10.285714, 'air_08cb3c4ee6cd6a22': RollingMean: 12.642857, 'air_08ef81d5b7a0d13f': RollingMean: 14.5, 'air_08f994758a1e76d4': RollingMean: 28., 'air_09040f6df960ddb8': RollingMean: 15.571429, 'air_0919d54f0c9a24b8': RollingMean: 35.857143, 'air_09661c0f3259cc04': RollingMean: 28.857143, 'air_09a845d5b5944b01': RollingMean: 6.928571, 'air_09fd1f5c58583141': RollingMean: 8.642857, 'air_0a74a5408a0b8642': RollingMean: 28.428571, 'air_0b184ec04c741a6a': RollingMean: 12.857143, 'air_0b1e72d2d4422b20': RollingMean: 20.928571, 'air_0b9038300f8b2b50': RollingMean: 11.428571, 'air_0e1eae99b8723bc1': RollingMean: 10.642857, 'air_0e7c11b9abc50163': RollingMean: 35.071429, 'air_0ead98dd07e7a82a': RollingMean: 13.571429, 'air_0f0cdeee6c9bf3d7': RollingMean: 22.071429, 'air_0f2f96335f274801': RollingMean: 9.928571, 'air_0f60e1576a7d397d': RollingMean: 6.214286, 'air_1033310359ceeac1': RollingMean: 18.642857, 'air_10393f12e9069760': RollingMean: 13.571429, 'air_105a7954e32dba9b': RollingMean: 51.5, 'air_10713fbf3071c361': RollingMean: 12.285714, 'air_10bbe8acd943d8f6': RollingMean: 30.857143, 'air_12c4fb7a423df20d': RollingMean: 20.071429, 'air_138ee734ac79ff90': RollingMean: 7.214286, 'air_138ff410757b845f': RollingMean: 54.428571, 'air_1408dd53f31a8a65': RollingMean: 27.857143, 'air_142e78ba7001da9c': RollingMean: 13.642857, 'air_1509881b22965b34': RollingMean: 19., 'air_152c1f08d7d20e07': RollingMean: 11.857143, 'air_15ae33469e9ea2dd': RollingMean: 10.285714, 'air_15e6e15c7ea2c162': RollingMean: 18.642857, 'air_16179d43b6ee5fd8': RollingMean: 7.714286, 'air_1653a6c513865af3': RollingMean: 32.5, 'air_168441ada3e878e1': RollingMean: 45.714286, 'air_16c4cfddeb2cf69b': RollingMean: 12.071429, 'air_16cf0a73233896de': RollingMean: 11.428571, 'air_1707a3f18bb0da07': RollingMean: 26.214286, 'air_17a6ab40f97fd4d8': RollingMean: 8.285714, 'air_17bed6dbf7c8b0fc': RollingMean: 19.785714, 'air_1979eaff8189d086': RollingMean: 9.071429, 'air_1ab60ce33bfed8a8': RollingMean: 9.642857, 'air_1ae94f514a0bce13': RollingMean: 7., 'air_1ba4e87ef7422183': RollingMean: 38.714286, 'air_1c0b150f9e696a5f': RollingMean: 100.785714, 'air_1c95a84924d72500': RollingMean: 8.714286, 'air_1d1e8860ae04f8e9': RollingMean: 15.071429, 'air_1d25ca6c76df48b4': RollingMean: 42.214286, 'air_1d3f797dd1f7cf1c': RollingMean: 33.571429, 'air_1dd8f6f47480d1a2': RollingMean: 37.428571, 'air_1dea9815ccd36620': RollingMean: 10.5, 'air_1e23210b584540e7': RollingMean: 3.857143, 'air_1e665503b8474c55': RollingMean: 5.785714, 'air_1eeff462acb24fb7': RollingMean: 16.428571, 'air_1f1390a8be2272b3': RollingMean: 18.214286, 'air_1f34e9beded2231a': RollingMean: 8.214286, 'air_1f7f8fa557bc0d55': RollingMean: 3.928571, 'air_2009041dbf9264de': RollingMean: 51.285714, 'air_20619d21192aa571': RollingMean: 16.785714, 'air_20add8092c9bb51d': RollingMean: 31.642857, 'air_2195cd5025a98033': RollingMean: 29.428571, 'air_21f5052d5330528d': RollingMean: 30.857143, 'air_220cba70c890b119': RollingMean: 7.928571, 'air_22682e965418936f': RollingMean: 10., 'air_228f10bec0bda9c8': RollingMean: 16.785714, 'air_229d7e508d9f1b5e': RollingMean: 9.571429, 'air_232dcee6f7c51d37': RollingMean: 5.571429, 'air_234d3dbf7f3d5a50': RollingMean: 7.642857, 'air_23e1b11aee2a1407': RollingMean: 46., 'air_23ee674e91469086': RollingMean: 20.714286, 'air_24b9b2a020826ede': RollingMean: 31.5, 'air_24e8414b9b07decb': RollingMean: 6.071429, 'air_2545dd3a00f265e2': RollingMean: 46.785714, 'air_256be208a979e023': RollingMean: 7.428571, 'air_2570ccb93badde68': RollingMean: 39.357143, 'air_258ad2619d7bff9a': RollingMean: 36.714286, 'air_258dc112912fc458': RollingMean: 61.428571, 'air_25c583983246b7b0': RollingMean: 24.428571, 'air_25d8e5cc57dd87d9': RollingMean: 24.928571, 'air_25e9888d30b386df': RollingMean: 4.928571, 'air_2634e41551e9807d': RollingMean: 14.857143, 'air_26c5bbeb7bb82bf1': RollingMean: 27.071429, 'air_26f10355d9b4d82a': RollingMean: 33.785714, 'air_2703dcb33192b181': RollingMean: 52.428571, 'air_275732a5db46f4d3': RollingMean: 15.928571, 'air_27e991812b0d9c92': RollingMean: 41.428571, 'air_28064154614b2e6c': RollingMean: 22.857143, 'air_287d2de7d3c93406': RollingMean: 13., 'air_28a9fa1ec0839375': RollingMean: 29.071429, 'air_28dbe91c4c9656be': RollingMean: 33.571429, 'air_290e7a57b390f78e': RollingMean: 13.857143, 'air_298513175efdf261': RollingMean: 26.428571, 'air_2a184c1745274b2b': RollingMean: 4.214286, 'air_2a24aec099333f39': RollingMean: 7.285714, 'air_2a3743e37aab04b4': RollingMean: 20.071429, 'air_2a485b92210c98b5': RollingMean: 18.142857, 'air_2a7f14da7fe0f699': RollingMean: 27.214286, 'air_2aab19554f91ff82': RollingMean: 49.714286, 'air_2ac361b97630e2df': RollingMean: 12.071429, 'air_2b8b29ddfd35018e': RollingMean: 10.928571, 'air_2b9bc9f5f5168ea1': RollingMean: 21.428571, 'air_2bffb19a24d11729': RollingMean: 12.428571, 'air_2c505f9ad67d4635': RollingMean: 16.5, 'air_2c6c79d597e48096': RollingMean: 17.928571, 'air_2c6fef1ce0e13a5a': RollingMean: 30.785714, 'air_2c989829acbd1c6b': RollingMean: 28.357143, 'air_2cee51fa6fdf6c0d': RollingMean: 16.571429, 'air_2d3afcb91762fe01': RollingMean: 45.142857, 'air_2d78d9a1f4dd02ca': RollingMean: 11.357143, 'air_2e7cb1f1a2a9cd6a': RollingMean: 27.571429, 'air_2f8ced25216df926': RollingMean: 14.142857, 'air_2fc149abe33adcb4': RollingMean: 35.214286, 'air_2fc478dc9f0a6b31': RollingMean: 11.428571, 'air_2fed81034f8834e5': RollingMean: 20.714286, 'air_303bac187b53083a': RollingMean: 10.428571, 'air_310e467e6e625004': RollingMean: 15.214286, 'air_3155ee23d92202da': RollingMean: 16.5, 'air_31c753b48a657b6c': RollingMean: 24.285714, 'air_32460819c7600037': RollingMean: 51.071429, 'air_324f7c39a8410e7c': RollingMean: 10.928571, 'air_326ca454ef3558bc': RollingMean: 22.214286, 'air_32b02ba5dc2027f4': RollingMean: 27.5, 'air_32c61b620a766138': RollingMean: 26.785714, 'air_32f5d7cd696e3c4a': RollingMean: 23.142857, 'air_33b01025210d6007': RollingMean: 11.785714, 'air_3440e0ea1b70a99b': RollingMean: 28., 'air_346ade7d29230634': RollingMean: 7.857143, 'air_347be2c4feeb408b': RollingMean: 21.071429, 'air_349278fa964bb12f': RollingMean: 19.214286, 'air_3525f11ef0bf0c35': RollingMean: 41.142857, 'air_35512c42db0868da': RollingMean: 6.071429, 'air_3561fd1c0bce6a95': RollingMean: 10.714286, 'air_35c4732dcbfe31be': RollingMean: 6.785714, 'air_36429b5ca4407b3e': RollingMean: 20., 'air_36bcf77d3382d36e': RollingMean: 32.5, 'air_37189c92b6c761ec': RollingMean: 14.857143, 'air_375a5241615b5e22': RollingMean: 7., 'air_382f5ace4e2247b8': RollingMean: 7.785714, 'air_383f5b2f8d345a49': RollingMean: 12.357143, 'air_38746ffe9aa20c7e': RollingMean: 3.785714, 'air_396166d47733d5c9': RollingMean: 24.714286, 'air_396942e6423a2145': RollingMean: 21.857143, 'air_397d3f32a7196aa2': RollingMean: 35.5, 'air_3980af67be35afdb': RollingMean: 15.785714, 'air_3982a2c4ea2ed431': RollingMean: 30.5, 'air_399904bdb7685ca0': RollingMean: 27.785714, 'air_39dccf7df20b1c6a': RollingMean: 26.714286, 'air_3a8a3f8fb5cd7f88': RollingMean: 21.214286, 'air_3aa839e8e0cb6c87': RollingMean: 27.5, 'air_3ac24136722e2291': RollingMean: 13.142857, 'air_3b20733899b5287f': RollingMean: 39.857143, 'air_3b6438b125086430': RollingMean: 13.285714, 'air_3bb99a1fe0583897': RollingMean: 40.285714, 'air_3bd49f98ab7f36ab': RollingMean: 13., 'air_3c05c8f26c611eb9': RollingMean: 25.285714, 'air_3c938075889fc059': RollingMean: 30.5, 'air_3cad29d1a23209d2': RollingMean: 8.357143, 'air_3caef3f76b8f26c5': RollingMean: 25.642857, 'air_3d3a2b509180e798': RollingMean: 17.714286, 'air_3e6cea17a9d2c0f1': RollingMean: 17.071429, 'air_3e93f3c81008696d': RollingMean: 34.214286, 'air_3f91d592acd6cc0b': RollingMean: 24.928571, 'air_401b39f97e56b939': RollingMean: 11.785714, 'air_4043b7ccfbffa732': RollingMean: 44.571429, 'air_4092cfbd95a3ac1b': RollingMean: 23., 'air_40953e2d8b4f2857': RollingMean: 15.428571, 'air_40f6193ea3ed1b91': RollingMean: 16.714286, 'air_414ff459ed18fa48': RollingMean: 16., 'air_41bbf6e1d9814c4b': RollingMean: 8.714286, 'air_421670f21da5ba31': RollingMean: 17.928571, 'air_4254c3fc3ad078bd': RollingMean: 11.357143, 'air_42c9aa6d617c5057': RollingMean: 42.714286, 'air_42d41eb58cad170e': RollingMean: 32.285714, 'air_43b65e4b05bff2d3': RollingMean: 20.642857, 'air_43d577e0c9460e64': RollingMean: 35.142857, 'air_4433ab8e9999915f': RollingMean: 21.142857, 'air_4481a87c1d7c9896': RollingMean: 20.928571, 'air_452100f5305dde64': RollingMean: 7.428571, 'air_45326ebb8dc72cfb': RollingMean: 29.142857, 'air_4570f52104fe0982': RollingMean: 7.357143, 'air_4579cb0669fd411b': RollingMean: 18.071429, 'air_457efe8c3a30ea17': RollingMean: 6.285714, 'air_464a62de0d57be1e': RollingMean: 26.857143, 'air_465bddfed3353b23': RollingMean: 27.071429, 'air_47070be6093f123e': RollingMean: 46.285714, 'air_472b19e3b5bffa41': RollingMean: 13.5, 'air_473cf23b9e7c0a37': RollingMean: 9.357143, 'air_473f98b212d37b4a': RollingMean: 27.357143, 'air_47beaffd3806c979': RollingMean: 19.214286, 'air_483eba479dc9910d': RollingMean: 21.642857, 'air_48e9fc98b62495a7': RollingMean: 17.071429, 'air_48f4da6223571da4': RollingMean: 22.571429, 'air_48ffd31594bc3263': RollingMean: 4.928571, 'air_49211568cab5fdee': RollingMean: 25., 'air_4974785f48853db9': RollingMean: 6.928571, 'air_4b251b9f8373f1ae': RollingMean: 26.928571, 'air_4b380b4db9d37883': RollingMean: 25.571429, 'air_4b55d8aea1d2b395': RollingMean: 34.285714, 'air_4b9085d0d46a6211': RollingMean: 28.142857, 'air_4beac252540f865e': RollingMean: 41.571429, 'air_4c2ed28f3f19ca52': RollingMean: 14.642857, 'air_4c665a2bfff0da3b': RollingMean: 8.857143, 'air_4c727b55acdee495': RollingMean: 15.357143, 'air_4cab15ad29c0ffbc': RollingMean: 21.214286, 'air_4cab91146e3d1897': RollingMean: 15., 'air_4cca5666eaf5c709': RollingMean: 34.428571, 'air_4ce7b17062a1bf73': RollingMean: 7.857143, 'air_4d21676ed11f0bac': RollingMean: 32.857143, 'air_4d71826793c09b22': RollingMean: 21.642857, 'air_4d90a22572fa1ec9': RollingMean: 25.785714, 'air_4de6d887a7b1c1fc': RollingMean: 15.357143, 'air_4dea8d17f6f59c56': RollingMean: 25.214286, 'air_4e1c38f68f435596': RollingMean: 34.071429, 'air_4f762e840b3996e1': RollingMean: 11.285714, 'air_4feeb8600f131e43': RollingMean: 57.714286, 'air_500641aca4cf673c': RollingMean: 15.142857, 'air_506fe758114df773': RollingMean: 28.428571, 'air_51281cd059d7b89b': RollingMean: 13.928571, 'air_51319e7acf0438cf': RollingMean: 13.571429, 'air_52a08ef3efdb4bb0': RollingMean: 47.928571, 'air_52e2a1fd42bc917a': RollingMean: 11.642857, 'air_536043fcf1a4f8a4': RollingMean: 30.071429, 'air_539d693f7317c62d': RollingMean: 18., 'air_546b353cbea4a45b': RollingMean: 17.357143, 'air_5485912b44f976de': RollingMean: 8.428571, 'air_54d6c25d33f5260e': RollingMean: 40.5, 'air_54ed43163b7596c4': RollingMean: 16.5, 'air_55390f784018349a': RollingMean: 46., 'air_55c3627912b9c849': RollingMean: 10., 'air_55e11c33d4758131': RollingMean: 22.071429, 'air_56cd12f31a0afc04': RollingMean: 29.785714, 'air_56cebcbd6906e04c': RollingMean: 17.357143, 'air_56ea46c14b2dd967': RollingMean: 42.214286, 'air_57013002b912772b': RollingMean: 5.571429, 'air_573ecdf81b157d22': RollingMean: 28.285714, 'air_57c9eea1a2b66e65': RollingMean: 14.357143, 'air_57ed725a1930a5b9': RollingMean: 15.642857, 'air_5878b6f2a9da12c1': RollingMean: 13.357143, 'air_59cc9b2b209c6331': RollingMean: 9.071429, 'air_5a9a6cbeeb434c08': RollingMean: 23.071429, 'air_5acc13d655a6e8b2': RollingMean: 24.071429, 'air_5afb1cca48ceaa19': RollingMean: 42.142857, 'air_5b6d18c470bbfaf9': RollingMean: 37.285714, 'air_5b704df317ed1962': RollingMean: 2.214286, 'air_5bd22f9cc1426a90': RollingMean: 32.357143, 'air_5c65468938c07fa5': RollingMean: 16.214286, 'air_5c7489c9ec755e2d': RollingMean: 33.428571, 'air_5c817ef28f236bdf': RollingMean: 42.428571, 'air_5cb030b9f0b91537': RollingMean: 11.071429, 'air_5cfc537125d97f16': RollingMean: 9.571429, 'air_5d7c744c3a2ef624': RollingMean: 32.428571, 'air_5d945ade487cdf4d': RollingMean: 13.5, 'air_5dea8a7a5bf5eb71': RollingMean: 28.285714, 'air_5e339a1f364cdb00': RollingMean: 11.285714, 'air_5e34c6fe6fabd10e': RollingMean: 20.357143, 'air_5e70fe82f9e4fab6': RollingMean: 15.5, 'air_5e939e005bd34633': RollingMean: 2.571429, 'air_5ed3198e4a5eed0f': RollingMean: 38.071429, 'air_5f3a3ef4cba110a4': RollingMean: 32.214286, 'air_5f6fa1b897fe80d5': RollingMean: 29.785714, 'air_5fbda8e9302f7c13': RollingMean: 27.928571, 'air_602ca92c0db34f8f': RollingMean: 19., 'air_609050e4e4f79ae1': RollingMean: 11.857143, 'air_60a7057184ec7ec7': RollingMean: 27.857143, 'air_60aa54ecbc602348': RollingMean: 5.428571, 'air_6108821ffafa9b72': RollingMean: 24.785714, 'air_614e2f7e76dff854': RollingMean: 11.857143, 'air_61668cc2b0778898': RollingMean: 8.428571, 'air_61b8d37c33617f21': RollingMean: 27.428571, 'air_61de73b097513f58': RollingMean: 9.714286, 'air_622375b4815cf5cb': RollingMean: 37.5, 'air_627cabe2fe53f33f': RollingMean: 14.5, 'air_629d9935273c82ae': RollingMean: 23.142857, 'air_629edf21ea38ac2d': RollingMean: 35.214286, 'air_632ba66e1f75aa28': RollingMean: 19.142857, 'air_638c35eb25e53eea': RollingMean: 19.5, 'air_63a750d8b4b6a976': RollingMean: 23.428571, 'air_63a88d81295195ed': RollingMean: 29.5, 'air_63b13c56b7201bd9': RollingMean: 22.285714, 'air_63e28ee0b0c955a7': RollingMean: 22., 'air_640cf4835f0d9ba3': RollingMean: 26.714286, 'air_6411203a47b5ec77': RollingMean: 8.928571, 'air_645cb18b33f938cf': RollingMean: 17.071429, 'air_646b93e336f0dded': RollingMean: 7.428571, 'air_64a5d5c1381837af': RollingMean: 40., 'air_64d4491ad8cdb1c6': RollingMean: 13.714286, 'air_650f9b9de0c5542c': RollingMean: 22.428571, 'air_657a0748462f85de': RollingMean: 8., 'air_65e294f1ae6df9c3': RollingMean: 22.214286, 'air_6607fe3671242ce3': RollingMean: 42.571429, 'air_670a0c1c4108bcea': RollingMean: 25., 'air_671b4bea84dafb67': RollingMean: 26.071429, 'air_673acd9fa5e0dd78': RollingMean: 6.214286, 'air_67483104fa38ef6c': RollingMean: 29.571429, 'air_675aa35cba456fd1': RollingMean: 43.642857, 'air_67f87c159d9e2ee2': RollingMean: 36.5, 'air_68147db09287bf74': RollingMean: 24., 'air_681b0c56328dd2af': RollingMean: 35., 'air_681f96e6a6595f82': RollingMean: 37.285714, 'air_68301bcb11e2f389': RollingMean: 26.428571, 'air_683371d9baabf410': RollingMean: 31.071429, 'air_6836438b543ba698': RollingMean: 12.928571, 'air_6873982b9e19c7ad': RollingMean: 5., 'air_68c1de82037d87e6': RollingMean: 25.785714, 'air_68cc910e7b307b09': RollingMean: 7.928571, 'air_68d075113f368946': RollingMean: 20.285714, 'air_6902e4ec305b3d08': RollingMean: 36.857143, 'air_694571ea13fb9e0e': RollingMean: 30.571429, 'air_6a15e4eae523189d': RollingMean: 17.785714, 'air_6b15edd1b4fbb96a': RollingMean: 31.785714, 'air_6b2268863b14a2af': RollingMean: 18.857143, 'air_6b65745d432fd77f': RollingMean: 23.571429, 'air_6b7678aae65d2d59': RollingMean: 6.928571, 'air_6b942d5ebbc759c2': RollingMean: 14.714286, 'air_6b9fa44a9cf504a1': RollingMean: 5.785714, 'air_6c1128955c58b690': RollingMean: 13., 'air_6c91a28278a16f64': RollingMean: 8.642857, 'air_6c952e3c6e590945': RollingMean: 15.642857, 'air_6ca1d941c8199a67': RollingMean: 26.071429, 'air_6cbe54f0aa30b615': RollingMean: 15.571429, 'air_6ced51c24fb54262': RollingMean: 7.785714, 'air_6d64dba2edd4fc0c': RollingMean: 9.428571, 'air_6d65542aa43b598b': RollingMean: 27.785714, 'air_6d65dd11d96e00fb': RollingMean: 4.428571, 'air_6e06824d0934dd81': RollingMean: 21.928571, 'air_6e3fd96320d24324': RollingMean: 8.285714, 'air_6e64fb5821402cd2': RollingMean: 8.357143, 'air_6ff5fca957798daa': RollingMean: 7.857143, 'air_707d4b6328f2c2df': RollingMean: 28.5, 'air_709262d948dd0b6e': RollingMean: 11.071429, 'air_70e9e8cd55879414': RollingMean: 10.928571, 'air_70f834596eb99fee': RollingMean: 21.428571, 'air_710d6537cb7623df': RollingMean: 32.071429, 'air_712dd258f7f91b4b': RollingMean: 15.714286, 'air_71903025d39a4571': RollingMean: 14.642857, 'air_722297e7f26db91d': RollingMean: 11.071429, 'air_728ff578acc6ac6e': RollingMean: 10.571429, 'air_72f5146cf0c49beb': RollingMean: 12.214286, 'air_735bcbe1763d6e98': RollingMean: 8.642857, 'air_73f316e6a18d8aa9': RollingMean: 22.785714, 'air_7420042ff75f9aca': RollingMean: 35., 'air_746211c0b532e8aa': RollingMean: 53.5, 'air_747f375eb3900e1e': RollingMean: 5.5, 'air_74cf22153214064c': RollingMean: 10.142857, 'air_7514d90009613cd6': RollingMean: 66.928571, 'air_754ae581ad80cc9f': RollingMean: 13.928571, 'air_75864c80d2fb334a': RollingMean: 10.357143, 'air_75bd5d1b6dc6670d': RollingMean: 14.142857, 'air_764f71040a413d4d': RollingMean: 49.428571, 'air_77488fa378cf98c3': RollingMean: 9., 'air_77dfc83450cbc89c': RollingMean: 41.571429, 'air_7831b00996701c0f': RollingMean: 23.5, 'air_789103bf53b8096b': RollingMean: 55.928571, 'air_789466e488705c93': RollingMean: 24.928571, 'air_78df4dc6a7e83e41': RollingMean: 17.5, 'air_79afb3f52b4d062c': RollingMean: 10., 'air_79f528087f49df06': RollingMean: 34.071429, 'air_7a81bd7fadcbf3d8': RollingMean: 5., 'air_7a946aada80376a4': RollingMean: 16., 'air_7bacc4d36fb094c9': RollingMean: 6.142857, 'air_7bc6ca04d7b0f3b8': RollingMean: 12.071429, 'air_7bda6048a4a78837': RollingMean: 23.857143, 'air_7c7774c66fb237f7': RollingMean: 8.714286, 'air_7cc17a324ae5c7dc': RollingMean: 14.714286, 'air_7cf5a02c0e01b647': RollingMean: 33.214286, 'air_7d65049f9d275c0d': RollingMean: 10.857143, 'air_7dacea2f22afccfb': RollingMean: 32.357143, 'air_7db266904cb0d72a': RollingMean: 13., 'air_7e12c5d27f44a8de': RollingMean: 23.785714, 'air_7ef9a5ea5c8fe39f': RollingMean: 11.142857, 'air_7f3dc18494bce98b': RollingMean: 14.071429, 'air_7f9e15afafcf4c75': RollingMean: 35.428571, 'air_7fbf7649eb13ad9b': RollingMean: 18.714286, 'air_800c02226e2e0288': RollingMean: 13.785714, 'air_8093d0b565e9dbdf': RollingMean: 37., 'air_8110d68cc869b85e': RollingMean: 45.642857, 'air_81546875de9c8e78': RollingMean: 4.642857, 'air_81a12d67c22e012f': RollingMean: 20.285714, 'air_81bd68142db76f58': RollingMean: 25.357143, 'air_81c2600146d07d16': RollingMean: 6.5, 'air_81c5dff692063446': RollingMean: 11.285714, 'air_820d1919cbecaa0a': RollingMean: 33.357143, 'air_82a6ae14151953ba': RollingMean: 37.357143, 'air_831658500aa7c846': RollingMean: 29.928571, 'air_832f9dbe9ee4ebd3': RollingMean: 13.285714, 'air_83db5aff8f50478e': RollingMean: 7.5, 'air_84060403939d8216': RollingMean: 15.285714, 'air_848616680ef061bd': RollingMean: 29.857143, 'air_84f6876ff7e83ae7': RollingMean: 19.214286, 'air_8523d6a70de49e6c': RollingMean: 33.785714, 'air_859feab8e3c9f98d': RollingMean: 22.571429, 'air_85bd13a49370c392': RollingMean: 11.928571, 'air_86cfbf2624576fad': RollingMean: 6.857143, 'air_86f7b2109e4abd65': RollingMean: 49.428571, 'air_87059630ab6fe47f': RollingMean: 3.714286, 'air_87078cf7903a648c': RollingMean: 6., 'air_87467487d21891dd': RollingMean: 13.571429, 'air_8764b3473ddcceaf': RollingMean: 5., 'air_876d7a23c47811cb': RollingMean: 15.928571, 'air_877f79706adbfb06': RollingMean: 12.142857, 'air_87ca98aa7664de94': RollingMean: 10.714286, 'air_87f9e1024b951f01': RollingMean: 11.285714, 'air_883ca28ef0ed3d55': RollingMean: 14.142857, 'air_88c8e34baa79217b': RollingMean: 30.714286, 'air_88ca84051ba95339': RollingMean: 18.642857, 'air_88f31db64991768a': RollingMean: 8.785714, 'air_890d7e28e8eaaa11': RollingMean: 8.071429, 'air_89e7328af22efe74': RollingMean: 34., 'air_8a1d21fad48374e8': RollingMean: 11.857143, 'air_8a59bb0c497b771e': RollingMean: 27.214286, 'air_8a906e5801eac81c': RollingMean: 23.357143, 'air_8b4a46dc521bfcfe': RollingMean: 29.071429, 'air_8c119d1f16049f20': RollingMean: 25.428571, 'air_8c3175aa5e4fc569': RollingMean: 80.071429, 'air_8cc350fd70ee0757': RollingMean: 32., 'air_8ce035ee1d8a56a6': RollingMean: 18.357143, 'air_8d50c64692322dff': RollingMean: 10.785714, 'air_8d61f49aa0373492': RollingMean: 45.642857, 'air_8e429650fcf7a0ae': RollingMean: 19.214286, 'air_8e4360a64dbd4c50': RollingMean: 20.357143, 'air_8e492076a1179383': RollingMean: 43.642857, 'air_8e8f42f047537154': RollingMean: 29.357143, 'air_8ec47c0f1e2c879e': RollingMean: 30.714286, 'air_8f13ef0f5e8c64dd': RollingMean: 6.285714, 'air_8f273fb9ad2fed6f': RollingMean: 13.071429, 'air_8f3b563416efc6ad': RollingMean: 13.642857, 'air_900d755ebd2f7bbd': RollingMean: 92.071429, 'air_901925b628677c2e': RollingMean: 9.142857, 'air_90213bcae4afa274': RollingMean: 27.357143, 'air_90bd5de52c166767': RollingMean: 22.071429, 'air_90ed0a2f24755533': RollingMean: 41.785714, 'air_90f0efbb702d77b7': RollingMean: 32.214286, 'air_9105a29b0eb250d2': RollingMean: 18.285714, 'air_91236b89d29567af': RollingMean: 23.071429, 'air_9152d9926e5c4a3a': RollingMean: 36.142857, 'air_915558a55c2bc56c': RollingMean: 17.785714, 'air_91beafbba9382b0a': RollingMean: 36.5, 'air_91d72e16c4bcba55': RollingMean: 14.785714, 'air_9241121af22ff1d6': RollingMean: 29.928571, 'air_929d8513e3cdb423': RollingMean: 7., 'air_931a8a4321b6e7d1': RollingMean: 5.142857, 'air_9352c401d5adb01b': RollingMean: 25.571429, 'air_9387ff95e886ebc7': RollingMean: 13.214286, 'air_938ef91ecdde6878': RollingMean: 23.214286, 'air_939964477035ef0b': RollingMean: 17.928571, 'air_93b9bb641f8fc982': RollingMean: 28.285714, 'air_93dd7070c9bf5453': RollingMean: 29.428571, 'air_93ebe490d4abb8e9': RollingMean: 17.714286, 'air_9438d67241c81314': RollingMean: 37., 'air_947eb2cae4f3e8f2': RollingMean: 32.071429, 'air_9483d65e9cc9a6b7': RollingMean: 15.285714, 'air_950381108f839348': RollingMean: 28.142857, 'air_95d28905941fd4cb': RollingMean: 30.785714, 'air_95e917913cd988f3': RollingMean: 23.642857, 'air_96005f79124e12bf': RollingMean: 40.428571, 'air_965b2e0cf4119003': RollingMean: 47.357143, 'air_96743eee94114261': RollingMean: 13.428571, 'air_96773a6236d279b1': RollingMean: 24.785714, 'air_968d72c12eed09f0': RollingMean: 18., 'air_96929a799b12a43e': RollingMean: 27.214286, 'air_96ec3cfe78cb0652': RollingMean: 18.5, 'air_97159fc4e90053fe': RollingMean: 24., 'air_97958e7fce98b6a3': RollingMean: 16.928571, 'air_97b2a9f975fc702c': RollingMean: 40.142857, 'air_97cf68dc1a9beac0': RollingMean: 13.428571, 'air_97e0f2feec4d577a': RollingMean: 15.071429, 'air_9828505fefc77d75': RollingMean: 12.5, 'air_98b54e32ccddd896': RollingMean: 18.214286, 'air_990a642a3deb2903': RollingMean: 33.5, 'air_99157b6163835eec': RollingMean: 33.785714, 'air_99a5183695b849f9': RollingMean: 30.928571, 'air_99b01136f451fc0e': RollingMean: 41.428571, 'air_99c3eae84130c1cb': RollingMean: 39.928571, 'air_9a30407764f4ff84': RollingMean: 17., 'air_9a6f6e7f623003d2': RollingMean: 3.142857, 'air_9aa32b3db0fab3a5': RollingMean: 17.142857, 'air_9aa92007e3628dbc': RollingMean: 31.642857, 'air_9ae7081cb77dc51e': RollingMean: 29.571429, 'air_9b13c7feb0a0c431': RollingMean: 10.714286, 'air_9b6af3db40da4ae2': RollingMean: 29.785714, 'air_9bbc673495e23532': RollingMean: 5.142857, 'air_9bf0ccac497c4af3': RollingMean: 44.571429, 'air_9bf595ef095572fb': RollingMean: 25.642857, 'air_9c6787aa03a45586': RollingMean: 92.142857, 'air_9ca2767761efff4d': RollingMean: 8.357143, 'air_9cd5e310f488bced': RollingMean: 14.642857, 'air_9cf2f1ba86229773': RollingMean: 34., 'air_9d3482b4680aee88': RollingMean: 9.785714, 'air_9d452a881f7f2bb7': RollingMean: 7.857143, 'air_9d474ec2448c700d': RollingMean: 12.857143, 'air_9d5a980b211e1795': RollingMean: 11.571429, 'air_9d93d95720f2e831': RollingMean: 8.285714, 'air_9dc9483f717d73ee': RollingMean: 5.571429, 'air_9dd7d38b0f1760c4': RollingMean: 2.5, 'air_9e920b758503ef54': RollingMean: 5.5, 'air_9efaa7ded03c5a71': RollingMean: 12.214286, 'air_9f277fb7a2c1d842': RollingMean: 12.142857, 'air_9fc607777ad76b26': RollingMean: 15.928571, 'air_a083834e7ffe187e': RollingMean: 20.285714, 'air_a11473cc1eb9a27f': RollingMean: 33.071429, 'air_a17f0778617c76e2': RollingMean: 38., 'air_a1fe8c588c8d2f30': RollingMean: 15.785714, 'air_a218912784bf767d': RollingMean: 14.357143, 'air_a21ffca0bea1661a': RollingMean: 1.071429, 'air_a239a44805932bab': RollingMean: 34.428571, 'air_a24bf50c3e90d583': RollingMean: 17.285714, 'air_a2567267116a3b75': RollingMean: 16.642857, 'air_a257c9749d8d0ff6': RollingMean: 18.428571, 'air_a271c9ba19e81d17': RollingMean: 28.857143, 'air_a2b29aa7feb4e36f': RollingMean: 16.642857, 'air_a304330715435390': RollingMean: 7.642857, 'air_a33461f4392ec62c': RollingMean: 28.142857, 'air_a373500730e2a9e0': RollingMean: 11., 'air_a38f25e3399d1b25': RollingMean: 41.857143, 'air_a41b032371a63427': RollingMean: 11.357143, 'air_a49f1cf0634f13e5': RollingMean: 22.214286, 'air_a510dcfe979f09eb': RollingMean: 13.714286, 'air_a546cbf478a8b6e4': RollingMean: 27.5, 'air_a55d17bd3f3033cb': RollingMean: 12.285714, 'air_a563896da3777078': RollingMean: 21.571429, 'air_a678e5b144ca24ce': RollingMean: 16.714286, 'air_a7404a854919e990': RollingMean: 7.357143, 'air_a8533b7a992bb0ca': RollingMean: 18.571429, 'air_a85f0c0c889f6b7e': RollingMean: 47.571429, 'air_a85f8c0bfd61889f': RollingMean: 15.142857, 'air_a88ac559064dec08': RollingMean: 32.142857, 'air_a9133955abccf071': RollingMean: 27.785714, 'air_a9178f19da58fe99': RollingMean: 7.428571, 'air_a9a380530c1e121f': RollingMean: 41.285714, 'air_aa0049fe3cc6f4d6': RollingMean: 10.142857, 'air_ab3ae0e410b20069': RollingMean: 17.928571, 'air_ab9746a0f83084b7': RollingMean: 6.714286, 'air_abcdc8115988a010': RollingMean: 10.857143, 'air_abf06fcca748dca5': RollingMean: 9.5, 'air_ac7a7427c9ae12a5': RollingMean: 62.071429, 'air_ad13e71e21235131': RollingMean: 16.071429, 'air_ad60f6b76c9df7ed': RollingMean: 23.357143, 'air_ad7777590c884721': RollingMean: 8.357143, 'air_add9a575623726c8': RollingMean: 40.285714, 'air_ade6e836ffd1da64': RollingMean: 10.214286, 'air_aed3a8b49abe4a48': RollingMean: 6.214286, 'air_af03c277a167b2bd': RollingMean: 26.642857, 'air_af24e3e817dea1e5': RollingMean: 15.357143, 'air_af63df35857b16e6': RollingMean: 23.857143, 'air_b0a6a4c5e95c74cf': RollingMean: 18.642857, 'air_b162fb07fbbdea33': RollingMean: 15.285714, 'air_b192fb5310436005': RollingMean: 8.785714, 'air_b1a72bf1ebf4b8ef': RollingMean: 52.285714, 'air_b1bb1fae86617d7a': RollingMean: 34.428571, 'air_b1d822f75c9fc615': RollingMean: 10.785714, 'air_b2395df0e874078d': RollingMean: 7.071429, 'air_b23d0f519291247d': RollingMean: 28.785714, 'air_b259b4e4a51a690d': RollingMean: 22.785714, 'air_b28bed4b2e7167b7': RollingMean: 16.928571, 'air_b2a639cc7e02edf1': RollingMean: 20.714286, 'air_b2d8bc9c88b85f96': RollingMean: 14.714286, 'air_b2d97bd2337c5ba7': RollingMean: 28.785714, 'air_b2dcec37b83e2494': RollingMean: 7.428571, 'air_b30fffd7ab1e75a5': RollingMean: 9.214286, 'air_b3180b74332ba886': RollingMean: 13.857143, 'air_b3a824511477a4ed': RollingMean: 6.214286, 'air_b439391e72899756': RollingMean: 19.928571, 'air_b45b8e456f53942a': RollingMean: 10.857143, 'air_b4f32bcc399da2b9': RollingMean: 29.071429, 'air_b5598d12d1b84890': RollingMean: 5.857143, 'air_b5bdd318005d9aa4': RollingMean: 34., 'air_b60cc7d6aee68194': RollingMean: 11.357143, 'air_b711b43ae472cb6b': RollingMean: 20.642857, 'air_b7fa3d2fca744dd2': RollingMean: 40.785714, 'air_b80fed1a07c817d2': RollingMean: 4.285714, 'air_b88192b35ac03c24': RollingMean: 17.5, 'air_b8925441167c3152': RollingMean: 2.071429, 'air_b8a5ee69e5fdcc5b': RollingMean: 34.142857, 'air_b8d9e1624baaadc2': RollingMean: 6.214286, 'air_b9e27558fb8bd5c4': RollingMean: 12.071429, 'air_ba495cccc8f0f237': RollingMean: 17.357143, 'air_ba937bf13d40fb24': RollingMean: 15.285714, 'air_bac5f4441db21db9': RollingMean: 41.785714, 'air_baf28ac9f13a307d': RollingMean: 13.571429, 'air_bb09595bab7d5cfb': RollingMean: 29.5, 'air_bb26d6d079594414': RollingMean: 13.285714, 'air_bb4ff06cd661ee9b': RollingMean: 34.428571, 'air_bbe1c1a47e09f161': RollingMean: 1.642857, 'air_bc991c51d6613745': RollingMean: 18.571429, 'air_bc9a129e11a2efe0': RollingMean: 30.785714, 'air_bcce1ea4350b7b72': RollingMean: 20.857143, 'air_bd74a9222edfdfe1': RollingMean: 14.285714, 'air_bdd32aa407c16335': RollingMean: 13.785714, 'air_bebd55ed63ab2422': RollingMean: 8.785714, 'air_bed603c423b7d9d4': RollingMean: 6.142857, 'air_bedd35489e666605': RollingMean: 38.642857, 'air_bf13014b6e3e60ca': RollingMean: 27., 'air_bf21b8350771879b': RollingMean: 23.857143, 'air_bf617aa68d5f1cfa': RollingMean: 6.857143, 'air_bf7591560077332d': RollingMean: 8.357143, 'air_bfafaed35e213fd7': RollingMean: 12.857143, 'air_bfda7731a6c6fc61': RollingMean: 19.071429, 'air_c027e2b560442808': RollingMean: 15.857143, 'air_c0385db498b391e5': RollingMean: 33.5, 'air_c1d5d165c055b866': RollingMean: 30.071429, 'air_c1ff20617c54fee7': RollingMean: 7.785714, 'air_c225148c0fcc5c72': RollingMean: 34.857143, 'air_c2626f5f86d57342': RollingMean: 15.928571, 'air_c26f027b5bc1f081': RollingMean: 5.071429, 'air_c28983412a7eefcf': RollingMean: 35.285714, 'air_c2c8435bdb3516d4': RollingMean: 31.214286, 'air_c31472d14e29cee8': RollingMean: 14.857143, 'air_c3585b0fba3998d0': RollingMean: 9.357143, 'air_c3bc011cca3bec65': RollingMean: 5.642857, 'air_c3dcaf3aeb18e20e': RollingMean: 17.428571, 'air_c47aa7493b15f297': RollingMean: 21.142857, 'air_c4fa5c562d5409ca': RollingMean: 13.785714, 'air_c52c63c781fe48f6': RollingMean: 27.714286, 'air_c5459218282bedd5': RollingMean: 22.571429, 'air_c66dbd2c37832d00': RollingMean: 16.714286, 'air_c6a164dd4060e960': RollingMean: 12.428571, 'air_c6aa2efba0ffc8eb': RollingMean: 28.857143, 'air_c6ffd6a93e6b68d6': RollingMean: 17.357143, 'air_c73d319ffabf287a': RollingMean: 15.285714, 'air_c759b6abeb552160': RollingMean: 5.071429, 'air_c77ee2b7d36da265': RollingMean: 42.071429, 'air_c7d30ab0e07f31d5': RollingMean: 17.142857, 'air_c7f78b4f3cba33ff': RollingMean: 22.785714, 'air_c8265ecc116f2284': RollingMean: 9.142857, 'air_c88467d88b2c8ecd': RollingMean: 18.428571, 'air_c8a657c8c5c93d69': RollingMean: 7.928571, 'air_c8c0ef02ed72053f': RollingMean: 26.5, 'air_c8fe396d6c46275d': RollingMean: 14.357143, 'air_c92745dfdd2ec68a': RollingMean: 18.428571, 'air_c9ed65554b6edffb': RollingMean: 12.928571, 'air_c9f6de13be8b8f25': RollingMean: 4.357143, 'air_ca1315af9e073bd1': RollingMean: 45.285714, 'air_ca6ae8d49a2f1eaf': RollingMean: 20.928571, 'air_ca957d3a1529fbd3': RollingMean: 33.714286, 'air_cadf9cfb510a1d78': RollingMean: 31.071429, 'air_caf996ac27206301': RollingMean: 5.642857, 'air_cb083b4789a8d3a2': RollingMean: 17.214286, 'air_cb25551c4cd8d9f3': RollingMean: 9., 'air_cb7467aed805e7fe': RollingMean: 36.571429, 'air_cb935ff8610ba3d3': RollingMean: 4.857143, 'air_cbe139af83feb388': RollingMean: 9.928571, 'air_cbe867adcf44e14f': RollingMean: 15.5, 'air_cc1a0e985ce63711': RollingMean: 27.785714, 'air_cc35590cd1da8554': RollingMean: 20.857143, 'air_ccd19a5bc5573ae5': RollingMean: 37.714286, 'air_cd4b301d5d3918d8': RollingMean: 6.928571, 'air_cd5f54969be9ed08': RollingMean: 7.214286, 'air_ced6297e5bdf5130': RollingMean: 24.428571, 'air_cf2229e64408d9fe': RollingMean: 17.857143, 'air_cf22e368c1a71d53': RollingMean: 37.285714, 'air_cf5ab75a0afb8af9': RollingMean: 45.214286, 'air_cfcc94797d2b5d3d': RollingMean: 17.285714, 'air_cfdeb326418194ff': RollingMean: 15.142857, 'air_d00161e19f08290b': RollingMean: 26., 'air_d00a15343325e5f7': RollingMean: 18.714286, 'air_d07e57b21109304a': RollingMean: 10.142857, 'air_d0a1e69685259c92': RollingMean: 29.285714, 'air_d0a7bd3339c3d12a': RollingMean: 39.785714, 'air_d0e8a085d8dc83aa': RollingMean: 7.571429, 'air_d138b593ebda55cc': RollingMean: 5.714286, 'air_d1418d6fd6d634f2': RollingMean: 16.285714, 'air_d186b2cb0b9ce022': RollingMean: 13.357143, 'air_d1f20424f76cc78e': RollingMean: 20.928571, 'air_d34c0861a2be94cb': RollingMean: 42.571429, 'air_d3e7b5952cd09ccb': RollingMean: 18.642857, 'air_d44d210d2994f01b': RollingMean: 6.071429, 'air_d473620754bf9fc2': RollingMean: 12.571429, 'air_d477b6339b8ce69f': RollingMean: 8.857143, 'air_d4981cdde163b172': RollingMean: 23.428571, 'air_d4b5a4b04c5f2d04': RollingMean: 12.642857, 'air_d4d218b451f82c3d': RollingMean: 9.785714, 'air_d500b48a8735fbd3': RollingMean: 18.071429, 'air_d54d6fcb116fbed3': RollingMean: 4.214286, 'air_d5e0a20370c325c7': RollingMean: 29.857143, 'air_d63cfa6d6ab78446': RollingMean: 18.428571, 'air_d69b08a175bc0387': RollingMean: 10.857143, 'air_d6b3e67261f07646': RollingMean: 13.714286, 'air_d8abb9e490abf94f': RollingMean: 12., 'air_d97dabf7aae60da5': RollingMean: 33.285714, 'air_d98380a4aeb0290b': RollingMean: 43.357143, 'air_daa7947e1c47f5ed': RollingMean: 34.357143, 'air_dabfbd0ec951925a': RollingMean: 7.642857, 'air_dad0b6a36138f309': RollingMean: 5.5, 'air_db1233ad855b34d5': RollingMean: 24.142857, 'air_db4b38ebe7a7ceff': RollingMean: 24.5, 'air_db80363d35f10926': RollingMean: 28.785714, 'air_dbf64f1ce38c7442': RollingMean: 14.928571, 'air_dc0e080ba0a5e5af': RollingMean: 8.928571, 'air_dc71c6cc06cd1aa2': RollingMean: 6.285714, 'air_de692863bb2dd758': RollingMean: 20.857143, 'air_de803f7e324936b8': RollingMean: 24.285714, 'air_de88770300008624': RollingMean: 20., 'air_dea0655f96947922': RollingMean: 38.571429, 'air_df507aec929ce5f6': RollingMean: 19.071429, 'air_df554c4527a1cfe6': RollingMean: 53.5, 'air_df5cf5cd03eb68d0': RollingMean: 7.857143, 'air_df843e6b22e8d540': RollingMean: 10.285714, 'air_df9355c47c5df9d3': RollingMean: 30.285714, 'air_dfad598ff642dab7': RollingMean: 26.071429, 'air_dfe068a1bf85f395': RollingMean: 36.428571, 'air_e00fe7853c0100d6': RollingMean: 20.214286, 'air_e0118664da63a2d0': RollingMean: 16.714286, 'air_e01d99390355408d': RollingMean: 11.571429, 'air_e053c561f32acc28': RollingMean: 17.928571, 'air_e08b9cf82057a170': RollingMean: 34.5, 'air_e0aee25b56a069f2': RollingMean: 14.214286, 'air_e0e69668214ff972': RollingMean: 9.785714, 'air_e0f241bd406810c0': RollingMean: 33.142857, 'air_e1b76fcb5208fb6b': RollingMean: 14.571429, 'air_e2208a79e2678432': RollingMean: 45.142857, 'air_e270aff84ac7e4c8': RollingMean: 25.928571, 'air_e3020992d5fe5dfd': RollingMean: 13.5, 'air_e34c631c766f2766': RollingMean: 24.642857, 'air_e42bdc3377d1eee7': RollingMean: 20.785714, 'air_e483f5b3c4f310e0': RollingMean: 5.285714, 'air_e524c6a9e06cc3a1': RollingMean: 8.785714, 'air_e55abd740f93ecc4': RollingMean: 46., 'air_e57dd6884595f60d': RollingMean: 39.428571, 'air_e58f669b6f1a08ce': RollingMean: 11.142857, 'air_e5cf003abcc5febb': RollingMean: 12.571429, 'air_e64de0a6bf0739af': RollingMean: 43.214286, 'air_e657ca554b0c008c': RollingMean: 25., 'air_e700e390226d9985': RollingMean: 15.928571, 'air_e76a668009c5dabc': RollingMean: 7.785714, 'air_e7d2ac6d53d1b744': RollingMean: 9.357143, 'air_e7fbee4e3cfe65c5': RollingMean: 36., 'air_e88bbe2ede3467aa': RollingMean: 22.857143, 'air_e89735e80d614a7e': RollingMean: 33.785714, 'air_e8ed9335d0c38333': RollingMean: 24.857143, 'air_e9ebf7fc520ac76a': RollingMean: 28.428571, 'air_ea6d0c3acf00b22a': RollingMean: 22.285714, 'air_ea7c16131980c837': RollingMean: 7.071429, 'air_eb120e6d384a17a8': RollingMean: 42.785714, 'air_eb20a89bba7dd3d0': RollingMean: 3., 'air_eb2d2653586315dd': RollingMean: 31.428571, 'air_eb5788dba285e725': RollingMean: 27.071429, 'air_ebd31e812960f517': RollingMean: 26.857143, 'air_ebe02c3090271fa9': RollingMean: 11.142857, 'air_ec0fad2def4dcff0': RollingMean: 15.142857, 'air_eca4a5a191e8d993': RollingMean: 28.357143, 'air_eca5e0064dc9314a': RollingMean: 34.571429, 'air_ecab54b57a71b10d': RollingMean: 13.071429, 'air_eceb97ad6a7d4c07': RollingMean: 27.857143, 'air_ecf7f141339f1d57': RollingMean: 19.857143, 'air_eda179770dfa9f91': RollingMean: 10.642857, 'air_edd5e3d696a5811b': RollingMean: 39.357143, 'air_ee3a01f0c71a769f': RollingMean: 25.285714, 'air_ee3ba9af184c6c82': RollingMean: 18.642857, 'air_eec5e572b9eb9c23': RollingMean: 11.142857, 'air_eeeadee005c006a2': RollingMean: 12.142857, 'air_ef47430bcd6f6a89': RollingMean: 14.857143, 'air_ef789667e2e6fe96': RollingMean: 36.357143, 'air_ef920fa6f4b085f6': RollingMean: 33.928571, 'air_efc80d3f96b3aff7': RollingMean: 9.428571, 'air_efd70b04de878f25': RollingMean: 31., 'air_efef1e3daecce07e': RollingMean: 40.428571, 'air_f068442ebb6c246c': RollingMean: 10.071429, 'air_f0c7272956e62f12': RollingMean: 4.285714, 'air_f0fb0975bdc2cdf9': RollingMean: 10.285714, 'air_f1094dbf2aef85d9': RollingMean: 6.714286, 'air_f180301886c21375': RollingMean: 14.428571, 'air_f183a514cb8ff4fa': RollingMean: 42.071429, 'air_f1f9027d4fa8f653': RollingMean: 26.5, 'air_f267dd70a6a6b5d3': RollingMean: 44.5, 'air_f26f36ec4dc5adb0': RollingMean: 38.785714, 'air_f2985de32bb792e0': RollingMean: 26.285714, 'air_f2c5a1f24279c531': RollingMean: 14.357143, 'air_f3602e4fa2f12993': RollingMean: 11.571429, 'air_f3f9824b7d70c3cf': RollingMean: 16.642857, 'air_f4936b91c9addbf0': RollingMean: 16., 'air_f593fa60ac3541e2': RollingMean: 13.642857, 'air_f690c42545146e0a': RollingMean: 12.928571, 'air_f6b2489ccf873c3b': RollingMean: 15.928571, 'air_f6bfd27e2e174d16': RollingMean: 12.928571, 'air_f6cdaf7b7fdc6d78': RollingMean: 10., 'air_f8233ad00755c35c': RollingMean: 29.571429, 'air_f85e21e543cf44f2': RollingMean: 4.785714, 'air_f88898cd09f40496': RollingMean: 8.857143, 'air_f911308e19d64236': RollingMean: 40.571429, 'air_f9168b23fdfc1e52': RollingMean: 19.5, 'air_f927b2da69a82341': RollingMean: 8.357143, 'air_f957c6d6467d4d90': RollingMean: 10.642857, 'air_f96765e800907c77': RollingMean: 43.857143, 'air_fa12b40b02fecfd8': RollingMean: 15., 'air_fa4ffc9057812fa2': RollingMean: 5.142857, 'air_fab092c35776a9b1': RollingMean: 10.714286, 'air_fb44f566d4f64a4e': RollingMean: 15.857143, 'air_fbadf737162a5ce3': RollingMean: 14.571429, 'air_fc477473134e9ae5': RollingMean: 16.857143, 'air_fcd4492c83f1c6b9': RollingMean: 22.357143, 'air_fcfbdcf7b1f82c6e': RollingMean: 35.642857, 'air_fd154088b1de6fa7': RollingMean: 6.428571, 'air_fd6aac1043520e83': RollingMean: 36.071429, 'air_fdc02ec4a3d21ea4': RollingMean: 8.071429, 'air_fdcfef8bd859f650': RollingMean: 3.642857, 'air_fe22ef5a9cbef123': RollingMean: 21.714286, 'air_fe58c074ec1445ea': RollingMean: 32.785714, 'air_fea5dc9594450608': RollingMean: 15.571429, 'air_fee8dcf4d619598e': RollingMean: 26.785714, 'air_fef9ccb3ba0da2f7': RollingMean: 9.714286, 'air_ffcc2d5087e1b476': RollingMean: 20., 'air_fff68b929994bfbd': RollingMean: 4.285714}), 'how': RollingMean: 0., 'target_name': 'target'} target_rollingmean_21_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_21_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 29.190476, 'air_0164b9927d20bcc3': RollingMean: 8.380952, 'air_0241aa3964b7f861': RollingMean: 8.904762, 'air_0328696196e46f18': RollingMean: 8.904762, 'air_034a3d5b40d5b1b1': RollingMean: 22.52381, 'air_036d4f1ee7285390': RollingMean: 21.238095, 'air_0382c794b73b51ad': RollingMean: 27.380952, 'air_03963426c9312048': RollingMean: 40.428571, 'air_04341b588bde96cd': RollingMean: 35.333333, 'air_049f6d5b402a31b2': RollingMean: 12.47619, 'air_04cae7c1bc9b2a0b': RollingMean: 19.238095, 'air_0585011fa179bcce': RollingMean: 5.333333, 'air_05c325d315cc17f5': RollingMean: 27., 'air_0647f17b4dc041c8': RollingMean: 30.52381, 'air_064e203265ee5753': RollingMean: 19.47619, 'air_066f0221b8a4d533': RollingMean: 12.428571, 'air_06f95ac5c33aca10': RollingMean: 30.047619, 'air_0728814bd98f7367': RollingMean: 8.095238, 'air_0768ab3910f7967f': RollingMean: 24.952381, 'air_07b314d83059c4d2': RollingMean: 36.380952, 'air_07bb665f9cdfbdfb': RollingMean: 26.333333, 'air_082908692355165e': RollingMean: 49.52381, 'air_083ddc520ea47e1e': RollingMean: 13.238095, 'air_0845d8395f30c6bb': RollingMean: 21.714286, 'air_084d98859256acf0': RollingMean: 15.761905, 'air_0867f7bebad6a649': RollingMean: 22.380952, 'air_08ba8cd01b3ba010': RollingMean: 10.666667, 'air_08cb3c4ee6cd6a22': RollingMean: 12.666667, 'air_08ef81d5b7a0d13f': RollingMean: 13.952381, 'air_08f994758a1e76d4': RollingMean: 27.714286, 'air_09040f6df960ddb8': RollingMean: 16., 'air_0919d54f0c9a24b8': RollingMean: 31.714286, 'air_09661c0f3259cc04': RollingMean: 28.47619, 'air_09a845d5b5944b01': RollingMean: 6.380952, 'air_09fd1f5c58583141': RollingMean: 8.333333, 'air_0a74a5408a0b8642': RollingMean: 28.095238, 'air_0b184ec04c741a6a': RollingMean: 11.190476, 'air_0b1e72d2d4422b20': RollingMean: 22.333333, 'air_0b9038300f8b2b50': RollingMean: 12.380952, 'air_0e1eae99b8723bc1': RollingMean: 11.952381, 'air_0e7c11b9abc50163': RollingMean: 36.904762, 'air_0ead98dd07e7a82a': RollingMean: 13.857143, 'air_0f0cdeee6c9bf3d7': RollingMean: 25.238095, 'air_0f2f96335f274801': RollingMean: 10., 'air_0f60e1576a7d397d': RollingMean: 6.619048, 'air_1033310359ceeac1': RollingMean: 18.809524, 'air_10393f12e9069760': RollingMean: 15.714286, 'air_105a7954e32dba9b': RollingMean: 50.857143, 'air_10713fbf3071c361': RollingMean: 12.047619, 'air_10bbe8acd943d8f6': RollingMean: 26.809524, 'air_12c4fb7a423df20d': RollingMean: 20.857143, 'air_138ee734ac79ff90': RollingMean: 8.714286, 'air_138ff410757b845f': RollingMean: 54.285714, 'air_1408dd53f31a8a65': RollingMean: 25.190476, 'air_142e78ba7001da9c': RollingMean: 13.619048, 'air_1509881b22965b34': RollingMean: 16.666667, 'air_152c1f08d7d20e07': RollingMean: 13.238095, 'air_15ae33469e9ea2dd': RollingMean: 11.428571, 'air_15e6e15c7ea2c162': RollingMean: 18.952381, 'air_16179d43b6ee5fd8': RollingMean: 8.285714, 'air_1653a6c513865af3': RollingMean: 32.52381, 'air_168441ada3e878e1': RollingMean: 41.47619, 'air_16c4cfddeb2cf69b': RollingMean: 11.52381, 'air_16cf0a73233896de': RollingMean: 10.52381, 'air_1707a3f18bb0da07': RollingMean: 27.285714, 'air_17a6ab40f97fd4d8': RollingMean: 6.904762, 'air_17bed6dbf7c8b0fc': RollingMean: 19.095238, 'air_1979eaff8189d086': RollingMean: 9.666667, 'air_1ab60ce33bfed8a8': RollingMean: 9.714286, 'air_1ae94f514a0bce13': RollingMean: 6.904762, 'air_1ba4e87ef7422183': RollingMean: 37.619048, 'air_1c0b150f9e696a5f': RollingMean: 107.238095, 'air_1c95a84924d72500': RollingMean: 9.619048, 'air_1d1e8860ae04f8e9': RollingMean: 12.904762, 'air_1d25ca6c76df48b4': RollingMean: 39.666667, 'air_1d3f797dd1f7cf1c': RollingMean: 33.428571, 'air_1dd8f6f47480d1a2': RollingMean: 37.857143, 'air_1dea9815ccd36620': RollingMean: 10.428571, 'air_1e23210b584540e7': RollingMean: 4.047619, 'air_1e665503b8474c55': RollingMean: 5.571429, 'air_1eeff462acb24fb7': RollingMean: 16.333333, 'air_1f1390a8be2272b3': RollingMean: 18.095238, 'air_1f34e9beded2231a': RollingMean: 8.095238, 'air_1f7f8fa557bc0d55': RollingMean: 3.619048, 'air_2009041dbf9264de': RollingMean: 50.047619, 'air_20619d21192aa571': RollingMean: 17.571429, 'air_20add8092c9bb51d': RollingMean: 30.428571, 'air_2195cd5025a98033': RollingMean: 28.809524, 'air_21f5052d5330528d': RollingMean: 32.809524, 'air_220cba70c890b119': RollingMean: 9.142857, 'air_22682e965418936f': RollingMean: 8.857143, 'air_228f10bec0bda9c8': RollingMean: 17.333333, 'air_229d7e508d9f1b5e': RollingMean: 10.809524, 'air_232dcee6f7c51d37': RollingMean: 5.571429, 'air_234d3dbf7f3d5a50': RollingMean: 7.571429, 'air_23e1b11aee2a1407': RollingMean: 45.571429, 'air_23ee674e91469086': RollingMean: 21.714286, 'air_24b9b2a020826ede': RollingMean: 31.666667, 'air_24e8414b9b07decb': RollingMean: 5.952381, 'air_2545dd3a00f265e2': RollingMean: 48.761905, 'air_256be208a979e023': RollingMean: 6.47619, 'air_2570ccb93badde68': RollingMean: 38.333333, 'air_258ad2619d7bff9a': RollingMean: 30.666667, 'air_258dc112912fc458': RollingMean: 61.571429, 'air_25c583983246b7b0': RollingMean: 20.047619, 'air_25d8e5cc57dd87d9': RollingMean: 26.190476, 'air_25e9888d30b386df': RollingMean: 4.285714, 'air_2634e41551e9807d': RollingMean: 13., 'air_26c5bbeb7bb82bf1': RollingMean: 28.952381, 'air_26f10355d9b4d82a': RollingMean: 32.52381, 'air_2703dcb33192b181': RollingMean: 49.428571, 'air_275732a5db46f4d3': RollingMean: 14.952381, 'air_27e991812b0d9c92': RollingMean: 44.238095, 'air_28064154614b2e6c': RollingMean: 21.714286, 'air_287d2de7d3c93406': RollingMean: 13.047619, 'air_28a9fa1ec0839375': RollingMean: 29.571429, 'air_28dbe91c4c9656be': RollingMean: 34.619048, 'air_290e7a57b390f78e': RollingMean: 13.714286, 'air_298513175efdf261': RollingMean: 27.285714, 'air_2a184c1745274b2b': RollingMean: 3.952381, 'air_2a24aec099333f39': RollingMean: 6.666667, 'air_2a3743e37aab04b4': RollingMean: 18.428571, 'air_2a485b92210c98b5': RollingMean: 21.571429, 'air_2a7f14da7fe0f699': RollingMean: 28.047619, 'air_2aab19554f91ff82': RollingMean: 51.285714, 'air_2ac361b97630e2df': RollingMean: 12., 'air_2b8b29ddfd35018e': RollingMean: 11.095238, 'air_2b9bc9f5f5168ea1': RollingMean: 20.190476, 'air_2bffb19a24d11729': RollingMean: 11.142857, 'air_2c505f9ad67d4635': RollingMean: 15.428571, 'air_2c6c79d597e48096': RollingMean: 16.380952, 'air_2c6fef1ce0e13a5a': RollingMean: 31.190476, 'air_2c989829acbd1c6b': RollingMean: 29.333333, 'air_2cee51fa6fdf6c0d': RollingMean: 16.619048, 'air_2d3afcb91762fe01': RollingMean: 44.857143, 'air_2d78d9a1f4dd02ca': RollingMean: 11.333333, 'air_2e7cb1f1a2a9cd6a': RollingMean: 27.666667, 'air_2f8ced25216df926': RollingMean: 13.714286, 'air_2fc149abe33adcb4': RollingMean: 34.857143, 'air_2fc478dc9f0a6b31': RollingMean: 11.714286, 'air_2fed81034f8834e5': RollingMean: 20.47619, 'air_303bac187b53083a': RollingMean: 10.047619, 'air_310e467e6e625004': RollingMean: 15.47619, 'air_3155ee23d92202da': RollingMean: 16.380952, 'air_31c753b48a657b6c': RollingMean: 24.904762, 'air_32460819c7600037': RollingMean: 50.761905, 'air_324f7c39a8410e7c': RollingMean: 11.809524, 'air_326ca454ef3558bc': RollingMean: 23.714286, 'air_32b02ba5dc2027f4': RollingMean: 29.285714, 'air_32c61b620a766138': RollingMean: 27.380952, 'air_32f5d7cd696e3c4a': RollingMean: 22.190476, 'air_33b01025210d6007': RollingMean: 10.285714, 'air_3440e0ea1b70a99b': RollingMean: 27.904762, 'air_346ade7d29230634': RollingMean: 7.761905, 'air_347be2c4feeb408b': RollingMean: 22.047619, 'air_349278fa964bb12f': RollingMean: 18.666667, 'air_3525f11ef0bf0c35': RollingMean: 39.761905, 'air_35512c42db0868da': RollingMean: 6.238095, 'air_3561fd1c0bce6a95': RollingMean: 12.142857, 'air_35c4732dcbfe31be': RollingMean: 6.571429, 'air_36429b5ca4407b3e': RollingMean: 18.904762, 'air_36bcf77d3382d36e': RollingMean: 32.285714, 'air_37189c92b6c761ec': RollingMean: 19.47619, 'air_375a5241615b5e22': RollingMean: 7.428571, 'air_382f5ace4e2247b8': RollingMean: 8.761905, 'air_383f5b2f8d345a49': RollingMean: 14.238095, 'air_38746ffe9aa20c7e': RollingMean: 4., 'air_396166d47733d5c9': RollingMean: 24.952381, 'air_396942e6423a2145': RollingMean: 19.333333, 'air_397d3f32a7196aa2': RollingMean: 31.190476, 'air_3980af67be35afdb': RollingMean: 17.380952, 'air_3982a2c4ea2ed431': RollingMean: 31.714286, 'air_399904bdb7685ca0': RollingMean: 26., 'air_39dccf7df20b1c6a': RollingMean: 24.809524, 'air_3a8a3f8fb5cd7f88': RollingMean: 21.285714, 'air_3aa839e8e0cb6c87': RollingMean: 27.380952, 'air_3ac24136722e2291': RollingMean: 14., 'air_3b20733899b5287f': RollingMean: 39.904762, 'air_3b6438b125086430': RollingMean: 13.380952, 'air_3bb99a1fe0583897': RollingMean: 37.904762, 'air_3bd49f98ab7f36ab': RollingMean: 11.809524, 'air_3c05c8f26c611eb9': RollingMean: 23.380952, 'air_3c938075889fc059': RollingMean: 28.428571, 'air_3cad29d1a23209d2': RollingMean: 9.142857, 'air_3caef3f76b8f26c5': RollingMean: 25.761905, 'air_3d3a2b509180e798': RollingMean: 17.142857, 'air_3e6cea17a9d2c0f1': RollingMean: 17.714286, 'air_3e93f3c81008696d': RollingMean: 34.47619, 'air_3f91d592acd6cc0b': RollingMean: 24.666667, 'air_401b39f97e56b939': RollingMean: 12.095238, 'air_4043b7ccfbffa732': RollingMean: 43.428571, 'air_4092cfbd95a3ac1b': RollingMean: 26.761905, 'air_40953e2d8b4f2857': RollingMean: 17.47619, 'air_40f6193ea3ed1b91': RollingMean: 15.190476, 'air_414ff459ed18fa48': RollingMean: 16.333333, 'air_41bbf6e1d9814c4b': RollingMean: 7.714286, 'air_421670f21da5ba31': RollingMean: 17.52381, 'air_4254c3fc3ad078bd': RollingMean: 10.904762, 'air_42c9aa6d617c5057': RollingMean: 43.142857, 'air_42d41eb58cad170e': RollingMean: 27.571429, 'air_43b65e4b05bff2d3': RollingMean: 21.952381, 'air_43d577e0c9460e64': RollingMean: 38.571429, 'air_4433ab8e9999915f': RollingMean: 19.619048, 'air_4481a87c1d7c9896': RollingMean: 22.095238, 'air_452100f5305dde64': RollingMean: 7.571429, 'air_45326ebb8dc72cfb': RollingMean: 30.714286, 'air_4570f52104fe0982': RollingMean: 7.285714, 'air_4579cb0669fd411b': RollingMean: 16.380952, 'air_457efe8c3a30ea17': RollingMean: 7.142857, 'air_464a62de0d57be1e': RollingMean: 25.619048, 'air_465bddfed3353b23': RollingMean: 25.952381, 'air_47070be6093f123e': RollingMean: 48.52381, 'air_472b19e3b5bffa41': RollingMean: 14.47619, 'air_473cf23b9e7c0a37': RollingMean: 8.571429, 'air_473f98b212d37b4a': RollingMean: 27.142857, 'air_47beaffd3806c979': RollingMean: 20.47619, 'air_483eba479dc9910d': RollingMean: 26.095238, 'air_48e9fc98b62495a7': RollingMean: 18.380952, 'air_48f4da6223571da4': RollingMean: 22.190476, 'air_48ffd31594bc3263': RollingMean: 4.857143, 'air_49211568cab5fdee': RollingMean: 23.380952, 'air_4974785f48853db9': RollingMean: 6.428571, 'air_4b251b9f8373f1ae': RollingMean: 27.714286, 'air_4b380b4db9d37883': RollingMean: 25.571429, 'air_4b55d8aea1d2b395': RollingMean: 36.904762, 'air_4b9085d0d46a6211': RollingMean: 29.428571, 'air_4beac252540f865e': RollingMean: 39.714286, 'air_4c2ed28f3f19ca52': RollingMean: 13.857143, 'air_4c665a2bfff0da3b': RollingMean: 8.666667, 'air_4c727b55acdee495': RollingMean: 14.809524, 'air_4cab15ad29c0ffbc': RollingMean: 20.047619, 'air_4cab91146e3d1897': RollingMean: 14.47619, 'air_4cca5666eaf5c709': RollingMean: 29.714286, 'air_4ce7b17062a1bf73': RollingMean: 9.047619, 'air_4d21676ed11f0bac': RollingMean: 30., 'air_4d71826793c09b22': RollingMean: 21.333333, 'air_4d90a22572fa1ec9': RollingMean: 26.238095, 'air_4de6d887a7b1c1fc': RollingMean: 15.52381, 'air_4dea8d17f6f59c56': RollingMean: 23.285714, 'air_4e1c38f68f435596': RollingMean: 35.952381, 'air_4f762e840b3996e1': RollingMean: 11.380952, 'air_4feeb8600f131e43': RollingMean: 57.428571, 'air_500641aca4cf673c': RollingMean: 15.666667, 'air_506fe758114df773': RollingMean: 27.619048, 'air_51281cd059d7b89b': RollingMean: 12.285714, 'air_51319e7acf0438cf': RollingMean: 12.952381, 'air_52a08ef3efdb4bb0': RollingMean: 44.904762, 'air_52e2a1fd42bc917a': RollingMean: 10.761905, 'air_536043fcf1a4f8a4': RollingMean: 28.238095, 'air_539d693f7317c62d': RollingMean: 17.285714, 'air_546b353cbea4a45b': RollingMean: 16.52381, 'air_5485912b44f976de': RollingMean: 8.809524, 'air_54d6c25d33f5260e': RollingMean: 38.47619, 'air_54ed43163b7596c4': RollingMean: 17.047619, 'air_55390f784018349a': RollingMean: 47.285714, 'air_55c3627912b9c849': RollingMean: 9.952381, 'air_55e11c33d4758131': RollingMean: 22.571429, 'air_56cd12f31a0afc04': RollingMean: 30.619048, 'air_56cebcbd6906e04c': RollingMean: 17.857143, 'air_56ea46c14b2dd967': RollingMean: 39.047619, 'air_57013002b912772b': RollingMean: 4.52381, 'air_573ecdf81b157d22': RollingMean: 28.380952, 'air_57c9eea1a2b66e65': RollingMean: 13.904762, 'air_57ed725a1930a5b9': RollingMean: 17., 'air_5878b6f2a9da12c1': RollingMean: 11.809524, 'air_59cc9b2b209c6331': RollingMean: 10.809524, 'air_5a9a6cbeeb434c08': RollingMean: 25.142857, 'air_5acc13d655a6e8b2': RollingMean: 25.238095, 'air_5afb1cca48ceaa19': RollingMean: 41.333333, 'air_5b6d18c470bbfaf9': RollingMean: 34.142857, 'air_5b704df317ed1962': RollingMean: 2.095238, 'air_5bd22f9cc1426a90': RollingMean: 29.095238, 'air_5c65468938c07fa5': RollingMean: 14., 'air_5c7489c9ec755e2d': RollingMean: 40.380952, 'air_5c817ef28f236bdf': RollingMean: 43.238095, 'air_5cb030b9f0b91537': RollingMean: 10.238095, 'air_5cfc537125d97f16': RollingMean: 9.952381, 'air_5d7c744c3a2ef624': RollingMean: 30.47619, 'air_5d945ade487cdf4d': RollingMean: 12.714286, 'air_5dea8a7a5bf5eb71': RollingMean: 31.047619, 'air_5e339a1f364cdb00': RollingMean: 10.428571, 'air_5e34c6fe6fabd10e': RollingMean: 18.809524, 'air_5e70fe82f9e4fab6': RollingMean: 15.857143, 'air_5e939e005bd34633': RollingMean: 3.190476, 'air_5ed3198e4a5eed0f': RollingMean: 39.190476, 'air_5f3a3ef4cba110a4': RollingMean: 31.47619, 'air_5f6fa1b897fe80d5': RollingMean: 28.809524, 'air_5fbda8e9302f7c13': RollingMean: 26.47619, 'air_602ca92c0db34f8f': RollingMean: 21.142857, 'air_609050e4e4f79ae1': RollingMean: 11.714286, 'air_60a7057184ec7ec7': RollingMean: 27.761905, 'air_60aa54ecbc602348': RollingMean: 5.095238, 'air_6108821ffafa9b72': RollingMean: 25.809524, 'air_614e2f7e76dff854': RollingMean: 11.285714, 'air_61668cc2b0778898': RollingMean: 10., 'air_61b8d37c33617f21': RollingMean: 27.52381, 'air_61de73b097513f58': RollingMean: 10.714286, 'air_622375b4815cf5cb': RollingMean: 35.142857, 'air_627cabe2fe53f33f': RollingMean: 14.904762, 'air_629d9935273c82ae': RollingMean: 21.428571, 'air_629edf21ea38ac2d': RollingMean: 37.571429, 'air_632ba66e1f75aa28': RollingMean: 17.857143, 'air_638c35eb25e53eea': RollingMean: 20.619048, 'air_63a750d8b4b6a976': RollingMean: 24.047619, 'air_63a88d81295195ed': RollingMean: 29.809524, 'air_63b13c56b7201bd9': RollingMean: 20., 'air_63e28ee0b0c955a7': RollingMean: 23.571429, 'air_640cf4835f0d9ba3': RollingMean: 24.47619, 'air_6411203a47b5ec77': RollingMean: 10.095238, 'air_645cb18b33f938cf': RollingMean: 16.047619, 'air_646b93e336f0dded': RollingMean: 6.619048, 'air_64a5d5c1381837af': RollingMean: 40.571429, 'air_64d4491ad8cdb1c6': RollingMean: 14.047619, 'air_650f9b9de0c5542c': RollingMean: 24., 'air_657a0748462f85de': RollingMean: 7.952381, 'air_65e294f1ae6df9c3': RollingMean: 23., 'air_6607fe3671242ce3': RollingMean: 40.190476, 'air_670a0c1c4108bcea': RollingMean: 25.285714, 'air_671b4bea84dafb67': RollingMean: 27., 'air_673acd9fa5e0dd78': RollingMean: 5.857143, 'air_67483104fa38ef6c': RollingMean: 30.904762, 'air_675aa35cba456fd1': RollingMean: 46.666667, 'air_67f87c159d9e2ee2': RollingMean: 39.47619, 'air_68147db09287bf74': RollingMean: 23.190476, 'air_681b0c56328dd2af': RollingMean: 33.285714, 'air_681f96e6a6595f82': RollingMean: 41.952381, 'air_68301bcb11e2f389': RollingMean: 24., 'air_683371d9baabf410': RollingMean: 32., 'air_6836438b543ba698': RollingMean: 13.285714, 'air_6873982b9e19c7ad': RollingMean: 5.190476, 'air_68c1de82037d87e6': RollingMean: 23.761905, 'air_68cc910e7b307b09': RollingMean: 7.428571, 'air_68d075113f368946': RollingMean: 20.142857, 'air_6902e4ec305b3d08': RollingMean: 38.761905, 'air_694571ea13fb9e0e': RollingMean: 30.904762, 'air_6a15e4eae523189d': RollingMean: 19.285714, 'air_6b15edd1b4fbb96a': RollingMean: 30.238095, 'air_6b2268863b14a2af': RollingMean: 18.238095, 'air_6b65745d432fd77f': RollingMean: 23.571429, 'air_6b7678aae65d2d59': RollingMean: 6.904762, 'air_6b942d5ebbc759c2': RollingMean: 13.571429, 'air_6b9fa44a9cf504a1': RollingMean: 6.142857, 'air_6c1128955c58b690': RollingMean: 14.095238, 'air_6c91a28278a16f64': RollingMean: 10.380952, 'air_6c952e3c6e590945': RollingMean: 15.285714, 'air_6ca1d941c8199a67': RollingMean: 26.619048, 'air_6cbe54f0aa30b615': RollingMean: 17., 'air_6ced51c24fb54262': RollingMean: 8.380952, 'air_6d64dba2edd4fc0c': RollingMean: 9.857143, 'air_6d65542aa43b598b': RollingMean: 28.095238, 'air_6d65dd11d96e00fb': RollingMean: 5.285714, 'air_6e06824d0934dd81': RollingMean: 21.285714, 'air_6e3fd96320d24324': RollingMean: 9.190476, 'air_6e64fb5821402cd2': RollingMean: 8.619048, 'air_6ff5fca957798daa': RollingMean: 7.190476, 'air_707d4b6328f2c2df': RollingMean: 29.571429, 'air_709262d948dd0b6e': RollingMean: 11., 'air_70e9e8cd55879414': RollingMean: 10.857143, 'air_70f834596eb99fee': RollingMean: 21.333333, 'air_710d6537cb7623df': RollingMean: 29.904762, 'air_712dd258f7f91b4b': RollingMean: 15.142857, 'air_71903025d39a4571': RollingMean: 14.809524, 'air_722297e7f26db91d': RollingMean: 11.904762, 'air_728ff578acc6ac6e': RollingMean: 9.809524, 'air_72f5146cf0c49beb': RollingMean: 13.238095, 'air_735bcbe1763d6e98': RollingMean: 8.047619, 'air_73f316e6a18d8aa9': RollingMean: 23.238095, 'air_7420042ff75f9aca': RollingMean: 35., 'air_746211c0b532e8aa': RollingMean: 49.190476, 'air_747f375eb3900e1e': RollingMean: 5.047619, 'air_74cf22153214064c': RollingMean: 11.095238, 'air_7514d90009613cd6': RollingMean: 56.857143, 'air_754ae581ad80cc9f': RollingMean: 14.380952, 'air_75864c80d2fb334a': RollingMean: 10.714286, 'air_75bd5d1b6dc6670d': RollingMean: 12.952381, 'air_764f71040a413d4d': RollingMean: 51.142857, 'air_77488fa378cf98c3': RollingMean: 8.904762, 'air_77dfc83450cbc89c': RollingMean: 43.857143, 'air_7831b00996701c0f': RollingMean: 25.952381, 'air_789103bf53b8096b': RollingMean: 58.52381, 'air_789466e488705c93': RollingMean: 25.285714, 'air_78df4dc6a7e83e41': RollingMean: 19.238095, 'air_79afb3f52b4d062c': RollingMean: 9., 'air_79f528087f49df06': RollingMean: 32.047619, 'air_7a81bd7fadcbf3d8': RollingMean: 6.190476, 'air_7a946aada80376a4': RollingMean: 13.952381, 'air_7bacc4d36fb094c9': RollingMean: 6.285714, 'air_7bc6ca04d7b0f3b8': RollingMean: 15.52381, 'air_7bda6048a4a78837': RollingMean: 24.714286, 'air_7c7774c66fb237f7': RollingMean: 8.714286, 'air_7cc17a324ae5c7dc': RollingMean: 15.095238, 'air_7cf5a02c0e01b647': RollingMean: 31.238095, 'air_7d65049f9d275c0d': RollingMean: 9.380952, 'air_7dacea2f22afccfb': RollingMean: 29.285714, 'air_7db266904cb0d72a': RollingMean: 14.047619, 'air_7e12c5d27f44a8de': RollingMean: 24.714286, 'air_7ef9a5ea5c8fe39f': RollingMean: 10.666667, 'air_7f3dc18494bce98b': RollingMean: 13.761905, 'air_7f9e15afafcf4c75': RollingMean: 35.190476, 'air_7fbf7649eb13ad9b': RollingMean: 19.333333, 'air_800c02226e2e0288': RollingMean: 14.952381, 'air_8093d0b565e9dbdf': RollingMean: 34.809524, 'air_8110d68cc869b85e': RollingMean: 45.904762, 'air_81546875de9c8e78': RollingMean: 5.428571, 'air_81a12d67c22e012f': RollingMean: 19.285714, 'air_81bd68142db76f58': RollingMean: 28.047619, 'air_81c2600146d07d16': RollingMean: 8.714286, 'air_81c5dff692063446': RollingMean: 9.380952, 'air_820d1919cbecaa0a': RollingMean: 33.619048, 'air_82a6ae14151953ba': RollingMean: 36.47619, 'air_831658500aa7c846': RollingMean: 30.428571, 'air_832f9dbe9ee4ebd3': RollingMean: 13.809524, 'air_83db5aff8f50478e': RollingMean: 8.190476, 'air_84060403939d8216': RollingMean: 14.809524, 'air_848616680ef061bd': RollingMean: 32.238095, 'air_84f6876ff7e83ae7': RollingMean: 19.380952, 'air_8523d6a70de49e6c': RollingMean: 31.380952, 'air_859feab8e3c9f98d': RollingMean: 22.238095, 'air_85bd13a49370c392': RollingMean: 11.47619, 'air_86cfbf2624576fad': RollingMean: 7.047619, 'air_86f7b2109e4abd65': RollingMean: 53.333333, 'air_87059630ab6fe47f': RollingMean: 4.380952, 'air_87078cf7903a648c': RollingMean: 6.142857, 'air_87467487d21891dd': RollingMean: 15.761905, 'air_8764b3473ddcceaf': RollingMean: 4.333333, 'air_876d7a23c47811cb': RollingMean: 16.142857, 'air_877f79706adbfb06': RollingMean: 11.952381, 'air_87ca98aa7664de94': RollingMean: 13.47619, 'air_87f9e1024b951f01': RollingMean: 12.380952, 'air_883ca28ef0ed3d55': RollingMean: 13.619048, 'air_88c8e34baa79217b': RollingMean: 28.571429, 'air_88ca84051ba95339': RollingMean: 18.238095, 'air_88f31db64991768a': RollingMean: 9.47619, 'air_890d7e28e8eaaa11': RollingMean: 7.333333, 'air_89e7328af22efe74': RollingMean: 40.857143, 'air_8a1d21fad48374e8': RollingMean: 12.380952, 'air_8a59bb0c497b771e': RollingMean: 27.809524, 'air_8a906e5801eac81c': RollingMean: 25.095238, 'air_8b4a46dc521bfcfe': RollingMean: 25.714286, 'air_8c119d1f16049f20': RollingMean: 24.095238, 'air_8c3175aa5e4fc569': RollingMean: 60.619048, 'air_8cc350fd70ee0757': RollingMean: 31.571429, 'air_8ce035ee1d8a56a6': RollingMean: 19., 'air_8d50c64692322dff': RollingMean: 10.238095, 'air_8d61f49aa0373492': RollingMean: 45.571429, 'air_8e429650fcf7a0ae': RollingMean: 20.047619, 'air_8e4360a64dbd4c50': RollingMean: 20.333333, 'air_8e492076a1179383': RollingMean: 59.285714, 'air_8e8f42f047537154': RollingMean: 29.095238, 'air_8ec47c0f1e2c879e': RollingMean: 30.619048, 'air_8f13ef0f5e8c64dd': RollingMean: 6.952381, 'air_8f273fb9ad2fed6f': RollingMean: 15.571429, 'air_8f3b563416efc6ad': RollingMean: 12.904762, 'air_900d755ebd2f7bbd': RollingMean: 82.2, 'air_901925b628677c2e': RollingMean: 9.333333, 'air_90213bcae4afa274': RollingMean: 26.761905, 'air_90bd5de52c166767': RollingMean: 23.142857, 'air_90ed0a2f24755533': RollingMean: 38.952381, 'air_90f0efbb702d77b7': RollingMean: 30.047619, 'air_9105a29b0eb250d2': RollingMean: 18.190476, 'air_91236b89d29567af': RollingMean: 20.571429, 'air_9152d9926e5c4a3a': RollingMean: 37.47619, 'air_915558a55c2bc56c': RollingMean: 18.190476, 'air_91beafbba9382b0a': RollingMean: 34.904762, 'air_91d72e16c4bcba55': RollingMean: 15.809524, 'air_9241121af22ff1d6': RollingMean: 31.619048, 'air_929d8513e3cdb423': RollingMean: 7.619048, 'air_931a8a4321b6e7d1': RollingMean: 5.714286, 'air_9352c401d5adb01b': RollingMean: 27.904762, 'air_9387ff95e886ebc7': RollingMean: 18.380952, 'air_938ef91ecdde6878': RollingMean: 22.047619, 'air_939964477035ef0b': RollingMean: 19.571429, 'air_93b9bb641f8fc982': RollingMean: 27.047619, 'air_93dd7070c9bf5453': RollingMean: 34.095238, 'air_93ebe490d4abb8e9': RollingMean: 16.47619, 'air_9438d67241c81314': RollingMean: 35.666667, 'air_947eb2cae4f3e8f2': RollingMean: 34., 'air_9483d65e9cc9a6b7': RollingMean: 14.47619, 'air_950381108f839348': RollingMean: 30.095238, 'air_95d28905941fd4cb': RollingMean: 29.47619, 'air_95e917913cd988f3': RollingMean: 24.428571, 'air_96005f79124e12bf': RollingMean: 41.619048, 'air_965b2e0cf4119003': RollingMean: 53.904762, 'air_96743eee94114261': RollingMean: 13.904762, 'air_96773a6236d279b1': RollingMean: 25.714286, 'air_968d72c12eed09f0': RollingMean: 19.285714, 'air_96929a799b12a43e': RollingMean: 27.761905, 'air_96ec3cfe78cb0652': RollingMean: 18.047619, 'air_97159fc4e90053fe': RollingMean: 23.714286, 'air_97958e7fce98b6a3': RollingMean: 19.095238, 'air_97b2a9f975fc702c': RollingMean: 34.285714, 'air_97cf68dc1a9beac0': RollingMean: 12.428571, 'air_97e0f2feec4d577a': RollingMean: 16.142857, 'air_9828505fefc77d75': RollingMean: 14.857143, 'air_98b54e32ccddd896': RollingMean: 17.952381, 'air_990a642a3deb2903': RollingMean: 34.428571, 'air_99157b6163835eec': RollingMean: 36.285714, 'air_99a5183695b849f9': RollingMean: 35.52381, 'air_99b01136f451fc0e': RollingMean: 40.761905, 'air_99c3eae84130c1cb': RollingMean: 41.095238, 'air_9a30407764f4ff84': RollingMean: 17.428571, 'air_9a6f6e7f623003d2': RollingMean: 3.190476, 'air_9aa32b3db0fab3a5': RollingMean: 16.095238, 'air_9aa92007e3628dbc': RollingMean: 31.333333, 'air_9ae7081cb77dc51e': RollingMean: 28.714286, 'air_9b13c7feb0a0c431': RollingMean: 10.52381, 'air_9b6af3db40da4ae2': RollingMean: 28., 'air_9bbc673495e23532': RollingMean: 4.571429, 'air_9bf0ccac497c4af3': RollingMean: 43.142857, 'air_9bf595ef095572fb': RollingMean: 25.571429, 'air_9c6787aa03a45586': RollingMean: 97.809524, 'air_9ca2767761efff4d': RollingMean: 10.190476, 'air_9cd5e310f488bced': RollingMean: 13.666667, 'air_9cf2f1ba86229773': RollingMean: 32.142857, 'air_9d3482b4680aee88': RollingMean: 10.190476, 'air_9d452a881f7f2bb7': RollingMean: 9.142857, 'air_9d474ec2448c700d': RollingMean: 12.380952, 'air_9d5a980b211e1795': RollingMean: 11.285714, 'air_9d93d95720f2e831': RollingMean: 7.52381, 'air_9dc9483f717d73ee': RollingMean: 5.47619, 'air_9dd7d38b0f1760c4': RollingMean: 2.428571, 'air_9e920b758503ef54': RollingMean: 5.571429, 'air_9efaa7ded03c5a71': RollingMean: 10.238095, 'air_9f277fb7a2c1d842': RollingMean: 11.571429, 'air_9fc607777ad76b26': RollingMean: 16.761905, 'air_a083834e7ffe187e': RollingMean: 19.904762, 'air_a11473cc1eb9a27f': RollingMean: 35.714286, 'air_a17f0778617c76e2': RollingMean: 37.571429, 'air_a1fe8c588c8d2f30': RollingMean: 15., 'air_a218912784bf767d': RollingMean: 14.809524, 'air_a21ffca0bea1661a': RollingMean: 1.095238, 'air_a239a44805932bab': RollingMean: 33.047619, 'air_a24bf50c3e90d583': RollingMean: 20.142857, 'air_a2567267116a3b75': RollingMean: 16.142857, 'air_a257c9749d8d0ff6': RollingMean: 19., 'air_a271c9ba19e81d17': RollingMean: 27.52381, 'air_a2b29aa7feb4e36f': RollingMean: 16.761905, 'air_a304330715435390': RollingMean: 7.714286, 'air_a33461f4392ec62c': RollingMean: 28.857143, 'air_a373500730e2a9e0': RollingMean: 10.47619, 'air_a38f25e3399d1b25': RollingMean: 43.571429, 'air_a41b032371a63427': RollingMean: 11.571429, 'air_a49f1cf0634f13e5': RollingMean: 24.238095, 'air_a510dcfe979f09eb': RollingMean: 14.571429, 'air_a546cbf478a8b6e4': RollingMean: 28.095238, 'air_a55d17bd3f3033cb': RollingMean: 12.952381, 'air_a563896da3777078': RollingMean: 21.714286, 'air_a678e5b144ca24ce': RollingMean: 15.52381, 'air_a7404a854919e990': RollingMean: 8., 'air_a8533b7a992bb0ca': RollingMean: 17.619048, 'air_a85f0c0c889f6b7e': RollingMean: 42.761905, 'air_a85f8c0bfd61889f': RollingMean: 12.952381, 'air_a88ac559064dec08': RollingMean: 33.809524, 'air_a9133955abccf071': RollingMean: 27.952381, 'air_a9178f19da58fe99': RollingMean: 6.857143, 'air_a9a380530c1e121f': RollingMean: 40.52381, 'air_aa0049fe3cc6f4d6': RollingMean: 9.380952, 'air_ab3ae0e410b20069': RollingMean: 16.52381, 'air_ab9746a0f83084b7': RollingMean: 8.857143, 'air_abcdc8115988a010': RollingMean: 11.809524, 'air_abf06fcca748dca5': RollingMean: 8.428571, 'air_ac7a7427c9ae12a5': RollingMean: 59.095238, 'air_ad13e71e21235131': RollingMean: 19.666667, 'air_ad60f6b76c9df7ed': RollingMean: 22.809524, 'air_ad7777590c884721': RollingMean: 8.142857, 'air_add9a575623726c8': RollingMean: 41.714286, 'air_ade6e836ffd1da64': RollingMean: 8.857143, 'air_aed3a8b49abe4a48': RollingMean: 5.857143, 'air_af03c277a167b2bd': RollingMean: 25.904762, 'air_af24e3e817dea1e5': RollingMean: 15.095238, 'air_af63df35857b16e6': RollingMean: 23.142857, 'air_b0a6a4c5e95c74cf': RollingMean: 17.190476, 'air_b162fb07fbbdea33': RollingMean: 14.809524, 'air_b192fb5310436005': RollingMean: 7.666667, 'air_b1a72bf1ebf4b8ef': RollingMean: 65.095238, 'air_b1bb1fae86617d7a': RollingMean: 35.333333, 'air_b1d822f75c9fc615': RollingMean: 10.142857, 'air_b2395df0e874078d': RollingMean: 6.47619, 'air_b23d0f519291247d': RollingMean: 26.428571, 'air_b259b4e4a51a690d': RollingMean: 21.047619, 'air_b28bed4b2e7167b7': RollingMean: 16., 'air_b2a639cc7e02edf1': RollingMean: 19.47619, 'air_b2d8bc9c88b85f96': RollingMean: 16.666667, 'air_b2d97bd2337c5ba7': RollingMean: 31.952381, 'air_b2dcec37b83e2494': RollingMean: 7.904762, 'air_b30fffd7ab1e75a5': RollingMean: 10.047619, 'air_b3180b74332ba886': RollingMean: 11.52381, 'air_b3a824511477a4ed': RollingMean: 6.380952, 'air_b439391e72899756': RollingMean: 19.190476, 'air_b45b8e456f53942a': RollingMean: 11., 'air_b4f32bcc399da2b9': RollingMean: 29.190476, 'air_b5598d12d1b84890': RollingMean: 5.380952, 'air_b5bdd318005d9aa4': RollingMean: 36.809524, 'air_b60cc7d6aee68194': RollingMean: 11.52381, 'air_b711b43ae472cb6b': RollingMean: 19.809524, 'air_b7fa3d2fca744dd2': RollingMean: 41.333333, 'air_b80fed1a07c817d2': RollingMean: 4.666667, 'air_b88192b35ac03c24': RollingMean: 15.666667, 'air_b8925441167c3152': RollingMean: 2.047619, 'air_b8a5ee69e5fdcc5b': RollingMean: 38., 'air_b8d9e1624baaadc2': RollingMean: 5.952381, 'air_b9e27558fb8bd5c4': RollingMean: 12.47619, 'air_ba495cccc8f0f237': RollingMean: 15.904762, 'air_ba937bf13d40fb24': RollingMean: 17.285714, 'air_bac5f4441db21db9': RollingMean: 40.761905, 'air_baf28ac9f13a307d': RollingMean: 15.619048, 'air_bb09595bab7d5cfb': RollingMean: 27.809524, 'air_bb26d6d079594414': RollingMean: 11.571429, 'air_bb4ff06cd661ee9b': RollingMean: 34.428571, 'air_bbe1c1a47e09f161': RollingMean: 1.52381, 'air_bc991c51d6613745': RollingMean: 20.095238, 'air_bc9a129e11a2efe0': RollingMean: 31.666667, 'air_bcce1ea4350b7b72': RollingMean: 18.52381, 'air_bd74a9222edfdfe1': RollingMean: 12.904762, 'air_bdd32aa407c16335': RollingMean: 16.428571, 'air_bebd55ed63ab2422': RollingMean: 8.571429, 'air_bed603c423b7d9d4': RollingMean: 5.428571, 'air_bedd35489e666605': RollingMean: 39.714286, 'air_bf13014b6e3e60ca': RollingMean: 32.952381, 'air_bf21b8350771879b': RollingMean: 26.809524, 'air_bf617aa68d5f1cfa': RollingMean: 6.809524, 'air_bf7591560077332d': RollingMean: 8.904762, 'air_bfafaed35e213fd7': RollingMean: 11.952381, 'air_bfda7731a6c6fc61': RollingMean: 19.428571, 'air_c027e2b560442808': RollingMean: 15., 'air_c0385db498b391e5': RollingMean: 31.904762, 'air_c1d5d165c055b866': RollingMean: 30., 'air_c1ff20617c54fee7': RollingMean: 7.809524, 'air_c225148c0fcc5c72': RollingMean: 36.52381, 'air_c2626f5f86d57342': RollingMean: 16.904762, 'air_c26f027b5bc1f081': RollingMean: 5.142857, 'air_c28983412a7eefcf': RollingMean: 38.809524, 'air_c2c8435bdb3516d4': RollingMean: 30.619048, 'air_c31472d14e29cee8': RollingMean: 13.142857, 'air_c3585b0fba3998d0': RollingMean: 8.904762, 'air_c3bc011cca3bec65': RollingMean: 7.666667, 'air_c3dcaf3aeb18e20e': RollingMean: 17., 'air_c47aa7493b15f297': RollingMean: 20.571429, 'air_c4fa5c562d5409ca': RollingMean: 13.52381, 'air_c52c63c781fe48f6': RollingMean: 27.761905, 'air_c5459218282bedd5': RollingMean: 23.809524, 'air_c66dbd2c37832d00': RollingMean: 17.285714, 'air_c6a164dd4060e960': RollingMean: 12.952381, 'air_c6aa2efba0ffc8eb': RollingMean: 29.571429, 'air_c6ffd6a93e6b68d6': RollingMean: 15.571429, 'air_c73d319ffabf287a': RollingMean: 16.095238, 'air_c759b6abeb552160': RollingMean: 9.095238, 'air_c77ee2b7d36da265': RollingMean: 42.095238, 'air_c7d30ab0e07f31d5': RollingMean: 16.095238, 'air_c7f78b4f3cba33ff': RollingMean: 23.952381, 'air_c8265ecc116f2284': RollingMean: 8.428571, 'air_c88467d88b2c8ecd': RollingMean: 19.952381, 'air_c8a657c8c5c93d69': RollingMean: 7.142857, 'air_c8c0ef02ed72053f': RollingMean: 27.190476, 'air_c8fe396d6c46275d': RollingMean: 16.142857, 'air_c92745dfdd2ec68a': RollingMean: 18.52381, 'air_c9ed65554b6edffb': RollingMean: 11.809524, 'air_c9f6de13be8b8f25': RollingMean: 3.809524, 'air_ca1315af9e073bd1': RollingMean: 42.380952, 'air_ca6ae8d49a2f1eaf': RollingMean: 21.380952, 'air_ca957d3a1529fbd3': RollingMean: 33.619048, 'air_cadf9cfb510a1d78': RollingMean: 32.666667, 'air_caf996ac27206301': RollingMean: 4.52381, 'air_cb083b4789a8d3a2': RollingMean: 17.52381, 'air_cb25551c4cd8d9f3': RollingMean: 9.428571, 'air_cb7467aed805e7fe': RollingMean: 36.095238, 'air_cb935ff8610ba3d3': RollingMean: 5.952381, 'air_cbe139af83feb388': RollingMean: 10.428571, 'air_cbe867adcf44e14f': RollingMean: 15.380952, 'air_cc1a0e985ce63711': RollingMean: 30., 'air_cc35590cd1da8554': RollingMean: 18.190476, 'air_ccd19a5bc5573ae5': RollingMean: 35.904762, 'air_cd4b301d5d3918d8': RollingMean: 6.47619, 'air_cd5f54969be9ed08': RollingMean: 7.142857, 'air_ced6297e5bdf5130': RollingMean: 25.809524, 'air_cf2229e64408d9fe': RollingMean: 16.571429, 'air_cf22e368c1a71d53': RollingMean: 36.428571, 'air_cf5ab75a0afb8af9': RollingMean: 45.190476, 'air_cfcc94797d2b5d3d': RollingMean: 17.333333, 'air_cfdeb326418194ff': RollingMean: 14.761905, 'air_d00161e19f08290b': RollingMean: 26.666667, 'air_d00a15343325e5f7': RollingMean: 17.571429, 'air_d07e57b21109304a': RollingMean: 11.142857, 'air_d0a1e69685259c92': RollingMean: 33.190476, 'air_d0a7bd3339c3d12a': RollingMean: 36.714286, 'air_d0e8a085d8dc83aa': RollingMean: 7.952381, 'air_d138b593ebda55cc': RollingMean: 5.380952, 'air_d1418d6fd6d634f2': RollingMean: 15.714286, 'air_d186b2cb0b9ce022': RollingMean: 13.380952, 'air_d1f20424f76cc78e': RollingMean: 19.333333, 'air_d34c0861a2be94cb': RollingMean: 43.142857, 'air_d3e7b5952cd09ccb': RollingMean: 19.714286, 'air_d44d210d2994f01b': RollingMean: 5.714286, 'air_d473620754bf9fc2': RollingMean: 12.904762, 'air_d477b6339b8ce69f': RollingMean: 8.047619, 'air_d4981cdde163b172': RollingMean: 24.714286, 'air_d4b5a4b04c5f2d04': RollingMean: 12.619048, 'air_d4d218b451f82c3d': RollingMean: 9.904762, 'air_d500b48a8735fbd3': RollingMean: 17.47619, 'air_d54d6fcb116fbed3': RollingMean: 4.428571, 'air_d5e0a20370c325c7': RollingMean: 29.190476, 'air_d63cfa6d6ab78446': RollingMean: 18.238095, 'air_d69b08a175bc0387': RollingMean: 10.809524, 'air_d6b3e67261f07646': RollingMean: 14.666667, 'air_d8abb9e490abf94f': RollingMean: 12.380952, 'air_d97dabf7aae60da5': RollingMean: 33.190476, 'air_d98380a4aeb0290b': RollingMean: 42.761905, 'air_daa7947e1c47f5ed': RollingMean: 34., 'air_dabfbd0ec951925a': RollingMean: 7.142857, 'air_dad0b6a36138f309': RollingMean: 5.571429, 'air_db1233ad855b34d5': RollingMean: 25.52381, 'air_db4b38ebe7a7ceff': RollingMean: 23.714286, 'air_db80363d35f10926': RollingMean: 28.714286, 'air_dbf64f1ce38c7442': RollingMean: 15.47619, 'air_dc0e080ba0a5e5af': RollingMean: 9.047619, 'air_dc71c6cc06cd1aa2': RollingMean: 6.190476, 'air_de692863bb2dd758': RollingMean: 21.238095, 'air_de803f7e324936b8': RollingMean: 25.619048, 'air_de88770300008624': RollingMean: 17.428571, 'air_dea0655f96947922': RollingMean: 35.809524, 'air_df507aec929ce5f6': RollingMean: 23.285714, 'air_df554c4527a1cfe6': RollingMean: 52.952381, 'air_df5cf5cd03eb68d0': RollingMean: 7.380952, 'air_df843e6b22e8d540': RollingMean: 11.952381, 'air_df9355c47c5df9d3': RollingMean: 27.52381, 'air_dfad598ff642dab7': RollingMean: 25.952381, 'air_dfe068a1bf85f395': RollingMean: 37.857143, 'air_e00fe7853c0100d6': RollingMean: 20.904762, 'air_e0118664da63a2d0': RollingMean: 16.333333, 'air_e01d99390355408d': RollingMean: 11.333333, 'air_e053c561f32acc28': RollingMean: 19.666667, 'air_e08b9cf82057a170': RollingMean: 33.333333, 'air_e0aee25b56a069f2': RollingMean: 14.428571, 'air_e0e69668214ff972': RollingMean: 9.904762, 'air_e0f241bd406810c0': RollingMean: 32.285714, 'air_e1b76fcb5208fb6b': RollingMean: 13.52381, 'air_e2208a79e2678432': RollingMean: 47.714286, 'air_e270aff84ac7e4c8': RollingMean: 24.571429, 'air_e3020992d5fe5dfd': RollingMean: 12.190476, 'air_e34c631c766f2766': RollingMean: 24.47619, 'air_e42bdc3377d1eee7': RollingMean: 21.190476, 'air_e483f5b3c4f310e0': RollingMean: 5.380952, 'air_e524c6a9e06cc3a1': RollingMean: 8.238095, 'air_e55abd740f93ecc4': RollingMean: 45.666667, 'air_e57dd6884595f60d': RollingMean: 38.571429, 'air_e58f669b6f1a08ce': RollingMean: 10.761905, 'air_e5cf003abcc5febb': RollingMean: 11.428571, 'air_e64de0a6bf0739af': RollingMean: 45.809524, 'air_e657ca554b0c008c': RollingMean: 22.571429, 'air_e700e390226d9985': RollingMean: 16.619048, 'air_e76a668009c5dabc': RollingMean: 7.952381, 'air_e7d2ac6d53d1b744': RollingMean: 9.952381, 'air_e7fbee4e3cfe65c5': RollingMean: 33.095238, 'air_e88bbe2ede3467aa': RollingMean: 24.380952, 'air_e89735e80d614a7e': RollingMean: 32.428571, 'air_e8ed9335d0c38333': RollingMean: 26.52381, 'air_e9ebf7fc520ac76a': RollingMean: 28.904762, 'air_ea6d0c3acf00b22a': RollingMean: 21.714286, 'air_ea7c16131980c837': RollingMean: 7.190476, 'air_eb120e6d384a17a8': RollingMean: 42.809524, 'air_eb20a89bba7dd3d0': RollingMean: 2.952381, 'air_eb2d2653586315dd': RollingMean: 32.380952, 'air_eb5788dba285e725': RollingMean: 23.52381, 'air_ebd31e812960f517': RollingMean: 26.47619, 'air_ebe02c3090271fa9': RollingMean: 11.333333, 'air_ec0fad2def4dcff0': RollingMean: 15.904762, 'air_eca4a5a191e8d993': RollingMean: 25.714286, 'air_eca5e0064dc9314a': RollingMean: 34.571429, 'air_ecab54b57a71b10d': RollingMean: 13.285714, 'air_eceb97ad6a7d4c07': RollingMean: 26.571429, 'air_ecf7f141339f1d57': RollingMean: 18.428571, 'air_eda179770dfa9f91': RollingMean: 9.428571, 'air_edd5e3d696a5811b': RollingMean: 42.761905, 'air_ee3a01f0c71a769f': RollingMean: 26.761905, 'air_ee3ba9af184c6c82': RollingMean: 16.904762, 'air_eec5e572b9eb9c23': RollingMean: 14.52381, 'air_eeeadee005c006a2': RollingMean: 11.952381, 'air_ef47430bcd6f6a89': RollingMean: 13.809524, 'air_ef789667e2e6fe96': RollingMean: 33.190476, 'air_ef920fa6f4b085f6': RollingMean: 37.857143, 'air_efc80d3f96b3aff7': RollingMean: 9.190476, 'air_efd70b04de878f25': RollingMean: 31.428571, 'air_efef1e3daecce07e': RollingMean: 40.666667, 'air_f068442ebb6c246c': RollingMean: 11.47619, 'air_f0c7272956e62f12': RollingMean: 3.714286, 'air_f0fb0975bdc2cdf9': RollingMean: 11.285714, 'air_f1094dbf2aef85d9': RollingMean: 6.142857, 'air_f180301886c21375': RollingMean: 14.142857, 'air_f183a514cb8ff4fa': RollingMean: 43.238095, 'air_f1f9027d4fa8f653': RollingMean: 28.714286, 'air_f267dd70a6a6b5d3': RollingMean: 57.714286, 'air_f26f36ec4dc5adb0': RollingMean: 38.809524, 'air_f2985de32bb792e0': RollingMean: 31.47619, 'air_f2c5a1f24279c531': RollingMean: 15.428571, 'air_f3602e4fa2f12993': RollingMean: 12.52381, 'air_f3f9824b7d70c3cf': RollingMean: 15.809524, 'air_f4936b91c9addbf0': RollingMean: 15.666667, 'air_f593fa60ac3541e2': RollingMean: 12.285714, 'air_f690c42545146e0a': RollingMean: 12.142857, 'air_f6b2489ccf873c3b': RollingMean: 15.857143, 'air_f6bfd27e2e174d16': RollingMean: 12.714286, 'air_f6cdaf7b7fdc6d78': RollingMean: 8.714286, 'air_f8233ad00755c35c': RollingMean: 30.333333, 'air_f85e21e543cf44f2': RollingMean: 5.666667, 'air_f88898cd09f40496': RollingMean: 7.952381, 'air_f911308e19d64236': RollingMean: 43.666667, 'air_f9168b23fdfc1e52': RollingMean: 17.47619, 'air_f927b2da69a82341': RollingMean: 10.47619, 'air_f957c6d6467d4d90': RollingMean: 10.47619, 'air_f96765e800907c77': RollingMean: 45.190476, 'air_fa12b40b02fecfd8': RollingMean: 15.52381, 'air_fa4ffc9057812fa2': RollingMean: 4.904762, 'air_fab092c35776a9b1': RollingMean: 10.809524, 'air_fb44f566d4f64a4e': RollingMean: 15.809524, 'air_fbadf737162a5ce3': RollingMean: 15.047619, 'air_fc477473134e9ae5': RollingMean: 13.333333, 'air_fcd4492c83f1c6b9': RollingMean: 21.714286, 'air_fcfbdcf7b1f82c6e': RollingMean: 37.47619, 'air_fd154088b1de6fa7': RollingMean: 6.52381, 'air_fd6aac1043520e83': RollingMean: 33.380952, 'air_fdc02ec4a3d21ea4': RollingMean: 7.619048, 'air_fdcfef8bd859f650': RollingMean: 3.52381, 'air_fe22ef5a9cbef123': RollingMean: 23.142857, 'air_fe58c074ec1445ea': RollingMean: 31.285714, 'air_fea5dc9594450608': RollingMean: 15.428571, 'air_fee8dcf4d619598e': RollingMean: 26.190476, 'air_fef9ccb3ba0da2f7': RollingMean: 8.857143, 'air_ffcc2d5087e1b476': RollingMean: 19.952381, 'air_fff68b929994bfbd': RollingMean: 4.428571}), 'how': RollingMean: 0., 'target_name': 'target'} ~['area_name', 'date', 'genre_name', 'latitude', 'longitude', 'store_id'] {'blacklist': {'area_name', 'date', 'genre_name', 'latitude', 'longitude', 'store_id'}} StandardScaler {'counts': Counter({'target_rollingmean_21_by_store_id': 252108, 'target_rollingmean_14_by_store_id': 252108, 'target_rollingmean_7_by_store_id': 252108, 'weekday': 252108, 'is_weekend': 252108}), 'means': defaultdict(<class 'float'>, {'is_weekend': 0.27469576530693085, 'target_rollingmean_14_by_store_id': 20.897453818387582, 'target_rollingmean_21_by_store_id': 20.901790561001423, 'target_rollingmean_7_by_store_id': 20.895968753375726, 'weekday': 3.0196780744759444}), 'vars': defaultdict(<class 'float'>, {'is_weekend': 0.1992380018293697, 'target_rollingmean_14_by_store_id': 139.4838567930601, 'target_rollingmean_21_by_store_id': 135.6375578980582, 'target_rollingmean_7_by_store_id': 148.74195644058028, 'weekday': 3.7017047339732057})} LinearRegression {'_weights': {'target_rollingmean_21_by_store_id': 8.267976730511927, 'target_rollingmean_14_by_store_id': 3.68602087744672, 'target_rollingmean_7_by_store_id': -0.7363408297986024, 'weekday': 6.318523886308725, 'is_weekend': -2.378332613642489}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 19.93106280511352, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 252108})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; }","title":"The art of using pipelines"},{"location":"examples/the-art-of-using-pipelines/#the-art-of-using-pipelines","text":"Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems. Both scikit-learn and pandas make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's pipeline module, however the pipe method from pandas is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations. Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not! In this notebook we'll manipulate data from the Kaggle Recruit Restaurants Visitor Forecasting competition . The data is directly available through river 's datasets module. from pprint import pprint from river import datasets for x , y in datasets . Restaurants (): pprint ( x ) pprint ( y ) break {'area_name': 'T\u014dky\u014d-to Nerima-ku Toyotamakita', 'date': datetime.datetime(2016, 1, 1, 0, 0), 'genre_name': 'Izakaya', 'is_holiday': True, 'latitude': 35.7356234, 'longitude': 139.6516577, 'store_id': 'air_04341b588bde96cd'} 10 We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model. from river import feature_extraction from river import linear_model from river import metrics from river import preprocessing from river import stats means = ( feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) scaler = preprocessing . StandardScaler () lin_reg = linear_model . LinearRegression () metric = metrics . MAE () for x , y in datasets . Restaurants (): # Derive date features x [ 'weekday' ] = x [ 'date' ] . weekday () x [ 'is_weekend' ] = x [ 'date' ] . weekday () in ( 5 , 6 ) # Process the rolling means of the target for mean in means : x = { ** x , ** mean . transform_one ( x )} mean . learn_one ( x , y ) # Remove the key/value pairs that aren't features for key in [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ]: x . pop ( key ) # Rescale the data x = scaler . learn_one ( x ) . transform_one ( x ) # Fit the linear regression y_pred = lin_reg . predict_one ( x ) lin_reg . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.465114 We're not using many features. We can print the last x to get an idea of the features (don't forget they've been scaled!) pprint ( x ) {'is_holiday': -0.23103573677646685, 'is_weekend': 1.6249280076334165, 'target_rollingmean_14_by_store_id': -1.4125913815779154, 'target_rollingmean_21_by_store_id': -1.3980979075298519, 'target_rollingmean_7_by_store_id': -1.3502314499809096, 'weekday': 1.0292832579142892} The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as river is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline \u00e0 la sklearn . from river import compose def get_date_features ( x ): weekday = x [ 'date' ] . weekday () return { 'weekday' : weekday , 'is_weekend' : weekday in ( 5 , 6 )} model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) metric = metrics . MAE () for x , y in datasets . Restaurants (): # Make a prediction without using the target y_pred = model . predict_one ( x ) # Update the model using the target model . learn_one ( x , y ) # Update the metric using the out-of-fold prediction metric . update ( y , y_pred ) print ( metric ) MAE: 8.38533 We use a Pipeline to arrange each step in a sequential order. A TransformerUnion is used to merge multiple feature extractors into a single transformer. The for loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called progressive_val_score part of the evaluate module. We can use it to replace the for loop. from river import evaluate model = compose . Pipeline ( ( 'features' , compose . TransformerUnion ( ( 'date_features' , compose . FuncTransformer ( get_date_features )), ( 'last_7_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 ))), ( 'last_14_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 ))), ( 'last_21_mean' , feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 ))) )), ( 'drop_non_features' , compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' )), ( 'scale' , preprocessing . StandardScaler ()), ( 'lin_reg' , linear_model . LinearRegression ()) ) evaluate . progressive_val_score ( dataset = datasets . Restaurants (), model = model , metric = metrics . MAE ()) MAE: 8.38533 Notice that you couldn't have used the progressive_val_score method if you wrote the model in a procedural manner. Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However river has some special tricks up it's sleeve to save you from a lot of pain. The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then river automatically infers one. model = compose . Pipeline ( compose . TransformerUnion ( compose . FuncTransformer ( get_date_features ), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )), feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Under the hood a Pipeline inherits from collections.OrderedDict . Indeed this makes sense because if you think about it a Pipeline is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a Pipeline the same way you would manipulate an ordinary dict . For instance we can print the name of each step by using the keys method. for name in model . steps : print ( name ) TransformerUnion Discard StandardScaler LinearRegression The first step is a FeatureUnion and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious. The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the + operator to merge Transformer s into a TransformerUnion . model = compose . Pipeline ( compose . FuncTransformer ( get_date_features ) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + \\ feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )), compose . Discard ( 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ), preprocessing . StandardScaler (), linear_model . LinearRegression () ) evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Likewhise we can use the | operator to assemble steps into a Pipeline . model = ( compose . FuncTransformer ( get_date_features ) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 7 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 14 )) + feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( 21 )) ) to_discard = [ 'store_id' , 'date' , 'genre_name' , 'area_name' , 'latitude' , 'longitude' ] model = model | compose . Discard ( * to_discard ) | preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a FuncTransformer , which can be quite handy. model = get_date_features for n in [ 7 , 14 , 21 ]: model += feature_extraction . TargetAgg ( by = 'store_id' , how = stats . RollingMean ( n )) model |= compose . Discard ( * to_discard ) model |= preprocessing . StandardScaler () model |= linear_model . LinearRegression () evaluate . progressive_val_score ( datasets . Restaurants (), model , metrics . MAE ()) MAE: 8.38533 Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own! Before finishing we can take an interactive look at our pipeline. model get_date_features def get_date_features(x): weekday = x['date'].weekday() return {'weekday': weekday, 'is_weekend': weekday in (5, 6)} target_rollingmean_7_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_7_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 31.571429, 'air_0164b9927d20bcc3': RollingMean: 6.428571, 'air_0241aa3964b7f861': RollingMean: 11.428571, 'air_0328696196e46f18': RollingMean: 10., 'air_034a3d5b40d5b1b1': RollingMean: 28.428571, 'air_036d4f1ee7285390': RollingMean: 27.142857, 'air_0382c794b73b51ad': RollingMean: 29.142857, 'air_03963426c9312048': RollingMean: 45.714286, 'air_04341b588bde96cd': RollingMean: 32.857143, 'air_049f6d5b402a31b2': RollingMean: 16.571429, 'air_04cae7c1bc9b2a0b': RollingMean: 18.571429, 'air_0585011fa179bcce': RollingMean: 5.571429, 'air_05c325d315cc17f5': RollingMean: 29.714286, 'air_0647f17b4dc041c8': RollingMean: 29.714286, 'air_064e203265ee5753': RollingMean: 19.142857, 'air_066f0221b8a4d533': RollingMean: 11.428571, 'air_06f95ac5c33aca10': RollingMean: 27.142857, 'air_0728814bd98f7367': RollingMean: 8.285714, 'air_0768ab3910f7967f': RollingMean: 32.571429, 'air_07b314d83059c4d2': RollingMean: 41.285714, 'air_07bb665f9cdfbdfb': RollingMean: 24.142857, 'air_082908692355165e': RollingMean: 48., 'air_083ddc520ea47e1e': RollingMean: 14.857143, 'air_0845d8395f30c6bb': RollingMean: 25.285714, 'air_084d98859256acf0': RollingMean: 12.714286, 'air_0867f7bebad6a649': RollingMean: 19.142857, 'air_08ba8cd01b3ba010': RollingMean: 10., 'air_08cb3c4ee6cd6a22': RollingMean: 10.857143, 'air_08ef81d5b7a0d13f': RollingMean: 12.857143, 'air_08f994758a1e76d4': RollingMean: 29.285714, 'air_09040f6df960ddb8': RollingMean: 17.571429, 'air_0919d54f0c9a24b8': RollingMean: 37., 'air_09661c0f3259cc04': RollingMean: 26.428571, 'air_09a845d5b5944b01': RollingMean: 7.571429, 'air_09fd1f5c58583141': RollingMean: 8.571429, 'air_0a74a5408a0b8642': RollingMean: 28.857143, 'air_0b184ec04c741a6a': RollingMean: 11.857143, 'air_0b1e72d2d4422b20': RollingMean: 20.857143, 'air_0b9038300f8b2b50': RollingMean: 13.428571, 'air_0e1eae99b8723bc1': RollingMean: 12.571429, 'air_0e7c11b9abc50163': RollingMean: 37., 'air_0ead98dd07e7a82a': RollingMean: 10.571429, 'air_0f0cdeee6c9bf3d7': RollingMean: 25.571429, 'air_0f2f96335f274801': RollingMean: 11.428571, 'air_0f60e1576a7d397d': RollingMean: 4.857143, 'air_1033310359ceeac1': RollingMean: 20.857143, 'air_10393f12e9069760': RollingMean: 13.285714, 'air_105a7954e32dba9b': RollingMean: 50.714286, 'air_10713fbf3071c361': RollingMean: 12.857143, 'air_10bbe8acd943d8f6': RollingMean: 28., 'air_12c4fb7a423df20d': RollingMean: 21.142857, 'air_138ee734ac79ff90': RollingMean: 6.285714, 'air_138ff410757b845f': RollingMean: 49.714286, 'air_1408dd53f31a8a65': RollingMean: 24.285714, 'air_142e78ba7001da9c': RollingMean: 14.142857, 'air_1509881b22965b34': RollingMean: 17., 'air_152c1f08d7d20e07': RollingMean: 12., 'air_15ae33469e9ea2dd': RollingMean: 8.285714, 'air_15e6e15c7ea2c162': RollingMean: 19.571429, 'air_16179d43b6ee5fd8': RollingMean: 8.428571, 'air_1653a6c513865af3': RollingMean: 35., 'air_168441ada3e878e1': RollingMean: 54.571429, 'air_16c4cfddeb2cf69b': RollingMean: 9.428571, 'air_16cf0a73233896de': RollingMean: 13., 'air_1707a3f18bb0da07': RollingMean: 23.142857, 'air_17a6ab40f97fd4d8': RollingMean: 6.571429, 'air_17bed6dbf7c8b0fc': RollingMean: 20.428571, 'air_1979eaff8189d086': RollingMean: 9.857143, 'air_1ab60ce33bfed8a8': RollingMean: 10.142857, 'air_1ae94f514a0bce13': RollingMean: 6.571429, 'air_1ba4e87ef7422183': RollingMean: 35.571429, 'air_1c0b150f9e696a5f': RollingMean: 100.714286, 'air_1c95a84924d72500': RollingMean: 9.285714, 'air_1d1e8860ae04f8e9': RollingMean: 15.142857, 'air_1d25ca6c76df48b4': RollingMean: 42.857143, 'air_1d3f797dd1f7cf1c': RollingMean: 39., 'air_1dd8f6f47480d1a2': RollingMean: 39.142857, 'air_1dea9815ccd36620': RollingMean: 10.285714, 'air_1e23210b584540e7': RollingMean: 3.857143, 'air_1e665503b8474c55': RollingMean: 6.142857, 'air_1eeff462acb24fb7': RollingMean: 19.571429, 'air_1f1390a8be2272b3': RollingMean: 19., 'air_1f34e9beded2231a': RollingMean: 9., 'air_1f7f8fa557bc0d55': RollingMean: 3.714286, 'air_2009041dbf9264de': RollingMean: 52.285714, 'air_20619d21192aa571': RollingMean: 13.285714, 'air_20add8092c9bb51d': RollingMean: 33.571429, 'air_2195cd5025a98033': RollingMean: 34.142857, 'air_21f5052d5330528d': RollingMean: 31.857143, 'air_220cba70c890b119': RollingMean: 8.714286, 'air_22682e965418936f': RollingMean: 10.142857, 'air_228f10bec0bda9c8': RollingMean: 17.142857, 'air_229d7e508d9f1b5e': RollingMean: 11.428571, 'air_232dcee6f7c51d37': RollingMean: 6.857143, 'air_234d3dbf7f3d5a50': RollingMean: 6.428571, 'air_23e1b11aee2a1407': RollingMean: 46.857143, 'air_23ee674e91469086': RollingMean: 21.142857, 'air_24b9b2a020826ede': RollingMean: 32.714286, 'air_24e8414b9b07decb': RollingMean: 5.857143, 'air_2545dd3a00f265e2': RollingMean: 56., 'air_256be208a979e023': RollingMean: 8., 'air_2570ccb93badde68': RollingMean: 37.428571, 'air_258ad2619d7bff9a': RollingMean: 38.428571, 'air_258dc112912fc458': RollingMean: 67.571429, 'air_25c583983246b7b0': RollingMean: 29.857143, 'air_25d8e5cc57dd87d9': RollingMean: 26.285714, 'air_25e9888d30b386df': RollingMean: 5.142857, 'air_2634e41551e9807d': RollingMean: 18.428571, 'air_26c5bbeb7bb82bf1': RollingMean: 27.142857, 'air_26f10355d9b4d82a': RollingMean: 33.285714, 'air_2703dcb33192b181': RollingMean: 54.142857, 'air_275732a5db46f4d3': RollingMean: 18.714286, 'air_27e991812b0d9c92': RollingMean: 42., 'air_28064154614b2e6c': RollingMean: 22.857143, 'air_287d2de7d3c93406': RollingMean: 11.857143, 'air_28a9fa1ec0839375': RollingMean: 34.142857, 'air_28dbe91c4c9656be': RollingMean: 33.571429, 'air_290e7a57b390f78e': RollingMean: 14.285714, 'air_298513175efdf261': RollingMean: 24.857143, 'air_2a184c1745274b2b': RollingMean: 3.428571, 'air_2a24aec099333f39': RollingMean: 8.857143, 'air_2a3743e37aab04b4': RollingMean: 17.428571, 'air_2a485b92210c98b5': RollingMean: 22.857143, 'air_2a7f14da7fe0f699': RollingMean: 25., 'air_2aab19554f91ff82': RollingMean: 43.571429, 'air_2ac361b97630e2df': RollingMean: 13.285714, 'air_2b8b29ddfd35018e': RollingMean: 8.714286, 'air_2b9bc9f5f5168ea1': RollingMean: 21., 'air_2bffb19a24d11729': RollingMean: 11., 'air_2c505f9ad67d4635': RollingMean: 16.571429, 'air_2c6c79d597e48096': RollingMean: 14.714286, 'air_2c6fef1ce0e13a5a': RollingMean: 28.714286, 'air_2c989829acbd1c6b': RollingMean: 28.142857, 'air_2cee51fa6fdf6c0d': RollingMean: 17.142857, 'air_2d3afcb91762fe01': RollingMean: 51., 'air_2d78d9a1f4dd02ca': RollingMean: 11.714286, 'air_2e7cb1f1a2a9cd6a': RollingMean: 31.428571, 'air_2f8ced25216df926': RollingMean: 12.714286, 'air_2fc149abe33adcb4': RollingMean: 38.285714, 'air_2fc478dc9f0a6b31': RollingMean: 12.857143, 'air_2fed81034f8834e5': RollingMean: 23.857143, 'air_303bac187b53083a': RollingMean: 9.571429, 'air_310e467e6e625004': RollingMean: 16.714286, 'air_3155ee23d92202da': RollingMean: 14.428571, 'air_31c753b48a657b6c': RollingMean: 21.285714, 'air_32460819c7600037': RollingMean: 46.857143, 'air_324f7c39a8410e7c': RollingMean: 12.428571, 'air_326ca454ef3558bc': RollingMean: 23.714286, 'air_32b02ba5dc2027f4': RollingMean: 29., 'air_32c61b620a766138': RollingMean: 28., 'air_32f5d7cd696e3c4a': RollingMean: 20.714286, 'air_33b01025210d6007': RollingMean: 12., 'air_3440e0ea1b70a99b': RollingMean: 30., 'air_346ade7d29230634': RollingMean: 7., 'air_347be2c4feeb408b': RollingMean: 22.571429, 'air_349278fa964bb12f': RollingMean: 19.714286, 'air_3525f11ef0bf0c35': RollingMean: 44.714286, 'air_35512c42db0868da': RollingMean: 5.142857, 'air_3561fd1c0bce6a95': RollingMean: 11.714286, 'air_35c4732dcbfe31be': RollingMean: 8.714286, 'air_36429b5ca4407b3e': RollingMean: 20., 'air_36bcf77d3382d36e': RollingMean: 31.285714, 'air_37189c92b6c761ec': RollingMean: 20.285714, 'air_375a5241615b5e22': RollingMean: 7.142857, 'air_382f5ace4e2247b8': RollingMean: 8.857143, 'air_383f5b2f8d345a49': RollingMean: 12.714286, 'air_38746ffe9aa20c7e': RollingMean: 4.571429, 'air_396166d47733d5c9': RollingMean: 31., 'air_396942e6423a2145': RollingMean: 24.428571, 'air_397d3f32a7196aa2': RollingMean: 33.142857, 'air_3980af67be35afdb': RollingMean: 20., 'air_3982a2c4ea2ed431': RollingMean: 33.428571, 'air_399904bdb7685ca0': RollingMean: 29., 'air_39dccf7df20b1c6a': RollingMean: 26.142857, 'air_3a8a3f8fb5cd7f88': RollingMean: 22.428571, 'air_3aa839e8e0cb6c87': RollingMean: 29.857143, 'air_3ac24136722e2291': RollingMean: 15.142857, 'air_3b20733899b5287f': RollingMean: 41.857143, 'air_3b6438b125086430': RollingMean: 13.857143, 'air_3bb99a1fe0583897': RollingMean: 39.428571, 'air_3bd49f98ab7f36ab': RollingMean: 15.857143, 'air_3c05c8f26c611eb9': RollingMean: 22.571429, 'air_3c938075889fc059': RollingMean: 27.285714, 'air_3cad29d1a23209d2': RollingMean: 9.142857, 'air_3caef3f76b8f26c5': RollingMean: 25.428571, 'air_3d3a2b509180e798': RollingMean: 17., 'air_3e6cea17a9d2c0f1': RollingMean: 18.714286, 'air_3e93f3c81008696d': RollingMean: 39.714286, 'air_3f91d592acd6cc0b': RollingMean: 21.714286, 'air_401b39f97e56b939': RollingMean: 11.285714, 'air_4043b7ccfbffa732': RollingMean: 47.857143, 'air_4092cfbd95a3ac1b': RollingMean: 27.714286, 'air_40953e2d8b4f2857': RollingMean: 17.428571, 'air_40f6193ea3ed1b91': RollingMean: 17.857143, 'air_414ff459ed18fa48': RollingMean: 14.285714, 'air_41bbf6e1d9814c4b': RollingMean: 7.714286, 'air_421670f21da5ba31': RollingMean: 18.428571, 'air_4254c3fc3ad078bd': RollingMean: 12.285714, 'air_42c9aa6d617c5057': RollingMean: 47., 'air_42d41eb58cad170e': RollingMean: 33.571429, 'air_43b65e4b05bff2d3': RollingMean: 19.571429, 'air_43d577e0c9460e64': RollingMean: 32., 'air_4433ab8e9999915f': RollingMean: 21.142857, 'air_4481a87c1d7c9896': RollingMean: 24.142857, 'air_452100f5305dde64': RollingMean: 7.714286, 'air_45326ebb8dc72cfb': RollingMean: 22.428571, 'air_4570f52104fe0982': RollingMean: 8.571429, 'air_4579cb0669fd411b': RollingMean: 19., 'air_457efe8c3a30ea17': RollingMean: 6.857143, 'air_464a62de0d57be1e': RollingMean: 26.428571, 'air_465bddfed3353b23': RollingMean: 30.285714, 'air_47070be6093f123e': RollingMean: 44.285714, 'air_472b19e3b5bffa41': RollingMean: 14., 'air_473cf23b9e7c0a37': RollingMean: 10.142857, 'air_473f98b212d37b4a': RollingMean: 27.714286, 'air_47beaffd3806c979': RollingMean: 18.571429, 'air_483eba479dc9910d': RollingMean: 19.285714, 'air_48e9fc98b62495a7': RollingMean: 23.142857, 'air_48f4da6223571da4': RollingMean: 21.428571, 'air_48ffd31594bc3263': RollingMean: 4., 'air_49211568cab5fdee': RollingMean: 25.285714, 'air_4974785f48853db9': RollingMean: 7.285714, 'air_4b251b9f8373f1ae': RollingMean: 25.857143, 'air_4b380b4db9d37883': RollingMean: 25.285714, 'air_4b55d8aea1d2b395': RollingMean: 36.142857, 'air_4b9085d0d46a6211': RollingMean: 21.285714, 'air_4beac252540f865e': RollingMean: 47.571429, 'air_4c2ed28f3f19ca52': RollingMean: 14.571429, 'air_4c665a2bfff0da3b': RollingMean: 8.857143, 'air_4c727b55acdee495': RollingMean: 14.285714, 'air_4cab15ad29c0ffbc': RollingMean: 19.142857, 'air_4cab91146e3d1897': RollingMean: 16., 'air_4cca5666eaf5c709': RollingMean: 37.571429, 'air_4ce7b17062a1bf73': RollingMean: 6., 'air_4d21676ed11f0bac': RollingMean: 30.714286, 'air_4d71826793c09b22': RollingMean: 20.857143, 'air_4d90a22572fa1ec9': RollingMean: 25.285714, 'air_4de6d887a7b1c1fc': RollingMean: 16.142857, 'air_4dea8d17f6f59c56': RollingMean: 27.857143, 'air_4e1c38f68f435596': RollingMean: 33.285714, 'air_4f762e840b3996e1': RollingMean: 10., 'air_4feeb8600f131e43': RollingMean: 55.714286, 'air_500641aca4cf673c': RollingMean: 18., 'air_506fe758114df773': RollingMean: 32.571429, 'air_51281cd059d7b89b': RollingMean: 16.571429, 'air_51319e7acf0438cf': RollingMean: 13.857143, 'air_52a08ef3efdb4bb0': RollingMean: 35.428571, 'air_52e2a1fd42bc917a': RollingMean: 11.142857, 'air_536043fcf1a4f8a4': RollingMean: 29.571429, 'air_539d693f7317c62d': RollingMean: 18.571429, 'air_546b353cbea4a45b': RollingMean: 14.285714, 'air_5485912b44f976de': RollingMean: 8., 'air_54d6c25d33f5260e': RollingMean: 45., 'air_54ed43163b7596c4': RollingMean: 14., 'air_55390f784018349a': RollingMean: 48.142857, 'air_55c3627912b9c849': RollingMean: 8.714286, 'air_55e11c33d4758131': RollingMean: 22.571429, 'air_56cd12f31a0afc04': RollingMean: 29.571429, 'air_56cebcbd6906e04c': RollingMean: 24.285714, 'air_56ea46c14b2dd967': RollingMean: 45.142857, 'air_57013002b912772b': RollingMean: 6.285714, 'air_573ecdf81b157d22': RollingMean: 25.857143, 'air_57c9eea1a2b66e65': RollingMean: 14.285714, 'air_57ed725a1930a5b9': RollingMean: 14., 'air_5878b6f2a9da12c1': RollingMean: 14., 'air_59cc9b2b209c6331': RollingMean: 9.285714, 'air_5a9a6cbeeb434c08': RollingMean: 23.714286, 'air_5acc13d655a6e8b2': RollingMean: 23.714286, 'air_5afb1cca48ceaa19': RollingMean: 51.571429, 'air_5b6d18c470bbfaf9': RollingMean: 39.285714, 'air_5b704df317ed1962': RollingMean: 2.142857, 'air_5bd22f9cc1426a90': RollingMean: 37.285714, 'air_5c65468938c07fa5': RollingMean: 11.428571, 'air_5c7489c9ec755e2d': RollingMean: 39.714286, 'air_5c817ef28f236bdf': RollingMean: 45.428571, 'air_5cb030b9f0b91537': RollingMean: 11.142857, 'air_5cfc537125d97f16': RollingMean: 8.428571, 'air_5d7c744c3a2ef624': RollingMean: 32.428571, 'air_5d945ade487cdf4d': RollingMean: 17., 'air_5dea8a7a5bf5eb71': RollingMean: 32.285714, 'air_5e339a1f364cdb00': RollingMean: 13.571429, 'air_5e34c6fe6fabd10e': RollingMean: 18.285714, 'air_5e70fe82f9e4fab6': RollingMean: 17.857143, 'air_5e939e005bd34633': RollingMean: 1.857143, 'air_5ed3198e4a5eed0f': RollingMean: 34.571429, 'air_5f3a3ef4cba110a4': RollingMean: 34.571429, 'air_5f6fa1b897fe80d5': RollingMean: 26., 'air_5fbda8e9302f7c13': RollingMean: 26.714286, 'air_602ca92c0db34f8f': RollingMean: 16.857143, 'air_609050e4e4f79ae1': RollingMean: 10.571429, 'air_60a7057184ec7ec7': RollingMean: 30.428571, 'air_60aa54ecbc602348': RollingMean: 5.714286, 'air_6108821ffafa9b72': RollingMean: 26., 'air_614e2f7e76dff854': RollingMean: 11.571429, 'air_61668cc2b0778898': RollingMean: 9.285714, 'air_61b8d37c33617f21': RollingMean: 28.857143, 'air_61de73b097513f58': RollingMean: 8.714286, 'air_622375b4815cf5cb': RollingMean: 44.857143, 'air_627cabe2fe53f33f': RollingMean: 14.571429, 'air_629d9935273c82ae': RollingMean: 27.142857, 'air_629edf21ea38ac2d': RollingMean: 39.142857, 'air_632ba66e1f75aa28': RollingMean: 20.142857, 'air_638c35eb25e53eea': RollingMean: 23.571429, 'air_63a750d8b4b6a976': RollingMean: 30.142857, 'air_63a88d81295195ed': RollingMean: 29.571429, 'air_63b13c56b7201bd9': RollingMean: 26.285714, 'air_63e28ee0b0c955a7': RollingMean: 25.857143, 'air_640cf4835f0d9ba3': RollingMean: 30., 'air_6411203a47b5ec77': RollingMean: 10., 'air_645cb18b33f938cf': RollingMean: 13.571429, 'air_646b93e336f0dded': RollingMean: 8.142857, 'air_64a5d5c1381837af': RollingMean: 38.428571, 'air_64d4491ad8cdb1c6': RollingMean: 14.714286, 'air_650f9b9de0c5542c': RollingMean: 23.857143, 'air_657a0748462f85de': RollingMean: 8.285714, 'air_65e294f1ae6df9c3': RollingMean: 18.857143, 'air_6607fe3671242ce3': RollingMean: 44.142857, 'air_670a0c1c4108bcea': RollingMean: 27.857143, 'air_671b4bea84dafb67': RollingMean: 26., 'air_673acd9fa5e0dd78': RollingMean: 7.142857, 'air_67483104fa38ef6c': RollingMean: 30.428571, 'air_675aa35cba456fd1': RollingMean: 43.285714, 'air_67f87c159d9e2ee2': RollingMean: 39.857143, 'air_68147db09287bf74': RollingMean: 21.285714, 'air_681b0c56328dd2af': RollingMean: 35.428571, 'air_681f96e6a6595f82': RollingMean: 35.857143, 'air_68301bcb11e2f389': RollingMean: 27.142857, 'air_683371d9baabf410': RollingMean: 31.714286, 'air_6836438b543ba698': RollingMean: 11.571429, 'air_6873982b9e19c7ad': RollingMean: 6.285714, 'air_68c1de82037d87e6': RollingMean: 25., 'air_68cc910e7b307b09': RollingMean: 9.428571, 'air_68d075113f368946': RollingMean: 23.857143, 'air_6902e4ec305b3d08': RollingMean: 38.428571, 'air_694571ea13fb9e0e': RollingMean: 29.285714, 'air_6a15e4eae523189d': RollingMean: 17.857143, 'air_6b15edd1b4fbb96a': RollingMean: 31., 'air_6b2268863b14a2af': RollingMean: 20.285714, 'air_6b65745d432fd77f': RollingMean: 23.428571, 'air_6b7678aae65d2d59': RollingMean: 9., 'air_6b942d5ebbc759c2': RollingMean: 12.857143, 'air_6b9fa44a9cf504a1': RollingMean: 4.857143, 'air_6c1128955c58b690': RollingMean: 14.285714, 'air_6c91a28278a16f64': RollingMean: 9.142857, 'air_6c952e3c6e590945': RollingMean: 15.571429, 'air_6ca1d941c8199a67': RollingMean: 28.571429, 'air_6cbe54f0aa30b615': RollingMean: 13.714286, 'air_6ced51c24fb54262': RollingMean: 9.142857, 'air_6d64dba2edd4fc0c': RollingMean: 5.142857, 'air_6d65542aa43b598b': RollingMean: 30., 'air_6d65dd11d96e00fb': RollingMean: 5.285714, 'air_6e06824d0934dd81': RollingMean: 23.285714, 'air_6e3fd96320d24324': RollingMean: 7.857143, 'air_6e64fb5821402cd2': RollingMean: 8.142857, 'air_6ff5fca957798daa': RollingMean: 7.285714, 'air_707d4b6328f2c2df': RollingMean: 28.857143, 'air_709262d948dd0b6e': RollingMean: 14.714286, 'air_70e9e8cd55879414': RollingMean: 10.857143, 'air_70f834596eb99fee': RollingMean: 21., 'air_710d6537cb7623df': RollingMean: 31.714286, 'air_712dd258f7f91b4b': RollingMean: 20.571429, 'air_71903025d39a4571': RollingMean: 15.142857, 'air_722297e7f26db91d': RollingMean: 12.285714, 'air_728ff578acc6ac6e': RollingMean: 11.857143, 'air_72f5146cf0c49beb': RollingMean: 13., 'air_735bcbe1763d6e98': RollingMean: 10.285714, 'air_73f316e6a18d8aa9': RollingMean: 23.714286, 'air_7420042ff75f9aca': RollingMean: 33.285714, 'air_746211c0b532e8aa': RollingMean: 64.142857, 'air_747f375eb3900e1e': RollingMean: 4.428571, 'air_74cf22153214064c': RollingMean: 13.714286, 'air_7514d90009613cd6': RollingMean: 75.714286, 'air_754ae581ad80cc9f': RollingMean: 10.857143, 'air_75864c80d2fb334a': RollingMean: 11.571429, 'air_75bd5d1b6dc6670d': RollingMean: 11.857143, 'air_764f71040a413d4d': RollingMean: 54.428571, 'air_77488fa378cf98c3': RollingMean: 6.857143, 'air_77dfc83450cbc89c': RollingMean: 42.714286, 'air_7831b00996701c0f': RollingMean: 23.428571, 'air_789103bf53b8096b': RollingMean: 54., 'air_789466e488705c93': RollingMean: 22.714286, 'air_78df4dc6a7e83e41': RollingMean: 16.857143, 'air_79afb3f52b4d062c': RollingMean: 9., 'air_79f528087f49df06': RollingMean: 31., 'air_7a81bd7fadcbf3d8': RollingMean: 4., 'air_7a946aada80376a4': RollingMean: 16.142857, 'air_7bacc4d36fb094c9': RollingMean: 6.571429, 'air_7bc6ca04d7b0f3b8': RollingMean: 8.428571, 'air_7bda6048a4a78837': RollingMean: 24.857143, 'air_7c7774c66fb237f7': RollingMean: 7.571429, 'air_7cc17a324ae5c7dc': RollingMean: 14.714286, 'air_7cf5a02c0e01b647': RollingMean: 33.857143, 'air_7d65049f9d275c0d': RollingMean: 11.571429, 'air_7dacea2f22afccfb': RollingMean: 38.142857, 'air_7db266904cb0d72a': RollingMean: 13.571429, 'air_7e12c5d27f44a8de': RollingMean: 25.571429, 'air_7ef9a5ea5c8fe39f': RollingMean: 10.857143, 'air_7f3dc18494bce98b': RollingMean: 15.428571, 'air_7f9e15afafcf4c75': RollingMean: 41.857143, 'air_7fbf7649eb13ad9b': RollingMean: 19.571429, 'air_800c02226e2e0288': RollingMean: 15.285714, 'air_8093d0b565e9dbdf': RollingMean: 39.142857, 'air_8110d68cc869b85e': RollingMean: 51.857143, 'air_81546875de9c8e78': RollingMean: 5., 'air_81a12d67c22e012f': RollingMean: 19.714286, 'air_81bd68142db76f58': RollingMean: 17.714286, 'air_81c2600146d07d16': RollingMean: 6.142857, 'air_81c5dff692063446': RollingMean: 14.857143, 'air_820d1919cbecaa0a': RollingMean: 32.714286, 'air_82a6ae14151953ba': RollingMean: 41.714286, 'air_831658500aa7c846': RollingMean: 31.571429, 'air_832f9dbe9ee4ebd3': RollingMean: 12.428571, 'air_83db5aff8f50478e': RollingMean: 6., 'air_84060403939d8216': RollingMean: 15.571429, 'air_848616680ef061bd': RollingMean: 29.571429, 'air_84f6876ff7e83ae7': RollingMean: 18.428571, 'air_8523d6a70de49e6c': RollingMean: 31.571429, 'air_859feab8e3c9f98d': RollingMean: 25.285714, 'air_85bd13a49370c392': RollingMean: 12.571429, 'air_86cfbf2624576fad': RollingMean: 7.571429, 'air_86f7b2109e4abd65': RollingMean: 47.571429, 'air_87059630ab6fe47f': RollingMean: 4.285714, 'air_87078cf7903a648c': RollingMean: 6.285714, 'air_87467487d21891dd': RollingMean: 10.142857, 'air_8764b3473ddcceaf': RollingMean: 4.857143, 'air_876d7a23c47811cb': RollingMean: 16.571429, 'air_877f79706adbfb06': RollingMean: 10.285714, 'air_87ca98aa7664de94': RollingMean: 13.714286, 'air_87f9e1024b951f01': RollingMean: 12.142857, 'air_883ca28ef0ed3d55': RollingMean: 13.857143, 'air_88c8e34baa79217b': RollingMean: 35.571429, 'air_88ca84051ba95339': RollingMean: 16.428571, 'air_88f31db64991768a': RollingMean: 8.428571, 'air_890d7e28e8eaaa11': RollingMean: 9.714286, 'air_89e7328af22efe74': RollingMean: 41., 'air_8a1d21fad48374e8': RollingMean: 13.857143, 'air_8a59bb0c497b771e': RollingMean: 23.857143, 'air_8a906e5801eac81c': RollingMean: 21.857143, 'air_8b4a46dc521bfcfe': RollingMean: 31.714286, 'air_8c119d1f16049f20': RollingMean: 29.857143, 'air_8c3175aa5e4fc569': RollingMean: 131.714286, 'air_8cc350fd70ee0757': RollingMean: 37.285714, 'air_8ce035ee1d8a56a6': RollingMean: 19.571429, 'air_8d50c64692322dff': RollingMean: 12.571429, 'air_8d61f49aa0373492': RollingMean: 49.714286, 'air_8e429650fcf7a0ae': RollingMean: 19.428571, 'air_8e4360a64dbd4c50': RollingMean: 20., 'air_8e492076a1179383': RollingMean: 43.571429, 'air_8e8f42f047537154': RollingMean: 28.285714, 'air_8ec47c0f1e2c879e': RollingMean: 32., 'air_8f13ef0f5e8c64dd': RollingMean: 5., 'air_8f273fb9ad2fed6f': RollingMean: 12.857143, 'air_8f3b563416efc6ad': RollingMean: 15., 'air_900d755ebd2f7bbd': RollingMean: 97.285714, 'air_901925b628677c2e': RollingMean: 8.571429, 'air_90213bcae4afa274': RollingMean: 29.571429, 'air_90bd5de52c166767': RollingMean: 21.714286, 'air_90ed0a2f24755533': RollingMean: 50., 'air_90f0efbb702d77b7': RollingMean: 30.857143, 'air_9105a29b0eb250d2': RollingMean: 16.857143, 'air_91236b89d29567af': RollingMean: 27.428571, 'air_9152d9926e5c4a3a': RollingMean: 31.142857, 'air_915558a55c2bc56c': RollingMean: 17.142857, 'air_91beafbba9382b0a': RollingMean: 36.428571, 'air_91d72e16c4bcba55': RollingMean: 15.714286, 'air_9241121af22ff1d6': RollingMean: 27.285714, 'air_929d8513e3cdb423': RollingMean: 8.285714, 'air_931a8a4321b6e7d1': RollingMean: 4., 'air_9352c401d5adb01b': RollingMean: 25.571429, 'air_9387ff95e886ebc7': RollingMean: 11.857143, 'air_938ef91ecdde6878': RollingMean: 27., 'air_939964477035ef0b': RollingMean: 16.857143, 'air_93b9bb641f8fc982': RollingMean: 24.857143, 'air_93dd7070c9bf5453': RollingMean: 29.142857, 'air_93ebe490d4abb8e9': RollingMean: 21.285714, 'air_9438d67241c81314': RollingMean: 31.714286, 'air_947eb2cae4f3e8f2': RollingMean: 36.714286, 'air_9483d65e9cc9a6b7': RollingMean: 15.428571, 'air_950381108f839348': RollingMean: 32.142857, 'air_95d28905941fd4cb': RollingMean: 36.428571, 'air_95e917913cd988f3': RollingMean: 25.714286, 'air_96005f79124e12bf': RollingMean: 42.714286, 'air_965b2e0cf4119003': RollingMean: 40., 'air_96743eee94114261': RollingMean: 15.571429, 'air_96773a6236d279b1': RollingMean: 25.428571, 'air_968d72c12eed09f0': RollingMean: 17.285714, 'air_96929a799b12a43e': RollingMean: 27.428571, 'air_96ec3cfe78cb0652': RollingMean: 21.285714, 'air_97159fc4e90053fe': RollingMean: 24.142857, 'air_97958e7fce98b6a3': RollingMean: 17.857143, 'air_97b2a9f975fc702c': RollingMean: 40.428571, 'air_97cf68dc1a9beac0': RollingMean: 14.571429, 'air_97e0f2feec4d577a': RollingMean: 18.714286, 'air_9828505fefc77d75': RollingMean: 12.714286, 'air_98b54e32ccddd896': RollingMean: 17.428571, 'air_990a642a3deb2903': RollingMean: 33.428571, 'air_99157b6163835eec': RollingMean: 34.428571, 'air_99a5183695b849f9': RollingMean: 25.857143, 'air_99b01136f451fc0e': RollingMean: 43.857143, 'air_99c3eae84130c1cb': RollingMean: 41.857143, 'air_9a30407764f4ff84': RollingMean: 20., 'air_9a6f6e7f623003d2': RollingMean: 2.857143, 'air_9aa32b3db0fab3a5': RollingMean: 15.857143, 'air_9aa92007e3628dbc': RollingMean: 36.571429, 'air_9ae7081cb77dc51e': RollingMean: 32.857143, 'air_9b13c7feb0a0c431': RollingMean: 12.714286, 'air_9b6af3db40da4ae2': RollingMean: 29., 'air_9bbc673495e23532': RollingMean: 4.571429, 'air_9bf0ccac497c4af3': RollingMean: 47.285714, 'air_9bf595ef095572fb': RollingMean: 28.142857, 'air_9c6787aa03a45586': RollingMean: 86., 'air_9ca2767761efff4d': RollingMean: 9.142857, 'air_9cd5e310f488bced': RollingMean: 11.571429, 'air_9cf2f1ba86229773': RollingMean: 34.285714, 'air_9d3482b4680aee88': RollingMean: 9.571429, 'air_9d452a881f7f2bb7': RollingMean: 9.285714, 'air_9d474ec2448c700d': RollingMean: 11.857143, 'air_9d5a980b211e1795': RollingMean: 11.285714, 'air_9d93d95720f2e831': RollingMean: 8.857143, 'air_9dc9483f717d73ee': RollingMean: 4.714286, 'air_9dd7d38b0f1760c4': RollingMean: 3., 'air_9e920b758503ef54': RollingMean: 7.285714, 'air_9efaa7ded03c5a71': RollingMean: 13.857143, 'air_9f277fb7a2c1d842': RollingMean: 9.857143, 'air_9fc607777ad76b26': RollingMean: 16.285714, 'air_a083834e7ffe187e': RollingMean: 20.857143, 'air_a11473cc1eb9a27f': RollingMean: 31.714286, 'air_a17f0778617c76e2': RollingMean: 30.285714, 'air_a1fe8c588c8d2f30': RollingMean: 17.285714, 'air_a218912784bf767d': RollingMean: 13., 'air_a21ffca0bea1661a': RollingMean: 1.142857, 'air_a239a44805932bab': RollingMean: 35.142857, 'air_a24bf50c3e90d583': RollingMean: 18.285714, 'air_a2567267116a3b75': RollingMean: 16.285714, 'air_a257c9749d8d0ff6': RollingMean: 18.142857, 'air_a271c9ba19e81d17': RollingMean: 29.142857, 'air_a2b29aa7feb4e36f': RollingMean: 21.571429, 'air_a304330715435390': RollingMean: 8.142857, 'air_a33461f4392ec62c': RollingMean: 30.142857, 'air_a373500730e2a9e0': RollingMean: 10.142857, 'air_a38f25e3399d1b25': RollingMean: 41.142857, 'air_a41b032371a63427': RollingMean: 9.428571, 'air_a49f1cf0634f13e5': RollingMean: 16.857143, 'air_a510dcfe979f09eb': RollingMean: 12.714286, 'air_a546cbf478a8b6e4': RollingMean: 28.714286, 'air_a55d17bd3f3033cb': RollingMean: 12.142857, 'air_a563896da3777078': RollingMean: 23.142857, 'air_a678e5b144ca24ce': RollingMean: 18.571429, 'air_a7404a854919e990': RollingMean: 8.571429, 'air_a8533b7a992bb0ca': RollingMean: 19.285714, 'air_a85f0c0c889f6b7e': RollingMean: 50.571429, 'air_a85f8c0bfd61889f': RollingMean: 17.142857, 'air_a88ac559064dec08': RollingMean: 28., 'air_a9133955abccf071': RollingMean: 27., 'air_a9178f19da58fe99': RollingMean: 7.714286, 'air_a9a380530c1e121f': RollingMean: 45.285714, 'air_aa0049fe3cc6f4d6': RollingMean: 11.142857, 'air_ab3ae0e410b20069': RollingMean: 16.714286, 'air_ab9746a0f83084b7': RollingMean: 1.142857, 'air_abcdc8115988a010': RollingMean: 9.857143, 'air_abf06fcca748dca5': RollingMean: 9., 'air_ac7a7427c9ae12a5': RollingMean: 63.857143, 'air_ad13e71e21235131': RollingMean: 10.285714, 'air_ad60f6b76c9df7ed': RollingMean: 30.571429, 'air_ad7777590c884721': RollingMean: 9.428571, 'air_add9a575623726c8': RollingMean: 47.571429, 'air_ade6e836ffd1da64': RollingMean: 11.571429, 'air_aed3a8b49abe4a48': RollingMean: 5., 'air_af03c277a167b2bd': RollingMean: 36.285714, 'air_af24e3e817dea1e5': RollingMean: 15.285714, 'air_af63df35857b16e6': RollingMean: 25.714286, 'air_b0a6a4c5e95c74cf': RollingMean: 19.571429, 'air_b162fb07fbbdea33': RollingMean: 15.285714, 'air_b192fb5310436005': RollingMean: 8., 'air_b1a72bf1ebf4b8ef': RollingMean: 46.285714, 'air_b1bb1fae86617d7a': RollingMean: 36.428571, 'air_b1d822f75c9fc615': RollingMean: 10.714286, 'air_b2395df0e874078d': RollingMean: 6.571429, 'air_b23d0f519291247d': RollingMean: 34., 'air_b259b4e4a51a690d': RollingMean: 18.142857, 'air_b28bed4b2e7167b7': RollingMean: 19.142857, 'air_b2a639cc7e02edf1': RollingMean: 20.142857, 'air_b2d8bc9c88b85f96': RollingMean: 8.142857, 'air_b2d97bd2337c5ba7': RollingMean: 36.142857, 'air_b2dcec37b83e2494': RollingMean: 7.857143, 'air_b30fffd7ab1e75a5': RollingMean: 9.428571, 'air_b3180b74332ba886': RollingMean: 14.285714, 'air_b3a824511477a4ed': RollingMean: 6.285714, 'air_b439391e72899756': RollingMean: 20.142857, 'air_b45b8e456f53942a': RollingMean: 9.285714, 'air_b4f32bcc399da2b9': RollingMean: 20.428571, 'air_b5598d12d1b84890': RollingMean: 5.428571, 'air_b5bdd318005d9aa4': RollingMean: 39.428571, 'air_b60cc7d6aee68194': RollingMean: 13.285714, 'air_b711b43ae472cb6b': RollingMean: 22.571429, 'air_b7fa3d2fca744dd2': RollingMean: 41.428571, 'air_b80fed1a07c817d2': RollingMean: 4.142857, 'air_b88192b35ac03c24': RollingMean: 21., 'air_b8925441167c3152': RollingMean: 1.714286, 'air_b8a5ee69e5fdcc5b': RollingMean: 31.571429, 'air_b8d9e1624baaadc2': RollingMean: 6.857143, 'air_b9e27558fb8bd5c4': RollingMean: 13.714286, 'air_ba495cccc8f0f237': RollingMean: 18.714286, 'air_ba937bf13d40fb24': RollingMean: 15.714286, 'air_bac5f4441db21db9': RollingMean: 45.857143, 'air_baf28ac9f13a307d': RollingMean: 14.571429, 'air_bb09595bab7d5cfb': RollingMean: 33.857143, 'air_bb26d6d079594414': RollingMean: 11.428571, 'air_bb4ff06cd661ee9b': RollingMean: 33.285714, 'air_bbe1c1a47e09f161': RollingMean: 1.428571, 'air_bc991c51d6613745': RollingMean: 19.714286, 'air_bc9a129e11a2efe0': RollingMean: 30.857143, 'air_bcce1ea4350b7b72': RollingMean: 26., 'air_bd74a9222edfdfe1': RollingMean: 13.428571, 'air_bdd32aa407c16335': RollingMean: 16.714286, 'air_bebd55ed63ab2422': RollingMean: 10.857143, 'air_bed603c423b7d9d4': RollingMean: 5.857143, 'air_bedd35489e666605': RollingMean: 35.857143, 'air_bf13014b6e3e60ca': RollingMean: 31.857143, 'air_bf21b8350771879b': RollingMean: 22.142857, 'air_bf617aa68d5f1cfa': RollingMean: 7.571429, 'air_bf7591560077332d': RollingMean: 9.142857, 'air_bfafaed35e213fd7': RollingMean: 14.714286, 'air_bfda7731a6c6fc61': RollingMean: 20.285714, 'air_c027e2b560442808': RollingMean: 17.285714, 'air_c0385db498b391e5': RollingMean: 34.285714, 'air_c1d5d165c055b866': RollingMean: 30.142857, 'air_c1ff20617c54fee7': RollingMean: 8.142857, 'air_c225148c0fcc5c72': RollingMean: 39.428571, 'air_c2626f5f86d57342': RollingMean: 16.142857, 'air_c26f027b5bc1f081': RollingMean: 5.857143, 'air_c28983412a7eefcf': RollingMean: 40.571429, 'air_c2c8435bdb3516d4': RollingMean: 35., 'air_c31472d14e29cee8': RollingMean: 16.714286, 'air_c3585b0fba3998d0': RollingMean: 9.285714, 'air_c3bc011cca3bec65': RollingMean: 4.285714, 'air_c3dcaf3aeb18e20e': RollingMean: 15.714286, 'air_c47aa7493b15f297': RollingMean: 23., 'air_c4fa5c562d5409ca': RollingMean: 15.857143, 'air_c52c63c781fe48f6': RollingMean: 29.428571, 'air_c5459218282bedd5': RollingMean: 21.714286, 'air_c66dbd2c37832d00': RollingMean: 16.714286, 'air_c6a164dd4060e960': RollingMean: 15.857143, 'air_c6aa2efba0ffc8eb': RollingMean: 26.428571, 'air_c6ffd6a93e6b68d6': RollingMean: 15.714286, 'air_c73d319ffabf287a': RollingMean: 18.428571, 'air_c759b6abeb552160': RollingMean: 7.142857, 'air_c77ee2b7d36da265': RollingMean: 42., 'air_c7d30ab0e07f31d5': RollingMean: 20.285714, 'air_c7f78b4f3cba33ff': RollingMean: 25.142857, 'air_c8265ecc116f2284': RollingMean: 9.142857, 'air_c88467d88b2c8ecd': RollingMean: 19.428571, 'air_c8a657c8c5c93d69': RollingMean: 9.142857, 'air_c8c0ef02ed72053f': RollingMean: 27.428571, 'air_c8fe396d6c46275d': RollingMean: 18., 'air_c92745dfdd2ec68a': RollingMean: 21.857143, 'air_c9ed65554b6edffb': RollingMean: 12.714286, 'air_c9f6de13be8b8f25': RollingMean: 3.571429, 'air_ca1315af9e073bd1': RollingMean: 43.714286, 'air_ca6ae8d49a2f1eaf': RollingMean: 24.142857, 'air_ca957d3a1529fbd3': RollingMean: 31.285714, 'air_cadf9cfb510a1d78': RollingMean: 29.714286, 'air_caf996ac27206301': RollingMean: 4.714286, 'air_cb083b4789a8d3a2': RollingMean: 18.714286, 'air_cb25551c4cd8d9f3': RollingMean: 5.714286, 'air_cb7467aed805e7fe': RollingMean: 40.428571, 'air_cb935ff8610ba3d3': RollingMean: 4.142857, 'air_cbe139af83feb388': RollingMean: 10.428571, 'air_cbe867adcf44e14f': RollingMean: 15.714286, 'air_cc1a0e985ce63711': RollingMean: 27.285714, 'air_cc35590cd1da8554': RollingMean: 24.571429, 'air_ccd19a5bc5573ae5': RollingMean: 34.857143, 'air_cd4b301d5d3918d8': RollingMean: 5.857143, 'air_cd5f54969be9ed08': RollingMean: 6.285714, 'air_ced6297e5bdf5130': RollingMean: 23.428571, 'air_cf2229e64408d9fe': RollingMean: 20.428571, 'air_cf22e368c1a71d53': RollingMean: 34.857143, 'air_cf5ab75a0afb8af9': RollingMean: 48., 'air_cfcc94797d2b5d3d': RollingMean: 18.285714, 'air_cfdeb326418194ff': RollingMean: 17.285714, 'air_d00161e19f08290b': RollingMean: 28.714286, 'air_d00a15343325e5f7': RollingMean: 21.142857, 'air_d07e57b21109304a': RollingMean: 11.428571, 'air_d0a1e69685259c92': RollingMean: 32., 'air_d0a7bd3339c3d12a': RollingMean: 46.714286, 'air_d0e8a085d8dc83aa': RollingMean: 6.571429, 'air_d138b593ebda55cc': RollingMean: 6., 'air_d1418d6fd6d634f2': RollingMean: 17.714286, 'air_d186b2cb0b9ce022': RollingMean: 14.428571, 'air_d1f20424f76cc78e': RollingMean: 19.857143, 'air_d34c0861a2be94cb': RollingMean: 41.285714, 'air_d3e7b5952cd09ccb': RollingMean: 17.857143, 'air_d44d210d2994f01b': RollingMean: 7., 'air_d473620754bf9fc2': RollingMean: 11., 'air_d477b6339b8ce69f': RollingMean: 11., 'air_d4981cdde163b172': RollingMean: 27.142857, 'air_d4b5a4b04c5f2d04': RollingMean: 15.142857, 'air_d4d218b451f82c3d': RollingMean: 9.428571, 'air_d500b48a8735fbd3': RollingMean: 20.142857, 'air_d54d6fcb116fbed3': RollingMean: 4.714286, 'air_d5e0a20370c325c7': RollingMean: 33.285714, 'air_d63cfa6d6ab78446': RollingMean: 20.857143, 'air_d69b08a175bc0387': RollingMean: 13.571429, 'air_d6b3e67261f07646': RollingMean: 11.714286, 'air_d8abb9e490abf94f': RollingMean: 12.714286, 'air_d97dabf7aae60da5': RollingMean: 34.714286, 'air_d98380a4aeb0290b': RollingMean: 44.285714, 'air_daa7947e1c47f5ed': RollingMean: 31.142857, 'air_dabfbd0ec951925a': RollingMean: 8.142857, 'air_dad0b6a36138f309': RollingMean: 5.142857, 'air_db1233ad855b34d5': RollingMean: 25.857143, 'air_db4b38ebe7a7ceff': RollingMean: 19.714286, 'air_db80363d35f10926': RollingMean: 36.142857, 'air_dbf64f1ce38c7442': RollingMean: 14., 'air_dc0e080ba0a5e5af': RollingMean: 9.428571, 'air_dc71c6cc06cd1aa2': RollingMean: 6.857143, 'air_de692863bb2dd758': RollingMean: 22., 'air_de803f7e324936b8': RollingMean: 24.571429, 'air_de88770300008624': RollingMean: 21.857143, 'air_dea0655f96947922': RollingMean: 37., 'air_df507aec929ce5f6': RollingMean: 24.857143, 'air_df554c4527a1cfe6': RollingMean: 57.285714, 'air_df5cf5cd03eb68d0': RollingMean: 7.285714, 'air_df843e6b22e8d540': RollingMean: 12.285714, 'air_df9355c47c5df9d3': RollingMean: 32.428571, 'air_dfad598ff642dab7': RollingMean: 28.285714, 'air_dfe068a1bf85f395': RollingMean: 41.142857, 'air_e00fe7853c0100d6': RollingMean: 18.142857, 'air_e0118664da63a2d0': RollingMean: 15.428571, 'air_e01d99390355408d': RollingMean: 10.571429, 'air_e053c561f32acc28': RollingMean: 16.428571, 'air_e08b9cf82057a170': RollingMean: 34., 'air_e0aee25b56a069f2': RollingMean: 14.857143, 'air_e0e69668214ff972': RollingMean: 11.142857, 'air_e0f241bd406810c0': RollingMean: 34.714286, 'air_e1b76fcb5208fb6b': RollingMean: 18.714286, 'air_e2208a79e2678432': RollingMean: 47.428571, 'air_e270aff84ac7e4c8': RollingMean: 22.714286, 'air_e3020992d5fe5dfd': RollingMean: 14.285714, 'air_e34c631c766f2766': RollingMean: 24.857143, 'air_e42bdc3377d1eee7': RollingMean: 23.428571, 'air_e483f5b3c4f310e0': RollingMean: 6., 'air_e524c6a9e06cc3a1': RollingMean: 9., 'air_e55abd740f93ecc4': RollingMean: 45.285714, 'air_e57dd6884595f60d': RollingMean: 37.714286, 'air_e58f669b6f1a08ce': RollingMean: 12.142857, 'air_e5cf003abcc5febb': RollingMean: 18.285714, 'air_e64de0a6bf0739af': RollingMean: 38.285714, 'air_e657ca554b0c008c': RollingMean: 28.285714, 'air_e700e390226d9985': RollingMean: 18.714286, 'air_e76a668009c5dabc': RollingMean: 9.285714, 'air_e7d2ac6d53d1b744': RollingMean: 11.714286, 'air_e7fbee4e3cfe65c5': RollingMean: 39., 'air_e88bbe2ede3467aa': RollingMean: 21.285714, 'air_e89735e80d614a7e': RollingMean: 31.428571, 'air_e8ed9335d0c38333': RollingMean: 30.142857, 'air_e9ebf7fc520ac76a': RollingMean: 29.142857, 'air_ea6d0c3acf00b22a': RollingMean: 25.571429, 'air_ea7c16131980c837': RollingMean: 7., 'air_eb120e6d384a17a8': RollingMean: 48.428571, 'air_eb20a89bba7dd3d0': RollingMean: 3.142857, 'air_eb2d2653586315dd': RollingMean: 35.285714, 'air_eb5788dba285e725': RollingMean: 31.142857, 'air_ebd31e812960f517': RollingMean: 30.571429, 'air_ebe02c3090271fa9': RollingMean: 11.285714, 'air_ec0fad2def4dcff0': RollingMean: 16., 'air_eca4a5a191e8d993': RollingMean: 32.571429, 'air_eca5e0064dc9314a': RollingMean: 33.714286, 'air_ecab54b57a71b10d': RollingMean: 13., 'air_eceb97ad6a7d4c07': RollingMean: 30.714286, 'air_ecf7f141339f1d57': RollingMean: 20.142857, 'air_eda179770dfa9f91': RollingMean: 10.428571, 'air_edd5e3d696a5811b': RollingMean: 38.142857, 'air_ee3a01f0c71a769f': RollingMean: 28.142857, 'air_ee3ba9af184c6c82': RollingMean: 21.285714, 'air_eec5e572b9eb9c23': RollingMean: 15.571429, 'air_eeeadee005c006a2': RollingMean: 13.714286, 'air_ef47430bcd6f6a89': RollingMean: 13.857143, 'air_ef789667e2e6fe96': RollingMean: 36.428571, 'air_ef920fa6f4b085f6': RollingMean: 32.142857, 'air_efc80d3f96b3aff7': RollingMean: 8.428571, 'air_efd70b04de878f25': RollingMean: 30., 'air_efef1e3daecce07e': RollingMean: 43.428571, 'air_f068442ebb6c246c': RollingMean: 11.571429, 'air_f0c7272956e62f12': RollingMean: 5.428571, 'air_f0fb0975bdc2cdf9': RollingMean: 10.142857, 'air_f1094dbf2aef85d9': RollingMean: 7.142857, 'air_f180301886c21375': RollingMean: 18.142857, 'air_f183a514cb8ff4fa': RollingMean: 38., 'air_f1f9027d4fa8f653': RollingMean: 22., 'air_f267dd70a6a6b5d3': RollingMean: 31.857143, 'air_f26f36ec4dc5adb0': RollingMean: 39.857143, 'air_f2985de32bb792e0': RollingMean: 24.714286, 'air_f2c5a1f24279c531': RollingMean: 18.571429, 'air_f3602e4fa2f12993': RollingMean: 11., 'air_f3f9824b7d70c3cf': RollingMean: 15.714286, 'air_f4936b91c9addbf0': RollingMean: 16.142857, 'air_f593fa60ac3541e2': RollingMean: 8., 'air_f690c42545146e0a': RollingMean: 11.714286, 'air_f6b2489ccf873c3b': RollingMean: 16.285714, 'air_f6bfd27e2e174d16': RollingMean: 14.142857, 'air_f6cdaf7b7fdc6d78': RollingMean: 12.285714, 'air_f8233ad00755c35c': RollingMean: 28.714286, 'air_f85e21e543cf44f2': RollingMean: 5.714286, 'air_f88898cd09f40496': RollingMean: 7.714286, 'air_f911308e19d64236': RollingMean: 47., 'air_f9168b23fdfc1e52': RollingMean: 14.142857, 'air_f927b2da69a82341': RollingMean: 9.428571, 'air_f957c6d6467d4d90': RollingMean: 9.285714, 'air_f96765e800907c77': RollingMean: 38.428571, 'air_fa12b40b02fecfd8': RollingMean: 14.571429, 'air_fa4ffc9057812fa2': RollingMean: 6., 'air_fab092c35776a9b1': RollingMean: 10.571429, 'air_fb44f566d4f64a4e': RollingMean: 14.857143, 'air_fbadf737162a5ce3': RollingMean: 15., 'air_fc477473134e9ae5': RollingMean: 15.714286, 'air_fcd4492c83f1c6b9': RollingMean: 24.142857, 'air_fcfbdcf7b1f82c6e': RollingMean: 39.857143, 'air_fd154088b1de6fa7': RollingMean: 4.428571, 'air_fd6aac1043520e83': RollingMean: 39.428571, 'air_fdc02ec4a3d21ea4': RollingMean: 8.142857, 'air_fdcfef8bd859f650': RollingMean: 3.714286, 'air_fe22ef5a9cbef123': RollingMean: 24.428571, 'air_fe58c074ec1445ea': RollingMean: 35., 'air_fea5dc9594450608': RollingMean: 17.428571, 'air_fee8dcf4d619598e': RollingMean: 29.571429, 'air_fef9ccb3ba0da2f7': RollingMean: 8.714286, 'air_ffcc2d5087e1b476': RollingMean: 24.714286, 'air_fff68b929994bfbd': RollingMean: 4.142857}), 'how': RollingMean: 0., 'target_name': 'target'} target_rollingmean_14_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_14_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 28.428571, 'air_0164b9927d20bcc3': RollingMean: 6.571429, 'air_0241aa3964b7f861': RollingMean: 9.928571, 'air_0328696196e46f18': RollingMean: 8.142857, 'air_034a3d5b40d5b1b1': RollingMean: 24.285714, 'air_036d4f1ee7285390': RollingMean: 22.714286, 'air_0382c794b73b51ad': RollingMean: 26.571429, 'air_03963426c9312048': RollingMean: 47.142857, 'air_04341b588bde96cd': RollingMean: 32.5, 'air_049f6d5b402a31b2': RollingMean: 13.571429, 'air_04cae7c1bc9b2a0b': RollingMean: 20., 'air_0585011fa179bcce': RollingMean: 4.785714, 'air_05c325d315cc17f5': RollingMean: 28.142857, 'air_0647f17b4dc041c8': RollingMean: 30.928571, 'air_064e203265ee5753': RollingMean: 18.714286, 'air_066f0221b8a4d533': RollingMean: 11.142857, 'air_06f95ac5c33aca10': RollingMean: 29.714286, 'air_0728814bd98f7367': RollingMean: 7.785714, 'air_0768ab3910f7967f': RollingMean: 28.214286, 'air_07b314d83059c4d2': RollingMean: 38.571429, 'air_07bb665f9cdfbdfb': RollingMean: 25.785714, 'air_082908692355165e': RollingMean: 46.071429, 'air_083ddc520ea47e1e': RollingMean: 12.642857, 'air_0845d8395f30c6bb': RollingMean: 22.714286, 'air_084d98859256acf0': RollingMean: 16.142857, 'air_0867f7bebad6a649': RollingMean: 21., 'air_08ba8cd01b3ba010': RollingMean: 10.285714, 'air_08cb3c4ee6cd6a22': RollingMean: 12.642857, 'air_08ef81d5b7a0d13f': RollingMean: 14.5, 'air_08f994758a1e76d4': RollingMean: 28., 'air_09040f6df960ddb8': RollingMean: 15.571429, 'air_0919d54f0c9a24b8': RollingMean: 35.857143, 'air_09661c0f3259cc04': RollingMean: 28.857143, 'air_09a845d5b5944b01': RollingMean: 6.928571, 'air_09fd1f5c58583141': RollingMean: 8.642857, 'air_0a74a5408a0b8642': RollingMean: 28.428571, 'air_0b184ec04c741a6a': RollingMean: 12.857143, 'air_0b1e72d2d4422b20': RollingMean: 20.928571, 'air_0b9038300f8b2b50': RollingMean: 11.428571, 'air_0e1eae99b8723bc1': RollingMean: 10.642857, 'air_0e7c11b9abc50163': RollingMean: 35.071429, 'air_0ead98dd07e7a82a': RollingMean: 13.571429, 'air_0f0cdeee6c9bf3d7': RollingMean: 22.071429, 'air_0f2f96335f274801': RollingMean: 9.928571, 'air_0f60e1576a7d397d': RollingMean: 6.214286, 'air_1033310359ceeac1': RollingMean: 18.642857, 'air_10393f12e9069760': RollingMean: 13.571429, 'air_105a7954e32dba9b': RollingMean: 51.5, 'air_10713fbf3071c361': RollingMean: 12.285714, 'air_10bbe8acd943d8f6': RollingMean: 30.857143, 'air_12c4fb7a423df20d': RollingMean: 20.071429, 'air_138ee734ac79ff90': RollingMean: 7.214286, 'air_138ff410757b845f': RollingMean: 54.428571, 'air_1408dd53f31a8a65': RollingMean: 27.857143, 'air_142e78ba7001da9c': RollingMean: 13.642857, 'air_1509881b22965b34': RollingMean: 19., 'air_152c1f08d7d20e07': RollingMean: 11.857143, 'air_15ae33469e9ea2dd': RollingMean: 10.285714, 'air_15e6e15c7ea2c162': RollingMean: 18.642857, 'air_16179d43b6ee5fd8': RollingMean: 7.714286, 'air_1653a6c513865af3': RollingMean: 32.5, 'air_168441ada3e878e1': RollingMean: 45.714286, 'air_16c4cfddeb2cf69b': RollingMean: 12.071429, 'air_16cf0a73233896de': RollingMean: 11.428571, 'air_1707a3f18bb0da07': RollingMean: 26.214286, 'air_17a6ab40f97fd4d8': RollingMean: 8.285714, 'air_17bed6dbf7c8b0fc': RollingMean: 19.785714, 'air_1979eaff8189d086': RollingMean: 9.071429, 'air_1ab60ce33bfed8a8': RollingMean: 9.642857, 'air_1ae94f514a0bce13': RollingMean: 7., 'air_1ba4e87ef7422183': RollingMean: 38.714286, 'air_1c0b150f9e696a5f': RollingMean: 100.785714, 'air_1c95a84924d72500': RollingMean: 8.714286, 'air_1d1e8860ae04f8e9': RollingMean: 15.071429, 'air_1d25ca6c76df48b4': RollingMean: 42.214286, 'air_1d3f797dd1f7cf1c': RollingMean: 33.571429, 'air_1dd8f6f47480d1a2': RollingMean: 37.428571, 'air_1dea9815ccd36620': RollingMean: 10.5, 'air_1e23210b584540e7': RollingMean: 3.857143, 'air_1e665503b8474c55': RollingMean: 5.785714, 'air_1eeff462acb24fb7': RollingMean: 16.428571, 'air_1f1390a8be2272b3': RollingMean: 18.214286, 'air_1f34e9beded2231a': RollingMean: 8.214286, 'air_1f7f8fa557bc0d55': RollingMean: 3.928571, 'air_2009041dbf9264de': RollingMean: 51.285714, 'air_20619d21192aa571': RollingMean: 16.785714, 'air_20add8092c9bb51d': RollingMean: 31.642857, 'air_2195cd5025a98033': RollingMean: 29.428571, 'air_21f5052d5330528d': RollingMean: 30.857143, 'air_220cba70c890b119': RollingMean: 7.928571, 'air_22682e965418936f': RollingMean: 10., 'air_228f10bec0bda9c8': RollingMean: 16.785714, 'air_229d7e508d9f1b5e': RollingMean: 9.571429, 'air_232dcee6f7c51d37': RollingMean: 5.571429, 'air_234d3dbf7f3d5a50': RollingMean: 7.642857, 'air_23e1b11aee2a1407': RollingMean: 46., 'air_23ee674e91469086': RollingMean: 20.714286, 'air_24b9b2a020826ede': RollingMean: 31.5, 'air_24e8414b9b07decb': RollingMean: 6.071429, 'air_2545dd3a00f265e2': RollingMean: 46.785714, 'air_256be208a979e023': RollingMean: 7.428571, 'air_2570ccb93badde68': RollingMean: 39.357143, 'air_258ad2619d7bff9a': RollingMean: 36.714286, 'air_258dc112912fc458': RollingMean: 61.428571, 'air_25c583983246b7b0': RollingMean: 24.428571, 'air_25d8e5cc57dd87d9': RollingMean: 24.928571, 'air_25e9888d30b386df': RollingMean: 4.928571, 'air_2634e41551e9807d': RollingMean: 14.857143, 'air_26c5bbeb7bb82bf1': RollingMean: 27.071429, 'air_26f10355d9b4d82a': RollingMean: 33.785714, 'air_2703dcb33192b181': RollingMean: 52.428571, 'air_275732a5db46f4d3': RollingMean: 15.928571, 'air_27e991812b0d9c92': RollingMean: 41.428571, 'air_28064154614b2e6c': RollingMean: 22.857143, 'air_287d2de7d3c93406': RollingMean: 13., 'air_28a9fa1ec0839375': RollingMean: 29.071429, 'air_28dbe91c4c9656be': RollingMean: 33.571429, 'air_290e7a57b390f78e': RollingMean: 13.857143, 'air_298513175efdf261': RollingMean: 26.428571, 'air_2a184c1745274b2b': RollingMean: 4.214286, 'air_2a24aec099333f39': RollingMean: 7.285714, 'air_2a3743e37aab04b4': RollingMean: 20.071429, 'air_2a485b92210c98b5': RollingMean: 18.142857, 'air_2a7f14da7fe0f699': RollingMean: 27.214286, 'air_2aab19554f91ff82': RollingMean: 49.714286, 'air_2ac361b97630e2df': RollingMean: 12.071429, 'air_2b8b29ddfd35018e': RollingMean: 10.928571, 'air_2b9bc9f5f5168ea1': RollingMean: 21.428571, 'air_2bffb19a24d11729': RollingMean: 12.428571, 'air_2c505f9ad67d4635': RollingMean: 16.5, 'air_2c6c79d597e48096': RollingMean: 17.928571, 'air_2c6fef1ce0e13a5a': RollingMean: 30.785714, 'air_2c989829acbd1c6b': RollingMean: 28.357143, 'air_2cee51fa6fdf6c0d': RollingMean: 16.571429, 'air_2d3afcb91762fe01': RollingMean: 45.142857, 'air_2d78d9a1f4dd02ca': RollingMean: 11.357143, 'air_2e7cb1f1a2a9cd6a': RollingMean: 27.571429, 'air_2f8ced25216df926': RollingMean: 14.142857, 'air_2fc149abe33adcb4': RollingMean: 35.214286, 'air_2fc478dc9f0a6b31': RollingMean: 11.428571, 'air_2fed81034f8834e5': RollingMean: 20.714286, 'air_303bac187b53083a': RollingMean: 10.428571, 'air_310e467e6e625004': RollingMean: 15.214286, 'air_3155ee23d92202da': RollingMean: 16.5, 'air_31c753b48a657b6c': RollingMean: 24.285714, 'air_32460819c7600037': RollingMean: 51.071429, 'air_324f7c39a8410e7c': RollingMean: 10.928571, 'air_326ca454ef3558bc': RollingMean: 22.214286, 'air_32b02ba5dc2027f4': RollingMean: 27.5, 'air_32c61b620a766138': RollingMean: 26.785714, 'air_32f5d7cd696e3c4a': RollingMean: 23.142857, 'air_33b01025210d6007': RollingMean: 11.785714, 'air_3440e0ea1b70a99b': RollingMean: 28., 'air_346ade7d29230634': RollingMean: 7.857143, 'air_347be2c4feeb408b': RollingMean: 21.071429, 'air_349278fa964bb12f': RollingMean: 19.214286, 'air_3525f11ef0bf0c35': RollingMean: 41.142857, 'air_35512c42db0868da': RollingMean: 6.071429, 'air_3561fd1c0bce6a95': RollingMean: 10.714286, 'air_35c4732dcbfe31be': RollingMean: 6.785714, 'air_36429b5ca4407b3e': RollingMean: 20., 'air_36bcf77d3382d36e': RollingMean: 32.5, 'air_37189c92b6c761ec': RollingMean: 14.857143, 'air_375a5241615b5e22': RollingMean: 7., 'air_382f5ace4e2247b8': RollingMean: 7.785714, 'air_383f5b2f8d345a49': RollingMean: 12.357143, 'air_38746ffe9aa20c7e': RollingMean: 3.785714, 'air_396166d47733d5c9': RollingMean: 24.714286, 'air_396942e6423a2145': RollingMean: 21.857143, 'air_397d3f32a7196aa2': RollingMean: 35.5, 'air_3980af67be35afdb': RollingMean: 15.785714, 'air_3982a2c4ea2ed431': RollingMean: 30.5, 'air_399904bdb7685ca0': RollingMean: 27.785714, 'air_39dccf7df20b1c6a': RollingMean: 26.714286, 'air_3a8a3f8fb5cd7f88': RollingMean: 21.214286, 'air_3aa839e8e0cb6c87': RollingMean: 27.5, 'air_3ac24136722e2291': RollingMean: 13.142857, 'air_3b20733899b5287f': RollingMean: 39.857143, 'air_3b6438b125086430': RollingMean: 13.285714, 'air_3bb99a1fe0583897': RollingMean: 40.285714, 'air_3bd49f98ab7f36ab': RollingMean: 13., 'air_3c05c8f26c611eb9': RollingMean: 25.285714, 'air_3c938075889fc059': RollingMean: 30.5, 'air_3cad29d1a23209d2': RollingMean: 8.357143, 'air_3caef3f76b8f26c5': RollingMean: 25.642857, 'air_3d3a2b509180e798': RollingMean: 17.714286, 'air_3e6cea17a9d2c0f1': RollingMean: 17.071429, 'air_3e93f3c81008696d': RollingMean: 34.214286, 'air_3f91d592acd6cc0b': RollingMean: 24.928571, 'air_401b39f97e56b939': RollingMean: 11.785714, 'air_4043b7ccfbffa732': RollingMean: 44.571429, 'air_4092cfbd95a3ac1b': RollingMean: 23., 'air_40953e2d8b4f2857': RollingMean: 15.428571, 'air_40f6193ea3ed1b91': RollingMean: 16.714286, 'air_414ff459ed18fa48': RollingMean: 16., 'air_41bbf6e1d9814c4b': RollingMean: 8.714286, 'air_421670f21da5ba31': RollingMean: 17.928571, 'air_4254c3fc3ad078bd': RollingMean: 11.357143, 'air_42c9aa6d617c5057': RollingMean: 42.714286, 'air_42d41eb58cad170e': RollingMean: 32.285714, 'air_43b65e4b05bff2d3': RollingMean: 20.642857, 'air_43d577e0c9460e64': RollingMean: 35.142857, 'air_4433ab8e9999915f': RollingMean: 21.142857, 'air_4481a87c1d7c9896': RollingMean: 20.928571, 'air_452100f5305dde64': RollingMean: 7.428571, 'air_45326ebb8dc72cfb': RollingMean: 29.142857, 'air_4570f52104fe0982': RollingMean: 7.357143, 'air_4579cb0669fd411b': RollingMean: 18.071429, 'air_457efe8c3a30ea17': RollingMean: 6.285714, 'air_464a62de0d57be1e': RollingMean: 26.857143, 'air_465bddfed3353b23': RollingMean: 27.071429, 'air_47070be6093f123e': RollingMean: 46.285714, 'air_472b19e3b5bffa41': RollingMean: 13.5, 'air_473cf23b9e7c0a37': RollingMean: 9.357143, 'air_473f98b212d37b4a': RollingMean: 27.357143, 'air_47beaffd3806c979': RollingMean: 19.214286, 'air_483eba479dc9910d': RollingMean: 21.642857, 'air_48e9fc98b62495a7': RollingMean: 17.071429, 'air_48f4da6223571da4': RollingMean: 22.571429, 'air_48ffd31594bc3263': RollingMean: 4.928571, 'air_49211568cab5fdee': RollingMean: 25., 'air_4974785f48853db9': RollingMean: 6.928571, 'air_4b251b9f8373f1ae': RollingMean: 26.928571, 'air_4b380b4db9d37883': RollingMean: 25.571429, 'air_4b55d8aea1d2b395': RollingMean: 34.285714, 'air_4b9085d0d46a6211': RollingMean: 28.142857, 'air_4beac252540f865e': RollingMean: 41.571429, 'air_4c2ed28f3f19ca52': RollingMean: 14.642857, 'air_4c665a2bfff0da3b': RollingMean: 8.857143, 'air_4c727b55acdee495': RollingMean: 15.357143, 'air_4cab15ad29c0ffbc': RollingMean: 21.214286, 'air_4cab91146e3d1897': RollingMean: 15., 'air_4cca5666eaf5c709': RollingMean: 34.428571, 'air_4ce7b17062a1bf73': RollingMean: 7.857143, 'air_4d21676ed11f0bac': RollingMean: 32.857143, 'air_4d71826793c09b22': RollingMean: 21.642857, 'air_4d90a22572fa1ec9': RollingMean: 25.785714, 'air_4de6d887a7b1c1fc': RollingMean: 15.357143, 'air_4dea8d17f6f59c56': RollingMean: 25.214286, 'air_4e1c38f68f435596': RollingMean: 34.071429, 'air_4f762e840b3996e1': RollingMean: 11.285714, 'air_4feeb8600f131e43': RollingMean: 57.714286, 'air_500641aca4cf673c': RollingMean: 15.142857, 'air_506fe758114df773': RollingMean: 28.428571, 'air_51281cd059d7b89b': RollingMean: 13.928571, 'air_51319e7acf0438cf': RollingMean: 13.571429, 'air_52a08ef3efdb4bb0': RollingMean: 47.928571, 'air_52e2a1fd42bc917a': RollingMean: 11.642857, 'air_536043fcf1a4f8a4': RollingMean: 30.071429, 'air_539d693f7317c62d': RollingMean: 18., 'air_546b353cbea4a45b': RollingMean: 17.357143, 'air_5485912b44f976de': RollingMean: 8.428571, 'air_54d6c25d33f5260e': RollingMean: 40.5, 'air_54ed43163b7596c4': RollingMean: 16.5, 'air_55390f784018349a': RollingMean: 46., 'air_55c3627912b9c849': RollingMean: 10., 'air_55e11c33d4758131': RollingMean: 22.071429, 'air_56cd12f31a0afc04': RollingMean: 29.785714, 'air_56cebcbd6906e04c': RollingMean: 17.357143, 'air_56ea46c14b2dd967': RollingMean: 42.214286, 'air_57013002b912772b': RollingMean: 5.571429, 'air_573ecdf81b157d22': RollingMean: 28.285714, 'air_57c9eea1a2b66e65': RollingMean: 14.357143, 'air_57ed725a1930a5b9': RollingMean: 15.642857, 'air_5878b6f2a9da12c1': RollingMean: 13.357143, 'air_59cc9b2b209c6331': RollingMean: 9.071429, 'air_5a9a6cbeeb434c08': RollingMean: 23.071429, 'air_5acc13d655a6e8b2': RollingMean: 24.071429, 'air_5afb1cca48ceaa19': RollingMean: 42.142857, 'air_5b6d18c470bbfaf9': RollingMean: 37.285714, 'air_5b704df317ed1962': RollingMean: 2.214286, 'air_5bd22f9cc1426a90': RollingMean: 32.357143, 'air_5c65468938c07fa5': RollingMean: 16.214286, 'air_5c7489c9ec755e2d': RollingMean: 33.428571, 'air_5c817ef28f236bdf': RollingMean: 42.428571, 'air_5cb030b9f0b91537': RollingMean: 11.071429, 'air_5cfc537125d97f16': RollingMean: 9.571429, 'air_5d7c744c3a2ef624': RollingMean: 32.428571, 'air_5d945ade487cdf4d': RollingMean: 13.5, 'air_5dea8a7a5bf5eb71': RollingMean: 28.285714, 'air_5e339a1f364cdb00': RollingMean: 11.285714, 'air_5e34c6fe6fabd10e': RollingMean: 20.357143, 'air_5e70fe82f9e4fab6': RollingMean: 15.5, 'air_5e939e005bd34633': RollingMean: 2.571429, 'air_5ed3198e4a5eed0f': RollingMean: 38.071429, 'air_5f3a3ef4cba110a4': RollingMean: 32.214286, 'air_5f6fa1b897fe80d5': RollingMean: 29.785714, 'air_5fbda8e9302f7c13': RollingMean: 27.928571, 'air_602ca92c0db34f8f': RollingMean: 19., 'air_609050e4e4f79ae1': RollingMean: 11.857143, 'air_60a7057184ec7ec7': RollingMean: 27.857143, 'air_60aa54ecbc602348': RollingMean: 5.428571, 'air_6108821ffafa9b72': RollingMean: 24.785714, 'air_614e2f7e76dff854': RollingMean: 11.857143, 'air_61668cc2b0778898': RollingMean: 8.428571, 'air_61b8d37c33617f21': RollingMean: 27.428571, 'air_61de73b097513f58': RollingMean: 9.714286, 'air_622375b4815cf5cb': RollingMean: 37.5, 'air_627cabe2fe53f33f': RollingMean: 14.5, 'air_629d9935273c82ae': RollingMean: 23.142857, 'air_629edf21ea38ac2d': RollingMean: 35.214286, 'air_632ba66e1f75aa28': RollingMean: 19.142857, 'air_638c35eb25e53eea': RollingMean: 19.5, 'air_63a750d8b4b6a976': RollingMean: 23.428571, 'air_63a88d81295195ed': RollingMean: 29.5, 'air_63b13c56b7201bd9': RollingMean: 22.285714, 'air_63e28ee0b0c955a7': RollingMean: 22., 'air_640cf4835f0d9ba3': RollingMean: 26.714286, 'air_6411203a47b5ec77': RollingMean: 8.928571, 'air_645cb18b33f938cf': RollingMean: 17.071429, 'air_646b93e336f0dded': RollingMean: 7.428571, 'air_64a5d5c1381837af': RollingMean: 40., 'air_64d4491ad8cdb1c6': RollingMean: 13.714286, 'air_650f9b9de0c5542c': RollingMean: 22.428571, 'air_657a0748462f85de': RollingMean: 8., 'air_65e294f1ae6df9c3': RollingMean: 22.214286, 'air_6607fe3671242ce3': RollingMean: 42.571429, 'air_670a0c1c4108bcea': RollingMean: 25., 'air_671b4bea84dafb67': RollingMean: 26.071429, 'air_673acd9fa5e0dd78': RollingMean: 6.214286, 'air_67483104fa38ef6c': RollingMean: 29.571429, 'air_675aa35cba456fd1': RollingMean: 43.642857, 'air_67f87c159d9e2ee2': RollingMean: 36.5, 'air_68147db09287bf74': RollingMean: 24., 'air_681b0c56328dd2af': RollingMean: 35., 'air_681f96e6a6595f82': RollingMean: 37.285714, 'air_68301bcb11e2f389': RollingMean: 26.428571, 'air_683371d9baabf410': RollingMean: 31.071429, 'air_6836438b543ba698': RollingMean: 12.928571, 'air_6873982b9e19c7ad': RollingMean: 5., 'air_68c1de82037d87e6': RollingMean: 25.785714, 'air_68cc910e7b307b09': RollingMean: 7.928571, 'air_68d075113f368946': RollingMean: 20.285714, 'air_6902e4ec305b3d08': RollingMean: 36.857143, 'air_694571ea13fb9e0e': RollingMean: 30.571429, 'air_6a15e4eae523189d': RollingMean: 17.785714, 'air_6b15edd1b4fbb96a': RollingMean: 31.785714, 'air_6b2268863b14a2af': RollingMean: 18.857143, 'air_6b65745d432fd77f': RollingMean: 23.571429, 'air_6b7678aae65d2d59': RollingMean: 6.928571, 'air_6b942d5ebbc759c2': RollingMean: 14.714286, 'air_6b9fa44a9cf504a1': RollingMean: 5.785714, 'air_6c1128955c58b690': RollingMean: 13., 'air_6c91a28278a16f64': RollingMean: 8.642857, 'air_6c952e3c6e590945': RollingMean: 15.642857, 'air_6ca1d941c8199a67': RollingMean: 26.071429, 'air_6cbe54f0aa30b615': RollingMean: 15.571429, 'air_6ced51c24fb54262': RollingMean: 7.785714, 'air_6d64dba2edd4fc0c': RollingMean: 9.428571, 'air_6d65542aa43b598b': RollingMean: 27.785714, 'air_6d65dd11d96e00fb': RollingMean: 4.428571, 'air_6e06824d0934dd81': RollingMean: 21.928571, 'air_6e3fd96320d24324': RollingMean: 8.285714, 'air_6e64fb5821402cd2': RollingMean: 8.357143, 'air_6ff5fca957798daa': RollingMean: 7.857143, 'air_707d4b6328f2c2df': RollingMean: 28.5, 'air_709262d948dd0b6e': RollingMean: 11.071429, 'air_70e9e8cd55879414': RollingMean: 10.928571, 'air_70f834596eb99fee': RollingMean: 21.428571, 'air_710d6537cb7623df': RollingMean: 32.071429, 'air_712dd258f7f91b4b': RollingMean: 15.714286, 'air_71903025d39a4571': RollingMean: 14.642857, 'air_722297e7f26db91d': RollingMean: 11.071429, 'air_728ff578acc6ac6e': RollingMean: 10.571429, 'air_72f5146cf0c49beb': RollingMean: 12.214286, 'air_735bcbe1763d6e98': RollingMean: 8.642857, 'air_73f316e6a18d8aa9': RollingMean: 22.785714, 'air_7420042ff75f9aca': RollingMean: 35., 'air_746211c0b532e8aa': RollingMean: 53.5, 'air_747f375eb3900e1e': RollingMean: 5.5, 'air_74cf22153214064c': RollingMean: 10.142857, 'air_7514d90009613cd6': RollingMean: 66.928571, 'air_754ae581ad80cc9f': RollingMean: 13.928571, 'air_75864c80d2fb334a': RollingMean: 10.357143, 'air_75bd5d1b6dc6670d': RollingMean: 14.142857, 'air_764f71040a413d4d': RollingMean: 49.428571, 'air_77488fa378cf98c3': RollingMean: 9., 'air_77dfc83450cbc89c': RollingMean: 41.571429, 'air_7831b00996701c0f': RollingMean: 23.5, 'air_789103bf53b8096b': RollingMean: 55.928571, 'air_789466e488705c93': RollingMean: 24.928571, 'air_78df4dc6a7e83e41': RollingMean: 17.5, 'air_79afb3f52b4d062c': RollingMean: 10., 'air_79f528087f49df06': RollingMean: 34.071429, 'air_7a81bd7fadcbf3d8': RollingMean: 5., 'air_7a946aada80376a4': RollingMean: 16., 'air_7bacc4d36fb094c9': RollingMean: 6.142857, 'air_7bc6ca04d7b0f3b8': RollingMean: 12.071429, 'air_7bda6048a4a78837': RollingMean: 23.857143, 'air_7c7774c66fb237f7': RollingMean: 8.714286, 'air_7cc17a324ae5c7dc': RollingMean: 14.714286, 'air_7cf5a02c0e01b647': RollingMean: 33.214286, 'air_7d65049f9d275c0d': RollingMean: 10.857143, 'air_7dacea2f22afccfb': RollingMean: 32.357143, 'air_7db266904cb0d72a': RollingMean: 13., 'air_7e12c5d27f44a8de': RollingMean: 23.785714, 'air_7ef9a5ea5c8fe39f': RollingMean: 11.142857, 'air_7f3dc18494bce98b': RollingMean: 14.071429, 'air_7f9e15afafcf4c75': RollingMean: 35.428571, 'air_7fbf7649eb13ad9b': RollingMean: 18.714286, 'air_800c02226e2e0288': RollingMean: 13.785714, 'air_8093d0b565e9dbdf': RollingMean: 37., 'air_8110d68cc869b85e': RollingMean: 45.642857, 'air_81546875de9c8e78': RollingMean: 4.642857, 'air_81a12d67c22e012f': RollingMean: 20.285714, 'air_81bd68142db76f58': RollingMean: 25.357143, 'air_81c2600146d07d16': RollingMean: 6.5, 'air_81c5dff692063446': RollingMean: 11.285714, 'air_820d1919cbecaa0a': RollingMean: 33.357143, 'air_82a6ae14151953ba': RollingMean: 37.357143, 'air_831658500aa7c846': RollingMean: 29.928571, 'air_832f9dbe9ee4ebd3': RollingMean: 13.285714, 'air_83db5aff8f50478e': RollingMean: 7.5, 'air_84060403939d8216': RollingMean: 15.285714, 'air_848616680ef061bd': RollingMean: 29.857143, 'air_84f6876ff7e83ae7': RollingMean: 19.214286, 'air_8523d6a70de49e6c': RollingMean: 33.785714, 'air_859feab8e3c9f98d': RollingMean: 22.571429, 'air_85bd13a49370c392': RollingMean: 11.928571, 'air_86cfbf2624576fad': RollingMean: 6.857143, 'air_86f7b2109e4abd65': RollingMean: 49.428571, 'air_87059630ab6fe47f': RollingMean: 3.714286, 'air_87078cf7903a648c': RollingMean: 6., 'air_87467487d21891dd': RollingMean: 13.571429, 'air_8764b3473ddcceaf': RollingMean: 5., 'air_876d7a23c47811cb': RollingMean: 15.928571, 'air_877f79706adbfb06': RollingMean: 12.142857, 'air_87ca98aa7664de94': RollingMean: 10.714286, 'air_87f9e1024b951f01': RollingMean: 11.285714, 'air_883ca28ef0ed3d55': RollingMean: 14.142857, 'air_88c8e34baa79217b': RollingMean: 30.714286, 'air_88ca84051ba95339': RollingMean: 18.642857, 'air_88f31db64991768a': RollingMean: 8.785714, 'air_890d7e28e8eaaa11': RollingMean: 8.071429, 'air_89e7328af22efe74': RollingMean: 34., 'air_8a1d21fad48374e8': RollingMean: 11.857143, 'air_8a59bb0c497b771e': RollingMean: 27.214286, 'air_8a906e5801eac81c': RollingMean: 23.357143, 'air_8b4a46dc521bfcfe': RollingMean: 29.071429, 'air_8c119d1f16049f20': RollingMean: 25.428571, 'air_8c3175aa5e4fc569': RollingMean: 80.071429, 'air_8cc350fd70ee0757': RollingMean: 32., 'air_8ce035ee1d8a56a6': RollingMean: 18.357143, 'air_8d50c64692322dff': RollingMean: 10.785714, 'air_8d61f49aa0373492': RollingMean: 45.642857, 'air_8e429650fcf7a0ae': RollingMean: 19.214286, 'air_8e4360a64dbd4c50': RollingMean: 20.357143, 'air_8e492076a1179383': RollingMean: 43.642857, 'air_8e8f42f047537154': RollingMean: 29.357143, 'air_8ec47c0f1e2c879e': RollingMean: 30.714286, 'air_8f13ef0f5e8c64dd': RollingMean: 6.285714, 'air_8f273fb9ad2fed6f': RollingMean: 13.071429, 'air_8f3b563416efc6ad': RollingMean: 13.642857, 'air_900d755ebd2f7bbd': RollingMean: 92.071429, 'air_901925b628677c2e': RollingMean: 9.142857, 'air_90213bcae4afa274': RollingMean: 27.357143, 'air_90bd5de52c166767': RollingMean: 22.071429, 'air_90ed0a2f24755533': RollingMean: 41.785714, 'air_90f0efbb702d77b7': RollingMean: 32.214286, 'air_9105a29b0eb250d2': RollingMean: 18.285714, 'air_91236b89d29567af': RollingMean: 23.071429, 'air_9152d9926e5c4a3a': RollingMean: 36.142857, 'air_915558a55c2bc56c': RollingMean: 17.785714, 'air_91beafbba9382b0a': RollingMean: 36.5, 'air_91d72e16c4bcba55': RollingMean: 14.785714, 'air_9241121af22ff1d6': RollingMean: 29.928571, 'air_929d8513e3cdb423': RollingMean: 7., 'air_931a8a4321b6e7d1': RollingMean: 5.142857, 'air_9352c401d5adb01b': RollingMean: 25.571429, 'air_9387ff95e886ebc7': RollingMean: 13.214286, 'air_938ef91ecdde6878': RollingMean: 23.214286, 'air_939964477035ef0b': RollingMean: 17.928571, 'air_93b9bb641f8fc982': RollingMean: 28.285714, 'air_93dd7070c9bf5453': RollingMean: 29.428571, 'air_93ebe490d4abb8e9': RollingMean: 17.714286, 'air_9438d67241c81314': RollingMean: 37., 'air_947eb2cae4f3e8f2': RollingMean: 32.071429, 'air_9483d65e9cc9a6b7': RollingMean: 15.285714, 'air_950381108f839348': RollingMean: 28.142857, 'air_95d28905941fd4cb': RollingMean: 30.785714, 'air_95e917913cd988f3': RollingMean: 23.642857, 'air_96005f79124e12bf': RollingMean: 40.428571, 'air_965b2e0cf4119003': RollingMean: 47.357143, 'air_96743eee94114261': RollingMean: 13.428571, 'air_96773a6236d279b1': RollingMean: 24.785714, 'air_968d72c12eed09f0': RollingMean: 18., 'air_96929a799b12a43e': RollingMean: 27.214286, 'air_96ec3cfe78cb0652': RollingMean: 18.5, 'air_97159fc4e90053fe': RollingMean: 24., 'air_97958e7fce98b6a3': RollingMean: 16.928571, 'air_97b2a9f975fc702c': RollingMean: 40.142857, 'air_97cf68dc1a9beac0': RollingMean: 13.428571, 'air_97e0f2feec4d577a': RollingMean: 15.071429, 'air_9828505fefc77d75': RollingMean: 12.5, 'air_98b54e32ccddd896': RollingMean: 18.214286, 'air_990a642a3deb2903': RollingMean: 33.5, 'air_99157b6163835eec': RollingMean: 33.785714, 'air_99a5183695b849f9': RollingMean: 30.928571, 'air_99b01136f451fc0e': RollingMean: 41.428571, 'air_99c3eae84130c1cb': RollingMean: 39.928571, 'air_9a30407764f4ff84': RollingMean: 17., 'air_9a6f6e7f623003d2': RollingMean: 3.142857, 'air_9aa32b3db0fab3a5': RollingMean: 17.142857, 'air_9aa92007e3628dbc': RollingMean: 31.642857, 'air_9ae7081cb77dc51e': RollingMean: 29.571429, 'air_9b13c7feb0a0c431': RollingMean: 10.714286, 'air_9b6af3db40da4ae2': RollingMean: 29.785714, 'air_9bbc673495e23532': RollingMean: 5.142857, 'air_9bf0ccac497c4af3': RollingMean: 44.571429, 'air_9bf595ef095572fb': RollingMean: 25.642857, 'air_9c6787aa03a45586': RollingMean: 92.142857, 'air_9ca2767761efff4d': RollingMean: 8.357143, 'air_9cd5e310f488bced': RollingMean: 14.642857, 'air_9cf2f1ba86229773': RollingMean: 34., 'air_9d3482b4680aee88': RollingMean: 9.785714, 'air_9d452a881f7f2bb7': RollingMean: 7.857143, 'air_9d474ec2448c700d': RollingMean: 12.857143, 'air_9d5a980b211e1795': RollingMean: 11.571429, 'air_9d93d95720f2e831': RollingMean: 8.285714, 'air_9dc9483f717d73ee': RollingMean: 5.571429, 'air_9dd7d38b0f1760c4': RollingMean: 2.5, 'air_9e920b758503ef54': RollingMean: 5.5, 'air_9efaa7ded03c5a71': RollingMean: 12.214286, 'air_9f277fb7a2c1d842': RollingMean: 12.142857, 'air_9fc607777ad76b26': RollingMean: 15.928571, 'air_a083834e7ffe187e': RollingMean: 20.285714, 'air_a11473cc1eb9a27f': RollingMean: 33.071429, 'air_a17f0778617c76e2': RollingMean: 38., 'air_a1fe8c588c8d2f30': RollingMean: 15.785714, 'air_a218912784bf767d': RollingMean: 14.357143, 'air_a21ffca0bea1661a': RollingMean: 1.071429, 'air_a239a44805932bab': RollingMean: 34.428571, 'air_a24bf50c3e90d583': RollingMean: 17.285714, 'air_a2567267116a3b75': RollingMean: 16.642857, 'air_a257c9749d8d0ff6': RollingMean: 18.428571, 'air_a271c9ba19e81d17': RollingMean: 28.857143, 'air_a2b29aa7feb4e36f': RollingMean: 16.642857, 'air_a304330715435390': RollingMean: 7.642857, 'air_a33461f4392ec62c': RollingMean: 28.142857, 'air_a373500730e2a9e0': RollingMean: 11., 'air_a38f25e3399d1b25': RollingMean: 41.857143, 'air_a41b032371a63427': RollingMean: 11.357143, 'air_a49f1cf0634f13e5': RollingMean: 22.214286, 'air_a510dcfe979f09eb': RollingMean: 13.714286, 'air_a546cbf478a8b6e4': RollingMean: 27.5, 'air_a55d17bd3f3033cb': RollingMean: 12.285714, 'air_a563896da3777078': RollingMean: 21.571429, 'air_a678e5b144ca24ce': RollingMean: 16.714286, 'air_a7404a854919e990': RollingMean: 7.357143, 'air_a8533b7a992bb0ca': RollingMean: 18.571429, 'air_a85f0c0c889f6b7e': RollingMean: 47.571429, 'air_a85f8c0bfd61889f': RollingMean: 15.142857, 'air_a88ac559064dec08': RollingMean: 32.142857, 'air_a9133955abccf071': RollingMean: 27.785714, 'air_a9178f19da58fe99': RollingMean: 7.428571, 'air_a9a380530c1e121f': RollingMean: 41.285714, 'air_aa0049fe3cc6f4d6': RollingMean: 10.142857, 'air_ab3ae0e410b20069': RollingMean: 17.928571, 'air_ab9746a0f83084b7': RollingMean: 6.714286, 'air_abcdc8115988a010': RollingMean: 10.857143, 'air_abf06fcca748dca5': RollingMean: 9.5, 'air_ac7a7427c9ae12a5': RollingMean: 62.071429, 'air_ad13e71e21235131': RollingMean: 16.071429, 'air_ad60f6b76c9df7ed': RollingMean: 23.357143, 'air_ad7777590c884721': RollingMean: 8.357143, 'air_add9a575623726c8': RollingMean: 40.285714, 'air_ade6e836ffd1da64': RollingMean: 10.214286, 'air_aed3a8b49abe4a48': RollingMean: 6.214286, 'air_af03c277a167b2bd': RollingMean: 26.642857, 'air_af24e3e817dea1e5': RollingMean: 15.357143, 'air_af63df35857b16e6': RollingMean: 23.857143, 'air_b0a6a4c5e95c74cf': RollingMean: 18.642857, 'air_b162fb07fbbdea33': RollingMean: 15.285714, 'air_b192fb5310436005': RollingMean: 8.785714, 'air_b1a72bf1ebf4b8ef': RollingMean: 52.285714, 'air_b1bb1fae86617d7a': RollingMean: 34.428571, 'air_b1d822f75c9fc615': RollingMean: 10.785714, 'air_b2395df0e874078d': RollingMean: 7.071429, 'air_b23d0f519291247d': RollingMean: 28.785714, 'air_b259b4e4a51a690d': RollingMean: 22.785714, 'air_b28bed4b2e7167b7': RollingMean: 16.928571, 'air_b2a639cc7e02edf1': RollingMean: 20.714286, 'air_b2d8bc9c88b85f96': RollingMean: 14.714286, 'air_b2d97bd2337c5ba7': RollingMean: 28.785714, 'air_b2dcec37b83e2494': RollingMean: 7.428571, 'air_b30fffd7ab1e75a5': RollingMean: 9.214286, 'air_b3180b74332ba886': RollingMean: 13.857143, 'air_b3a824511477a4ed': RollingMean: 6.214286, 'air_b439391e72899756': RollingMean: 19.928571, 'air_b45b8e456f53942a': RollingMean: 10.857143, 'air_b4f32bcc399da2b9': RollingMean: 29.071429, 'air_b5598d12d1b84890': RollingMean: 5.857143, 'air_b5bdd318005d9aa4': RollingMean: 34., 'air_b60cc7d6aee68194': RollingMean: 11.357143, 'air_b711b43ae472cb6b': RollingMean: 20.642857, 'air_b7fa3d2fca744dd2': RollingMean: 40.785714, 'air_b80fed1a07c817d2': RollingMean: 4.285714, 'air_b88192b35ac03c24': RollingMean: 17.5, 'air_b8925441167c3152': RollingMean: 2.071429, 'air_b8a5ee69e5fdcc5b': RollingMean: 34.142857, 'air_b8d9e1624baaadc2': RollingMean: 6.214286, 'air_b9e27558fb8bd5c4': RollingMean: 12.071429, 'air_ba495cccc8f0f237': RollingMean: 17.357143, 'air_ba937bf13d40fb24': RollingMean: 15.285714, 'air_bac5f4441db21db9': RollingMean: 41.785714, 'air_baf28ac9f13a307d': RollingMean: 13.571429, 'air_bb09595bab7d5cfb': RollingMean: 29.5, 'air_bb26d6d079594414': RollingMean: 13.285714, 'air_bb4ff06cd661ee9b': RollingMean: 34.428571, 'air_bbe1c1a47e09f161': RollingMean: 1.642857, 'air_bc991c51d6613745': RollingMean: 18.571429, 'air_bc9a129e11a2efe0': RollingMean: 30.785714, 'air_bcce1ea4350b7b72': RollingMean: 20.857143, 'air_bd74a9222edfdfe1': RollingMean: 14.285714, 'air_bdd32aa407c16335': RollingMean: 13.785714, 'air_bebd55ed63ab2422': RollingMean: 8.785714, 'air_bed603c423b7d9d4': RollingMean: 6.142857, 'air_bedd35489e666605': RollingMean: 38.642857, 'air_bf13014b6e3e60ca': RollingMean: 27., 'air_bf21b8350771879b': RollingMean: 23.857143, 'air_bf617aa68d5f1cfa': RollingMean: 6.857143, 'air_bf7591560077332d': RollingMean: 8.357143, 'air_bfafaed35e213fd7': RollingMean: 12.857143, 'air_bfda7731a6c6fc61': RollingMean: 19.071429, 'air_c027e2b560442808': RollingMean: 15.857143, 'air_c0385db498b391e5': RollingMean: 33.5, 'air_c1d5d165c055b866': RollingMean: 30.071429, 'air_c1ff20617c54fee7': RollingMean: 7.785714, 'air_c225148c0fcc5c72': RollingMean: 34.857143, 'air_c2626f5f86d57342': RollingMean: 15.928571, 'air_c26f027b5bc1f081': RollingMean: 5.071429, 'air_c28983412a7eefcf': RollingMean: 35.285714, 'air_c2c8435bdb3516d4': RollingMean: 31.214286, 'air_c31472d14e29cee8': RollingMean: 14.857143, 'air_c3585b0fba3998d0': RollingMean: 9.357143, 'air_c3bc011cca3bec65': RollingMean: 5.642857, 'air_c3dcaf3aeb18e20e': RollingMean: 17.428571, 'air_c47aa7493b15f297': RollingMean: 21.142857, 'air_c4fa5c562d5409ca': RollingMean: 13.785714, 'air_c52c63c781fe48f6': RollingMean: 27.714286, 'air_c5459218282bedd5': RollingMean: 22.571429, 'air_c66dbd2c37832d00': RollingMean: 16.714286, 'air_c6a164dd4060e960': RollingMean: 12.428571, 'air_c6aa2efba0ffc8eb': RollingMean: 28.857143, 'air_c6ffd6a93e6b68d6': RollingMean: 17.357143, 'air_c73d319ffabf287a': RollingMean: 15.285714, 'air_c759b6abeb552160': RollingMean: 5.071429, 'air_c77ee2b7d36da265': RollingMean: 42.071429, 'air_c7d30ab0e07f31d5': RollingMean: 17.142857, 'air_c7f78b4f3cba33ff': RollingMean: 22.785714, 'air_c8265ecc116f2284': RollingMean: 9.142857, 'air_c88467d88b2c8ecd': RollingMean: 18.428571, 'air_c8a657c8c5c93d69': RollingMean: 7.928571, 'air_c8c0ef02ed72053f': RollingMean: 26.5, 'air_c8fe396d6c46275d': RollingMean: 14.357143, 'air_c92745dfdd2ec68a': RollingMean: 18.428571, 'air_c9ed65554b6edffb': RollingMean: 12.928571, 'air_c9f6de13be8b8f25': RollingMean: 4.357143, 'air_ca1315af9e073bd1': RollingMean: 45.285714, 'air_ca6ae8d49a2f1eaf': RollingMean: 20.928571, 'air_ca957d3a1529fbd3': RollingMean: 33.714286, 'air_cadf9cfb510a1d78': RollingMean: 31.071429, 'air_caf996ac27206301': RollingMean: 5.642857, 'air_cb083b4789a8d3a2': RollingMean: 17.214286, 'air_cb25551c4cd8d9f3': RollingMean: 9., 'air_cb7467aed805e7fe': RollingMean: 36.571429, 'air_cb935ff8610ba3d3': RollingMean: 4.857143, 'air_cbe139af83feb388': RollingMean: 9.928571, 'air_cbe867adcf44e14f': RollingMean: 15.5, 'air_cc1a0e985ce63711': RollingMean: 27.785714, 'air_cc35590cd1da8554': RollingMean: 20.857143, 'air_ccd19a5bc5573ae5': RollingMean: 37.714286, 'air_cd4b301d5d3918d8': RollingMean: 6.928571, 'air_cd5f54969be9ed08': RollingMean: 7.214286, 'air_ced6297e5bdf5130': RollingMean: 24.428571, 'air_cf2229e64408d9fe': RollingMean: 17.857143, 'air_cf22e368c1a71d53': RollingMean: 37.285714, 'air_cf5ab75a0afb8af9': RollingMean: 45.214286, 'air_cfcc94797d2b5d3d': RollingMean: 17.285714, 'air_cfdeb326418194ff': RollingMean: 15.142857, 'air_d00161e19f08290b': RollingMean: 26., 'air_d00a15343325e5f7': RollingMean: 18.714286, 'air_d07e57b21109304a': RollingMean: 10.142857, 'air_d0a1e69685259c92': RollingMean: 29.285714, 'air_d0a7bd3339c3d12a': RollingMean: 39.785714, 'air_d0e8a085d8dc83aa': RollingMean: 7.571429, 'air_d138b593ebda55cc': RollingMean: 5.714286, 'air_d1418d6fd6d634f2': RollingMean: 16.285714, 'air_d186b2cb0b9ce022': RollingMean: 13.357143, 'air_d1f20424f76cc78e': RollingMean: 20.928571, 'air_d34c0861a2be94cb': RollingMean: 42.571429, 'air_d3e7b5952cd09ccb': RollingMean: 18.642857, 'air_d44d210d2994f01b': RollingMean: 6.071429, 'air_d473620754bf9fc2': RollingMean: 12.571429, 'air_d477b6339b8ce69f': RollingMean: 8.857143, 'air_d4981cdde163b172': RollingMean: 23.428571, 'air_d4b5a4b04c5f2d04': RollingMean: 12.642857, 'air_d4d218b451f82c3d': RollingMean: 9.785714, 'air_d500b48a8735fbd3': RollingMean: 18.071429, 'air_d54d6fcb116fbed3': RollingMean: 4.214286, 'air_d5e0a20370c325c7': RollingMean: 29.857143, 'air_d63cfa6d6ab78446': RollingMean: 18.428571, 'air_d69b08a175bc0387': RollingMean: 10.857143, 'air_d6b3e67261f07646': RollingMean: 13.714286, 'air_d8abb9e490abf94f': RollingMean: 12., 'air_d97dabf7aae60da5': RollingMean: 33.285714, 'air_d98380a4aeb0290b': RollingMean: 43.357143, 'air_daa7947e1c47f5ed': RollingMean: 34.357143, 'air_dabfbd0ec951925a': RollingMean: 7.642857, 'air_dad0b6a36138f309': RollingMean: 5.5, 'air_db1233ad855b34d5': RollingMean: 24.142857, 'air_db4b38ebe7a7ceff': RollingMean: 24.5, 'air_db80363d35f10926': RollingMean: 28.785714, 'air_dbf64f1ce38c7442': RollingMean: 14.928571, 'air_dc0e080ba0a5e5af': RollingMean: 8.928571, 'air_dc71c6cc06cd1aa2': RollingMean: 6.285714, 'air_de692863bb2dd758': RollingMean: 20.857143, 'air_de803f7e324936b8': RollingMean: 24.285714, 'air_de88770300008624': RollingMean: 20., 'air_dea0655f96947922': RollingMean: 38.571429, 'air_df507aec929ce5f6': RollingMean: 19.071429, 'air_df554c4527a1cfe6': RollingMean: 53.5, 'air_df5cf5cd03eb68d0': RollingMean: 7.857143, 'air_df843e6b22e8d540': RollingMean: 10.285714, 'air_df9355c47c5df9d3': RollingMean: 30.285714, 'air_dfad598ff642dab7': RollingMean: 26.071429, 'air_dfe068a1bf85f395': RollingMean: 36.428571, 'air_e00fe7853c0100d6': RollingMean: 20.214286, 'air_e0118664da63a2d0': RollingMean: 16.714286, 'air_e01d99390355408d': RollingMean: 11.571429, 'air_e053c561f32acc28': RollingMean: 17.928571, 'air_e08b9cf82057a170': RollingMean: 34.5, 'air_e0aee25b56a069f2': RollingMean: 14.214286, 'air_e0e69668214ff972': RollingMean: 9.785714, 'air_e0f241bd406810c0': RollingMean: 33.142857, 'air_e1b76fcb5208fb6b': RollingMean: 14.571429, 'air_e2208a79e2678432': RollingMean: 45.142857, 'air_e270aff84ac7e4c8': RollingMean: 25.928571, 'air_e3020992d5fe5dfd': RollingMean: 13.5, 'air_e34c631c766f2766': RollingMean: 24.642857, 'air_e42bdc3377d1eee7': RollingMean: 20.785714, 'air_e483f5b3c4f310e0': RollingMean: 5.285714, 'air_e524c6a9e06cc3a1': RollingMean: 8.785714, 'air_e55abd740f93ecc4': RollingMean: 46., 'air_e57dd6884595f60d': RollingMean: 39.428571, 'air_e58f669b6f1a08ce': RollingMean: 11.142857, 'air_e5cf003abcc5febb': RollingMean: 12.571429, 'air_e64de0a6bf0739af': RollingMean: 43.214286, 'air_e657ca554b0c008c': RollingMean: 25., 'air_e700e390226d9985': RollingMean: 15.928571, 'air_e76a668009c5dabc': RollingMean: 7.785714, 'air_e7d2ac6d53d1b744': RollingMean: 9.357143, 'air_e7fbee4e3cfe65c5': RollingMean: 36., 'air_e88bbe2ede3467aa': RollingMean: 22.857143, 'air_e89735e80d614a7e': RollingMean: 33.785714, 'air_e8ed9335d0c38333': RollingMean: 24.857143, 'air_e9ebf7fc520ac76a': RollingMean: 28.428571, 'air_ea6d0c3acf00b22a': RollingMean: 22.285714, 'air_ea7c16131980c837': RollingMean: 7.071429, 'air_eb120e6d384a17a8': RollingMean: 42.785714, 'air_eb20a89bba7dd3d0': RollingMean: 3., 'air_eb2d2653586315dd': RollingMean: 31.428571, 'air_eb5788dba285e725': RollingMean: 27.071429, 'air_ebd31e812960f517': RollingMean: 26.857143, 'air_ebe02c3090271fa9': RollingMean: 11.142857, 'air_ec0fad2def4dcff0': RollingMean: 15.142857, 'air_eca4a5a191e8d993': RollingMean: 28.357143, 'air_eca5e0064dc9314a': RollingMean: 34.571429, 'air_ecab54b57a71b10d': RollingMean: 13.071429, 'air_eceb97ad6a7d4c07': RollingMean: 27.857143, 'air_ecf7f141339f1d57': RollingMean: 19.857143, 'air_eda179770dfa9f91': RollingMean: 10.642857, 'air_edd5e3d696a5811b': RollingMean: 39.357143, 'air_ee3a01f0c71a769f': RollingMean: 25.285714, 'air_ee3ba9af184c6c82': RollingMean: 18.642857, 'air_eec5e572b9eb9c23': RollingMean: 11.142857, 'air_eeeadee005c006a2': RollingMean: 12.142857, 'air_ef47430bcd6f6a89': RollingMean: 14.857143, 'air_ef789667e2e6fe96': RollingMean: 36.357143, 'air_ef920fa6f4b085f6': RollingMean: 33.928571, 'air_efc80d3f96b3aff7': RollingMean: 9.428571, 'air_efd70b04de878f25': RollingMean: 31., 'air_efef1e3daecce07e': RollingMean: 40.428571, 'air_f068442ebb6c246c': RollingMean: 10.071429, 'air_f0c7272956e62f12': RollingMean: 4.285714, 'air_f0fb0975bdc2cdf9': RollingMean: 10.285714, 'air_f1094dbf2aef85d9': RollingMean: 6.714286, 'air_f180301886c21375': RollingMean: 14.428571, 'air_f183a514cb8ff4fa': RollingMean: 42.071429, 'air_f1f9027d4fa8f653': RollingMean: 26.5, 'air_f267dd70a6a6b5d3': RollingMean: 44.5, 'air_f26f36ec4dc5adb0': RollingMean: 38.785714, 'air_f2985de32bb792e0': RollingMean: 26.285714, 'air_f2c5a1f24279c531': RollingMean: 14.357143, 'air_f3602e4fa2f12993': RollingMean: 11.571429, 'air_f3f9824b7d70c3cf': RollingMean: 16.642857, 'air_f4936b91c9addbf0': RollingMean: 16., 'air_f593fa60ac3541e2': RollingMean: 13.642857, 'air_f690c42545146e0a': RollingMean: 12.928571, 'air_f6b2489ccf873c3b': RollingMean: 15.928571, 'air_f6bfd27e2e174d16': RollingMean: 12.928571, 'air_f6cdaf7b7fdc6d78': RollingMean: 10., 'air_f8233ad00755c35c': RollingMean: 29.571429, 'air_f85e21e543cf44f2': RollingMean: 4.785714, 'air_f88898cd09f40496': RollingMean: 8.857143, 'air_f911308e19d64236': RollingMean: 40.571429, 'air_f9168b23fdfc1e52': RollingMean: 19.5, 'air_f927b2da69a82341': RollingMean: 8.357143, 'air_f957c6d6467d4d90': RollingMean: 10.642857, 'air_f96765e800907c77': RollingMean: 43.857143, 'air_fa12b40b02fecfd8': RollingMean: 15., 'air_fa4ffc9057812fa2': RollingMean: 5.142857, 'air_fab092c35776a9b1': RollingMean: 10.714286, 'air_fb44f566d4f64a4e': RollingMean: 15.857143, 'air_fbadf737162a5ce3': RollingMean: 14.571429, 'air_fc477473134e9ae5': RollingMean: 16.857143, 'air_fcd4492c83f1c6b9': RollingMean: 22.357143, 'air_fcfbdcf7b1f82c6e': RollingMean: 35.642857, 'air_fd154088b1de6fa7': RollingMean: 6.428571, 'air_fd6aac1043520e83': RollingMean: 36.071429, 'air_fdc02ec4a3d21ea4': RollingMean: 8.071429, 'air_fdcfef8bd859f650': RollingMean: 3.642857, 'air_fe22ef5a9cbef123': RollingMean: 21.714286, 'air_fe58c074ec1445ea': RollingMean: 32.785714, 'air_fea5dc9594450608': RollingMean: 15.571429, 'air_fee8dcf4d619598e': RollingMean: 26.785714, 'air_fef9ccb3ba0da2f7': RollingMean: 9.714286, 'air_ffcc2d5087e1b476': RollingMean: 20., 'air_fff68b929994bfbd': RollingMean: 4.285714}), 'how': RollingMean: 0., 'target_name': 'target'} target_rollingmean_21_by_store_id {'by': ['store_id'], 'feature_name': 'target_rollingmean_21_by_store_id', 'groups': defaultdict(functools.partial(<function deepcopy at 0x7f804bfae700>, RollingMean: 0.), {'air_00a91d42b08b08d9': RollingMean: 29.190476, 'air_0164b9927d20bcc3': RollingMean: 8.380952, 'air_0241aa3964b7f861': RollingMean: 8.904762, 'air_0328696196e46f18': RollingMean: 8.904762, 'air_034a3d5b40d5b1b1': RollingMean: 22.52381, 'air_036d4f1ee7285390': RollingMean: 21.238095, 'air_0382c794b73b51ad': RollingMean: 27.380952, 'air_03963426c9312048': RollingMean: 40.428571, 'air_04341b588bde96cd': RollingMean: 35.333333, 'air_049f6d5b402a31b2': RollingMean: 12.47619, 'air_04cae7c1bc9b2a0b': RollingMean: 19.238095, 'air_0585011fa179bcce': RollingMean: 5.333333, 'air_05c325d315cc17f5': RollingMean: 27., 'air_0647f17b4dc041c8': RollingMean: 30.52381, 'air_064e203265ee5753': RollingMean: 19.47619, 'air_066f0221b8a4d533': RollingMean: 12.428571, 'air_06f95ac5c33aca10': RollingMean: 30.047619, 'air_0728814bd98f7367': RollingMean: 8.095238, 'air_0768ab3910f7967f': RollingMean: 24.952381, 'air_07b314d83059c4d2': RollingMean: 36.380952, 'air_07bb665f9cdfbdfb': RollingMean: 26.333333, 'air_082908692355165e': RollingMean: 49.52381, 'air_083ddc520ea47e1e': RollingMean: 13.238095, 'air_0845d8395f30c6bb': RollingMean: 21.714286, 'air_084d98859256acf0': RollingMean: 15.761905, 'air_0867f7bebad6a649': RollingMean: 22.380952, 'air_08ba8cd01b3ba010': RollingMean: 10.666667, 'air_08cb3c4ee6cd6a22': RollingMean: 12.666667, 'air_08ef81d5b7a0d13f': RollingMean: 13.952381, 'air_08f994758a1e76d4': RollingMean: 27.714286, 'air_09040f6df960ddb8': RollingMean: 16., 'air_0919d54f0c9a24b8': RollingMean: 31.714286, 'air_09661c0f3259cc04': RollingMean: 28.47619, 'air_09a845d5b5944b01': RollingMean: 6.380952, 'air_09fd1f5c58583141': RollingMean: 8.333333, 'air_0a74a5408a0b8642': RollingMean: 28.095238, 'air_0b184ec04c741a6a': RollingMean: 11.190476, 'air_0b1e72d2d4422b20': RollingMean: 22.333333, 'air_0b9038300f8b2b50': RollingMean: 12.380952, 'air_0e1eae99b8723bc1': RollingMean: 11.952381, 'air_0e7c11b9abc50163': RollingMean: 36.904762, 'air_0ead98dd07e7a82a': RollingMean: 13.857143, 'air_0f0cdeee6c9bf3d7': RollingMean: 25.238095, 'air_0f2f96335f274801': RollingMean: 10., 'air_0f60e1576a7d397d': RollingMean: 6.619048, 'air_1033310359ceeac1': RollingMean: 18.809524, 'air_10393f12e9069760': RollingMean: 15.714286, 'air_105a7954e32dba9b': RollingMean: 50.857143, 'air_10713fbf3071c361': RollingMean: 12.047619, 'air_10bbe8acd943d8f6': RollingMean: 26.809524, 'air_12c4fb7a423df20d': RollingMean: 20.857143, 'air_138ee734ac79ff90': RollingMean: 8.714286, 'air_138ff410757b845f': RollingMean: 54.285714, 'air_1408dd53f31a8a65': RollingMean: 25.190476, 'air_142e78ba7001da9c': RollingMean: 13.619048, 'air_1509881b22965b34': RollingMean: 16.666667, 'air_152c1f08d7d20e07': RollingMean: 13.238095, 'air_15ae33469e9ea2dd': RollingMean: 11.428571, 'air_15e6e15c7ea2c162': RollingMean: 18.952381, 'air_16179d43b6ee5fd8': RollingMean: 8.285714, 'air_1653a6c513865af3': RollingMean: 32.52381, 'air_168441ada3e878e1': RollingMean: 41.47619, 'air_16c4cfddeb2cf69b': RollingMean: 11.52381, 'air_16cf0a73233896de': RollingMean: 10.52381, 'air_1707a3f18bb0da07': RollingMean: 27.285714, 'air_17a6ab40f97fd4d8': RollingMean: 6.904762, 'air_17bed6dbf7c8b0fc': RollingMean: 19.095238, 'air_1979eaff8189d086': RollingMean: 9.666667, 'air_1ab60ce33bfed8a8': RollingMean: 9.714286, 'air_1ae94f514a0bce13': RollingMean: 6.904762, 'air_1ba4e87ef7422183': RollingMean: 37.619048, 'air_1c0b150f9e696a5f': RollingMean: 107.238095, 'air_1c95a84924d72500': RollingMean: 9.619048, 'air_1d1e8860ae04f8e9': RollingMean: 12.904762, 'air_1d25ca6c76df48b4': RollingMean: 39.666667, 'air_1d3f797dd1f7cf1c': RollingMean: 33.428571, 'air_1dd8f6f47480d1a2': RollingMean: 37.857143, 'air_1dea9815ccd36620': RollingMean: 10.428571, 'air_1e23210b584540e7': RollingMean: 4.047619, 'air_1e665503b8474c55': RollingMean: 5.571429, 'air_1eeff462acb24fb7': RollingMean: 16.333333, 'air_1f1390a8be2272b3': RollingMean: 18.095238, 'air_1f34e9beded2231a': RollingMean: 8.095238, 'air_1f7f8fa557bc0d55': RollingMean: 3.619048, 'air_2009041dbf9264de': RollingMean: 50.047619, 'air_20619d21192aa571': RollingMean: 17.571429, 'air_20add8092c9bb51d': RollingMean: 30.428571, 'air_2195cd5025a98033': RollingMean: 28.809524, 'air_21f5052d5330528d': RollingMean: 32.809524, 'air_220cba70c890b119': RollingMean: 9.142857, 'air_22682e965418936f': RollingMean: 8.857143, 'air_228f10bec0bda9c8': RollingMean: 17.333333, 'air_229d7e508d9f1b5e': RollingMean: 10.809524, 'air_232dcee6f7c51d37': RollingMean: 5.571429, 'air_234d3dbf7f3d5a50': RollingMean: 7.571429, 'air_23e1b11aee2a1407': RollingMean: 45.571429, 'air_23ee674e91469086': RollingMean: 21.714286, 'air_24b9b2a020826ede': RollingMean: 31.666667, 'air_24e8414b9b07decb': RollingMean: 5.952381, 'air_2545dd3a00f265e2': RollingMean: 48.761905, 'air_256be208a979e023': RollingMean: 6.47619, 'air_2570ccb93badde68': RollingMean: 38.333333, 'air_258ad2619d7bff9a': RollingMean: 30.666667, 'air_258dc112912fc458': RollingMean: 61.571429, 'air_25c583983246b7b0': RollingMean: 20.047619, 'air_25d8e5cc57dd87d9': RollingMean: 26.190476, 'air_25e9888d30b386df': RollingMean: 4.285714, 'air_2634e41551e9807d': RollingMean: 13., 'air_26c5bbeb7bb82bf1': RollingMean: 28.952381, 'air_26f10355d9b4d82a': RollingMean: 32.52381, 'air_2703dcb33192b181': RollingMean: 49.428571, 'air_275732a5db46f4d3': RollingMean: 14.952381, 'air_27e991812b0d9c92': RollingMean: 44.238095, 'air_28064154614b2e6c': RollingMean: 21.714286, 'air_287d2de7d3c93406': RollingMean: 13.047619, 'air_28a9fa1ec0839375': RollingMean: 29.571429, 'air_28dbe91c4c9656be': RollingMean: 34.619048, 'air_290e7a57b390f78e': RollingMean: 13.714286, 'air_298513175efdf261': RollingMean: 27.285714, 'air_2a184c1745274b2b': RollingMean: 3.952381, 'air_2a24aec099333f39': RollingMean: 6.666667, 'air_2a3743e37aab04b4': RollingMean: 18.428571, 'air_2a485b92210c98b5': RollingMean: 21.571429, 'air_2a7f14da7fe0f699': RollingMean: 28.047619, 'air_2aab19554f91ff82': RollingMean: 51.285714, 'air_2ac361b97630e2df': RollingMean: 12., 'air_2b8b29ddfd35018e': RollingMean: 11.095238, 'air_2b9bc9f5f5168ea1': RollingMean: 20.190476, 'air_2bffb19a24d11729': RollingMean: 11.142857, 'air_2c505f9ad67d4635': RollingMean: 15.428571, 'air_2c6c79d597e48096': RollingMean: 16.380952, 'air_2c6fef1ce0e13a5a': RollingMean: 31.190476, 'air_2c989829acbd1c6b': RollingMean: 29.333333, 'air_2cee51fa6fdf6c0d': RollingMean: 16.619048, 'air_2d3afcb91762fe01': RollingMean: 44.857143, 'air_2d78d9a1f4dd02ca': RollingMean: 11.333333, 'air_2e7cb1f1a2a9cd6a': RollingMean: 27.666667, 'air_2f8ced25216df926': RollingMean: 13.714286, 'air_2fc149abe33adcb4': RollingMean: 34.857143, 'air_2fc478dc9f0a6b31': RollingMean: 11.714286, 'air_2fed81034f8834e5': RollingMean: 20.47619, 'air_303bac187b53083a': RollingMean: 10.047619, 'air_310e467e6e625004': RollingMean: 15.47619, 'air_3155ee23d92202da': RollingMean: 16.380952, 'air_31c753b48a657b6c': RollingMean: 24.904762, 'air_32460819c7600037': RollingMean: 50.761905, 'air_324f7c39a8410e7c': RollingMean: 11.809524, 'air_326ca454ef3558bc': RollingMean: 23.714286, 'air_32b02ba5dc2027f4': RollingMean: 29.285714, 'air_32c61b620a766138': RollingMean: 27.380952, 'air_32f5d7cd696e3c4a': RollingMean: 22.190476, 'air_33b01025210d6007': RollingMean: 10.285714, 'air_3440e0ea1b70a99b': RollingMean: 27.904762, 'air_346ade7d29230634': RollingMean: 7.761905, 'air_347be2c4feeb408b': RollingMean: 22.047619, 'air_349278fa964bb12f': RollingMean: 18.666667, 'air_3525f11ef0bf0c35': RollingMean: 39.761905, 'air_35512c42db0868da': RollingMean: 6.238095, 'air_3561fd1c0bce6a95': RollingMean: 12.142857, 'air_35c4732dcbfe31be': RollingMean: 6.571429, 'air_36429b5ca4407b3e': RollingMean: 18.904762, 'air_36bcf77d3382d36e': RollingMean: 32.285714, 'air_37189c92b6c761ec': RollingMean: 19.47619, 'air_375a5241615b5e22': RollingMean: 7.428571, 'air_382f5ace4e2247b8': RollingMean: 8.761905, 'air_383f5b2f8d345a49': RollingMean: 14.238095, 'air_38746ffe9aa20c7e': RollingMean: 4., 'air_396166d47733d5c9': RollingMean: 24.952381, 'air_396942e6423a2145': RollingMean: 19.333333, 'air_397d3f32a7196aa2': RollingMean: 31.190476, 'air_3980af67be35afdb': RollingMean: 17.380952, 'air_3982a2c4ea2ed431': RollingMean: 31.714286, 'air_399904bdb7685ca0': RollingMean: 26., 'air_39dccf7df20b1c6a': RollingMean: 24.809524, 'air_3a8a3f8fb5cd7f88': RollingMean: 21.285714, 'air_3aa839e8e0cb6c87': RollingMean: 27.380952, 'air_3ac24136722e2291': RollingMean: 14., 'air_3b20733899b5287f': RollingMean: 39.904762, 'air_3b6438b125086430': RollingMean: 13.380952, 'air_3bb99a1fe0583897': RollingMean: 37.904762, 'air_3bd49f98ab7f36ab': RollingMean: 11.809524, 'air_3c05c8f26c611eb9': RollingMean: 23.380952, 'air_3c938075889fc059': RollingMean: 28.428571, 'air_3cad29d1a23209d2': RollingMean: 9.142857, 'air_3caef3f76b8f26c5': RollingMean: 25.761905, 'air_3d3a2b509180e798': RollingMean: 17.142857, 'air_3e6cea17a9d2c0f1': RollingMean: 17.714286, 'air_3e93f3c81008696d': RollingMean: 34.47619, 'air_3f91d592acd6cc0b': RollingMean: 24.666667, 'air_401b39f97e56b939': RollingMean: 12.095238, 'air_4043b7ccfbffa732': RollingMean: 43.428571, 'air_4092cfbd95a3ac1b': RollingMean: 26.761905, 'air_40953e2d8b4f2857': RollingMean: 17.47619, 'air_40f6193ea3ed1b91': RollingMean: 15.190476, 'air_414ff459ed18fa48': RollingMean: 16.333333, 'air_41bbf6e1d9814c4b': RollingMean: 7.714286, 'air_421670f21da5ba31': RollingMean: 17.52381, 'air_4254c3fc3ad078bd': RollingMean: 10.904762, 'air_42c9aa6d617c5057': RollingMean: 43.142857, 'air_42d41eb58cad170e': RollingMean: 27.571429, 'air_43b65e4b05bff2d3': RollingMean: 21.952381, 'air_43d577e0c9460e64': RollingMean: 38.571429, 'air_4433ab8e9999915f': RollingMean: 19.619048, 'air_4481a87c1d7c9896': RollingMean: 22.095238, 'air_452100f5305dde64': RollingMean: 7.571429, 'air_45326ebb8dc72cfb': RollingMean: 30.714286, 'air_4570f52104fe0982': RollingMean: 7.285714, 'air_4579cb0669fd411b': RollingMean: 16.380952, 'air_457efe8c3a30ea17': RollingMean: 7.142857, 'air_464a62de0d57be1e': RollingMean: 25.619048, 'air_465bddfed3353b23': RollingMean: 25.952381, 'air_47070be6093f123e': RollingMean: 48.52381, 'air_472b19e3b5bffa41': RollingMean: 14.47619, 'air_473cf23b9e7c0a37': RollingMean: 8.571429, 'air_473f98b212d37b4a': RollingMean: 27.142857, 'air_47beaffd3806c979': RollingMean: 20.47619, 'air_483eba479dc9910d': RollingMean: 26.095238, 'air_48e9fc98b62495a7': RollingMean: 18.380952, 'air_48f4da6223571da4': RollingMean: 22.190476, 'air_48ffd31594bc3263': RollingMean: 4.857143, 'air_49211568cab5fdee': RollingMean: 23.380952, 'air_4974785f48853db9': RollingMean: 6.428571, 'air_4b251b9f8373f1ae': RollingMean: 27.714286, 'air_4b380b4db9d37883': RollingMean: 25.571429, 'air_4b55d8aea1d2b395': RollingMean: 36.904762, 'air_4b9085d0d46a6211': RollingMean: 29.428571, 'air_4beac252540f865e': RollingMean: 39.714286, 'air_4c2ed28f3f19ca52': RollingMean: 13.857143, 'air_4c665a2bfff0da3b': RollingMean: 8.666667, 'air_4c727b55acdee495': RollingMean: 14.809524, 'air_4cab15ad29c0ffbc': RollingMean: 20.047619, 'air_4cab91146e3d1897': RollingMean: 14.47619, 'air_4cca5666eaf5c709': RollingMean: 29.714286, 'air_4ce7b17062a1bf73': RollingMean: 9.047619, 'air_4d21676ed11f0bac': RollingMean: 30., 'air_4d71826793c09b22': RollingMean: 21.333333, 'air_4d90a22572fa1ec9': RollingMean: 26.238095, 'air_4de6d887a7b1c1fc': RollingMean: 15.52381, 'air_4dea8d17f6f59c56': RollingMean: 23.285714, 'air_4e1c38f68f435596': RollingMean: 35.952381, 'air_4f762e840b3996e1': RollingMean: 11.380952, 'air_4feeb8600f131e43': RollingMean: 57.428571, 'air_500641aca4cf673c': RollingMean: 15.666667, 'air_506fe758114df773': RollingMean: 27.619048, 'air_51281cd059d7b89b': RollingMean: 12.285714, 'air_51319e7acf0438cf': RollingMean: 12.952381, 'air_52a08ef3efdb4bb0': RollingMean: 44.904762, 'air_52e2a1fd42bc917a': RollingMean: 10.761905, 'air_536043fcf1a4f8a4': RollingMean: 28.238095, 'air_539d693f7317c62d': RollingMean: 17.285714, 'air_546b353cbea4a45b': RollingMean: 16.52381, 'air_5485912b44f976de': RollingMean: 8.809524, 'air_54d6c25d33f5260e': RollingMean: 38.47619, 'air_54ed43163b7596c4': RollingMean: 17.047619, 'air_55390f784018349a': RollingMean: 47.285714, 'air_55c3627912b9c849': RollingMean: 9.952381, 'air_55e11c33d4758131': RollingMean: 22.571429, 'air_56cd12f31a0afc04': RollingMean: 30.619048, 'air_56cebcbd6906e04c': RollingMean: 17.857143, 'air_56ea46c14b2dd967': RollingMean: 39.047619, 'air_57013002b912772b': RollingMean: 4.52381, 'air_573ecdf81b157d22': RollingMean: 28.380952, 'air_57c9eea1a2b66e65': RollingMean: 13.904762, 'air_57ed725a1930a5b9': RollingMean: 17., 'air_5878b6f2a9da12c1': RollingMean: 11.809524, 'air_59cc9b2b209c6331': RollingMean: 10.809524, 'air_5a9a6cbeeb434c08': RollingMean: 25.142857, 'air_5acc13d655a6e8b2': RollingMean: 25.238095, 'air_5afb1cca48ceaa19': RollingMean: 41.333333, 'air_5b6d18c470bbfaf9': RollingMean: 34.142857, 'air_5b704df317ed1962': RollingMean: 2.095238, 'air_5bd22f9cc1426a90': RollingMean: 29.095238, 'air_5c65468938c07fa5': RollingMean: 14., 'air_5c7489c9ec755e2d': RollingMean: 40.380952, 'air_5c817ef28f236bdf': RollingMean: 43.238095, 'air_5cb030b9f0b91537': RollingMean: 10.238095, 'air_5cfc537125d97f16': RollingMean: 9.952381, 'air_5d7c744c3a2ef624': RollingMean: 30.47619, 'air_5d945ade487cdf4d': RollingMean: 12.714286, 'air_5dea8a7a5bf5eb71': RollingMean: 31.047619, 'air_5e339a1f364cdb00': RollingMean: 10.428571, 'air_5e34c6fe6fabd10e': RollingMean: 18.809524, 'air_5e70fe82f9e4fab6': RollingMean: 15.857143, 'air_5e939e005bd34633': RollingMean: 3.190476, 'air_5ed3198e4a5eed0f': RollingMean: 39.190476, 'air_5f3a3ef4cba110a4': RollingMean: 31.47619, 'air_5f6fa1b897fe80d5': RollingMean: 28.809524, 'air_5fbda8e9302f7c13': RollingMean: 26.47619, 'air_602ca92c0db34f8f': RollingMean: 21.142857, 'air_609050e4e4f79ae1': RollingMean: 11.714286, 'air_60a7057184ec7ec7': RollingMean: 27.761905, 'air_60aa54ecbc602348': RollingMean: 5.095238, 'air_6108821ffafa9b72': RollingMean: 25.809524, 'air_614e2f7e76dff854': RollingMean: 11.285714, 'air_61668cc2b0778898': RollingMean: 10., 'air_61b8d37c33617f21': RollingMean: 27.52381, 'air_61de73b097513f58': RollingMean: 10.714286, 'air_622375b4815cf5cb': RollingMean: 35.142857, 'air_627cabe2fe53f33f': RollingMean: 14.904762, 'air_629d9935273c82ae': RollingMean: 21.428571, 'air_629edf21ea38ac2d': RollingMean: 37.571429, 'air_632ba66e1f75aa28': RollingMean: 17.857143, 'air_638c35eb25e53eea': RollingMean: 20.619048, 'air_63a750d8b4b6a976': RollingMean: 24.047619, 'air_63a88d81295195ed': RollingMean: 29.809524, 'air_63b13c56b7201bd9': RollingMean: 20., 'air_63e28ee0b0c955a7': RollingMean: 23.571429, 'air_640cf4835f0d9ba3': RollingMean: 24.47619, 'air_6411203a47b5ec77': RollingMean: 10.095238, 'air_645cb18b33f938cf': RollingMean: 16.047619, 'air_646b93e336f0dded': RollingMean: 6.619048, 'air_64a5d5c1381837af': RollingMean: 40.571429, 'air_64d4491ad8cdb1c6': RollingMean: 14.047619, 'air_650f9b9de0c5542c': RollingMean: 24., 'air_657a0748462f85de': RollingMean: 7.952381, 'air_65e294f1ae6df9c3': RollingMean: 23., 'air_6607fe3671242ce3': RollingMean: 40.190476, 'air_670a0c1c4108bcea': RollingMean: 25.285714, 'air_671b4bea84dafb67': RollingMean: 27., 'air_673acd9fa5e0dd78': RollingMean: 5.857143, 'air_67483104fa38ef6c': RollingMean: 30.904762, 'air_675aa35cba456fd1': RollingMean: 46.666667, 'air_67f87c159d9e2ee2': RollingMean: 39.47619, 'air_68147db09287bf74': RollingMean: 23.190476, 'air_681b0c56328dd2af': RollingMean: 33.285714, 'air_681f96e6a6595f82': RollingMean: 41.952381, 'air_68301bcb11e2f389': RollingMean: 24., 'air_683371d9baabf410': RollingMean: 32., 'air_6836438b543ba698': RollingMean: 13.285714, 'air_6873982b9e19c7ad': RollingMean: 5.190476, 'air_68c1de82037d87e6': RollingMean: 23.761905, 'air_68cc910e7b307b09': RollingMean: 7.428571, 'air_68d075113f368946': RollingMean: 20.142857, 'air_6902e4ec305b3d08': RollingMean: 38.761905, 'air_694571ea13fb9e0e': RollingMean: 30.904762, 'air_6a15e4eae523189d': RollingMean: 19.285714, 'air_6b15edd1b4fbb96a': RollingMean: 30.238095, 'air_6b2268863b14a2af': RollingMean: 18.238095, 'air_6b65745d432fd77f': RollingMean: 23.571429, 'air_6b7678aae65d2d59': RollingMean: 6.904762, 'air_6b942d5ebbc759c2': RollingMean: 13.571429, 'air_6b9fa44a9cf504a1': RollingMean: 6.142857, 'air_6c1128955c58b690': RollingMean: 14.095238, 'air_6c91a28278a16f64': RollingMean: 10.380952, 'air_6c952e3c6e590945': RollingMean: 15.285714, 'air_6ca1d941c8199a67': RollingMean: 26.619048, 'air_6cbe54f0aa30b615': RollingMean: 17., 'air_6ced51c24fb54262': RollingMean: 8.380952, 'air_6d64dba2edd4fc0c': RollingMean: 9.857143, 'air_6d65542aa43b598b': RollingMean: 28.095238, 'air_6d65dd11d96e00fb': RollingMean: 5.285714, 'air_6e06824d0934dd81': RollingMean: 21.285714, 'air_6e3fd96320d24324': RollingMean: 9.190476, 'air_6e64fb5821402cd2': RollingMean: 8.619048, 'air_6ff5fca957798daa': RollingMean: 7.190476, 'air_707d4b6328f2c2df': RollingMean: 29.571429, 'air_709262d948dd0b6e': RollingMean: 11., 'air_70e9e8cd55879414': RollingMean: 10.857143, 'air_70f834596eb99fee': RollingMean: 21.333333, 'air_710d6537cb7623df': RollingMean: 29.904762, 'air_712dd258f7f91b4b': RollingMean: 15.142857, 'air_71903025d39a4571': RollingMean: 14.809524, 'air_722297e7f26db91d': RollingMean: 11.904762, 'air_728ff578acc6ac6e': RollingMean: 9.809524, 'air_72f5146cf0c49beb': RollingMean: 13.238095, 'air_735bcbe1763d6e98': RollingMean: 8.047619, 'air_73f316e6a18d8aa9': RollingMean: 23.238095, 'air_7420042ff75f9aca': RollingMean: 35., 'air_746211c0b532e8aa': RollingMean: 49.190476, 'air_747f375eb3900e1e': RollingMean: 5.047619, 'air_74cf22153214064c': RollingMean: 11.095238, 'air_7514d90009613cd6': RollingMean: 56.857143, 'air_754ae581ad80cc9f': RollingMean: 14.380952, 'air_75864c80d2fb334a': RollingMean: 10.714286, 'air_75bd5d1b6dc6670d': RollingMean: 12.952381, 'air_764f71040a413d4d': RollingMean: 51.142857, 'air_77488fa378cf98c3': RollingMean: 8.904762, 'air_77dfc83450cbc89c': RollingMean: 43.857143, 'air_7831b00996701c0f': RollingMean: 25.952381, 'air_789103bf53b8096b': RollingMean: 58.52381, 'air_789466e488705c93': RollingMean: 25.285714, 'air_78df4dc6a7e83e41': RollingMean: 19.238095, 'air_79afb3f52b4d062c': RollingMean: 9., 'air_79f528087f49df06': RollingMean: 32.047619, 'air_7a81bd7fadcbf3d8': RollingMean: 6.190476, 'air_7a946aada80376a4': RollingMean: 13.952381, 'air_7bacc4d36fb094c9': RollingMean: 6.285714, 'air_7bc6ca04d7b0f3b8': RollingMean: 15.52381, 'air_7bda6048a4a78837': RollingMean: 24.714286, 'air_7c7774c66fb237f7': RollingMean: 8.714286, 'air_7cc17a324ae5c7dc': RollingMean: 15.095238, 'air_7cf5a02c0e01b647': RollingMean: 31.238095, 'air_7d65049f9d275c0d': RollingMean: 9.380952, 'air_7dacea2f22afccfb': RollingMean: 29.285714, 'air_7db266904cb0d72a': RollingMean: 14.047619, 'air_7e12c5d27f44a8de': RollingMean: 24.714286, 'air_7ef9a5ea5c8fe39f': RollingMean: 10.666667, 'air_7f3dc18494bce98b': RollingMean: 13.761905, 'air_7f9e15afafcf4c75': RollingMean: 35.190476, 'air_7fbf7649eb13ad9b': RollingMean: 19.333333, 'air_800c02226e2e0288': RollingMean: 14.952381, 'air_8093d0b565e9dbdf': RollingMean: 34.809524, 'air_8110d68cc869b85e': RollingMean: 45.904762, 'air_81546875de9c8e78': RollingMean: 5.428571, 'air_81a12d67c22e012f': RollingMean: 19.285714, 'air_81bd68142db76f58': RollingMean: 28.047619, 'air_81c2600146d07d16': RollingMean: 8.714286, 'air_81c5dff692063446': RollingMean: 9.380952, 'air_820d1919cbecaa0a': RollingMean: 33.619048, 'air_82a6ae14151953ba': RollingMean: 36.47619, 'air_831658500aa7c846': RollingMean: 30.428571, 'air_832f9dbe9ee4ebd3': RollingMean: 13.809524, 'air_83db5aff8f50478e': RollingMean: 8.190476, 'air_84060403939d8216': RollingMean: 14.809524, 'air_848616680ef061bd': RollingMean: 32.238095, 'air_84f6876ff7e83ae7': RollingMean: 19.380952, 'air_8523d6a70de49e6c': RollingMean: 31.380952, 'air_859feab8e3c9f98d': RollingMean: 22.238095, 'air_85bd13a49370c392': RollingMean: 11.47619, 'air_86cfbf2624576fad': RollingMean: 7.047619, 'air_86f7b2109e4abd65': RollingMean: 53.333333, 'air_87059630ab6fe47f': RollingMean: 4.380952, 'air_87078cf7903a648c': RollingMean: 6.142857, 'air_87467487d21891dd': RollingMean: 15.761905, 'air_8764b3473ddcceaf': RollingMean: 4.333333, 'air_876d7a23c47811cb': RollingMean: 16.142857, 'air_877f79706adbfb06': RollingMean: 11.952381, 'air_87ca98aa7664de94': RollingMean: 13.47619, 'air_87f9e1024b951f01': RollingMean: 12.380952, 'air_883ca28ef0ed3d55': RollingMean: 13.619048, 'air_88c8e34baa79217b': RollingMean: 28.571429, 'air_88ca84051ba95339': RollingMean: 18.238095, 'air_88f31db64991768a': RollingMean: 9.47619, 'air_890d7e28e8eaaa11': RollingMean: 7.333333, 'air_89e7328af22efe74': RollingMean: 40.857143, 'air_8a1d21fad48374e8': RollingMean: 12.380952, 'air_8a59bb0c497b771e': RollingMean: 27.809524, 'air_8a906e5801eac81c': RollingMean: 25.095238, 'air_8b4a46dc521bfcfe': RollingMean: 25.714286, 'air_8c119d1f16049f20': RollingMean: 24.095238, 'air_8c3175aa5e4fc569': RollingMean: 60.619048, 'air_8cc350fd70ee0757': RollingMean: 31.571429, 'air_8ce035ee1d8a56a6': RollingMean: 19., 'air_8d50c64692322dff': RollingMean: 10.238095, 'air_8d61f49aa0373492': RollingMean: 45.571429, 'air_8e429650fcf7a0ae': RollingMean: 20.047619, 'air_8e4360a64dbd4c50': RollingMean: 20.333333, 'air_8e492076a1179383': RollingMean: 59.285714, 'air_8e8f42f047537154': RollingMean: 29.095238, 'air_8ec47c0f1e2c879e': RollingMean: 30.619048, 'air_8f13ef0f5e8c64dd': RollingMean: 6.952381, 'air_8f273fb9ad2fed6f': RollingMean: 15.571429, 'air_8f3b563416efc6ad': RollingMean: 12.904762, 'air_900d755ebd2f7bbd': RollingMean: 82.2, 'air_901925b628677c2e': RollingMean: 9.333333, 'air_90213bcae4afa274': RollingMean: 26.761905, 'air_90bd5de52c166767': RollingMean: 23.142857, 'air_90ed0a2f24755533': RollingMean: 38.952381, 'air_90f0efbb702d77b7': RollingMean: 30.047619, 'air_9105a29b0eb250d2': RollingMean: 18.190476, 'air_91236b89d29567af': RollingMean: 20.571429, 'air_9152d9926e5c4a3a': RollingMean: 37.47619, 'air_915558a55c2bc56c': RollingMean: 18.190476, 'air_91beafbba9382b0a': RollingMean: 34.904762, 'air_91d72e16c4bcba55': RollingMean: 15.809524, 'air_9241121af22ff1d6': RollingMean: 31.619048, 'air_929d8513e3cdb423': RollingMean: 7.619048, 'air_931a8a4321b6e7d1': RollingMean: 5.714286, 'air_9352c401d5adb01b': RollingMean: 27.904762, 'air_9387ff95e886ebc7': RollingMean: 18.380952, 'air_938ef91ecdde6878': RollingMean: 22.047619, 'air_939964477035ef0b': RollingMean: 19.571429, 'air_93b9bb641f8fc982': RollingMean: 27.047619, 'air_93dd7070c9bf5453': RollingMean: 34.095238, 'air_93ebe490d4abb8e9': RollingMean: 16.47619, 'air_9438d67241c81314': RollingMean: 35.666667, 'air_947eb2cae4f3e8f2': RollingMean: 34., 'air_9483d65e9cc9a6b7': RollingMean: 14.47619, 'air_950381108f839348': RollingMean: 30.095238, 'air_95d28905941fd4cb': RollingMean: 29.47619, 'air_95e917913cd988f3': RollingMean: 24.428571, 'air_96005f79124e12bf': RollingMean: 41.619048, 'air_965b2e0cf4119003': RollingMean: 53.904762, 'air_96743eee94114261': RollingMean: 13.904762, 'air_96773a6236d279b1': RollingMean: 25.714286, 'air_968d72c12eed09f0': RollingMean: 19.285714, 'air_96929a799b12a43e': RollingMean: 27.761905, 'air_96ec3cfe78cb0652': RollingMean: 18.047619, 'air_97159fc4e90053fe': RollingMean: 23.714286, 'air_97958e7fce98b6a3': RollingMean: 19.095238, 'air_97b2a9f975fc702c': RollingMean: 34.285714, 'air_97cf68dc1a9beac0': RollingMean: 12.428571, 'air_97e0f2feec4d577a': RollingMean: 16.142857, 'air_9828505fefc77d75': RollingMean: 14.857143, 'air_98b54e32ccddd896': RollingMean: 17.952381, 'air_990a642a3deb2903': RollingMean: 34.428571, 'air_99157b6163835eec': RollingMean: 36.285714, 'air_99a5183695b849f9': RollingMean: 35.52381, 'air_99b01136f451fc0e': RollingMean: 40.761905, 'air_99c3eae84130c1cb': RollingMean: 41.095238, 'air_9a30407764f4ff84': RollingMean: 17.428571, 'air_9a6f6e7f623003d2': RollingMean: 3.190476, 'air_9aa32b3db0fab3a5': RollingMean: 16.095238, 'air_9aa92007e3628dbc': RollingMean: 31.333333, 'air_9ae7081cb77dc51e': RollingMean: 28.714286, 'air_9b13c7feb0a0c431': RollingMean: 10.52381, 'air_9b6af3db40da4ae2': RollingMean: 28., 'air_9bbc673495e23532': RollingMean: 4.571429, 'air_9bf0ccac497c4af3': RollingMean: 43.142857, 'air_9bf595ef095572fb': RollingMean: 25.571429, 'air_9c6787aa03a45586': RollingMean: 97.809524, 'air_9ca2767761efff4d': RollingMean: 10.190476, 'air_9cd5e310f488bced': RollingMean: 13.666667, 'air_9cf2f1ba86229773': RollingMean: 32.142857, 'air_9d3482b4680aee88': RollingMean: 10.190476, 'air_9d452a881f7f2bb7': RollingMean: 9.142857, 'air_9d474ec2448c700d': RollingMean: 12.380952, 'air_9d5a980b211e1795': RollingMean: 11.285714, 'air_9d93d95720f2e831': RollingMean: 7.52381, 'air_9dc9483f717d73ee': RollingMean: 5.47619, 'air_9dd7d38b0f1760c4': RollingMean: 2.428571, 'air_9e920b758503ef54': RollingMean: 5.571429, 'air_9efaa7ded03c5a71': RollingMean: 10.238095, 'air_9f277fb7a2c1d842': RollingMean: 11.571429, 'air_9fc607777ad76b26': RollingMean: 16.761905, 'air_a083834e7ffe187e': RollingMean: 19.904762, 'air_a11473cc1eb9a27f': RollingMean: 35.714286, 'air_a17f0778617c76e2': RollingMean: 37.571429, 'air_a1fe8c588c8d2f30': RollingMean: 15., 'air_a218912784bf767d': RollingMean: 14.809524, 'air_a21ffca0bea1661a': RollingMean: 1.095238, 'air_a239a44805932bab': RollingMean: 33.047619, 'air_a24bf50c3e90d583': RollingMean: 20.142857, 'air_a2567267116a3b75': RollingMean: 16.142857, 'air_a257c9749d8d0ff6': RollingMean: 19., 'air_a271c9ba19e81d17': RollingMean: 27.52381, 'air_a2b29aa7feb4e36f': RollingMean: 16.761905, 'air_a304330715435390': RollingMean: 7.714286, 'air_a33461f4392ec62c': RollingMean: 28.857143, 'air_a373500730e2a9e0': RollingMean: 10.47619, 'air_a38f25e3399d1b25': RollingMean: 43.571429, 'air_a41b032371a63427': RollingMean: 11.571429, 'air_a49f1cf0634f13e5': RollingMean: 24.238095, 'air_a510dcfe979f09eb': RollingMean: 14.571429, 'air_a546cbf478a8b6e4': RollingMean: 28.095238, 'air_a55d17bd3f3033cb': RollingMean: 12.952381, 'air_a563896da3777078': RollingMean: 21.714286, 'air_a678e5b144ca24ce': RollingMean: 15.52381, 'air_a7404a854919e990': RollingMean: 8., 'air_a8533b7a992bb0ca': RollingMean: 17.619048, 'air_a85f0c0c889f6b7e': RollingMean: 42.761905, 'air_a85f8c0bfd61889f': RollingMean: 12.952381, 'air_a88ac559064dec08': RollingMean: 33.809524, 'air_a9133955abccf071': RollingMean: 27.952381, 'air_a9178f19da58fe99': RollingMean: 6.857143, 'air_a9a380530c1e121f': RollingMean: 40.52381, 'air_aa0049fe3cc6f4d6': RollingMean: 9.380952, 'air_ab3ae0e410b20069': RollingMean: 16.52381, 'air_ab9746a0f83084b7': RollingMean: 8.857143, 'air_abcdc8115988a010': RollingMean: 11.809524, 'air_abf06fcca748dca5': RollingMean: 8.428571, 'air_ac7a7427c9ae12a5': RollingMean: 59.095238, 'air_ad13e71e21235131': RollingMean: 19.666667, 'air_ad60f6b76c9df7ed': RollingMean: 22.809524, 'air_ad7777590c884721': RollingMean: 8.142857, 'air_add9a575623726c8': RollingMean: 41.714286, 'air_ade6e836ffd1da64': RollingMean: 8.857143, 'air_aed3a8b49abe4a48': RollingMean: 5.857143, 'air_af03c277a167b2bd': RollingMean: 25.904762, 'air_af24e3e817dea1e5': RollingMean: 15.095238, 'air_af63df35857b16e6': RollingMean: 23.142857, 'air_b0a6a4c5e95c74cf': RollingMean: 17.190476, 'air_b162fb07fbbdea33': RollingMean: 14.809524, 'air_b192fb5310436005': RollingMean: 7.666667, 'air_b1a72bf1ebf4b8ef': RollingMean: 65.095238, 'air_b1bb1fae86617d7a': RollingMean: 35.333333, 'air_b1d822f75c9fc615': RollingMean: 10.142857, 'air_b2395df0e874078d': RollingMean: 6.47619, 'air_b23d0f519291247d': RollingMean: 26.428571, 'air_b259b4e4a51a690d': RollingMean: 21.047619, 'air_b28bed4b2e7167b7': RollingMean: 16., 'air_b2a639cc7e02edf1': RollingMean: 19.47619, 'air_b2d8bc9c88b85f96': RollingMean: 16.666667, 'air_b2d97bd2337c5ba7': RollingMean: 31.952381, 'air_b2dcec37b83e2494': RollingMean: 7.904762, 'air_b30fffd7ab1e75a5': RollingMean: 10.047619, 'air_b3180b74332ba886': RollingMean: 11.52381, 'air_b3a824511477a4ed': RollingMean: 6.380952, 'air_b439391e72899756': RollingMean: 19.190476, 'air_b45b8e456f53942a': RollingMean: 11., 'air_b4f32bcc399da2b9': RollingMean: 29.190476, 'air_b5598d12d1b84890': RollingMean: 5.380952, 'air_b5bdd318005d9aa4': RollingMean: 36.809524, 'air_b60cc7d6aee68194': RollingMean: 11.52381, 'air_b711b43ae472cb6b': RollingMean: 19.809524, 'air_b7fa3d2fca744dd2': RollingMean: 41.333333, 'air_b80fed1a07c817d2': RollingMean: 4.666667, 'air_b88192b35ac03c24': RollingMean: 15.666667, 'air_b8925441167c3152': RollingMean: 2.047619, 'air_b8a5ee69e5fdcc5b': RollingMean: 38., 'air_b8d9e1624baaadc2': RollingMean: 5.952381, 'air_b9e27558fb8bd5c4': RollingMean: 12.47619, 'air_ba495cccc8f0f237': RollingMean: 15.904762, 'air_ba937bf13d40fb24': RollingMean: 17.285714, 'air_bac5f4441db21db9': RollingMean: 40.761905, 'air_baf28ac9f13a307d': RollingMean: 15.619048, 'air_bb09595bab7d5cfb': RollingMean: 27.809524, 'air_bb26d6d079594414': RollingMean: 11.571429, 'air_bb4ff06cd661ee9b': RollingMean: 34.428571, 'air_bbe1c1a47e09f161': RollingMean: 1.52381, 'air_bc991c51d6613745': RollingMean: 20.095238, 'air_bc9a129e11a2efe0': RollingMean: 31.666667, 'air_bcce1ea4350b7b72': RollingMean: 18.52381, 'air_bd74a9222edfdfe1': RollingMean: 12.904762, 'air_bdd32aa407c16335': RollingMean: 16.428571, 'air_bebd55ed63ab2422': RollingMean: 8.571429, 'air_bed603c423b7d9d4': RollingMean: 5.428571, 'air_bedd35489e666605': RollingMean: 39.714286, 'air_bf13014b6e3e60ca': RollingMean: 32.952381, 'air_bf21b8350771879b': RollingMean: 26.809524, 'air_bf617aa68d5f1cfa': RollingMean: 6.809524, 'air_bf7591560077332d': RollingMean: 8.904762, 'air_bfafaed35e213fd7': RollingMean: 11.952381, 'air_bfda7731a6c6fc61': RollingMean: 19.428571, 'air_c027e2b560442808': RollingMean: 15., 'air_c0385db498b391e5': RollingMean: 31.904762, 'air_c1d5d165c055b866': RollingMean: 30., 'air_c1ff20617c54fee7': RollingMean: 7.809524, 'air_c225148c0fcc5c72': RollingMean: 36.52381, 'air_c2626f5f86d57342': RollingMean: 16.904762, 'air_c26f027b5bc1f081': RollingMean: 5.142857, 'air_c28983412a7eefcf': RollingMean: 38.809524, 'air_c2c8435bdb3516d4': RollingMean: 30.619048, 'air_c31472d14e29cee8': RollingMean: 13.142857, 'air_c3585b0fba3998d0': RollingMean: 8.904762, 'air_c3bc011cca3bec65': RollingMean: 7.666667, 'air_c3dcaf3aeb18e20e': RollingMean: 17., 'air_c47aa7493b15f297': RollingMean: 20.571429, 'air_c4fa5c562d5409ca': RollingMean: 13.52381, 'air_c52c63c781fe48f6': RollingMean: 27.761905, 'air_c5459218282bedd5': RollingMean: 23.809524, 'air_c66dbd2c37832d00': RollingMean: 17.285714, 'air_c6a164dd4060e960': RollingMean: 12.952381, 'air_c6aa2efba0ffc8eb': RollingMean: 29.571429, 'air_c6ffd6a93e6b68d6': RollingMean: 15.571429, 'air_c73d319ffabf287a': RollingMean: 16.095238, 'air_c759b6abeb552160': RollingMean: 9.095238, 'air_c77ee2b7d36da265': RollingMean: 42.095238, 'air_c7d30ab0e07f31d5': RollingMean: 16.095238, 'air_c7f78b4f3cba33ff': RollingMean: 23.952381, 'air_c8265ecc116f2284': RollingMean: 8.428571, 'air_c88467d88b2c8ecd': RollingMean: 19.952381, 'air_c8a657c8c5c93d69': RollingMean: 7.142857, 'air_c8c0ef02ed72053f': RollingMean: 27.190476, 'air_c8fe396d6c46275d': RollingMean: 16.142857, 'air_c92745dfdd2ec68a': RollingMean: 18.52381, 'air_c9ed65554b6edffb': RollingMean: 11.809524, 'air_c9f6de13be8b8f25': RollingMean: 3.809524, 'air_ca1315af9e073bd1': RollingMean: 42.380952, 'air_ca6ae8d49a2f1eaf': RollingMean: 21.380952, 'air_ca957d3a1529fbd3': RollingMean: 33.619048, 'air_cadf9cfb510a1d78': RollingMean: 32.666667, 'air_caf996ac27206301': RollingMean: 4.52381, 'air_cb083b4789a8d3a2': RollingMean: 17.52381, 'air_cb25551c4cd8d9f3': RollingMean: 9.428571, 'air_cb7467aed805e7fe': RollingMean: 36.095238, 'air_cb935ff8610ba3d3': RollingMean: 5.952381, 'air_cbe139af83feb388': RollingMean: 10.428571, 'air_cbe867adcf44e14f': RollingMean: 15.380952, 'air_cc1a0e985ce63711': RollingMean: 30., 'air_cc35590cd1da8554': RollingMean: 18.190476, 'air_ccd19a5bc5573ae5': RollingMean: 35.904762, 'air_cd4b301d5d3918d8': RollingMean: 6.47619, 'air_cd5f54969be9ed08': RollingMean: 7.142857, 'air_ced6297e5bdf5130': RollingMean: 25.809524, 'air_cf2229e64408d9fe': RollingMean: 16.571429, 'air_cf22e368c1a71d53': RollingMean: 36.428571, 'air_cf5ab75a0afb8af9': RollingMean: 45.190476, 'air_cfcc94797d2b5d3d': RollingMean: 17.333333, 'air_cfdeb326418194ff': RollingMean: 14.761905, 'air_d00161e19f08290b': RollingMean: 26.666667, 'air_d00a15343325e5f7': RollingMean: 17.571429, 'air_d07e57b21109304a': RollingMean: 11.142857, 'air_d0a1e69685259c92': RollingMean: 33.190476, 'air_d0a7bd3339c3d12a': RollingMean: 36.714286, 'air_d0e8a085d8dc83aa': RollingMean: 7.952381, 'air_d138b593ebda55cc': RollingMean: 5.380952, 'air_d1418d6fd6d634f2': RollingMean: 15.714286, 'air_d186b2cb0b9ce022': RollingMean: 13.380952, 'air_d1f20424f76cc78e': RollingMean: 19.333333, 'air_d34c0861a2be94cb': RollingMean: 43.142857, 'air_d3e7b5952cd09ccb': RollingMean: 19.714286, 'air_d44d210d2994f01b': RollingMean: 5.714286, 'air_d473620754bf9fc2': RollingMean: 12.904762, 'air_d477b6339b8ce69f': RollingMean: 8.047619, 'air_d4981cdde163b172': RollingMean: 24.714286, 'air_d4b5a4b04c5f2d04': RollingMean: 12.619048, 'air_d4d218b451f82c3d': RollingMean: 9.904762, 'air_d500b48a8735fbd3': RollingMean: 17.47619, 'air_d54d6fcb116fbed3': RollingMean: 4.428571, 'air_d5e0a20370c325c7': RollingMean: 29.190476, 'air_d63cfa6d6ab78446': RollingMean: 18.238095, 'air_d69b08a175bc0387': RollingMean: 10.809524, 'air_d6b3e67261f07646': RollingMean: 14.666667, 'air_d8abb9e490abf94f': RollingMean: 12.380952, 'air_d97dabf7aae60da5': RollingMean: 33.190476, 'air_d98380a4aeb0290b': RollingMean: 42.761905, 'air_daa7947e1c47f5ed': RollingMean: 34., 'air_dabfbd0ec951925a': RollingMean: 7.142857, 'air_dad0b6a36138f309': RollingMean: 5.571429, 'air_db1233ad855b34d5': RollingMean: 25.52381, 'air_db4b38ebe7a7ceff': RollingMean: 23.714286, 'air_db80363d35f10926': RollingMean: 28.714286, 'air_dbf64f1ce38c7442': RollingMean: 15.47619, 'air_dc0e080ba0a5e5af': RollingMean: 9.047619, 'air_dc71c6cc06cd1aa2': RollingMean: 6.190476, 'air_de692863bb2dd758': RollingMean: 21.238095, 'air_de803f7e324936b8': RollingMean: 25.619048, 'air_de88770300008624': RollingMean: 17.428571, 'air_dea0655f96947922': RollingMean: 35.809524, 'air_df507aec929ce5f6': RollingMean: 23.285714, 'air_df554c4527a1cfe6': RollingMean: 52.952381, 'air_df5cf5cd03eb68d0': RollingMean: 7.380952, 'air_df843e6b22e8d540': RollingMean: 11.952381, 'air_df9355c47c5df9d3': RollingMean: 27.52381, 'air_dfad598ff642dab7': RollingMean: 25.952381, 'air_dfe068a1bf85f395': RollingMean: 37.857143, 'air_e00fe7853c0100d6': RollingMean: 20.904762, 'air_e0118664da63a2d0': RollingMean: 16.333333, 'air_e01d99390355408d': RollingMean: 11.333333, 'air_e053c561f32acc28': RollingMean: 19.666667, 'air_e08b9cf82057a170': RollingMean: 33.333333, 'air_e0aee25b56a069f2': RollingMean: 14.428571, 'air_e0e69668214ff972': RollingMean: 9.904762, 'air_e0f241bd406810c0': RollingMean: 32.285714, 'air_e1b76fcb5208fb6b': RollingMean: 13.52381, 'air_e2208a79e2678432': RollingMean: 47.714286, 'air_e270aff84ac7e4c8': RollingMean: 24.571429, 'air_e3020992d5fe5dfd': RollingMean: 12.190476, 'air_e34c631c766f2766': RollingMean: 24.47619, 'air_e42bdc3377d1eee7': RollingMean: 21.190476, 'air_e483f5b3c4f310e0': RollingMean: 5.380952, 'air_e524c6a9e06cc3a1': RollingMean: 8.238095, 'air_e55abd740f93ecc4': RollingMean: 45.666667, 'air_e57dd6884595f60d': RollingMean: 38.571429, 'air_e58f669b6f1a08ce': RollingMean: 10.761905, 'air_e5cf003abcc5febb': RollingMean: 11.428571, 'air_e64de0a6bf0739af': RollingMean: 45.809524, 'air_e657ca554b0c008c': RollingMean: 22.571429, 'air_e700e390226d9985': RollingMean: 16.619048, 'air_e76a668009c5dabc': RollingMean: 7.952381, 'air_e7d2ac6d53d1b744': RollingMean: 9.952381, 'air_e7fbee4e3cfe65c5': RollingMean: 33.095238, 'air_e88bbe2ede3467aa': RollingMean: 24.380952, 'air_e89735e80d614a7e': RollingMean: 32.428571, 'air_e8ed9335d0c38333': RollingMean: 26.52381, 'air_e9ebf7fc520ac76a': RollingMean: 28.904762, 'air_ea6d0c3acf00b22a': RollingMean: 21.714286, 'air_ea7c16131980c837': RollingMean: 7.190476, 'air_eb120e6d384a17a8': RollingMean: 42.809524, 'air_eb20a89bba7dd3d0': RollingMean: 2.952381, 'air_eb2d2653586315dd': RollingMean: 32.380952, 'air_eb5788dba285e725': RollingMean: 23.52381, 'air_ebd31e812960f517': RollingMean: 26.47619, 'air_ebe02c3090271fa9': RollingMean: 11.333333, 'air_ec0fad2def4dcff0': RollingMean: 15.904762, 'air_eca4a5a191e8d993': RollingMean: 25.714286, 'air_eca5e0064dc9314a': RollingMean: 34.571429, 'air_ecab54b57a71b10d': RollingMean: 13.285714, 'air_eceb97ad6a7d4c07': RollingMean: 26.571429, 'air_ecf7f141339f1d57': RollingMean: 18.428571, 'air_eda179770dfa9f91': RollingMean: 9.428571, 'air_edd5e3d696a5811b': RollingMean: 42.761905, 'air_ee3a01f0c71a769f': RollingMean: 26.761905, 'air_ee3ba9af184c6c82': RollingMean: 16.904762, 'air_eec5e572b9eb9c23': RollingMean: 14.52381, 'air_eeeadee005c006a2': RollingMean: 11.952381, 'air_ef47430bcd6f6a89': RollingMean: 13.809524, 'air_ef789667e2e6fe96': RollingMean: 33.190476, 'air_ef920fa6f4b085f6': RollingMean: 37.857143, 'air_efc80d3f96b3aff7': RollingMean: 9.190476, 'air_efd70b04de878f25': RollingMean: 31.428571, 'air_efef1e3daecce07e': RollingMean: 40.666667, 'air_f068442ebb6c246c': RollingMean: 11.47619, 'air_f0c7272956e62f12': RollingMean: 3.714286, 'air_f0fb0975bdc2cdf9': RollingMean: 11.285714, 'air_f1094dbf2aef85d9': RollingMean: 6.142857, 'air_f180301886c21375': RollingMean: 14.142857, 'air_f183a514cb8ff4fa': RollingMean: 43.238095, 'air_f1f9027d4fa8f653': RollingMean: 28.714286, 'air_f267dd70a6a6b5d3': RollingMean: 57.714286, 'air_f26f36ec4dc5adb0': RollingMean: 38.809524, 'air_f2985de32bb792e0': RollingMean: 31.47619, 'air_f2c5a1f24279c531': RollingMean: 15.428571, 'air_f3602e4fa2f12993': RollingMean: 12.52381, 'air_f3f9824b7d70c3cf': RollingMean: 15.809524, 'air_f4936b91c9addbf0': RollingMean: 15.666667, 'air_f593fa60ac3541e2': RollingMean: 12.285714, 'air_f690c42545146e0a': RollingMean: 12.142857, 'air_f6b2489ccf873c3b': RollingMean: 15.857143, 'air_f6bfd27e2e174d16': RollingMean: 12.714286, 'air_f6cdaf7b7fdc6d78': RollingMean: 8.714286, 'air_f8233ad00755c35c': RollingMean: 30.333333, 'air_f85e21e543cf44f2': RollingMean: 5.666667, 'air_f88898cd09f40496': RollingMean: 7.952381, 'air_f911308e19d64236': RollingMean: 43.666667, 'air_f9168b23fdfc1e52': RollingMean: 17.47619, 'air_f927b2da69a82341': RollingMean: 10.47619, 'air_f957c6d6467d4d90': RollingMean: 10.47619, 'air_f96765e800907c77': RollingMean: 45.190476, 'air_fa12b40b02fecfd8': RollingMean: 15.52381, 'air_fa4ffc9057812fa2': RollingMean: 4.904762, 'air_fab092c35776a9b1': RollingMean: 10.809524, 'air_fb44f566d4f64a4e': RollingMean: 15.809524, 'air_fbadf737162a5ce3': RollingMean: 15.047619, 'air_fc477473134e9ae5': RollingMean: 13.333333, 'air_fcd4492c83f1c6b9': RollingMean: 21.714286, 'air_fcfbdcf7b1f82c6e': RollingMean: 37.47619, 'air_fd154088b1de6fa7': RollingMean: 6.52381, 'air_fd6aac1043520e83': RollingMean: 33.380952, 'air_fdc02ec4a3d21ea4': RollingMean: 7.619048, 'air_fdcfef8bd859f650': RollingMean: 3.52381, 'air_fe22ef5a9cbef123': RollingMean: 23.142857, 'air_fe58c074ec1445ea': RollingMean: 31.285714, 'air_fea5dc9594450608': RollingMean: 15.428571, 'air_fee8dcf4d619598e': RollingMean: 26.190476, 'air_fef9ccb3ba0da2f7': RollingMean: 8.857143, 'air_ffcc2d5087e1b476': RollingMean: 19.952381, 'air_fff68b929994bfbd': RollingMean: 4.428571}), 'how': RollingMean: 0., 'target_name': 'target'} ~['area_name', 'date', 'genre_name', 'latitude', 'longitude', 'store_id'] {'blacklist': {'area_name', 'date', 'genre_name', 'latitude', 'longitude', 'store_id'}} StandardScaler {'counts': Counter({'target_rollingmean_21_by_store_id': 252108, 'target_rollingmean_14_by_store_id': 252108, 'target_rollingmean_7_by_store_id': 252108, 'weekday': 252108, 'is_weekend': 252108}), 'means': defaultdict(<class 'float'>, {'is_weekend': 0.27469576530693085, 'target_rollingmean_14_by_store_id': 20.897453818387582, 'target_rollingmean_21_by_store_id': 20.901790561001423, 'target_rollingmean_7_by_store_id': 20.895968753375726, 'weekday': 3.0196780744759444}), 'vars': defaultdict(<class 'float'>, {'is_weekend': 0.1992380018293697, 'target_rollingmean_14_by_store_id': 139.4838567930601, 'target_rollingmean_21_by_store_id': 135.6375578980582, 'target_rollingmean_7_by_store_id': 148.74195644058028, 'weekday': 3.7017047339732057})} LinearRegression {'_weights': {'target_rollingmean_21_by_store_id': 8.267976730511927, 'target_rollingmean_14_by_store_id': 3.68602087744672, 'target_rollingmean_7_by_store_id': -0.7363408297986024, 'weekday': 6.318523886308725, 'is_weekend': -2.378332613642489}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 19.93106280511352, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Squared({}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 252108})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; }","title":"The art of using pipelines"},{"location":"faq/faq/","text":"Frequently Asked Questions \u00b6 Do all classifiers support multi-class classification? No, they don't. Although binary classification can be seen as a special case of multi-class classification, there are many optimizations that can be performed if we know that there are only two classes. It would be annoying to have to check whether this is the case in an online setting. All in all we find that separating both cases leads to much cleaner code. Note that the multiclass module contains wrapper models that enable you to perform multi-class classification with binary classifiers. How do I know if a classifier supports multi-class classification? Each classifier that is part of river is either a base.BinaryClassifier or a base.MultiClassifier . You can use Python's isinstance function to check for a particular classifier, as so: >>> from river import base >>> from river import linear_model >>> classifier = linear_model . LogisticRegression () >>> isinstance ( classifier , base . BinaryClassifier ) True >>> isinstance ( classifier , base . MultiClassifier ) False Why doesn't river do any input validation? Python encourages a coding style called EAFP , which stands for \"Easier to Ask for Forgiveness than Permission\". The idea is to assume that runtime errors don't occur, and instead use try/expects to catch errors. The great benefit is that we don't have to drown our code with if statements, which is symptomatic of the LBYL style , which stands for \"look before you leap\". This makes our implementations much more readable than, say, scikit-learn, which does a lot of input validation. The catch is that users have to be careful to use sane inputs. As always, there is no free lunch ! What about reinforcement learning? Reinforcement learning works in an online manner because of the nature of the task. Reinforcement learning can be therefore be seen as a subcase of online machine learning. However, we prefer not to support it because there are already many existing opensource libraries dedicated to it. What are the differences between scikit-learn's online learning algorithm which have a partial_fit method and their equivalents in river? The algorithms from sklearn that support incremental learning are mostly meant for mini-batch learning. In a pure streaming context where the observations arrive one by one, then river is much faster than sklearn . This is mostly because sklearn incurs a lot of overhead by performing data checks. Also, sklearn assumes that you're always using the same number of features. This is not the case with river because it use dictionaries which allows you to drop and add features as you wish. How do I save and load models? >>> from river import ensemble >>> import pickle >>> model = ensemble . AdaptiveRandomForestClassifier () # save >>> with open ( 'model.pkl' , 'wb' ) as f : ... pickle . dump ( model , f ) # load >>> with open ( 'model.pkl' , 'rb' ) as f : ... model = pickle . load ( f ) We also encourage you to try out dill and cloudpickle . What about neural networks? There are many great open-source libraries for building neural network models. We don't feel that we can bring anything of value to the existing Python ecosystem. However, we are open to implementing compatibility wrappers for popular libraries such as PyTorch and Keras. Who are the authors of this library? We are research engineers, graduate students, PhDs and machine learning researchers. The members of the develompent team are mainly located in France, Brazil and New Zealand.","title":"Frequently Asked Questions"},{"location":"faq/faq/#frequently-asked-questions","text":"Do all classifiers support multi-class classification? No, they don't. Although binary classification can be seen as a special case of multi-class classification, there are many optimizations that can be performed if we know that there are only two classes. It would be annoying to have to check whether this is the case in an online setting. All in all we find that separating both cases leads to much cleaner code. Note that the multiclass module contains wrapper models that enable you to perform multi-class classification with binary classifiers. How do I know if a classifier supports multi-class classification? Each classifier that is part of river is either a base.BinaryClassifier or a base.MultiClassifier . You can use Python's isinstance function to check for a particular classifier, as so: >>> from river import base >>> from river import linear_model >>> classifier = linear_model . LogisticRegression () >>> isinstance ( classifier , base . BinaryClassifier ) True >>> isinstance ( classifier , base . MultiClassifier ) False Why doesn't river do any input validation? Python encourages a coding style called EAFP , which stands for \"Easier to Ask for Forgiveness than Permission\". The idea is to assume that runtime errors don't occur, and instead use try/expects to catch errors. The great benefit is that we don't have to drown our code with if statements, which is symptomatic of the LBYL style , which stands for \"look before you leap\". This makes our implementations much more readable than, say, scikit-learn, which does a lot of input validation. The catch is that users have to be careful to use sane inputs. As always, there is no free lunch ! What about reinforcement learning? Reinforcement learning works in an online manner because of the nature of the task. Reinforcement learning can be therefore be seen as a subcase of online machine learning. However, we prefer not to support it because there are already many existing opensource libraries dedicated to it. What are the differences between scikit-learn's online learning algorithm which have a partial_fit method and their equivalents in river? The algorithms from sklearn that support incremental learning are mostly meant for mini-batch learning. In a pure streaming context where the observations arrive one by one, then river is much faster than sklearn . This is mostly because sklearn incurs a lot of overhead by performing data checks. Also, sklearn assumes that you're always using the same number of features. This is not the case with river because it use dictionaries which allows you to drop and add features as you wish. How do I save and load models? >>> from river import ensemble >>> import pickle >>> model = ensemble . AdaptiveRandomForestClassifier () # save >>> with open ( 'model.pkl' , 'wb' ) as f : ... pickle . dump ( model , f ) # load >>> with open ( 'model.pkl' , 'rb' ) as f : ... model = pickle . load ( f ) We also encourage you to try out dill and cloudpickle . What about neural networks? There are many great open-source libraries for building neural network models. We don't feel that we can bring anything of value to the existing Python ecosystem. However, we are open to implementing compatibility wrappers for popular libraries such as PyTorch and Keras. Who are the authors of this library? We are research engineers, graduate students, PhDs and machine learning researchers. The members of the develompent team are mainly located in France, Brazil and New Zealand.","title":"Frequently Asked Questions"},{"location":"getting-started/getting-started/","text":"Getting started \u00b6 First things first, make sure you have installed river . In river , features are represented with dictionaries, where the keys correspond to the features names. For instance: import datetime as dt x = { 'shop' : 'Ikea' , 'city' : 'Stockholm' , 'date' : dt . datetime ( 2020 , 6 , 1 ), 'sales' : 42 } It is up to you, the user, to decide how to stream your data. river offers a stream module which has various utilities for handling streaming data, such as stream.iter_csv . For the sake of example, river also provides a datasets module which contains various streaming datasets. For example, the datasets.Phishing dataset contains records of phishing attempts on web pages. from river import datasets dataset = datasets . Phishing () print ( dataset ) Phishing websites. This dataset contains features from web pages that are classified as phishing or not. Name Phishing Task Binary classification Samples 1,250 Features 9 Sparse False Path /Users/max.halford/projects/river/river/datasets/phishing.csv.gz The dataset is a streaming dataset, and therefore doesn't sit in memory. Instead, we can loop over each sample with a for loop: for x , y in dataset : pass print ( x ) {'empty_server_form_handler': 1.0, 'popup_window': 0.5, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 1.0, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 0, 'ip_in_url': 0} print ( y ) False Typically, models learn via a learn_one(x, y) method, which takes as input some features and a target value. Being able to learn with a single instance gives a lot of flexibility. For instance, a model can be updated whenever a new sample arrives from a stream. To exemplify this, let's train a logistic regression on the above dataset. from river import linear_model model = linear_model . LogisticRegression () for x , y in dataset : model . learn_one ( x , y ) Predictions can be obtained by calling a model's predict_one method. In the case of a classifier, we can also use predict_proba_one to produce probability estimates. model = linear_model . LogisticRegression () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) print ( y_pred ) {False: 0.7731541581376543, True: 0.22684584186234572} The metrics module gives access to many metrics that are commonly used in machine learning. Like the rest of river , these metrics can be updated with one element at a time: from river import metrics model = linear_model . LogisticRegression () metric = metrics . ROCAUC () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.893565 A common way to improve the performance of a logistic regression is to scale the data. This can be done by using a preprocessing.StandardScaler . In particular, we can define a pipeline to organise our model into a sequence of steps: from river import compose from river import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) model StandardScaler {'counts': Counter(), 'means': defaultdict(<class 'float'>, {}), 'vars': defaultdict(<class 'float'>, {})} LogisticRegression {'_weights': {}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 0.0, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Log({'weight_pos': 1.0, 'weight_neg': 1.0}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 0})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } metric = metrics . ROCAUC () for x , y in datasets . Phishing (): y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.950363 As we can see, the model is performing much better now that the data is being scaled. Under the hood, the standard scaler maintains a running average and a running variance for each feature in the dataset. Each feature is thus scaled according to the average and the variance seen up to every given point in time. This concludes this short guide to getting started with river . There is a lot more to discover and understand. Head towards the user guide for recipes on how to perform common machine learning tasks. You may also consult the API reference , which is a catalogue of all the modules that river exposes. Finally, the examples section contains comprehensive examples for various usecases.","title":"In a nutshell"},{"location":"getting-started/getting-started/#getting-started","text":"First things first, make sure you have installed river . In river , features are represented with dictionaries, where the keys correspond to the features names. For instance: import datetime as dt x = { 'shop' : 'Ikea' , 'city' : 'Stockholm' , 'date' : dt . datetime ( 2020 , 6 , 1 ), 'sales' : 42 } It is up to you, the user, to decide how to stream your data. river offers a stream module which has various utilities for handling streaming data, such as stream.iter_csv . For the sake of example, river also provides a datasets module which contains various streaming datasets. For example, the datasets.Phishing dataset contains records of phishing attempts on web pages. from river import datasets dataset = datasets . Phishing () print ( dataset ) Phishing websites. This dataset contains features from web pages that are classified as phishing or not. Name Phishing Task Binary classification Samples 1,250 Features 9 Sparse False Path /Users/max.halford/projects/river/river/datasets/phishing.csv.gz The dataset is a streaming dataset, and therefore doesn't sit in memory. Instead, we can loop over each sample with a for loop: for x , y in dataset : pass print ( x ) {'empty_server_form_handler': 1.0, 'popup_window': 0.5, 'https': 1.0, 'request_from_other_domain': 1.0, 'anchor_from_other_domain': 1.0, 'is_popular': 0.5, 'long_url': 0.0, 'age_of_domain': 0, 'ip_in_url': 0} print ( y ) False Typically, models learn via a learn_one(x, y) method, which takes as input some features and a target value. Being able to learn with a single instance gives a lot of flexibility. For instance, a model can be updated whenever a new sample arrives from a stream. To exemplify this, let's train a logistic regression on the above dataset. from river import linear_model model = linear_model . LogisticRegression () for x , y in dataset : model . learn_one ( x , y ) Predictions can be obtained by calling a model's predict_one method. In the case of a classifier, we can also use predict_proba_one to produce probability estimates. model = linear_model . LogisticRegression () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) print ( y_pred ) {False: 0.7731541581376543, True: 0.22684584186234572} The metrics module gives access to many metrics that are commonly used in machine learning. Like the rest of river , these metrics can be updated with one element at a time: from river import metrics model = linear_model . LogisticRegression () metric = metrics . ROCAUC () for x , y in dataset : y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.893565 A common way to improve the performance of a logistic regression is to scale the data. This can be done by using a preprocessing.StandardScaler . In particular, we can define a pipeline to organise our model into a sequence of steps: from river import compose from river import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) model StandardScaler {'counts': Counter(), 'means': defaultdict(<class 'float'>, {}), 'vars': defaultdict(<class 'float'>, {})} LogisticRegression {'_weights': {}, '_y_name': None, 'clip_gradient': 1000000000000.0, 'initializer': Zeros (), 'intercept': 0.0, 'intercept_init': 0.0, 'intercept_lr': Constant({'learning_rate': 0.01}), 'l2': 0.0, 'loss': Log({'weight_pos': 1.0, 'weight_neg': 1.0}), 'optimizer': SGD({'lr': Constant({'learning_rate': 0.01}), 'n_iterations': 0})} .estimator { padding: 1em; border-style: solid; background: white; }</p> <p>.pipeline { display: flex; flex-direction: column; align-items: center; background: linear-gradient(#000, #000) no-repeat center / 3px 100%; }</p> <p>.union { display: flex; flex-direction: row; align-items: center; justify-content: center; padding: 1em; border-style: solid; background: white }</p> <p>/<em> Vertical spacing between steps </em>/</p> <p>.estimator + .estimator, .estimator + .union, .union + .estimator { margin-top: 2em; }</p> <p>.union &gt; .estimator { margin-top: 0; }</p> <p>/<em> Spacing within a union of estimators </em>/</p> <p>.union &gt; .estimator + .estimator, .pipeline + .estimator, .estimator + .pipeline, .pipeline + .pipeline { margin-left: 1em; }</p> <p>/<em> Typography </em>/ .estimator-params { display: block; white-space: pre-wrap; font-size: 120%; margin-bottom: -1em; }</p> <p>.estimator &gt; code { background-color: white !important; }</p> <p>.estimator-name { display: inline; margin: 0; font-size: 130%; }</p> <p>/<em> Toggle </em>/</p> <p>summary { display: flex; align-items:center; cursor: pointer; }</p> <p>summary &gt; div { width: 100%; } metric = metrics . ROCAUC () for x , y in datasets . Phishing (): y_pred = model . predict_proba_one ( x ) model . learn_one ( x , y ) metric . update ( y , y_pred ) metric ROCAUC: 0.950363 As we can see, the model is performing much better now that the data is being scaled. Under the hood, the standard scaler maintains a running average and a running variance for each feature in the dataset. Each feature is thus scaled according to the average and the variance seen up to every given point in time. This concludes this short guide to getting started with river . There is a lot more to discover and understand. Head towards the user guide for recipes on how to perform common machine learning tasks. You may also consult the API reference , which is a catalogue of all the modules that river exposes. Finally, the examples section contains comprehensive examples for various usecases.","title":"Getting started"},{"location":"getting-started/installation/","text":"Installation \u00b6 river is intended to work with Python 3.6 or above . Installation can be done via pip : pip install river You can install the latest development version from GitHub as so: pip install git+https://github.com/online-ml/river --upgrade Or, through SSH: pip install git+ssh://git@github.com/online-ml/river.git --upgrade Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"},{"location":"getting-started/installation/#installation","text":"river is intended to work with Python 3.6 or above . Installation can be done via pip : pip install river You can install the latest development version from GitHub as so: pip install git+https://github.com/online-ml/river --upgrade Or, through SSH: pip install git+ssh://git@github.com/online-ml/river.git --upgrade Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"},{"location":"releases/0.0.2/","text":"0.0.2 - 2019-02-13 \u00b6 PyPI GitHub compat \u00b6 Added sklearn wrappers. ensemble \u00b6 Added ensemble.HedgeClassifier . feature_selection \u00b6 Added feature_selection.RandomDiscarder . feature_extraction \u00b6 Added feature_extraction.TargetEncoder . impute \u00b6 Added impute.NumericImputer . optim \u00b6 Added optim.AbsoluteLoss . Added optim.HingeLoss . Added optim.EpsilonInsensitiveHingeLoss . stats \u00b6 Added stats.NUnique . Added stats.Min . Added stats.Max . Added stats.PeakToPeak . Added stats.Kurtosis . Added stats.Skew . Added stats.Sum . Added stats.EWMean . Made sure the running statistics produce the same results as pandas.DataFrame.rolling method.","title":"0.0.2 - 2019-02-13"},{"location":"releases/0.0.2/#002-2019-02-13","text":"PyPI GitHub","title":"0.0.2 - 2019-02-13"},{"location":"releases/0.0.2/#compat","text":"Added sklearn wrappers.","title":"compat"},{"location":"releases/0.0.2/#ensemble","text":"Added ensemble.HedgeClassifier .","title":"ensemble"},{"location":"releases/0.0.2/#feature_selection","text":"Added feature_selection.RandomDiscarder .","title":"feature_selection"},{"location":"releases/0.0.2/#feature_extraction","text":"Added feature_extraction.TargetEncoder .","title":"feature_extraction"},{"location":"releases/0.0.2/#impute","text":"Added impute.NumericImputer .","title":"impute"},{"location":"releases/0.0.2/#optim","text":"Added optim.AbsoluteLoss . Added optim.HingeLoss . Added optim.EpsilonInsensitiveHingeLoss .","title":"optim"},{"location":"releases/0.0.2/#stats","text":"Added stats.NUnique . Added stats.Min . Added stats.Max . Added stats.PeakToPeak . Added stats.Kurtosis . Added stats.Skew . Added stats.Sum . Added stats.EWMean . Made sure the running statistics produce the same results as pandas.DataFrame.rolling method.","title":"stats"},{"location":"releases/0.0.3/","text":"0.0.3 - 2019-03-21 \u00b6 PyPI GitHub base \u00b6 Calling fit_one now returns the calling instance, not the out-of-fold prediction/transform; fit_predict_one , fit_predict_proba_one , and fit_transform_one are available to reproduce the previous behavior. Binary classifiers now output a dict with probabilities for False and True when calling predict_proba_one , which solves the interface issues of having multi-class classifiers do binary classification. compat \u00b6 Added compat.convert_river_to_sklearn . compose \u00b6 Added compose.BoxCoxTransformRegressor . Added compose.TargetModifierRegressor . datasets \u00b6 Added datasets.fetch_restaurants . Added datasets.load_airline . dist \u00b6 Added dist.Multinomial . Added dist.Normal . ensemble \u00b6 Added ensemble.BaggingRegressor . feature_extraction \u00b6 Added feature_extraction.TargetGroupBy . impute \u00b6 Added impute.CategoricalImputer . linear_model \u00b6 Added linear_model.FMRegressor . Removed all the passive-aggressive estimators. metrics \u00b6 Added metrics.Accuracy . Added metrics.MAE . Added metrics.MSE . Added metrics.RMSE . Added metrics.RMSLE . Added metrics.SMAPE . Added metrics.Precision . Added metrics.Recall . Added metrics.F1 . model_selection \u00b6 model_selection.online_score can now be passed a metrics.Metric instead of an sklearn metric; it also checks that the provided metric can be used with the accompanying model. naive_bayes \u00b6 Added naive_bayes.GaussianNB . optim \u00b6 Added optim.PassiveAggressiveI . Added optim.PassiveAggressiveII . preprocessing \u00b6 Added preprocessing.Discarder . Added preprocessing.PolynomialExtender . Added preprocessing.FuncTransformer . reco \u00b6 Added reco.SVD . stats \u00b6 Added stats.Mode . Added stats.Quantile . Added stats.RollingQuantile . Added stats.Entropy . Added stats.RollingMin . Added stats.RollingMax . Added stats.RollingMode . Added stats.RollingSum . Added stats.RollingPeakToPeak . stream \u00b6 Added stream.iter_csv . tree \u00b6 Added tree.MondrianTreeClassifier . Added tree.MondrianTreeRegressor .","title":"0.0.3 - 2019-03-21"},{"location":"releases/0.0.3/#003-2019-03-21","text":"PyPI GitHub","title":"0.0.3 - 2019-03-21"},{"location":"releases/0.0.3/#base","text":"Calling fit_one now returns the calling instance, not the out-of-fold prediction/transform; fit_predict_one , fit_predict_proba_one , and fit_transform_one are available to reproduce the previous behavior. Binary classifiers now output a dict with probabilities for False and True when calling predict_proba_one , which solves the interface issues of having multi-class classifiers do binary classification.","title":"base"},{"location":"releases/0.0.3/#compat","text":"Added compat.convert_river_to_sklearn .","title":"compat"},{"location":"releases/0.0.3/#compose","text":"Added compose.BoxCoxTransformRegressor . Added compose.TargetModifierRegressor .","title":"compose"},{"location":"releases/0.0.3/#datasets","text":"Added datasets.fetch_restaurants . Added datasets.load_airline .","title":"datasets"},{"location":"releases/0.0.3/#dist","text":"Added dist.Multinomial . Added dist.Normal .","title":"dist"},{"location":"releases/0.0.3/#ensemble","text":"Added ensemble.BaggingRegressor .","title":"ensemble"},{"location":"releases/0.0.3/#feature_extraction","text":"Added feature_extraction.TargetGroupBy .","title":"feature_extraction"},{"location":"releases/0.0.3/#impute","text":"Added impute.CategoricalImputer .","title":"impute"},{"location":"releases/0.0.3/#linear_model","text":"Added linear_model.FMRegressor . Removed all the passive-aggressive estimators.","title":"linear_model"},{"location":"releases/0.0.3/#metrics","text":"Added metrics.Accuracy . Added metrics.MAE . Added metrics.MSE . Added metrics.RMSE . Added metrics.RMSLE . Added metrics.SMAPE . Added metrics.Precision . Added metrics.Recall . Added metrics.F1 .","title":"metrics"},{"location":"releases/0.0.3/#model_selection","text":"model_selection.online_score can now be passed a metrics.Metric instead of an sklearn metric; it also checks that the provided metric can be used with the accompanying model.","title":"model_selection"},{"location":"releases/0.0.3/#naive_bayes","text":"Added naive_bayes.GaussianNB .","title":"naive_bayes"},{"location":"releases/0.0.3/#optim","text":"Added optim.PassiveAggressiveI . Added optim.PassiveAggressiveII .","title":"optim"},{"location":"releases/0.0.3/#preprocessing","text":"Added preprocessing.Discarder . Added preprocessing.PolynomialExtender . Added preprocessing.FuncTransformer .","title":"preprocessing"},{"location":"releases/0.0.3/#reco","text":"Added reco.SVD .","title":"reco"},{"location":"releases/0.0.3/#stats","text":"Added stats.Mode . Added stats.Quantile . Added stats.RollingQuantile . Added stats.Entropy . Added stats.RollingMin . Added stats.RollingMax . Added stats.RollingMode . Added stats.RollingSum . Added stats.RollingPeakToPeak .","title":"stats"},{"location":"releases/0.0.3/#stream","text":"Added stream.iter_csv .","title":"stream"},{"location":"releases/0.0.3/#tree","text":"Added tree.MondrianTreeClassifier . Added tree.MondrianTreeRegressor .","title":"tree"},{"location":"releases/0.1.0/","text":"0.1.0 - 2019-05-08 \u00b6 PyPI GitHub base \u00b6 Removed the fit_predict_one estimator method. Removed the fit_predict_proba_one estimator method. Removed the fit_transform_one estimator method. compat \u00b6 Added compat.convert_sklearn_to_river . compat.convert_river_to_sklearn now returns an sklearn.pipeline.Pipeline when provided with a compose.Pipeline . compose \u00b6 Added compose.Discard . Added compose.Select . Added compose.SplitRegressor . The draw method of compose.Pipeline now works properly for arbitrary amounts of nesting, including multiple nested compose.FeatureUnion . datasets \u00b6 Added datasets.fetch_electricity . dummy \u00b6 Added dummy.NoChangeClassifier . Added dummy.PriorClassifier . Added dummy.StatisticRegressor . feature_extraction \u00b6 Added feature_extraction.Differ . Renamed feature_extraction.GroupBy to feature_extraction.Agg . Renamed feature_extraction.TargetGroupBy to feature_extraction.TargetAgg . feature_selection \u00b6 Added feature_selection.SelectKBest . Added feature_selection.VarianceThreshold . impute \u00b6 Added impute.StatImputer . Removed impute.CategoricalImputer . Removed impute.NumericImputer . linear_model \u00b6 Added linear_model.PAClassifier . Added linear_model.PARegressor . Added linear_model.SoftmaxRegression . metrics \u00b6 Added metrics.ConfusionMatrix . Added metrics.CrossEntropy . Added metrics.MacroF1 . Added metrics.MacroPrecision . Added metrics.MacroRecall . Added metrics.MicroF1 . Added metrics.MicroPrecision . Added metrics.MicroRecall . Each metric now has a bigger_is_better property to indicate if a high value is better than a low one or not. optim \u00b6 Added optim.OptimalLR . Added optim.CrossEntropy . Removed optim.PassiveAggressiveI . Removed optim.PassiveAggressiveII . preprocessing \u00b6 Removed preprocessing.Discarder . Added on and sparse parameters to preprocessing.OneHotEncoder . stats \u00b6 Added stats.Covariance . Added stats.PearsonCorrelation . Added stats.SmoothMean . utils \u00b6 Added utils.check_estimator . Added utils.Histogram . Added utils.SortedWindow . Added utils.Window .","title":"0.1.0 - 2019-05-08"},{"location":"releases/0.1.0/#010-2019-05-08","text":"PyPI GitHub","title":"0.1.0 - 2019-05-08"},{"location":"releases/0.1.0/#base","text":"Removed the fit_predict_one estimator method. Removed the fit_predict_proba_one estimator method. Removed the fit_transform_one estimator method.","title":"base"},{"location":"releases/0.1.0/#compat","text":"Added compat.convert_sklearn_to_river . compat.convert_river_to_sklearn now returns an sklearn.pipeline.Pipeline when provided with a compose.Pipeline .","title":"compat"},{"location":"releases/0.1.0/#compose","text":"Added compose.Discard . Added compose.Select . Added compose.SplitRegressor . The draw method of compose.Pipeline now works properly for arbitrary amounts of nesting, including multiple nested compose.FeatureUnion .","title":"compose"},{"location":"releases/0.1.0/#datasets","text":"Added datasets.fetch_electricity .","title":"datasets"},{"location":"releases/0.1.0/#dummy","text":"Added dummy.NoChangeClassifier . Added dummy.PriorClassifier . Added dummy.StatisticRegressor .","title":"dummy"},{"location":"releases/0.1.0/#feature_extraction","text":"Added feature_extraction.Differ . Renamed feature_extraction.GroupBy to feature_extraction.Agg . Renamed feature_extraction.TargetGroupBy to feature_extraction.TargetAgg .","title":"feature_extraction"},{"location":"releases/0.1.0/#feature_selection","text":"Added feature_selection.SelectKBest . Added feature_selection.VarianceThreshold .","title":"feature_selection"},{"location":"releases/0.1.0/#impute","text":"Added impute.StatImputer . Removed impute.CategoricalImputer . Removed impute.NumericImputer .","title":"impute"},{"location":"releases/0.1.0/#linear_model","text":"Added linear_model.PAClassifier . Added linear_model.PARegressor . Added linear_model.SoftmaxRegression .","title":"linear_model"},{"location":"releases/0.1.0/#metrics","text":"Added metrics.ConfusionMatrix . Added metrics.CrossEntropy . Added metrics.MacroF1 . Added metrics.MacroPrecision . Added metrics.MacroRecall . Added metrics.MicroF1 . Added metrics.MicroPrecision . Added metrics.MicroRecall . Each metric now has a bigger_is_better property to indicate if a high value is better than a low one or not.","title":"metrics"},{"location":"releases/0.1.0/#optim","text":"Added optim.OptimalLR . Added optim.CrossEntropy . Removed optim.PassiveAggressiveI . Removed optim.PassiveAggressiveII .","title":"optim"},{"location":"releases/0.1.0/#preprocessing","text":"Removed preprocessing.Discarder . Added on and sparse parameters to preprocessing.OneHotEncoder .","title":"preprocessing"},{"location":"releases/0.1.0/#stats","text":"Added stats.Covariance . Added stats.PearsonCorrelation . Added stats.SmoothMean .","title":"stats"},{"location":"releases/0.1.0/#utils","text":"Added utils.check_estimator . Added utils.Histogram . Added utils.SortedWindow . Added utils.Window .","title":"utils"},{"location":"releases/0.2.0/","text":"0.2.0 - 2019-05-27 \u00b6 PyPI GitHub compose \u00b6 compose.Pipeline now has a debug_one . compose.Discard and compose.Select now take variadic inputs, which means you don't have to provide a list of features to exclude/include. datasets \u00b6 Added datasets.fetch_bikes feature_extraction \u00b6 Classes that inherit from feature_extraction.VectorizerMixin can now directly be passed str instances instead of dict instances. feature_extraction.Agg and feature_extraction.TargetAgg can now aggregate on multiple attributes. metrics \u00b6 Added RollingAccuracy Added RollingCrossEntropy Added RollingF1 Added RollingLogLoss Added RollingMacroF1 Added RollingMacroPrecision Added RollingMacroRecall Added RollingMAE Added RollingMicroF1 Added RollingMicroPrecision Added RollingMicroRecall Added RollingMSE Added RollingPrecision Added RollingRecall Added RollingRMSE Added RollingRMSLE Added RollingSMAPE model_selection \u00b6 Added model_selection.online_qa_score . proba \u00b6 The dist module has been renamed to proba and is now public, for the moment it contains a single distribution called proba.Gaussian . naive_bayes \u00b6 Added naive_bayes.BernoulliNB . Added naive_bayes.ComplementNB . optim \u00b6 Added optim.AdaBound . tree \u00b6 Added tree.DecisionTreeClassifier . Removed tree.MondrianTreeClassifier and tree.MondrianTreeRegressor because their performance wasn't good enough. stats \u00b6 Added stats.AutoCorrelation . Added stats.EWVar . Rename stats.Variance to stats.Var and stats.RollingVariance to stats.RollingVar . stream \u00b6 Added stream.simulate_qa . utils \u00b6 Added utils.SDFT . Added utils.Skyline . Renamed the window_size parameter to size in utils.Window and utils.SortedWindow .","title":"0.2.0 - 2019-05-27"},{"location":"releases/0.2.0/#020-2019-05-27","text":"PyPI GitHub","title":"0.2.0 - 2019-05-27"},{"location":"releases/0.2.0/#compose","text":"compose.Pipeline now has a debug_one . compose.Discard and compose.Select now take variadic inputs, which means you don't have to provide a list of features to exclude/include.","title":"compose"},{"location":"releases/0.2.0/#datasets","text":"Added datasets.fetch_bikes","title":"datasets"},{"location":"releases/0.2.0/#feature_extraction","text":"Classes that inherit from feature_extraction.VectorizerMixin can now directly be passed str instances instead of dict instances. feature_extraction.Agg and feature_extraction.TargetAgg can now aggregate on multiple attributes.","title":"feature_extraction"},{"location":"releases/0.2.0/#metrics","text":"Added RollingAccuracy Added RollingCrossEntropy Added RollingF1 Added RollingLogLoss Added RollingMacroF1 Added RollingMacroPrecision Added RollingMacroRecall Added RollingMAE Added RollingMicroF1 Added RollingMicroPrecision Added RollingMicroRecall Added RollingMSE Added RollingPrecision Added RollingRecall Added RollingRMSE Added RollingRMSLE Added RollingSMAPE","title":"metrics"},{"location":"releases/0.2.0/#model_selection","text":"Added model_selection.online_qa_score .","title":"model_selection"},{"location":"releases/0.2.0/#proba","text":"The dist module has been renamed to proba and is now public, for the moment it contains a single distribution called proba.Gaussian .","title":"proba"},{"location":"releases/0.2.0/#naive_bayes","text":"Added naive_bayes.BernoulliNB . Added naive_bayes.ComplementNB .","title":"naive_bayes"},{"location":"releases/0.2.0/#optim","text":"Added optim.AdaBound .","title":"optim"},{"location":"releases/0.2.0/#tree","text":"Added tree.DecisionTreeClassifier . Removed tree.MondrianTreeClassifier and tree.MondrianTreeRegressor because their performance wasn't good enough.","title":"tree"},{"location":"releases/0.2.0/#stats","text":"Added stats.AutoCorrelation . Added stats.EWVar . Rename stats.Variance to stats.Var and stats.RollingVariance to stats.RollingVar .","title":"stats"},{"location":"releases/0.2.0/#stream","text":"Added stream.simulate_qa .","title":"stream"},{"location":"releases/0.2.0/#utils","text":"Added utils.SDFT . Added utils.Skyline . Renamed the window_size parameter to size in utils.Window and utils.SortedWindow .","title":"utils"},{"location":"releases/0.3.0/","text":"0.3.0 - 2019-06-23 \u00b6 PyPI GitHub datasets \u00b6 Added datasets.load_chick_weights . decomposition \u00b6 Added decomposition.LDA . ensemble \u00b6 Added ensemble.HedgeRegressor . Added ensemble.StackingBinaryClassifier . metrics \u00b6 Added metrics.FBeta Added metrics.MacroFBeta Added metrics.MicroFBeta Added metrics.MultiFBeta Added metrics.RollingFBeta Added metrics.RollingMacroFBeta Added metrics.RollingMicroFBeta Added metrics.RollingMultiFBeta Added metrics.Jaccard Added metrics.RollingConfusionMatrix Added metrics.RegressionMultiOutput Added metrics.MCC Added metrics.RollingMCC Added metrics.ROCAUC Renamed metrics.F1Score to metrics.F1 . multioutput \u00b6 Added multioutput.ClassifierChain . Added multioutput.RegressorChain . optim \u00b6 Added optim.QuantileLoss Added optim.MiniBatcher . preprocessing \u00b6 Added preprocessing.Normalizer . proba \u00b6 Added proba.Multinomial .","title":"0.3.0 - 2019-06-23"},{"location":"releases/0.3.0/#030-2019-06-23","text":"PyPI GitHub","title":"0.3.0 - 2019-06-23"},{"location":"releases/0.3.0/#datasets","text":"Added datasets.load_chick_weights .","title":"datasets"},{"location":"releases/0.3.0/#decomposition","text":"Added decomposition.LDA .","title":"decomposition"},{"location":"releases/0.3.0/#ensemble","text":"Added ensemble.HedgeRegressor . Added ensemble.StackingBinaryClassifier .","title":"ensemble"},{"location":"releases/0.3.0/#metrics","text":"Added metrics.FBeta Added metrics.MacroFBeta Added metrics.MicroFBeta Added metrics.MultiFBeta Added metrics.RollingFBeta Added metrics.RollingMacroFBeta Added metrics.RollingMicroFBeta Added metrics.RollingMultiFBeta Added metrics.Jaccard Added metrics.RollingConfusionMatrix Added metrics.RegressionMultiOutput Added metrics.MCC Added metrics.RollingMCC Added metrics.ROCAUC Renamed metrics.F1Score to metrics.F1 .","title":"metrics"},{"location":"releases/0.3.0/#multioutput","text":"Added multioutput.ClassifierChain . Added multioutput.RegressorChain .","title":"multioutput"},{"location":"releases/0.3.0/#optim","text":"Added optim.QuantileLoss Added optim.MiniBatcher .","title":"optim"},{"location":"releases/0.3.0/#preprocessing","text":"Added preprocessing.Normalizer .","title":"preprocessing"},{"location":"releases/0.3.0/#proba","text":"Added proba.Multinomial .","title":"proba"},{"location":"releases/0.4.1/","text":"0.4.1 - 2019-10-23 \u00b6 PyPI GitHub base \u00b6 Tests are now much more extensive, thanks mostly to the newly added estimator tags. compose \u00b6 Added compose.Renamer . datasets \u00b6 Added fetch_kdd99_http . Added fetch_sms . Added fetch_trec07p . ensemble \u00b6 Removed ensemble.HedgeBinaryClassifier because it's performance was subpar. Removed ensemble.GroupRegressor , as this should be a special case of ensemble.StackingRegressor . feature_extraction \u00b6 Fixed a bug where feature_extraction.CountVectorizer and feature_extraction.TFIDFVectorizer couldn't be pickled. linear_model \u00b6 linear_model.LogisticRegression and linear_model.LinearRegression now have an intercept_lr parameter. metrics \u00b6 Metrics can now be composed using the + operator, which is useful for evaluating multiple metrics at the same time. Added metrics.Rolling , which eliminates the need for a specific rolling implementation for each metric. Each metric can now be passed a sample_weight argument. Added metrics.WeightedF1 . Added metrics.WeightedFBeta . Added metrics.WeightedPrecision . Added metrics.WeightedRecall . neighbors \u00b6 Added neighbors.KNeighborsRegressor . Added neighbors.KNeighborsClassifier . optim \u00b6 Added optim.AdaMax . The optim module has been reorganized into submodules; namely optim.schedulers , optim.initializers , and optim.losses . The top-level now only contains optimizers. Some classes have been renamed accordingly. See the documentation for details. Renamed optim.VanillaSGD to optim.SGD . stats \u00b6 Added stats.IQR . Added stats.RollingIQR . Cythonized stats.Mean and stats.Var . stream \u00b6 Added stream.shuffle . stream.iter_csv now has fraction and seed parameters to sample rows, deterministically or not. Renamed stream.iter_numpy to stream.iter_array . stream.iter_csv can now read from gzipped files. time_series \u00b6 time_series.Detrender now has a window_size parameter for detrending with a rolling mean. tree \u00b6 Added tree.RandomForestClassifier . utils \u00b6 Fixed a bug where utils.dot could take longer than necessary.","title":"0.4.1 - 2019-10-23"},{"location":"releases/0.4.1/#041-2019-10-23","text":"PyPI GitHub","title":"0.4.1 - 2019-10-23"},{"location":"releases/0.4.1/#base","text":"Tests are now much more extensive, thanks mostly to the newly added estimator tags.","title":"base"},{"location":"releases/0.4.1/#compose","text":"Added compose.Renamer .","title":"compose"},{"location":"releases/0.4.1/#datasets","text":"Added fetch_kdd99_http . Added fetch_sms . Added fetch_trec07p .","title":"datasets"},{"location":"releases/0.4.1/#ensemble","text":"Removed ensemble.HedgeBinaryClassifier because it's performance was subpar. Removed ensemble.GroupRegressor , as this should be a special case of ensemble.StackingRegressor .","title":"ensemble"},{"location":"releases/0.4.1/#feature_extraction","text":"Fixed a bug where feature_extraction.CountVectorizer and feature_extraction.TFIDFVectorizer couldn't be pickled.","title":"feature_extraction"},{"location":"releases/0.4.1/#linear_model","text":"linear_model.LogisticRegression and linear_model.LinearRegression now have an intercept_lr parameter.","title":"linear_model"},{"location":"releases/0.4.1/#metrics","text":"Metrics can now be composed using the + operator, which is useful for evaluating multiple metrics at the same time. Added metrics.Rolling , which eliminates the need for a specific rolling implementation for each metric. Each metric can now be passed a sample_weight argument. Added metrics.WeightedF1 . Added metrics.WeightedFBeta . Added metrics.WeightedPrecision . Added metrics.WeightedRecall .","title":"metrics"},{"location":"releases/0.4.1/#neighbors","text":"Added neighbors.KNeighborsRegressor . Added neighbors.KNeighborsClassifier .","title":"neighbors"},{"location":"releases/0.4.1/#optim","text":"Added optim.AdaMax . The optim module has been reorganized into submodules; namely optim.schedulers , optim.initializers , and optim.losses . The top-level now only contains optimizers. Some classes have been renamed accordingly. See the documentation for details. Renamed optim.VanillaSGD to optim.SGD .","title":"optim"},{"location":"releases/0.4.1/#stats","text":"Added stats.IQR . Added stats.RollingIQR . Cythonized stats.Mean and stats.Var .","title":"stats"},{"location":"releases/0.4.1/#stream","text":"Added stream.shuffle . stream.iter_csv now has fraction and seed parameters to sample rows, deterministically or not. Renamed stream.iter_numpy to stream.iter_array . stream.iter_csv can now read from gzipped files.","title":"stream"},{"location":"releases/0.4.1/#time_series","text":"time_series.Detrender now has a window_size parameter for detrending with a rolling mean.","title":"time_series"},{"location":"releases/0.4.1/#tree","text":"Added tree.RandomForestClassifier .","title":"tree"},{"location":"releases/0.4.1/#utils","text":"Fixed a bug where utils.dot could take longer than necessary.","title":"utils"},{"location":"releases/0.4.3/","text":"0.4.3 - 2019-10-27 \u00b6 PyPI GitHub base \u00b6 Model that inherit from base.Wrapper (e.g. tree.RandomForestClassifier ) can now be pickled. datasets \u00b6 Added datasets.fetch_credit_card . utils \u00b6 Added the utils.math sub-module. tree \u00b6 Fixed the debug_one method of tree.DecisionTreeClassifier .","title":"0.4.3 - 2019-10-27"},{"location":"releases/0.4.3/#043-2019-10-27","text":"PyPI GitHub","title":"0.4.3 - 2019-10-27"},{"location":"releases/0.4.3/#base","text":"Model that inherit from base.Wrapper (e.g. tree.RandomForestClassifier ) can now be pickled.","title":"base"},{"location":"releases/0.4.3/#datasets","text":"Added datasets.fetch_credit_card .","title":"datasets"},{"location":"releases/0.4.3/#utils","text":"Added the utils.math sub-module.","title":"utils"},{"location":"releases/0.4.3/#tree","text":"Fixed the debug_one method of tree.DecisionTreeClassifier .","title":"tree"},{"location":"releases/0.4.4/","text":"0.4.4 - 2019-11-11 \u00b6 PyPI GitHub This release was mainly made to provide access to wheels <https://pythonwheels.com/> _ for Windows and MacOS. ensemble \u00b6 Added ensemble.AdaBoostClassifier . linear_model \u00b6 Added a clip_gradient parameter to linear_model.LinearRegression and linear_model.LogisticRegression . Gradient clipping was already implemented, but the maximum absolute value can now be set by the user. The intercept_lr parameter of linear_model.LinearRegression and linear_model.LogisticRegression can now be passed an instance of optim.schedulers.Scheduler as well as a float . metrics \u00b6 Fixed metrics.SMAPE , the implementation was missing a multiplication by 2. optim \u00b6 Added optim.schedulers.Optimal produces results that are identical to sklearn.linear_model.SGDRegressor and sklearn.linear_model.SGDClassifier when setting their learning_rate parameter to 'optimal' . time_series \u00b6 Added time_series.SNARIMAX , a generic model which encompasses well-known time series models such as ARIMA and NARX.","title":"0.4.4 - 2019-11-11"},{"location":"releases/0.4.4/#044-2019-11-11","text":"PyPI GitHub This release was mainly made to provide access to wheels <https://pythonwheels.com/> _ for Windows and MacOS.","title":"0.4.4 - 2019-11-11"},{"location":"releases/0.4.4/#ensemble","text":"Added ensemble.AdaBoostClassifier .","title":"ensemble"},{"location":"releases/0.4.4/#linear_model","text":"Added a clip_gradient parameter to linear_model.LinearRegression and linear_model.LogisticRegression . Gradient clipping was already implemented, but the maximum absolute value can now be set by the user. The intercept_lr parameter of linear_model.LinearRegression and linear_model.LogisticRegression can now be passed an instance of optim.schedulers.Scheduler as well as a float .","title":"linear_model"},{"location":"releases/0.4.4/#metrics","text":"Fixed metrics.SMAPE , the implementation was missing a multiplication by 2.","title":"metrics"},{"location":"releases/0.4.4/#optim","text":"Added optim.schedulers.Optimal produces results that are identical to sklearn.linear_model.SGDRegressor and sklearn.linear_model.SGDClassifier when setting their learning_rate parameter to 'optimal' .","title":"optim"},{"location":"releases/0.4.4/#time_series","text":"Added time_series.SNARIMAX , a generic model which encompasses well-known time series models such as ARIMA and NARX.","title":"time_series"},{"location":"releases/0.5.0/","text":"0.5.0 - 2020-03-13 \u00b6 PyPI GitHub compat \u00b6 Added compat.PyTorch2CremeRegressor . compat.SKL2CremeRegressor and compat.SKL2CremeClassifier now have an optional batch_size parameter in order to perform mini-batching. compose \u00b6 Renamed compose.Whitelister to compose.Select . Renamed compose.Blacklister to compose.Discard . facto \u00b6 Added facto.FFMClassifier . Added facto.FFMRegressor . Added facto.FwFMClassifier . Added facto.FwFMRegressor . Added facto.HOFMClassifier . Added facto.HOFMRegressor . Refactored facto.FMClassifier . Refactored facto.FMRegressor . feature_selection \u00b6 Added feature_selection.PoissonInclusion . Removed feature_selection.RandomDiscarder as it didn't make much sense. feature_extraction \u00b6 Renamed feature_extraction.CountVectorizer to feature_extraction.BagOfWords . Renamed feature_extraction.TFIDFVectorizer to feature_extraction.TFIDF . Added preprocessor and ngram_range parameters to feature_extraction.BagOfWords . Added preprocessor and ngram_range parameters to feature_extraction.TFIDF . datasets \u00b6 The datasets module has been overhauled. Each dataset is now a class (e.g. fetch_electricity has become datasets.Elec2 ). Added datasets.TrumpApproval . Added datasets.MaliciousURL . Added datasets.gen.SEA . Added datasets.Higgs . Added datasets.MovieLens100K . Added datasets.Bananas . Added datasets.Taxis . Added datasets.ImageSegments . Added datasets.SMTP impute \u00b6 Added impute.PreviousImputer . linear_model \u00b6 linear_model.FMClassifier has been moved to the facto module. linear_model.FMRegressor has been moved to the facto module. Added linear_model.ALMAClassifier . metrics \u00b6 Added metrics.ClassificationReport . Added metrics.TimeRolling . The implementation of metrics.ROCAUC was incorrect. Using the trapezoidal rule instead of Simpson's rule seems to be more robust. metrics.PerClass has been removed; it is recommended that you use metrics.ClassificationReport instead as it gives a better overview. meta \u00b6 Moved meta.TransformedTargetRegressor and meta.BoxCoxRegressor to this module (they were previously in the compose module). Added meta.PredClipper model_selection \u00b6 Added model_selection.expand_param_grid to generate a list of models from a grid of parameters. Added the model_selection.successive_halving method for selecting hyperparameters. The online_score and online_qa_score methods have been merged into a single method named model_selection.progressive_val_score . preprocessing \u00b6 Added preprocessing.RBFSampler . Added preprocessing.MaxAbsScaler . Added preprocessing.RobustScaler . Added preprocessing.Binarizer . Added with_mean and with_std parameters to preprocessing.StandardScaler . optim \u00b6 Added optim.losses.BinaryFocalLoss . Added the optim.AMSGrad optimizer. Added the optim.Nadam optimizer. Added optim.losses.Poisson . Fixed a performance bug in optim.NesterovMomentum . reco \u00b6 Added reco.FunkMF . Renamed reco.SVD to reco.BiasedMF . Renamed reco.SGDBaseline to reco.Baseline . Models now expect a dict input with user and item fields. sampling \u00b6 Added sampling.RandomUnderSampler . Added sampling.RandomOverSampler . Added sampling.RandomSampler . Added sampling.HardSamplingClassifier . Added sampling.HardSamplingRegressor . stats \u00b6 Added stats.AbsMax . Added stats.RollingAbsMax . stream \u00b6 Added stream.iter_libsvm . stream.iter_csv now supports reading from '.zip' files. Added stream.Cache . Added a drop parameter to stream.iter_csv to discard fields.","title":"0.5.0 - 2020-03-13"},{"location":"releases/0.5.0/#050-2020-03-13","text":"PyPI GitHub","title":"0.5.0 - 2020-03-13"},{"location":"releases/0.5.0/#compat","text":"Added compat.PyTorch2CremeRegressor . compat.SKL2CremeRegressor and compat.SKL2CremeClassifier now have an optional batch_size parameter in order to perform mini-batching.","title":"compat"},{"location":"releases/0.5.0/#compose","text":"Renamed compose.Whitelister to compose.Select . Renamed compose.Blacklister to compose.Discard .","title":"compose"},{"location":"releases/0.5.0/#facto","text":"Added facto.FFMClassifier . Added facto.FFMRegressor . Added facto.FwFMClassifier . Added facto.FwFMRegressor . Added facto.HOFMClassifier . Added facto.HOFMRegressor . Refactored facto.FMClassifier . Refactored facto.FMRegressor .","title":"facto"},{"location":"releases/0.5.0/#feature_selection","text":"Added feature_selection.PoissonInclusion . Removed feature_selection.RandomDiscarder as it didn't make much sense.","title":"feature_selection"},{"location":"releases/0.5.0/#feature_extraction","text":"Renamed feature_extraction.CountVectorizer to feature_extraction.BagOfWords . Renamed feature_extraction.TFIDFVectorizer to feature_extraction.TFIDF . Added preprocessor and ngram_range parameters to feature_extraction.BagOfWords . Added preprocessor and ngram_range parameters to feature_extraction.TFIDF .","title":"feature_extraction"},{"location":"releases/0.5.0/#datasets","text":"The datasets module has been overhauled. Each dataset is now a class (e.g. fetch_electricity has become datasets.Elec2 ). Added datasets.TrumpApproval . Added datasets.MaliciousURL . Added datasets.gen.SEA . Added datasets.Higgs . Added datasets.MovieLens100K . Added datasets.Bananas . Added datasets.Taxis . Added datasets.ImageSegments . Added datasets.SMTP","title":"datasets"},{"location":"releases/0.5.0/#impute","text":"Added impute.PreviousImputer .","title":"impute"},{"location":"releases/0.5.0/#linear_model","text":"linear_model.FMClassifier has been moved to the facto module. linear_model.FMRegressor has been moved to the facto module. Added linear_model.ALMAClassifier .","title":"linear_model"},{"location":"releases/0.5.0/#metrics","text":"Added metrics.ClassificationReport . Added metrics.TimeRolling . The implementation of metrics.ROCAUC was incorrect. Using the trapezoidal rule instead of Simpson's rule seems to be more robust. metrics.PerClass has been removed; it is recommended that you use metrics.ClassificationReport instead as it gives a better overview.","title":"metrics"},{"location":"releases/0.5.0/#meta","text":"Moved meta.TransformedTargetRegressor and meta.BoxCoxRegressor to this module (they were previously in the compose module). Added meta.PredClipper","title":"meta"},{"location":"releases/0.5.0/#model_selection","text":"Added model_selection.expand_param_grid to generate a list of models from a grid of parameters. Added the model_selection.successive_halving method for selecting hyperparameters. The online_score and online_qa_score methods have been merged into a single method named model_selection.progressive_val_score .","title":"model_selection"},{"location":"releases/0.5.0/#preprocessing","text":"Added preprocessing.RBFSampler . Added preprocessing.MaxAbsScaler . Added preprocessing.RobustScaler . Added preprocessing.Binarizer . Added with_mean and with_std parameters to preprocessing.StandardScaler .","title":"preprocessing"},{"location":"releases/0.5.0/#optim","text":"Added optim.losses.BinaryFocalLoss . Added the optim.AMSGrad optimizer. Added the optim.Nadam optimizer. Added optim.losses.Poisson . Fixed a performance bug in optim.NesterovMomentum .","title":"optim"},{"location":"releases/0.5.0/#reco","text":"Added reco.FunkMF . Renamed reco.SVD to reco.BiasedMF . Renamed reco.SGDBaseline to reco.Baseline . Models now expect a dict input with user and item fields.","title":"reco"},{"location":"releases/0.5.0/#sampling","text":"Added sampling.RandomUnderSampler . Added sampling.RandomOverSampler . Added sampling.RandomSampler . Added sampling.HardSamplingClassifier . Added sampling.HardSamplingRegressor .","title":"sampling"},{"location":"releases/0.5.0/#stats","text":"Added stats.AbsMax . Added stats.RollingAbsMax .","title":"stats"},{"location":"releases/0.5.0/#stream","text":"Added stream.iter_libsvm . stream.iter_csv now supports reading from '.zip' files. Added stream.Cache . Added a drop parameter to stream.iter_csv to discard fields.","title":"stream"},{"location":"releases/0.5.1/","text":"0.5.1 - 2020-03-29 \u00b6 PyPI GitHub compose \u00b6 compose.Pipeline and compose.TransformerUnion now variadic arguments as input instead of a list. This doesn't change anything when using the shorthand operators | and + . model_selection \u00b6 Removed model_selection.successive_halving Added model_selection.SuccessiveHalvingRegressor and model_selection.SuccessiveHalvingClassifier stream \u00b6 Added a copy parameter to stream.simulate_qa in order to handle unwanted feature modifications. tree \u00b6 Added a curtail_under parameter to tree.DecisionTreeClassifier . The speed and accuracy of both tree.DecisionTreeClassifier and tree.RandomForestClassifier has been slightly improved for numerical attributes. The esthetics of the tree.DecisionTreeClassifier.draw method have been improved.","title":"0.5.1 - 2020-03-29"},{"location":"releases/0.5.1/#051-2020-03-29","text":"PyPI GitHub","title":"0.5.1 - 2020-03-29"},{"location":"releases/0.5.1/#compose","text":"compose.Pipeline and compose.TransformerUnion now variadic arguments as input instead of a list. This doesn't change anything when using the shorthand operators | and + .","title":"compose"},{"location":"releases/0.5.1/#model_selection","text":"Removed model_selection.successive_halving Added model_selection.SuccessiveHalvingRegressor and model_selection.SuccessiveHalvingClassifier","title":"model_selection"},{"location":"releases/0.5.1/#stream","text":"Added a copy parameter to stream.simulate_qa in order to handle unwanted feature modifications.","title":"stream"},{"location":"releases/0.5.1/#tree","text":"Added a curtail_under parameter to tree.DecisionTreeClassifier . The speed and accuracy of both tree.DecisionTreeClassifier and tree.RandomForestClassifier has been slightly improved for numerical attributes. The esthetics of the tree.DecisionTreeClassifier.draw method have been improved.","title":"tree"},{"location":"releases/0.6.0/","text":"0.6.0 - 2020-06-09 \u00b6 base \u00b6 Added a new base class called SupervisedTransformer from which supervised transformers inherit from. Before this, supervised transformers has a is_supervised property. compose \u00b6 Added compose.SelectType , which allows selecting feature subsets based on their type. Added a score_one method to compose.Pipeline so that estimators from the anomaly module can be pipelined. Added compose.Grouper , which allows applying transformers within different subgroups. datasets \u00b6 Added datasets.Music , which is a dataset for multi-output binary classification. Added datasets.synth.Friedman , which is synthetic regression dataset. The datasets.gen module has been renamed to datasets.synth Each dataset now has a __repr__ method which displays some descriptive information. Added datasets.Insects , which has 10 variants. feature_extraction \u00b6 feature_extraction.Differ has been deprecated. We might put it back in a future if we find a better design. impute \u00b6 impute.StatImputer has been completely refactored. metrics \u00b6 In metrics.SMAPE , instead of raising a ZeroDivisionError , the convention is now to use 0 when both y_true and y_pred are equal to 0. model_selection \u00b6 Added the possibility to configure how the progress is printed in model_selection.progressive_val_score . For instance, the progress can now be printed to a file by providing the file argument. multiclass \u00b6 Added multiclass.OutputCodeClassifier . Added multiclass.OneVsOneClassifier . multioutput \u00b6 Fixed a bug where multioutput.ClassifierChain and multioutput.RegressorChain could not be pickled. stats \u00b6 Added stats.Shift , which can be used to compute statistics over a shifted version of a variable. Added stats.Link , which can be used to compose univariate statistics. Univariate statistics can now be composed via the | operator. Renamed stats.Covariance to stats.Cov . Renamed stats.PearsonCorrelation to stats.PearsonCorr . Renamed stats.AutoCorrelation to stats.AutoCorr . Added stats.RollingCov , which computes covariance between two variables over a window. Added stats.RollingPearsonCorr , which computes the Pearson correlation over a window. stream \u00b6 Added a stream.iter_sql utility method to work with SQLAlchemy. The target_name parameter of stream.iter_csv has been renamed to target . It can now be passed a list of values in order to support multi-output scenarios. Added stream.iter_arff for handling ARFF files. tree \u00b6 Cancelled the behavior where tree.DecisionTreeRegressor would raise an exception when no split was found.","title":"0.6.0 - 2020-06-09"},{"location":"releases/0.6.0/#060-2020-06-09","text":"","title":"0.6.0 - 2020-06-09"},{"location":"releases/0.6.0/#base","text":"Added a new base class called SupervisedTransformer from which supervised transformers inherit from. Before this, supervised transformers has a is_supervised property.","title":"base"},{"location":"releases/0.6.0/#compose","text":"Added compose.SelectType , which allows selecting feature subsets based on their type. Added a score_one method to compose.Pipeline so that estimators from the anomaly module can be pipelined. Added compose.Grouper , which allows applying transformers within different subgroups.","title":"compose"},{"location":"releases/0.6.0/#datasets","text":"Added datasets.Music , which is a dataset for multi-output binary classification. Added datasets.synth.Friedman , which is synthetic regression dataset. The datasets.gen module has been renamed to datasets.synth Each dataset now has a __repr__ method which displays some descriptive information. Added datasets.Insects , which has 10 variants.","title":"datasets"},{"location":"releases/0.6.0/#feature_extraction","text":"feature_extraction.Differ has been deprecated. We might put it back in a future if we find a better design.","title":"feature_extraction"},{"location":"releases/0.6.0/#impute","text":"impute.StatImputer has been completely refactored.","title":"impute"},{"location":"releases/0.6.0/#metrics","text":"In metrics.SMAPE , instead of raising a ZeroDivisionError , the convention is now to use 0 when both y_true and y_pred are equal to 0.","title":"metrics"},{"location":"releases/0.6.0/#model_selection","text":"Added the possibility to configure how the progress is printed in model_selection.progressive_val_score . For instance, the progress can now be printed to a file by providing the file argument.","title":"model_selection"},{"location":"releases/0.6.0/#multiclass","text":"Added multiclass.OutputCodeClassifier . Added multiclass.OneVsOneClassifier .","title":"multiclass"},{"location":"releases/0.6.0/#multioutput","text":"Fixed a bug where multioutput.ClassifierChain and multioutput.RegressorChain could not be pickled.","title":"multioutput"},{"location":"releases/0.6.0/#stats","text":"Added stats.Shift , which can be used to compute statistics over a shifted version of a variable. Added stats.Link , which can be used to compose univariate statistics. Univariate statistics can now be composed via the | operator. Renamed stats.Covariance to stats.Cov . Renamed stats.PearsonCorrelation to stats.PearsonCorr . Renamed stats.AutoCorrelation to stats.AutoCorr . Added stats.RollingCov , which computes covariance between two variables over a window. Added stats.RollingPearsonCorr , which computes the Pearson correlation over a window.","title":"stats"},{"location":"releases/0.6.0/#stream","text":"Added a stream.iter_sql utility method to work with SQLAlchemy. The target_name parameter of stream.iter_csv has been renamed to target . It can now be passed a list of values in order to support multi-output scenarios. Added stream.iter_arff for handling ARFF files.","title":"stream"},{"location":"releases/0.6.0/#tree","text":"Cancelled the behavior where tree.DecisionTreeRegressor would raise an exception when no split was found.","title":"tree"},{"location":"releases/0.6.1/","text":"0.6.1 - 2020-06-10 \u00b6 compose \u00b6 Fixed a bug that occurred when part of a compose.Transformer was a compose.Pipeline and wasn't properly handled.","title":"0.6.1 - 2020-06-10"},{"location":"releases/0.6.1/#061-2020-06-10","text":"","title":"0.6.1 - 2020-06-10"},{"location":"releases/0.6.1/#compose","text":"Fixed a bug that occurred when part of a compose.Transformer was a compose.Pipeline and wasn't properly handled.","title":"compose"},{"location":"releases/0.7.0/","text":"0.7.0 - 2021-04-16 \u00b6 Alas, no release notes for this one.","title":"0.7.0 - 2021-04-16"},{"location":"releases/0.7.0/#070-2021-04-16","text":"Alas, no release notes for this one.","title":"0.7.0 - 2021-04-16"},{"location":"releases/0.7.1/","text":"0.7.1 - 2021-06-13 \u00b6 Fixed an issue where scikit-learn was imported in sam_knn.py but wasn't specified as a dependency. stream \u00b6 Added drop_nones parameter to stream.iter_csv .","title":"0.7.1 - 2021-06-13"},{"location":"releases/0.7.1/#071-2021-06-13","text":"Fixed an issue where scikit-learn was imported in sam_knn.py but wasn't specified as a dependency.","title":"0.7.1 - 2021-06-13"},{"location":"releases/0.7.1/#stream","text":"Added drop_nones parameter to stream.iter_csv .","title":"stream"},{"location":"releases/0.7.2/","text":"0.7.2 \u00b6 expert \u00b6 Each expert model will now raise a NotEnoughModels exception if only a single model is passed.","title":"0.7.2"},{"location":"releases/0.7.2/#072","text":"","title":"0.7.2"},{"location":"releases/0.7.2/#expert","text":"Each expert model will now raise a NotEnoughModels exception if only a single model is passed.","title":"expert"},{"location":"releases/unreleased/","text":"Unreleased \u00b6 base \u00b6 The base.BinaryClassifier and base.MultiClassifier have been merge into base.Classifier . The 'binary_only' tag is now used to indicate whether or not a classifier support multi-class classification or not. compose \u00b6 Fixed some bugs related to mini-batching in compose.Pipeline . datasets \u00b6 Added datasets.SolarFlare , which is a small multi-output regression dataset. decomposition \u00b6 decomposition.LDA now takes as input word counts instead of raw text. expert \u00b6 Created this new module, which will regroup methods that perform expert learning, which boils down to managing multiple models. Moved ensemble.StackingBinaryClassifier to expert.StackingClassifier . Moved model_selection.SuccessiveHalvingClassifier to expert.SuccessiveHalvingClassifier . Moved model_selection.SuccessiveHalvingRegressor to expert.SuccessiveHalvingRegressor . Moved ensemble.HedgeRegressor to ensemble.EWARegressor . evaluate \u00b6 Created this new module, which will contains methods for evaluating models. feature_extraction \u00b6 Moved preprocessing.PolynomialExtender to feature_extraction.PolynomialExtender . Moved preprocessing.RBFSampler to feature_extraction.RBFSampler . linear_model \u00b6 Added linear_model.Perceptron , which is implemented as a special case of logistic regression. model_selection \u00b6 Deleted this module. multiclass \u00b6 multiclass.OneVsRestClassifier now supports mini-batching. optim \u00b6 Removed optim.MiniBatcher . Implemented optim.Averager , which allows doing averaged stochastic gradient descent. Removed optim.Perceptron . utils \u00b6 Moved model_selection.expand_param_grid to utils.expand_param_grid .","title":"Unreleased"},{"location":"releases/unreleased/#unreleased","text":"","title":"Unreleased"},{"location":"releases/unreleased/#base","text":"The base.BinaryClassifier and base.MultiClassifier have been merge into base.Classifier . The 'binary_only' tag is now used to indicate whether or not a classifier support multi-class classification or not.","title":"base"},{"location":"releases/unreleased/#compose","text":"Fixed some bugs related to mini-batching in compose.Pipeline .","title":"compose"},{"location":"releases/unreleased/#datasets","text":"Added datasets.SolarFlare , which is a small multi-output regression dataset.","title":"datasets"},{"location":"releases/unreleased/#decomposition","text":"decomposition.LDA now takes as input word counts instead of raw text.","title":"decomposition"},{"location":"releases/unreleased/#expert","text":"Created this new module, which will regroup methods that perform expert learning, which boils down to managing multiple models. Moved ensemble.StackingBinaryClassifier to expert.StackingClassifier . Moved model_selection.SuccessiveHalvingClassifier to expert.SuccessiveHalvingClassifier . Moved model_selection.SuccessiveHalvingRegressor to expert.SuccessiveHalvingRegressor . Moved ensemble.HedgeRegressor to ensemble.EWARegressor .","title":"expert"},{"location":"releases/unreleased/#evaluate","text":"Created this new module, which will contains methods for evaluating models.","title":"evaluate"},{"location":"releases/unreleased/#feature_extraction","text":"Moved preprocessing.PolynomialExtender to feature_extraction.PolynomialExtender . Moved preprocessing.RBFSampler to feature_extraction.RBFSampler .","title":"feature_extraction"},{"location":"releases/unreleased/#linear_model","text":"Added linear_model.Perceptron , which is implemented as a special case of logistic regression.","title":"linear_model"},{"location":"releases/unreleased/#model_selection","text":"Deleted this module.","title":"model_selection"},{"location":"releases/unreleased/#multiclass","text":"multiclass.OneVsRestClassifier now supports mini-batching.","title":"multiclass"},{"location":"releases/unreleased/#optim","text":"Removed optim.MiniBatcher . Implemented optim.Averager , which allows doing averaged stochastic gradient descent. Removed optim.Perceptron .","title":"optim"},{"location":"releases/unreleased/#utils","text":"Moved model_selection.expand_param_grid to utils.expand_param_grid .","title":"utils"},{"location":"user-guide/feature-extraction/","text":"Feature extraction \u00b6 To do.","title":"Feature extraction"},{"location":"user-guide/feature-extraction/#feature-extraction","text":"To do.","title":"Feature extraction"},{"location":"user-guide/hyperparameter-tuning/","text":"Hyperparameter tuning \u00b6 To do.","title":"Hyperparameter tuning"},{"location":"user-guide/hyperparameter-tuning/#hyperparameter-tuning","text":"To do.","title":"Hyperparameter tuning"},{"location":"user-guide/mini-batching/","text":"Mini-batching \u00b6 In its purest form, online machine learning encompasses models which learn with one sample at a time. This is the design which is used in river . The main downside of single-instance processing is that it doesn't scale to big data, at least not in the sense of traditional batch learning. Indeed, processing one sample at a time means that we are unable to fully take advantage of vectorisation and other computational tools that are taken for granted in batch learning. On top of this, processing a large dataset in river essentially involves a Python for loop, which might be too slow for some usecases. However, this doesn't mean that river is slow. In fact, for processing a single instance, river is actually a couple of orders of magnitude faster than libraries such as scikit-learn, PyTorch, and Tensorflow. The reason why is because river is designed from the ground up to process a single instance, whereas the majority of other libraries choose to care about batches of data. Both approaches offer different compromises, and the best choice depends on your usecase. In order to propose the best of both worlds, river offers some limited support for mini-batch learning. Some of river 's estimators implement *_many methods on top of their *_one counterparts. For instance, preprocessing.StandardScaler has a learn_many method as well as a transform_many method, in addition to learn_one and transform_one . Each mini-batch method takes as input a pandas.DataFrame . Supervised estimators also take as input a pandas.Series of target values. We choose to use pandas.DataFrames over numpy.ndarrays because of the simple fact that the former allows us to name each feature. This in turn allows us to offer a uniform interface for both single instance and mini-batch learning. As an example, we will build a simple pipeline that scales the data and trains a logistic regression. Indeed, the compose.Pipeline class can be applied to mini-batches, as long as each step is able to do so. from river import compose from river import linear_model from river import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) For this example, we will use datasets.Higgs . from river import datasets dataset = datasets . Higgs () if not dataset . is_downloaded : dataset . download () dataset Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB) Higgs dataset. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. Name Higgs Task Binary classification Samples 11,000,000 Features 28 Sparse False Path /home/runner/river_data/Higgs/HIGGS.csv.gz URL https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz Size 2.62 GB Downloaded True The easiest way to read the data in a mini-batch fashion is to use the read_csv from pandas . import pandas as pd names = [ 'target' , 'lepton pT' , 'lepton eta' , 'lepton phi' , 'missing energy magnitude' , 'missing energy phi' , 'jet 1 pt' , 'jet 1 eta' , 'jet 1 phi' , 'jet 1 b-tag' , 'jet 2 pt' , 'jet 2 eta' , 'jet 2 phi' , 'jet 2 b-tag' , 'jet 3 pt' , 'jet 3 eta' , 'jet 3 phi' , 'jet 3 b-tag' , 'jet 4 pt' , 'jet 4 eta' , 'jet 4 phi' , 'jet 4 b-tag' , 'm_jj' , 'm_jjj' , 'm_lv' , 'm_jlv' , 'm_bb' , 'm_wbb' , 'm_wwbb' ] for x in pd . read_csv ( dataset . path , names = names , chunksize = 8096 , nrows = 3e5 ): y = x . pop ( 'target' ) y_pred = model . predict_proba_many ( x ) model . learn_many ( x , y ) If you are familiar with scikit-learn, you might be aware that some of their estimators have a partial_fit method, which is similar to river's learn_many method. Here are some advantages that river has over scikit-learn: We guarantee that river's is just as fast, if not faster than scikit-learn. The differences are negligeable, but are slightly in favor of river. We take as input dataframes, which allows us to name each feature. The benefit is that you can add/remove/permute features between batches and everything will keep working. Estimators that support mini-batches also support single instance learning. This means that you can enjoy the best of both worlds. For instance, you can train with mini-batches and use predict_one to make predictions. Note that you can check which estimators can process mini-batches programmatically: import importlib import inspect def can_mini_batch ( obj ): return hasattr ( obj , 'learn_many' ) for module in importlib . import_module ( 'river' ) . __all__ : if module in [ 'datasets' , 'synth' ]: continue for name , obj in inspect . getmembers ( importlib . import_module ( f 'river. { module } ' ), can_mini_batch ): print ( name ) MiniBatchClassifier MiniBatchRegressor SKL2RiverClassifier SKL2RiverRegressor Pipeline LinearRegression LogisticRegression Perceptron OneVsRestClassifier StandardScaler Because mini-batch learning isn't treated as a first-class citizen, some of the river's functionalities require some work in order to play nicely with mini-batches. For instance, the objects from the metrics module have an update method that take as input a single pair (y_true, y_pred) . This might change in the future, depending on the demand. We plan to promote more models to the mini-batch regime. However, we will only be doing so for the methods that benefit the most from it, as well as those that are most popular. Indeed, river 's core philosophy will remain to cater to single instance learning.","title":"Mini-batching"},{"location":"user-guide/mini-batching/#mini-batching","text":"In its purest form, online machine learning encompasses models which learn with one sample at a time. This is the design which is used in river . The main downside of single-instance processing is that it doesn't scale to big data, at least not in the sense of traditional batch learning. Indeed, processing one sample at a time means that we are unable to fully take advantage of vectorisation and other computational tools that are taken for granted in batch learning. On top of this, processing a large dataset in river essentially involves a Python for loop, which might be too slow for some usecases. However, this doesn't mean that river is slow. In fact, for processing a single instance, river is actually a couple of orders of magnitude faster than libraries such as scikit-learn, PyTorch, and Tensorflow. The reason why is because river is designed from the ground up to process a single instance, whereas the majority of other libraries choose to care about batches of data. Both approaches offer different compromises, and the best choice depends on your usecase. In order to propose the best of both worlds, river offers some limited support for mini-batch learning. Some of river 's estimators implement *_many methods on top of their *_one counterparts. For instance, preprocessing.StandardScaler has a learn_many method as well as a transform_many method, in addition to learn_one and transform_one . Each mini-batch method takes as input a pandas.DataFrame . Supervised estimators also take as input a pandas.Series of target values. We choose to use pandas.DataFrames over numpy.ndarrays because of the simple fact that the former allows us to name each feature. This in turn allows us to offer a uniform interface for both single instance and mini-batch learning. As an example, we will build a simple pipeline that scales the data and trains a logistic regression. Indeed, the compose.Pipeline class can be applied to mini-batches, as long as each step is able to do so. from river import compose from river import linear_model from river import preprocessing model = compose . Pipeline ( preprocessing . StandardScaler (), linear_model . LogisticRegression () ) For this example, we will use datasets.Higgs . from river import datasets dataset = datasets . Higgs () if not dataset . is_downloaded : dataset . download () dataset Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz (2.62 GB) Higgs dataset. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. Name Higgs Task Binary classification Samples 11,000,000 Features 28 Sparse False Path /home/runner/river_data/Higgs/HIGGS.csv.gz URL https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz Size 2.62 GB Downloaded True The easiest way to read the data in a mini-batch fashion is to use the read_csv from pandas . import pandas as pd names = [ 'target' , 'lepton pT' , 'lepton eta' , 'lepton phi' , 'missing energy magnitude' , 'missing energy phi' , 'jet 1 pt' , 'jet 1 eta' , 'jet 1 phi' , 'jet 1 b-tag' , 'jet 2 pt' , 'jet 2 eta' , 'jet 2 phi' , 'jet 2 b-tag' , 'jet 3 pt' , 'jet 3 eta' , 'jet 3 phi' , 'jet 3 b-tag' , 'jet 4 pt' , 'jet 4 eta' , 'jet 4 phi' , 'jet 4 b-tag' , 'm_jj' , 'm_jjj' , 'm_lv' , 'm_jlv' , 'm_bb' , 'm_wbb' , 'm_wwbb' ] for x in pd . read_csv ( dataset . path , names = names , chunksize = 8096 , nrows = 3e5 ): y = x . pop ( 'target' ) y_pred = model . predict_proba_many ( x ) model . learn_many ( x , y ) If you are familiar with scikit-learn, you might be aware that some of their estimators have a partial_fit method, which is similar to river's learn_many method. Here are some advantages that river has over scikit-learn: We guarantee that river's is just as fast, if not faster than scikit-learn. The differences are negligeable, but are slightly in favor of river. We take as input dataframes, which allows us to name each feature. The benefit is that you can add/remove/permute features between batches and everything will keep working. Estimators that support mini-batches also support single instance learning. This means that you can enjoy the best of both worlds. For instance, you can train with mini-batches and use predict_one to make predictions. Note that you can check which estimators can process mini-batches programmatically: import importlib import inspect def can_mini_batch ( obj ): return hasattr ( obj , 'learn_many' ) for module in importlib . import_module ( 'river' ) . __all__ : if module in [ 'datasets' , 'synth' ]: continue for name , obj in inspect . getmembers ( importlib . import_module ( f 'river. { module } ' ), can_mini_batch ): print ( name ) MiniBatchClassifier MiniBatchRegressor SKL2RiverClassifier SKL2RiverRegressor Pipeline LinearRegression LogisticRegression Perceptron OneVsRestClassifier StandardScaler Because mini-batch learning isn't treated as a first-class citizen, some of the river's functionalities require some work in order to play nicely with mini-batches. For instance, the objects from the metrics module have an update method that take as input a single pair (y_true, y_pred) . This might change in the future, depending on the demand. We plan to promote more models to the mini-batch regime. However, we will only be doing so for the methods that benefit the most from it, as well as those that are most popular. Indeed, river 's core philosophy will remain to cater to single instance learning.","title":"Mini-batching"},{"location":"user-guide/model-evaluation/","text":"Model evaluation \u00b6 To do.","title":"Model evaluation"},{"location":"user-guide/model-evaluation/#model-evaluation","text":"To do.","title":"Model evaluation"},{"location":"user-guide/on-hoeffding-trees/","text":"Incremental decision trees in river: the Hoeffding Tree case \u00b6 Decision trees (DT) are popular learning models due to their inherently simplicity, flexibility and self-explainable structure. Moreover, when aggregated in ensembles, high predictive power might be achieved. Bagging and gradient boosting-based tree ensembles are very popular solutions in competition platforms such as Kaggle, and also among researchers. Although fairly lightweight, tradicional batch DTs cannot cope with data stream mining/online learning requirements, as they do multiple passes over the data and have to be retrained from scratch every time a new observation appears. The data stream literature has plenty of incremental DT (iDT) families that are better suited to online learning. Nonetheless, Hoeffding Trees (HT) are historically the most popular family of iDTs to date. In fact, HTs have some nice properties: one-pass learning regime; theoretical guarantees to converge to the batch DT model given enough observations and a stationary data distribution; small memory and running time footprint (in most cases); some of their variations can deal with non-stationary distributions. And the previous list goes on and on. Besides that, HTs also have the same advantages as batch DTs ( C4.5 / J48 , CART , M5 , etc.) do. We can inspect the structure of a HT to understand how decisions were made, which is a nice feature to have in online learning tasks. In river , HTs are first-class citizens, so we have multiple realizations of this framework that are suited to different learning tasks and scenarios. This brief introduction to HT does not aims at being extensive nor delving into algorithmic or implementation details of the HTs. Instead, we intend to provide a high-level overview of the HTs as they are envisioned in river , as well as their shared properties and important hyperparameters. In this guide, we are going to: summarize the differences accross the multiple HT versions available; learn how to inspect tree models; learn how to manage the memory usage of HTs; compare numerical tree splitters and understand their impact on the iDT induction process. Well, without further ado, let's go! First things first, we are going to start with some imports. import matplotlib.pyplot as plt from river import datasets from river import evaluate from river import metrics from river import preprocessing # we are going to use that later from river import synth # we are going to use some synthetic datasets too from river import tree 1. Trees, trees everywhere: gardening 101 with river \u00b6 At first glance, the amount of iDT algorithms in river might seem too much to handle, but in reality the distinction among them is easy to grasp. To facilitate our lives, here's a neat table listing the available HT models and summarizing their differences: Name Acronym Task Non-stationary? Comments Source Hoeffding Tree Classifier HTC Classification No Basic HT for classification tasks [1] Hoeffding Adaptive Tree Classifier HATC Classification Yes Modifies HTC by adding an instance of ADWIN to each node to detect and react to drift detection [2] Extremely Fast Decision Tree Classifier EFDT Classification No Deploys split decisions as soon as possible and periodically revisit decisions and redo them if necessary. Not as fast in practice as the name implies, but it tends to converge faster than HTC to the model generated by a batch DT [3] Hoeffding Tree Regressor HTR Regression No Basic HT for regression tasks. It is an adaptation of the FIRT/FIMT algorithm that bears some semblance to HTC [4] Hoeffding Adaptive Tree Regressor HATR Regression Yes Modifies HTR by adding an instance of ADWIN to each node to detect and react to drift detection - incremental Structured-Output Prediction Tree Regressor iSOUPT Multi-target regression No Multi-target version of HTR [5] Label Combination Hoeffding Tree Classifier LCHTC Multi-label classification No Creates a numerical code for each combination of the binary labels and uses HTC to learn from this encoded representation. At prediction time, decodes the modified representation to obtain the original label set - As we can see, although their application fields might overlap sometimes, the HT variations have specific situations in which they are better suited to work. Moreover, in river we provide a standardized API access to all the HT variants since they share many properties in common. 2. How to inspect tree models? \u00b6 We provide a handful of tools to inspect trained HTs in river . Here, we will provide some examples of how to access their inner structures, get useful information, and plot the iDT structure. Firstly, let's pick a toy dataset from which our tree will learn from. Here we are going to focus on the classification case, but the same operations apply to other learning tasks. We will select the Phishing dataset from the datasets module to exemplify the HTs' capabilities. dataset = datasets . Phishing () dataset Phishing websites. This dataset contains features from web pages that are classified as phishing or not. Name Phishing Task Binary classification Samples 1,250 Features 9 Sparse False Path /home/smastelini/Documents/river_main/river/datasets/phishing.csv.gz We are going to train an instance of HoeffdingTreeClassifier using this dataset. As everything else in river , training an iDT is a piece of cake! %% time model = tree . HoeffdingTreeClassifier ( grace_period = 50 ) for x , y in dataset : model . learn_one ( x , y ) model CPU times: user 167 ms, sys: 755 \u00b5s, total: 168 ms Wall time: 166 ms HoeffdingTreeClassifier ( grace_period=50 max_depth=inf split_criterion=\"info_gain\" split_confidence=1e-07 tie_threshold=0.05 leaf_prediction=\"nba\" nb_threshold=0 nominal_attributes=None splitter=GaussianSplitter ( n_splits=10 ) binary_split=False max_size=100 memory_estimate_period=1000000 stop_mem_management=False remove_poor_attrs=False merit_preprune=True ) That's it! We are not going to enter into details about some of the available parameters of HTC here. The user can refer to the documentation page for more information about that. Let's talk about model inspection :D At any time, we can easily get some statistics about our trained model by using the summary property: model . summary {'n_nodes': 5, 'n_branches': 2, 'n_leaves': 3, 'n_active_leaves': 3, 'n_inactive_leaves': 0, 'height': 3, 'total_observed_weight': 1250.0} This property show us the internal structure of the tree, including data concerning the memory-management routines that we are going to check later in this guide. We can also get a representation of the tree model as a pandas.DataFrame object: model . to_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } parent is_leaf depth stats feature threshold splitter splitters _disabled_attrs _last_split_attempt_at _mc_correct_weight _nb_correct_weight node 0 <NA> False 0 {True: 260.0, False: 390.0} empty_server_form_handler 0.545455 NaN NaN NaN NaN NaN NaN 1 0 True 1 {True: 443.4163997711022, False: 59.8769131081... NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 474.293313 249.0 248.0 2 0 False 1 {True: 71.58360022889778, False: 404.123086891... popup_window 0.090909 NaN NaN NaN NaN NaN NaN 3 2 True 2 {False: 31.426538522574834, True: 33.0} NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 59.426539 23.0 36.0 4 2 True 2 {False: 250.57346147742516, True: 6.0} NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 241.573461 210.0 196.0 Hmm, maybe not the clearest of the representations. What about drawing the tree structure instead? model . draw () Much better, huh? Lastly, we can check how the tree predicts one specific instance by using the debug_one method: x , y = next ( iter ( dataset )) # Let's select the first example in the stream x , y ({'empty_server_form_handler': 0.0, 'popup_window': 0.0, 'https': 0.0, 'request_from_other_domain': 0.0, 'anchor_from_other_domain': 0.0, 'is_popular': 0.5, 'long_url': 1.0, 'age_of_domain': 1, 'ip_in_url': 1}, True) print ( model . debug_one ( x )) empty_server_form_handler \u2264 0.5454545454545454 Class True: P(False) = 0.1 P(True) = 0.9 Our tree got this one right! The method debug_one is especially useful when we are dealing with a big tree model where drawing might not be the wisest of the choices (we will end up with a tree chart that has too much information to visually understand). Some additional hints: the max_depth parameter is our friend when building HTs that need to be constantly inspected. This parameter, which is available for every HT variant, triggers a pre-pruning mechanism that stops tree growth when the given depth is reached. we can also limit the depth when using the draw method. in the case of tree ensembles, individual trees can be accessed using the [index] operator. Then, the same set of inspection tools are available to play with! 3. Advanced gardening with river: grab your pruning shears and let's limit memory usage \u00b6 Online learning is well-suited to highly scalable processing centers with petabytes of data arriving intermittently, but it can also work with Internet of Things (IoT) devices operating at low power and with limited processing capability. Hence, making sure our trees are not going to use too much memory is a nice feature that can impact on both energy usage and the running time. HTs have memory-management routines that put the user in the control of computational resources that are available. In this brief guide, we are going to use a regression tree, since this kind of iDT typically spends more memory than the classification counterparts. However, the user can control the memory usage in the exact same way in river , regardless of the HT variant! We will rely on the Friedman synthetic dataset (data generator) from the synth module in our evaluation. To avoid doing tedious stuff, let's use the Track class from the evaluate module. Tracks are the standard way of benchmarking learning models in river . Since data generators can produce instances indefinitely, we will select a sample of size 10K for our tests. def friedman_track ( n_samples = 10_000 ): dataset = synth . Friedman ( seed = 42 ) . take ( n_samples ) track = evaluate . Track ( \"10K Friedman + MAE\" , dataset , metrics . MAE (), n_samples ) return track We are almost ready to go. Let's first define a simple function that parses the data generated by our tracks and plots it in a nice way. def plot_track ( track , metric_name , models , n_samples , n_checkpoints ): fig , ax = plt . subplots ( figsize = ( 5 , 5 ), nrows = 3 , dpi = 300 ) for model_name , model in models . items (): step = [] error = [] r_time = [] memory = [] for checkpoint in track ( n_samples ) . run ( model , n_checkpoints ): step . append ( checkpoint [ \"Step\" ]) error . append ( checkpoint [ metric_name ]) # Convert timedelta object into seconds r_time . append ( checkpoint [ \"Time\" ] . total_seconds ()) # Make sure the memory measurements are in MB raw_memory , unit = float ( checkpoint [ \"Memory\" ][: - 3 ]), checkpoint [ \"Memory\" ][ - 2 :] memory . append ( raw_memory * 2 **- 10 if unit == 'KB' else raw_memory ) ax [ 0 ] . grid ( True ) ax [ 1 ] . grid ( True ) ax [ 2 ] . grid ( True ) ax [ 0 ] . plot ( step , error , label = model_name ) ax [ 0 ] . set_ylabel ( metric_name ) ax [ 1 ] . plot ( step , r_time , label = model_name ) ax [ 1 ] . set_ylabel ( 'Time (seconds)' ) ax [ 2 ] . plot ( step , memory , label = model_name ) ax [ 2 ] . set_ylabel ( 'Memory (MB)' ) ax [ 2 ] . set_xlabel ( 'Instances' ) plt . legend () plt . tight_layout () plt . close () return fig plot_track ( friedman_track , \"MAE\" , { \"Unbounded HTR\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ())}, 10_000 , 100 ) As we can see, our tree uses more than 10 MB to keep its structure. Let's say we wanted to limit our memory usage to 5 MB. How could we do that? Note that we are using a illustration case here. In real applications, data may be unbounded, so the trees might grow indefinitely. HTs expose some parameters related to memory management. The user can refer to the documentation for more details on that matter. Here, we are going to focus on two parameters: max_size : determines the maximum amount of memory (in MB) that the HT can use. memory_estimate_period : intervals after which the memory-management is triggered. We are going to limit our HTR to 5 MB and perform memory checks at intervals of 500 instances. plot_track ( friedman_track , \"MAE\" , { \"Restricted HTR\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( max_size = 5 , memory_estimate_period = 500 ) ) }, 10_000 , 100 ) Note that as soon the memory usage reaches the limit that we determined (at the memory check intervals), HTR starts managing its resource usage to reduce the size. As a consequence, the running time also decreases. For more accurate management, the intervals between memory checks should be decreased. This action, however, has costs since the tree stops the learning process to estimate its size and alter its own structure. Too frequent memory checks might end up result in a slow learning process. Besides, by using fewer resources, the predictive performance can be negatively impacted. So, use this tool with caution! But how that works at all? HTs monitor the incoming feature values to perform split attempts. To do so, they rely on a class of algorithms called Attribute Observers (AO) or Splitters (spoiler alert!). Each leaf node in an HT keeps one AO per incoming feature. After pre-determined intervals ( grace_period parameter), leaves query their AOs for split candidates. Well, there are costs to monitor input features (mainly the numerical ones). In fact, AOs correspond to one of the most time and memory-consuming portions of the HTs. To manage memory usage, an HT firstly determines its least promising leaves, w.r.t. how likely they will be split. Then, these leaves' AOs are removed, and the tree nodes are said to be \"deactivated.\" That's it! The deactivated leaves do not perform split attempts anymore, but they continue to be updated to provide responses. They will be kept as leaves as long as there are not available resources to enable tree growth. These leaves can be activated again (meaning that new AOs will be created for them) if there is available memory, so don't worry! Hint: another indirect way to bound memory usage is to limit the tree depth. By default, the trees can grow indefinitely, but the max_depth parameter can control this behavior. plot_track ( friedman_track , \"MAE\" , { \"HTR with at most 5 levels\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( max_depth = 5 ) ) }, 10_000 , 100 ) 4. Branching and growth: splitters, the heart of the trees \u00b6 As previously stated, one of the core operations of iDT is, well, to grow. Plants and gardening-related jokes apart, growth in HTs is guided by their AOs or splitters, as mentioned in the end of Section 3. Nominal features can be easily monitored, since the feature partitions are well-defined beforehand. Numerical features, on the other hand, do not have an explicit best cut point. Still, numerical features are typically split by using a binary test: \\(\\le\\) or \\(>\\) . Therefore, numerical splitters must somehow summarize the incoming feature values and be able to evaluate the merit of split point candidates. There are diverse strategies to monitor numerical features and choices related to them, including which data structure will be used to keep a summary of the incoming feature and also how many split points are going to be evaluated during split attempts. Again, this guide does not intend to be an exhaustive delve into the iDT subject. In fact, each of the following aspects of the iDTs could be considered a separate research area: AOs, intervals between split attempts, split heuristics (e.g., info gain, variance reduction, and so on), tree depth and max size, and much more! Let's focus a bit into the AO matter. River provides a handful of splitters for classification and regression trees, which can be chosen using the parameter splitter . We will list the available tree splitters in the following sections and compare some of their chacteristics. Some notation: \\(n\\) : Number of observations seen so far. \\(c\\) : the number of classes. \\(s\\) : the number of split points to evaluate (which means that this is a user-given parameter). \\(h\\) : the number of histogram bins or hash slots. Tipically, \\(h \\ll n\\) . 4.1. Classification tree splitters \u00b6 The following table summarizes the available classification splitters. The user might refer to the documentation of each splitter for more details about their functioning. Splitter Description Insertion Memory Split candidate query Works with Naive Bayes leaves? Exhaustive Keeps all the observed input values and class counts in a Binary Search Tree (BST) \\(O(\\log n)\\) (average) or \\(O(n)\\) (worst case) \\(O(n)\\) \\(O(n)\\) No Histogram Builds a histogram for each class in order to discretize the input feature \\(O(\\log h)\\) \\(O(c h)\\) \\(O(c h)\\) Yes Gaussian Approximates the class distributions using Gaussian distributions \\(O(1)\\) \\(O(c)\\) \\(O(cs)\\) Yes Note that some of the splitters have configurable parameters that directly impact not only on their time and memory costs, but also on the final predictive performance. Examples: The number of split points can be configured in the Gaussian splitter. Increasing this number makes this splitter slower, but it also potentially increases the quality of the obtained query points, implying enhanced tree accuracy. The number of stored bins can be selected in the Histogram splitter. Increasing this number increases the memory footprint and running time of this splitter, but it also potentially makes its split candidates more accurate and positively impacts on the tree's final predictive performance. Next, we provide a brief comparison of the classification splitters using 10K instances of the Random RBF synthetic dataset. Note that the tree equiped with the Exhaustive splitter does not use Naive Bayes leaves. def random_rbf_track ( n_samples = 10_000 ): dataset = synth . RandomRBF ( seed_model = 7 , seed_sample = 42 ) . take ( n_samples ) track = evaluate . Track ( \"10K Random RBF + Accuracy\" , dataset , metrics . Accuracy (), n_samples ) return track plot_track ( random_rbf_track , \"Accuracy\" , { \"HTC + Exhaustive splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . ExhaustiveSplitter (), leaf_prediction = \"mc\" ), \"HTC + Histogram splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . HistogramSplitter () ), \"HTC + Gaussian splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . GaussianSplitter () ) }, 10_000 , 100 ) 4.2 Regression tree splitters \u00b6 The available regression tree splitters are summarized in the next table. The TE-BST costs are expressed in terms of \\(n^*\\) because the number of stored elements can be smaller than or equal to \\(n\\) . Splitter Description Insertion Memory Split candidate query Extended Binary Search Tree (E-BST) Stores all the observations and target statistics in a BST \\(O(\\log n)\\) (average) or \\(O(n)\\) (worst case) \\(O(n)\\) \\(O(n)\\) Truncated E-BST (TE-BST) Rounds the incoming data before passing it to the BST \\(O(\\log n^*)\\) (average) or \\(O(n^*)\\) (worst case) \\(O(n^*)\\) \\(O(n^*)\\) Quantization Observer (QO) Uses a hash-like structure to quantize the incoming data \\(O(1)\\) \\(O(h)\\) \\(O(h \\log h)\\) E-BST is an exhaustive algorithm, i.e., it works as batch solutions usually do, which might be prohibitive in real-world online scenarios. TE-BST and QO apply approximations to alleviate the costs involved in monitoring numerical data and performing split attempts. The number of desired decimal places to round the data (TE-BST) and the quantization radius (QO) are directly related to the running time, memory footprint, and error of the resulting tree model. We present a brief comparison of the available regression tree splitters using the 10K instances of the Friedman synthetic dataset. plot_track ( friedman_track , \"MAE\" , { \"HTR + E-BST\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . EBSTSplitter () ) ), \"HTR + TE-BST\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . TEBSTSplitter ( digits = 2 ) ) ), \"HTR + QO\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . QOSplitter () ) ), }, 10_000 , 100 ) Wrapping up \u00b6 This guide provides a walkthrough in the HTs available in river . We discussed about model inspection, memory management, and feature splits. Keep in mind that each HT variant has specific details and capabilities that are out-of-the-scope of this introductory material. The user is advised to check the documentation page of the tree models for detailed information.","title":"Incremental decision trees in river: the Hoeffding Tree case"},{"location":"user-guide/on-hoeffding-trees/#incremental-decision-trees-in-river-the-hoeffding-tree-case","text":"Decision trees (DT) are popular learning models due to their inherently simplicity, flexibility and self-explainable structure. Moreover, when aggregated in ensembles, high predictive power might be achieved. Bagging and gradient boosting-based tree ensembles are very popular solutions in competition platforms such as Kaggle, and also among researchers. Although fairly lightweight, tradicional batch DTs cannot cope with data stream mining/online learning requirements, as they do multiple passes over the data and have to be retrained from scratch every time a new observation appears. The data stream literature has plenty of incremental DT (iDT) families that are better suited to online learning. Nonetheless, Hoeffding Trees (HT) are historically the most popular family of iDTs to date. In fact, HTs have some nice properties: one-pass learning regime; theoretical guarantees to converge to the batch DT model given enough observations and a stationary data distribution; small memory and running time footprint (in most cases); some of their variations can deal with non-stationary distributions. And the previous list goes on and on. Besides that, HTs also have the same advantages as batch DTs ( C4.5 / J48 , CART , M5 , etc.) do. We can inspect the structure of a HT to understand how decisions were made, which is a nice feature to have in online learning tasks. In river , HTs are first-class citizens, so we have multiple realizations of this framework that are suited to different learning tasks and scenarios. This brief introduction to HT does not aims at being extensive nor delving into algorithmic or implementation details of the HTs. Instead, we intend to provide a high-level overview of the HTs as they are envisioned in river , as well as their shared properties and important hyperparameters. In this guide, we are going to: summarize the differences accross the multiple HT versions available; learn how to inspect tree models; learn how to manage the memory usage of HTs; compare numerical tree splitters and understand their impact on the iDT induction process. Well, without further ado, let's go! First things first, we are going to start with some imports. import matplotlib.pyplot as plt from river import datasets from river import evaluate from river import metrics from river import preprocessing # we are going to use that later from river import synth # we are going to use some synthetic datasets too from river import tree","title":"Incremental decision trees in river: the Hoeffding Tree case"},{"location":"user-guide/on-hoeffding-trees/#1-trees-trees-everywhere-gardening-101-with-river","text":"At first glance, the amount of iDT algorithms in river might seem too much to handle, but in reality the distinction among them is easy to grasp. To facilitate our lives, here's a neat table listing the available HT models and summarizing their differences: Name Acronym Task Non-stationary? Comments Source Hoeffding Tree Classifier HTC Classification No Basic HT for classification tasks [1] Hoeffding Adaptive Tree Classifier HATC Classification Yes Modifies HTC by adding an instance of ADWIN to each node to detect and react to drift detection [2] Extremely Fast Decision Tree Classifier EFDT Classification No Deploys split decisions as soon as possible and periodically revisit decisions and redo them if necessary. Not as fast in practice as the name implies, but it tends to converge faster than HTC to the model generated by a batch DT [3] Hoeffding Tree Regressor HTR Regression No Basic HT for regression tasks. It is an adaptation of the FIRT/FIMT algorithm that bears some semblance to HTC [4] Hoeffding Adaptive Tree Regressor HATR Regression Yes Modifies HTR by adding an instance of ADWIN to each node to detect and react to drift detection - incremental Structured-Output Prediction Tree Regressor iSOUPT Multi-target regression No Multi-target version of HTR [5] Label Combination Hoeffding Tree Classifier LCHTC Multi-label classification No Creates a numerical code for each combination of the binary labels and uses HTC to learn from this encoded representation. At prediction time, decodes the modified representation to obtain the original label set - As we can see, although their application fields might overlap sometimes, the HT variations have specific situations in which they are better suited to work. Moreover, in river we provide a standardized API access to all the HT variants since they share many properties in common.","title":"1. Trees, trees everywhere: gardening 101 with river"},{"location":"user-guide/on-hoeffding-trees/#2-how-to-inspect-tree-models","text":"We provide a handful of tools to inspect trained HTs in river . Here, we will provide some examples of how to access their inner structures, get useful information, and plot the iDT structure. Firstly, let's pick a toy dataset from which our tree will learn from. Here we are going to focus on the classification case, but the same operations apply to other learning tasks. We will select the Phishing dataset from the datasets module to exemplify the HTs' capabilities. dataset = datasets . Phishing () dataset Phishing websites. This dataset contains features from web pages that are classified as phishing or not. Name Phishing Task Binary classification Samples 1,250 Features 9 Sparse False Path /home/smastelini/Documents/river_main/river/datasets/phishing.csv.gz We are going to train an instance of HoeffdingTreeClassifier using this dataset. As everything else in river , training an iDT is a piece of cake! %% time model = tree . HoeffdingTreeClassifier ( grace_period = 50 ) for x , y in dataset : model . learn_one ( x , y ) model CPU times: user 167 ms, sys: 755 \u00b5s, total: 168 ms Wall time: 166 ms HoeffdingTreeClassifier ( grace_period=50 max_depth=inf split_criterion=\"info_gain\" split_confidence=1e-07 tie_threshold=0.05 leaf_prediction=\"nba\" nb_threshold=0 nominal_attributes=None splitter=GaussianSplitter ( n_splits=10 ) binary_split=False max_size=100 memory_estimate_period=1000000 stop_mem_management=False remove_poor_attrs=False merit_preprune=True ) That's it! We are not going to enter into details about some of the available parameters of HTC here. The user can refer to the documentation page for more information about that. Let's talk about model inspection :D At any time, we can easily get some statistics about our trained model by using the summary property: model . summary {'n_nodes': 5, 'n_branches': 2, 'n_leaves': 3, 'n_active_leaves': 3, 'n_inactive_leaves': 0, 'height': 3, 'total_observed_weight': 1250.0} This property show us the internal structure of the tree, including data concerning the memory-management routines that we are going to check later in this guide. We can also get a representation of the tree model as a pandas.DataFrame object: model . to_dataframe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } parent is_leaf depth stats feature threshold splitter splitters _disabled_attrs _last_split_attempt_at _mc_correct_weight _nb_correct_weight node 0 <NA> False 0 {True: 260.0, False: 390.0} empty_server_form_handler 0.545455 NaN NaN NaN NaN NaN NaN 1 0 True 1 {True: 443.4163997711022, False: 59.8769131081... NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 474.293313 249.0 248.0 2 0 False 1 {True: 71.58360022889778, False: 404.123086891... popup_window 0.090909 NaN NaN NaN NaN NaN NaN 3 2 True 2 {False: 31.426538522574834, True: 33.0} NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 59.426539 23.0 36.0 4 2 True 2 {False: 250.57346147742516, True: 6.0} NaN NaN GaussianSplitter {'empty_server_form_handler': GaussianSplitter... {} 241.573461 210.0 196.0 Hmm, maybe not the clearest of the representations. What about drawing the tree structure instead? model . draw () Much better, huh? Lastly, we can check how the tree predicts one specific instance by using the debug_one method: x , y = next ( iter ( dataset )) # Let's select the first example in the stream x , y ({'empty_server_form_handler': 0.0, 'popup_window': 0.0, 'https': 0.0, 'request_from_other_domain': 0.0, 'anchor_from_other_domain': 0.0, 'is_popular': 0.5, 'long_url': 1.0, 'age_of_domain': 1, 'ip_in_url': 1}, True) print ( model . debug_one ( x )) empty_server_form_handler \u2264 0.5454545454545454 Class True: P(False) = 0.1 P(True) = 0.9 Our tree got this one right! The method debug_one is especially useful when we are dealing with a big tree model where drawing might not be the wisest of the choices (we will end up with a tree chart that has too much information to visually understand). Some additional hints: the max_depth parameter is our friend when building HTs that need to be constantly inspected. This parameter, which is available for every HT variant, triggers a pre-pruning mechanism that stops tree growth when the given depth is reached. we can also limit the depth when using the draw method. in the case of tree ensembles, individual trees can be accessed using the [index] operator. Then, the same set of inspection tools are available to play with!","title":"2. How to inspect tree models?"},{"location":"user-guide/on-hoeffding-trees/#3-advanced-gardening-with-river-grab-your-pruning-shears-and-lets-limit-memory-usage","text":"Online learning is well-suited to highly scalable processing centers with petabytes of data arriving intermittently, but it can also work with Internet of Things (IoT) devices operating at low power and with limited processing capability. Hence, making sure our trees are not going to use too much memory is a nice feature that can impact on both energy usage and the running time. HTs have memory-management routines that put the user in the control of computational resources that are available. In this brief guide, we are going to use a regression tree, since this kind of iDT typically spends more memory than the classification counterparts. However, the user can control the memory usage in the exact same way in river , regardless of the HT variant! We will rely on the Friedman synthetic dataset (data generator) from the synth module in our evaluation. To avoid doing tedious stuff, let's use the Track class from the evaluate module. Tracks are the standard way of benchmarking learning models in river . Since data generators can produce instances indefinitely, we will select a sample of size 10K for our tests. def friedman_track ( n_samples = 10_000 ): dataset = synth . Friedman ( seed = 42 ) . take ( n_samples ) track = evaluate . Track ( \"10K Friedman + MAE\" , dataset , metrics . MAE (), n_samples ) return track We are almost ready to go. Let's first define a simple function that parses the data generated by our tracks and plots it in a nice way. def plot_track ( track , metric_name , models , n_samples , n_checkpoints ): fig , ax = plt . subplots ( figsize = ( 5 , 5 ), nrows = 3 , dpi = 300 ) for model_name , model in models . items (): step = [] error = [] r_time = [] memory = [] for checkpoint in track ( n_samples ) . run ( model , n_checkpoints ): step . append ( checkpoint [ \"Step\" ]) error . append ( checkpoint [ metric_name ]) # Convert timedelta object into seconds r_time . append ( checkpoint [ \"Time\" ] . total_seconds ()) # Make sure the memory measurements are in MB raw_memory , unit = float ( checkpoint [ \"Memory\" ][: - 3 ]), checkpoint [ \"Memory\" ][ - 2 :] memory . append ( raw_memory * 2 **- 10 if unit == 'KB' else raw_memory ) ax [ 0 ] . grid ( True ) ax [ 1 ] . grid ( True ) ax [ 2 ] . grid ( True ) ax [ 0 ] . plot ( step , error , label = model_name ) ax [ 0 ] . set_ylabel ( metric_name ) ax [ 1 ] . plot ( step , r_time , label = model_name ) ax [ 1 ] . set_ylabel ( 'Time (seconds)' ) ax [ 2 ] . plot ( step , memory , label = model_name ) ax [ 2 ] . set_ylabel ( 'Memory (MB)' ) ax [ 2 ] . set_xlabel ( 'Instances' ) plt . legend () plt . tight_layout () plt . close () return fig plot_track ( friedman_track , \"MAE\" , { \"Unbounded HTR\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ())}, 10_000 , 100 ) As we can see, our tree uses more than 10 MB to keep its structure. Let's say we wanted to limit our memory usage to 5 MB. How could we do that? Note that we are using a illustration case here. In real applications, data may be unbounded, so the trees might grow indefinitely. HTs expose some parameters related to memory management. The user can refer to the documentation for more details on that matter. Here, we are going to focus on two parameters: max_size : determines the maximum amount of memory (in MB) that the HT can use. memory_estimate_period : intervals after which the memory-management is triggered. We are going to limit our HTR to 5 MB and perform memory checks at intervals of 500 instances. plot_track ( friedman_track , \"MAE\" , { \"Restricted HTR\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( max_size = 5 , memory_estimate_period = 500 ) ) }, 10_000 , 100 ) Note that as soon the memory usage reaches the limit that we determined (at the memory check intervals), HTR starts managing its resource usage to reduce the size. As a consequence, the running time also decreases. For more accurate management, the intervals between memory checks should be decreased. This action, however, has costs since the tree stops the learning process to estimate its size and alter its own structure. Too frequent memory checks might end up result in a slow learning process. Besides, by using fewer resources, the predictive performance can be negatively impacted. So, use this tool with caution! But how that works at all? HTs monitor the incoming feature values to perform split attempts. To do so, they rely on a class of algorithms called Attribute Observers (AO) or Splitters (spoiler alert!). Each leaf node in an HT keeps one AO per incoming feature. After pre-determined intervals ( grace_period parameter), leaves query their AOs for split candidates. Well, there are costs to monitor input features (mainly the numerical ones). In fact, AOs correspond to one of the most time and memory-consuming portions of the HTs. To manage memory usage, an HT firstly determines its least promising leaves, w.r.t. how likely they will be split. Then, these leaves' AOs are removed, and the tree nodes are said to be \"deactivated.\" That's it! The deactivated leaves do not perform split attempts anymore, but they continue to be updated to provide responses. They will be kept as leaves as long as there are not available resources to enable tree growth. These leaves can be activated again (meaning that new AOs will be created for them) if there is available memory, so don't worry! Hint: another indirect way to bound memory usage is to limit the tree depth. By default, the trees can grow indefinitely, but the max_depth parameter can control this behavior. plot_track ( friedman_track , \"MAE\" , { \"HTR with at most 5 levels\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( max_depth = 5 ) ) }, 10_000 , 100 )","title":"3. Advanced gardening with river: grab your pruning shears and let's limit memory usage"},{"location":"user-guide/on-hoeffding-trees/#4-branching-and-growth-splitters-the-heart-of-the-trees","text":"As previously stated, one of the core operations of iDT is, well, to grow. Plants and gardening-related jokes apart, growth in HTs is guided by their AOs or splitters, as mentioned in the end of Section 3. Nominal features can be easily monitored, since the feature partitions are well-defined beforehand. Numerical features, on the other hand, do not have an explicit best cut point. Still, numerical features are typically split by using a binary test: \\(\\le\\) or \\(>\\) . Therefore, numerical splitters must somehow summarize the incoming feature values and be able to evaluate the merit of split point candidates. There are diverse strategies to monitor numerical features and choices related to them, including which data structure will be used to keep a summary of the incoming feature and also how many split points are going to be evaluated during split attempts. Again, this guide does not intend to be an exhaustive delve into the iDT subject. In fact, each of the following aspects of the iDTs could be considered a separate research area: AOs, intervals between split attempts, split heuristics (e.g., info gain, variance reduction, and so on), tree depth and max size, and much more! Let's focus a bit into the AO matter. River provides a handful of splitters for classification and regression trees, which can be chosen using the parameter splitter . We will list the available tree splitters in the following sections and compare some of their chacteristics. Some notation: \\(n\\) : Number of observations seen so far. \\(c\\) : the number of classes. \\(s\\) : the number of split points to evaluate (which means that this is a user-given parameter). \\(h\\) : the number of histogram bins or hash slots. Tipically, \\(h \\ll n\\) .","title":"4. Branching and growth: splitters, the heart of the trees"},{"location":"user-guide/on-hoeffding-trees/#41-classification-tree-splitters","text":"The following table summarizes the available classification splitters. The user might refer to the documentation of each splitter for more details about their functioning. Splitter Description Insertion Memory Split candidate query Works with Naive Bayes leaves? Exhaustive Keeps all the observed input values and class counts in a Binary Search Tree (BST) \\(O(\\log n)\\) (average) or \\(O(n)\\) (worst case) \\(O(n)\\) \\(O(n)\\) No Histogram Builds a histogram for each class in order to discretize the input feature \\(O(\\log h)\\) \\(O(c h)\\) \\(O(c h)\\) Yes Gaussian Approximates the class distributions using Gaussian distributions \\(O(1)\\) \\(O(c)\\) \\(O(cs)\\) Yes Note that some of the splitters have configurable parameters that directly impact not only on their time and memory costs, but also on the final predictive performance. Examples: The number of split points can be configured in the Gaussian splitter. Increasing this number makes this splitter slower, but it also potentially increases the quality of the obtained query points, implying enhanced tree accuracy. The number of stored bins can be selected in the Histogram splitter. Increasing this number increases the memory footprint and running time of this splitter, but it also potentially makes its split candidates more accurate and positively impacts on the tree's final predictive performance. Next, we provide a brief comparison of the classification splitters using 10K instances of the Random RBF synthetic dataset. Note that the tree equiped with the Exhaustive splitter does not use Naive Bayes leaves. def random_rbf_track ( n_samples = 10_000 ): dataset = synth . RandomRBF ( seed_model = 7 , seed_sample = 42 ) . take ( n_samples ) track = evaluate . Track ( \"10K Random RBF + Accuracy\" , dataset , metrics . Accuracy (), n_samples ) return track plot_track ( random_rbf_track , \"Accuracy\" , { \"HTC + Exhaustive splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . ExhaustiveSplitter (), leaf_prediction = \"mc\" ), \"HTC + Histogram splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . HistogramSplitter () ), \"HTC + Gaussian splitter\" : tree . HoeffdingTreeClassifier ( splitter = tree . splitter . GaussianSplitter () ) }, 10_000 , 100 )","title":"4.1. Classification tree splitters"},{"location":"user-guide/on-hoeffding-trees/#42-regression-tree-splitters","text":"The available regression tree splitters are summarized in the next table. The TE-BST costs are expressed in terms of \\(n^*\\) because the number of stored elements can be smaller than or equal to \\(n\\) . Splitter Description Insertion Memory Split candidate query Extended Binary Search Tree (E-BST) Stores all the observations and target statistics in a BST \\(O(\\log n)\\) (average) or \\(O(n)\\) (worst case) \\(O(n)\\) \\(O(n)\\) Truncated E-BST (TE-BST) Rounds the incoming data before passing it to the BST \\(O(\\log n^*)\\) (average) or \\(O(n^*)\\) (worst case) \\(O(n^*)\\) \\(O(n^*)\\) Quantization Observer (QO) Uses a hash-like structure to quantize the incoming data \\(O(1)\\) \\(O(h)\\) \\(O(h \\log h)\\) E-BST is an exhaustive algorithm, i.e., it works as batch solutions usually do, which might be prohibitive in real-world online scenarios. TE-BST and QO apply approximations to alleviate the costs involved in monitoring numerical data and performing split attempts. The number of desired decimal places to round the data (TE-BST) and the quantization radius (QO) are directly related to the running time, memory footprint, and error of the resulting tree model. We present a brief comparison of the available regression tree splitters using the 10K instances of the Friedman synthetic dataset. plot_track ( friedman_track , \"MAE\" , { \"HTR + E-BST\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . EBSTSplitter () ) ), \"HTR + TE-BST\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . TEBSTSplitter ( digits = 2 ) ) ), \"HTR + QO\" : ( preprocessing . StandardScaler () | tree . HoeffdingTreeRegressor ( splitter = tree . splitter . QOSplitter () ) ), }, 10_000 , 100 )","title":"4.2 Regression tree splitters"},{"location":"user-guide/on-hoeffding-trees/#wrapping-up","text":"This guide provides a walkthrough in the HTs available in river . We discussed about model inspection, memory management, and feature splits. Keep in mind that each HT variant has specific details and capabilities that are out-of-the-scope of this introductory material. The user is advised to check the documentation page of the tree models for detailed information.","title":"Wrapping up"},{"location":"user-guide/pipelines/","text":"Pipelines \u00b6 Pipelines are an integral part of river. We encourage their usage and apply them in many of their examples. The compose.Pipeline contains all the logic for building and applying pipelines. A pipeline is essentially a list of estimators that are applied in sequence. The only requirement is that the first n - 1 steps be transformers. The last step can be a regressor, a classifier, a clusterer, a transformer, etc. Here is an example: from river import compose from river import linear_model from river import preprocessing from river import feature_extraction model = compose . Pipeline ( preprocessing . StandardScaler (), feature_extraction . PolynomialExtender (), linear_model . LinearRegression () ) You can also use the | operator, as so: model = ( preprocessing . StandardScaler () | feature_extraction . PolynomialExtender () | linear_model . LinearRegression () ) Or, equally: model = preprocessing . StandardScaler () model |= feature_extraction . PolynomialExtender () model |= linear_model . LinearRegression () A pipeline has a draw method that can be used to visualize it: model compose.Pipeline inherits from base.Estimator , which means that it has a learn_one method. You would expect learn_one to update each estimator, but that's not actually what happens . Instead, the transformers are updated when predict_one (or predict_proba_one for that matter) is called. Indeed, in online machine learning, we can update the unsupervised parts of our model when a sample arrives. We don't have to wait for the ground truth to arrive in order to update unsupervised estimators that don't depend on it. In other words, in a pipeline, learn_one updates the supervised parts, whilst predict_one updates the unsupervised parts. It's important to be aware of this behavior, as it is quite different to what is done in other libraries that rely on batch machine learning. Here is a small example to illustrate the previous point: from river import datasets dataset = datasets . TrumpApproval () x , y = next ( iter ( dataset )) x , y ({'ordinal_date': 736389, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}, 43.75505) Let us call predict_one , which will update each transformer, but won't update the linear regression. model . predict_one ( x ) 0.0 The prediction is nil because each weight of the linear regression is equal to 0. model [ 'StandardScaler' ] . means defaultdict(float, {'ordinal_date': 736389.0, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}) As we can see, the means of each feature have been updated, even though we called predict_one and not learn_one . Note that if you call transform_one with a pipeline who's last step is not a transformer, then the output from the last transformer (which is thus the penultimate step) will be returned: model . transform_one ( x ) {'ordinal_date': 0.0, 'gallup': 0.0, 'ipsos': 0.0, 'morning_consult': 0.0, 'rasmussen': 0.0, 'you_gov': 0.0, 'ordinal_date*ordinal_date': 0.0, 'gallup*ordinal_date': 0.0, 'ipsos*ordinal_date': 0.0, 'morning_consult*ordinal_date': 0.0, 'ordinal_date*rasmussen': 0.0, 'ordinal_date*you_gov': 0.0, 'gallup*gallup': 0.0, 'gallup*ipsos': 0.0, 'gallup*morning_consult': 0.0, 'gallup*rasmussen': 0.0, 'gallup*you_gov': 0.0, 'ipsos*ipsos': 0.0, 'ipsos*morning_consult': 0.0, 'ipsos*rasmussen': 0.0, 'ipsos*you_gov': 0.0, 'morning_consult*morning_consult': 0.0, 'morning_consult*rasmussen': 0.0, 'morning_consult*you_gov': 0.0, 'rasmussen*rasmussen': 0.0, 'rasmussen*you_gov': 0.0, 'you_gov*you_gov': 0.0} In many cases, you might want to connect a step to multiple steps. For instance, you might to extract different kinds of features from a single input. An elegant way to do this is to use a compose.TransformerUnion . Essentially, the latter is a list of transformers who's results will be merged into a single dict when transform_one is called. As an example let's say that we want to apply a feature_extraction.RBFSampler as well as the feature_extraction.PolynomialExtender . This may be done as so: model = ( preprocessing . StandardScaler () | ( feature_extraction . PolynomialExtender () + feature_extraction . RBFSampler ()) | linear_model . LinearRegression () ) model Note that the + symbol acts as a shorthand notation for creating a compose.TransformerUnion , which means that we could have declared the above pipeline as so: model = ( preprocessing . StandardScaler () | compose . TransformerUnion ( feature_extraction . PolynomialExtender (), feature_extraction . RBFSampler () ) | linear_model . LinearRegression () ) Pipelines provide the benefit of removing a lot of cruft by taking care of tedious details for you. They also enable to clearly define what steps your model is made of. Finally, having your model in a single object means that you can move it around more easily. Note that you can include user-defined functions in a pipeline by using a compose.FuncTransformer .","title":"Pipelines"},{"location":"user-guide/pipelines/#pipelines","text":"Pipelines are an integral part of river. We encourage their usage and apply them in many of their examples. The compose.Pipeline contains all the logic for building and applying pipelines. A pipeline is essentially a list of estimators that are applied in sequence. The only requirement is that the first n - 1 steps be transformers. The last step can be a regressor, a classifier, a clusterer, a transformer, etc. Here is an example: from river import compose from river import linear_model from river import preprocessing from river import feature_extraction model = compose . Pipeline ( preprocessing . StandardScaler (), feature_extraction . PolynomialExtender (), linear_model . LinearRegression () ) You can also use the | operator, as so: model = ( preprocessing . StandardScaler () | feature_extraction . PolynomialExtender () | linear_model . LinearRegression () ) Or, equally: model = preprocessing . StandardScaler () model |= feature_extraction . PolynomialExtender () model |= linear_model . LinearRegression () A pipeline has a draw method that can be used to visualize it: model compose.Pipeline inherits from base.Estimator , which means that it has a learn_one method. You would expect learn_one to update each estimator, but that's not actually what happens . Instead, the transformers are updated when predict_one (or predict_proba_one for that matter) is called. Indeed, in online machine learning, we can update the unsupervised parts of our model when a sample arrives. We don't have to wait for the ground truth to arrive in order to update unsupervised estimators that don't depend on it. In other words, in a pipeline, learn_one updates the supervised parts, whilst predict_one updates the unsupervised parts. It's important to be aware of this behavior, as it is quite different to what is done in other libraries that rely on batch machine learning. Here is a small example to illustrate the previous point: from river import datasets dataset = datasets . TrumpApproval () x , y = next ( iter ( dataset )) x , y ({'ordinal_date': 736389, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}, 43.75505) Let us call predict_one , which will update each transformer, but won't update the linear regression. model . predict_one ( x ) 0.0 The prediction is nil because each weight of the linear regression is equal to 0. model [ 'StandardScaler' ] . means defaultdict(float, {'ordinal_date': 736389.0, 'gallup': 43.843213, 'ipsos': 46.19925042857143, 'morning_consult': 48.318749, 'rasmussen': 44.104692, 'you_gov': 43.636914000000004}) As we can see, the means of each feature have been updated, even though we called predict_one and not learn_one . Note that if you call transform_one with a pipeline who's last step is not a transformer, then the output from the last transformer (which is thus the penultimate step) will be returned: model . transform_one ( x ) {'ordinal_date': 0.0, 'gallup': 0.0, 'ipsos': 0.0, 'morning_consult': 0.0, 'rasmussen': 0.0, 'you_gov': 0.0, 'ordinal_date*ordinal_date': 0.0, 'gallup*ordinal_date': 0.0, 'ipsos*ordinal_date': 0.0, 'morning_consult*ordinal_date': 0.0, 'ordinal_date*rasmussen': 0.0, 'ordinal_date*you_gov': 0.0, 'gallup*gallup': 0.0, 'gallup*ipsos': 0.0, 'gallup*morning_consult': 0.0, 'gallup*rasmussen': 0.0, 'gallup*you_gov': 0.0, 'ipsos*ipsos': 0.0, 'ipsos*morning_consult': 0.0, 'ipsos*rasmussen': 0.0, 'ipsos*you_gov': 0.0, 'morning_consult*morning_consult': 0.0, 'morning_consult*rasmussen': 0.0, 'morning_consult*you_gov': 0.0, 'rasmussen*rasmussen': 0.0, 'rasmussen*you_gov': 0.0, 'you_gov*you_gov': 0.0} In many cases, you might want to connect a step to multiple steps. For instance, you might to extract different kinds of features from a single input. An elegant way to do this is to use a compose.TransformerUnion . Essentially, the latter is a list of transformers who's results will be merged into a single dict when transform_one is called. As an example let's say that we want to apply a feature_extraction.RBFSampler as well as the feature_extraction.PolynomialExtender . This may be done as so: model = ( preprocessing . StandardScaler () | ( feature_extraction . PolynomialExtender () + feature_extraction . RBFSampler ()) | linear_model . LinearRegression () ) model Note that the + symbol acts as a shorthand notation for creating a compose.TransformerUnion , which means that we could have declared the above pipeline as so: model = ( preprocessing . StandardScaler () | compose . TransformerUnion ( feature_extraction . PolynomialExtender (), feature_extraction . RBFSampler () ) | linear_model . LinearRegression () ) Pipelines provide the benefit of removing a lot of cruft by taking care of tedious details for you. They also enable to clearly define what steps your model is made of. Finally, having your model in a single object means that you can move it around more easily. Note that you can include user-defined functions in a pipeline by using a compose.FuncTransformer .","title":"Pipelines"},{"location":"user-guide/reading-data/","text":"Reading data \u00b6 In river , the features of a sample are stored inside a dictionary, which in Python is called a dict and is a native data structure. In other words, we don't use any sophisticated data structure, such as a numpy.ndarray or a pandas.DataFrame . The main advantage of using plain dict s is that it removes the overhead that comes with using the aforementioned data structures. This is important in a streaming context because we want to be able to process many individual samples in rapid succession. Another advantage is that dict s allow us to give names to our features. Finally, dict s are not typed, and can therefore store heterogeneous data. Another advantage which we haven't mentioned is that dict s play nicely with Python's standard library. Indeed, Python contains many tools that allow manipulating dict s. For instance, the csv.DictReader can be used to read a CSV file and convert each row to a dict . In fact, the stream.iter_csv method from river is just a wrapper on top of csv.DictReader that adds a few bells and whistles. river provides some out-of-the-box datasets to get you started. from river import datasets dataset = datasets . Bikes () dataset Bike sharing station information from the city of Toulouse. The goal is to predict the number of bikes in 5 different bike stations from the city of Toulouse. Name Bikes Task Regression Samples 182,470 Features 8 Sparse False Path /Users/max.halford/river_data/Bikes/toulouse_bikes.csv URL https://maxhalford.github.io/files/datasets/toulouse_bikes.zip Size 12.52 MB Downloaded True Note that when we say \"loaded\", we don't mean that the actual data is read from the disk. On the contrary, the dataset is a streaming data that can be iterated over one sample at a time. In Python lingo, it's a generator . Let's take a look at the first sample: x , y = next ( iter ( dataset )) x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} Each dataset is iterable, which means we can also do: for x , y in dataset : break x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} As we can see, the values have different types. Under the hood, calling for x, y in dataset simply iterates over a file and parses each value appropriately. We can do this ourselves by using stream.iter_csv : from river import stream X_y = stream . iter_csv ( dataset . path ) x , y = next ( X_y ) x , y ({'moment': '2016-04-01 00:00:07', 'bikes': '1', 'station': 'metro-canal-du-midi', 'clouds': '75', 'description': 'light rain', 'humidity': '81', 'pressure': '1017.0', 'temperature': '6.54', 'wind': '9.3'}, None) There are a couple things that are wrong. First of all, the numeric features have not been casted into numbers. Indeed, by default, stream.iter_csv assumes that everything is a string. A related issue is that the moment field hasn't been parsed into a datetime . Finally, the target field, which is bikes , hasn't been separated from the rest of the features. We can remedy to these issues by setting a few parameters: X_y = stream . iter_csv ( dataset . path , converters = { 'bikes' : int , 'clouds' : int , 'humidity' : int , 'pressure' : float , 'temperature' : float , 'wind' : float }, parse_dates = { 'moment' : '%Y-%m- %d %H:%M:%S' }, target = 'bikes' ) x , y = next ( X_y ) x , y ({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3}, 1) That's much better. We invite you to take a look at the stream module to see for yourself what other methods are available. Note that river is first and foremost a machine learning library, and therefore isn't as much concerned about reading data as it is about statistical algorithms. We do however believe that the fact that we use dictionary gives you, the user, a lot of freedom and flexibility. The stream module provides helper functions to read data from different formats. For instance, you can use the stream.iter_sklearn_dataset function to turn any scikit-learn dataset into a stream. from sklearn import datasets dataset = datasets . load_boston () for x , y in stream . iter_sklearn_dataset ( dataset ): break x , y ({'CRIM': 0.00632, 'ZN': 18.0, 'INDUS': 2.31, 'CHAS': 0.0, 'NOX': 0.538, 'RM': 6.575, 'AGE': 65.2, 'DIS': 4.09, 'RAD': 1.0, 'TAX': 296.0, 'PTRATIO': 15.3, 'B': 396.9, 'LSTAT': 4.98}, 24.0) To conclude, let us shortly mention the difference between proactive learning and reactive learning in the specific context of online machine learning. When we loop over a data with a for loop, we have the control over the data and the order in which it arrives. We are proactive in the sense that we, the user, are asking for the data to arrive. In contract, in a reactive situation, we don't have control on the data arrival. A typical example of such a situation is a web server, where web requests arrive in an arbitrary order. This is a situation where river shines. For instance, in a Flask application, you could define a route to make predictions with a river model as so: import flask app = flask . Flask ( __name__ ) @app . route ( '/' , methods = [ 'GET' ]) def predict (): payload = flask . request . json river_model = load_model () return river_model . predict_proba_one ( payload ) Likewise, a model can be updated whenever a request arrives as so: @app . route ( '/' , methods = [ 'POST' ]) def learn (): payload = flask . request . json river_model = load_model () river_model . learn_one ( payload [ 'features' ], payload [ 'target' ]) return {}, 201 To summarize, river can be used in many different ways. The fact that it uses dictionaries to represent features provides a lot of flexibility and space for creativity.","title":"Reading data"},{"location":"user-guide/reading-data/#reading-data","text":"In river , the features of a sample are stored inside a dictionary, which in Python is called a dict and is a native data structure. In other words, we don't use any sophisticated data structure, such as a numpy.ndarray or a pandas.DataFrame . The main advantage of using plain dict s is that it removes the overhead that comes with using the aforementioned data structures. This is important in a streaming context because we want to be able to process many individual samples in rapid succession. Another advantage is that dict s allow us to give names to our features. Finally, dict s are not typed, and can therefore store heterogeneous data. Another advantage which we haven't mentioned is that dict s play nicely with Python's standard library. Indeed, Python contains many tools that allow manipulating dict s. For instance, the csv.DictReader can be used to read a CSV file and convert each row to a dict . In fact, the stream.iter_csv method from river is just a wrapper on top of csv.DictReader that adds a few bells and whistles. river provides some out-of-the-box datasets to get you started. from river import datasets dataset = datasets . Bikes () dataset Bike sharing station information from the city of Toulouse. The goal is to predict the number of bikes in 5 different bike stations from the city of Toulouse. Name Bikes Task Regression Samples 182,470 Features 8 Sparse False Path /Users/max.halford/river_data/Bikes/toulouse_bikes.csv URL https://maxhalford.github.io/files/datasets/toulouse_bikes.zip Size 12.52 MB Downloaded True Note that when we say \"loaded\", we don't mean that the actual data is read from the disk. On the contrary, the dataset is a streaming data that can be iterated over one sample at a time. In Python lingo, it's a generator . Let's take a look at the first sample: x , y = next ( iter ( dataset )) x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} Each dataset is iterable, which means we can also do: for x , y in dataset : break x {'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3} As we can see, the values have different types. Under the hood, calling for x, y in dataset simply iterates over a file and parses each value appropriately. We can do this ourselves by using stream.iter_csv : from river import stream X_y = stream . iter_csv ( dataset . path ) x , y = next ( X_y ) x , y ({'moment': '2016-04-01 00:00:07', 'bikes': '1', 'station': 'metro-canal-du-midi', 'clouds': '75', 'description': 'light rain', 'humidity': '81', 'pressure': '1017.0', 'temperature': '6.54', 'wind': '9.3'}, None) There are a couple things that are wrong. First of all, the numeric features have not been casted into numbers. Indeed, by default, stream.iter_csv assumes that everything is a string. A related issue is that the moment field hasn't been parsed into a datetime . Finally, the target field, which is bikes , hasn't been separated from the rest of the features. We can remedy to these issues by setting a few parameters: X_y = stream . iter_csv ( dataset . path , converters = { 'bikes' : int , 'clouds' : int , 'humidity' : int , 'pressure' : float , 'temperature' : float , 'wind' : float }, parse_dates = { 'moment' : '%Y-%m- %d %H:%M:%S' }, target = 'bikes' ) x , y = next ( X_y ) x , y ({'moment': datetime.datetime(2016, 4, 1, 0, 0, 7), 'station': 'metro-canal-du-midi', 'clouds': 75, 'description': 'light rain', 'humidity': 81, 'pressure': 1017.0, 'temperature': 6.54, 'wind': 9.3}, 1) That's much better. We invite you to take a look at the stream module to see for yourself what other methods are available. Note that river is first and foremost a machine learning library, and therefore isn't as much concerned about reading data as it is about statistical algorithms. We do however believe that the fact that we use dictionary gives you, the user, a lot of freedom and flexibility. The stream module provides helper functions to read data from different formats. For instance, you can use the stream.iter_sklearn_dataset function to turn any scikit-learn dataset into a stream. from sklearn import datasets dataset = datasets . load_boston () for x , y in stream . iter_sklearn_dataset ( dataset ): break x , y ({'CRIM': 0.00632, 'ZN': 18.0, 'INDUS': 2.31, 'CHAS': 0.0, 'NOX': 0.538, 'RM': 6.575, 'AGE': 65.2, 'DIS': 4.09, 'RAD': 1.0, 'TAX': 296.0, 'PTRATIO': 15.3, 'B': 396.9, 'LSTAT': 4.98}, 24.0) To conclude, let us shortly mention the difference between proactive learning and reactive learning in the specific context of online machine learning. When we loop over a data with a for loop, we have the control over the data and the order in which it arrives. We are proactive in the sense that we, the user, are asking for the data to arrive. In contract, in a reactive situation, we don't have control on the data arrival. A typical example of such a situation is a web server, where web requests arrive in an arbitrary order. This is a situation where river shines. For instance, in a Flask application, you could define a route to make predictions with a river model as so: import flask app = flask . Flask ( __name__ ) @app . route ( '/' , methods = [ 'GET' ]) def predict (): payload = flask . request . json river_model = load_model () return river_model . predict_proba_one ( payload ) Likewise, a model can be updated whenever a request arrives as so: @app . route ( '/' , methods = [ 'POST' ]) def learn (): payload = flask . request . json river_model = load_model () river_model . learn_one ( payload [ 'features' ], payload [ 'target' ]) return {}, 201 To summarize, river can be used in many different ways. The fact that it uses dictionaries to represent features provides a lot of flexibility and space for creativity.","title":"Reading data"}]}